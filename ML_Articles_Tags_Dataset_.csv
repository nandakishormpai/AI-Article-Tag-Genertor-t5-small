title,text,url,authors,timestamp,wordlen,corrected_tags
How to Build a Reporting Dashboard using Dash and Plotly,"A method to select either a condensed data table or the complete data table.

One of the features that I wanted for the data table was the ability to show a “condensed” version of the table as well as the complete data table. Therefore, I included a radio button in the layouts.py file to select which version of the table to present:

Code Block 17: Radio Button in layouts.py

The callback for this functionality takes input from the radio button and outputs the columns to render in the data table:

Code Block 18: Callback for Radio Button in layouts.py File

This callback is a little bit more complicated since I am adding columns for conditional formatting (which I will go into below). Essentially, just as the callback below is changing the data presented in the data table based upon the dates selected using the callback statement, Output('datatable-paid-search', 'data' , this callback is changing the columns presented in the data table based upon the radio button selection using the callback statement, Output('datatable-paid-search', 'columns' .

Conditionally Color-Code Different Data Table cells

One of the features which the stakeholders wanted for the data table was the ability to have certain numbers or cells in the data table to be highlighted based upon a metric’s value; red for negative numbers for instance. However, conditional formatting of data table cells has three main issues.

There is lack of formatting functionality in Dash Data Tables at this time.

If a number is formatted prior to inclusion in a Dash Data Table (in pandas for instance), then data table functionality such as sorting and filtering does not work properly.

There is a bug in the Dash data table code in which conditional formatting does not work properly.

I ended up formatting the numbers in the data table in pandas despite the above limitations. I discovered that conditional formatting in Dash does not work properly for formatted numbers (numbers with commas, dollar signs, percent signs, etc.). Indeed, I found out that there is a bug with the method described in the Conditional Formatting — Highlighting Cells section of the Dash Data Table User Guide:

Code Block 19: Conditional Formatting — Highlighting Cells

The cell for New York City temperature shows up as green even though the value is less than 3.9.* I’ve tested this in other scenarios and it seems like the conditional formatting for numbers only uses the integer part of the condition (“3” but not “3.9”). The filter for Temperature used for conditional formatting somehow truncates the significant digits and only considers the integer part of a number. I posted to the Dash community forum about this bug, and it has since been fixed in a recent version of Dash.

*This has since been corrected in the Dash Documentation.

Conditional Formatting of Cells using Doppelganger Columns

Due to the above limitations with conditional formatting of cells, I came up with an alternative method in which I add “doppelganger” columns to both the pandas data frame and Dash data table. These doppelganger columns had either the value of the original column, or the value of the original column multiplied by 100 (to overcome the bug when the decimal portion of a value is not considered by conditional filtering). Then, the doppelganger columns can be added to the data table but are hidden from view with the following statements:

Code Block 20: Adding Doppelganger Columns

Then, the conditional cell formatting can be implemented using the following syntax:

Code Block 21: Conditional Cell Formatting

Essentially, the filter is applied on the “doppelganger” column, Revenue_YoY_percent_conditional (filtering cells in which the value is less than 0). However, the formatting is applied on the corresponding “real” column, Revenue YoY (%) . One can imagine other usages for this method of conditional formatting; for instance, highlighting outlier values.

The complete statement for the data table is below (with conditional formatting for odd and even rows, as well highlighting cells that are above a certain threshold using the doppelganger method):

Code Block 22: Data Table with Conditional Formatting

I describe the method to update the graphs using the selected rows in the data table below.",https://medium.com/p/4f4257c18a7f#58a3,['David Comfort'],2019-03-13 14:21:44.055000+00:00,668,"data table, radio button, Dash data table, doppelganger columns, conditional formatting"
A Brief Introduction to Change Point Detection using Python,"The ruptures Package

Charles Truong adapted the ruptures package from the R changepoint package. It specifically focuses on offline changepoint detection, where the whole sequence is analyzed. Out of all of the Python changepoint options, it is the best documented. We can install it using the basic pip install command:

pip install ruptures

The package offers a variety of search methods (binary segmentation, Pelt, window-based change detection, dynamic programming, etc.), as well as multiple cost functions to play around with. In this tutorial, we focus specifically on search methods.

Search Method Background

This section provides a brief background on some of the search methods available in the ruptures package, including binary segmentation, PELT, window-based change detection, and dynamic programming.

Pruned Exact Linear Time (PELT) search method: The PELT method is an exact method, and generally produces quick and consistent results. It detects change points through the minimization of costs (4). The algorithm has a computational cost of O(n), where n is the number of data points (4). For more info on the PELT method, check out this paper.

Dynamic programming search method: This is an exact method, which has a considerable computational cost of O(Qn² ), where Q is the max number of change points and n is the number of data points (4). For more info on the dynamic programming search method, check out this paper.

Binary segmentation search method: This method is arguably the most established in literature (4). Binary segmentation is an approximate method with an efficient computational cost of O (n log n), where n is the number of data points (4). The algorithm works by iteratively applying a single change point method to the entire sequence to determine if a split exists. If a split is detected, then the sequence splits into two sub-sequences (5). The same process is then applied to both sub-sequences, and so on (5). For more info on binary segmentation, check out this paper.

Window-based search method: This is a relatively simple approximate search method. The window-based search method “computes the discrepancy between two adjacent windows that move along with signal y” (6). When the two windows are highly dissimilar, a high discrepancy between the two values occurs, which is indicative of a change point (6). Upon generating a discrepancy curve, the algorithm locates optimal change point indices in the sequence (6). For more info on the window-based search method, check out this paper.

Code Example

In the below code, we perform change point detection using the search methods described above. We use the time series for daily WTI oil prices, from 2014 to now, pulled via the Energy Information Administration’s (EIA) API ( see this tutorial for more info on using the EIA API to pull data):

def retrieve_time_series(api, series_ID):

""""""

Return the time series dataframe,

based on API and unique Series ID

api: API that we're connected to

series_ID: string. Name of the series that we want

to pull from the EIA API

""""""

#Retrieve Data By Series ID

series_search = api.data_by_series(series=series_ID)

##Create a pandas dataframe from the retrieved time series

df = pd.DataFrame(series_search)

return df

""""""

Execution in main block

"""""" #Create EIA API using your specific API key

api_key = 'YOUR API KEY HERE'

api = eia.API(api_key)



#Pull the oil WTI price data

series_ID='PET.RWTC.D'

price_df=retrieve_time_series(api, series_ID)

price_df.reset_index(level=0, inplace=True)

#Rename the columns for easier analysis

price_df.rename(columns={'index':'Date',

price_df.columns[1]:'WTI_Price'},

inplace=True)

#Format the 'Date' column

price_df['Date']=price_df['Date'].astype(str).str[:-3]

#Convert the Date column into a date object

price_df['Date']=pd.to_datetime(price_df['Date'], format='%Y %m%d')

#Subset to only include data going back to 2014

price_df=price_df[(price_df['Date']>='2014-01-01')] #Convert the time series values to a numpy 1D array

points=np.array(price_df['WTI_Price'])



#RUPTURES PACKAGE

#Changepoint detection with the Pelt search method

model=""rbf""

algo = rpt.Pelt(model=model).fit(points)

result = algo.predict(pen=10)

rpt.display(points, result, figsize=(10, 6))

plt.title('Change Point Detection: Pelt Search Method')

plt.show()



#Changepoint detection with the Binary Segmentation search method

model = ""l2""

algo = rpt.Binseg(model=model).fit(points)

my_bkps = algo.predict(n_bkps=10)

# show results

rpt.show.display(points, my_bkps, figsize=(10, 6))

plt.title('Change Point Detection: Binary Segmentation Search Method')

plt.show()



#Changepoint detection with window-based search method

model = ""l2""

algo = rpt.Window(width=40, model=model).fit(points)

my_bkps = algo.predict(n_bkps=10)

rpt.show.display(points, my_bkps, figsize=(10, 6))

plt.title('Change Point Detection: Window-Based Search Method')

plt.show()



#Changepoint detection with dynamic programming search method

model = ""l1""

algo = rpt.Dynp(model=model, min_size=3, jump=5).fit(points)

my_bkps = algo.predict(n_bkps=10)

rpt.show.display(points, my_bkps, figsize=(10, 6))

plt.title('Change Point Detection: Dynamic Programming Search Method')

plt.show()

Snapshot of the WTI Oil Price Time Series, pulled via the EIA API

Change Point Detection with Pelt Search Method, WTI Oil Price Time Series, 2014-Present

Change Point Detection with Binary Segmentation Search Method, WTI Oil Price Time Series, 2014-Present

Change Point Detection with Window-Based Search Method, WTI Oil Price Time Series, 2014-Present

Change Point Detection with Dynamic Programming Search Method, WTI Oil Price Time Series, 2014-Present

As you can see in the graphics above, the detected change points in the sequence differ based on the search method used. The optimal search method depends on what you value most when subsetting the time series. The PELT and dynamic programming methods are both exact (as opposed to approximate) methods, so they are generally more accurate.",https://towardsdatascience.com/a-brief-introduction-to-change-point-detection-using-python-d9bcb5299aa7,['Kirsten Perry'],2019-08-15 11:16:23.649000+00:00,757,"ruptures, changepoint detection, Python, PELT, binary segmentation"
Demystifying Tensorflow Estimator,"Tensorflow is one of the most popular machine learning library out there. It is one of the few framework that lets you work at the lowest and highest abstraction of machine learning. You can do useful machine learning using high level APIs that handle everything or you might build a custom model from ground up controlling everything from model architecture to training loops

You can find and run complete working example of the code snippets here

What is Tensorflow Estimator??

Similar to keras’s Model API, Tensorflow Estimator is a high level API that encapsulates training, evaluation, prediction and serving. In other words you don’t have to write training, evaluation or prediction loops, esitmator does it all for you. It also gives you a gives you a reusable model which you can save to or load from anywhere.

Why use Estimator API?

Estimator is an easy to use API that handles lots of complexity for your but still provide enough customization opportunities. You can design your model once and it will run seamlessly on your local machine with or without GPUs as well as on a multi-server distributed environment with GPUs and TPUs !!!

It safely runs a distributed training loop that takes care of loading data, checkpoints and write summaries for tensor board

How does it work?

Estimator API allows you to use pre-made as well as custom estimators. I will go through working with pre-made estimators here and leave custom estimators for another time

You can make any pre-made estimator work by going through following steps

a: Dataset input functions

These functions take in dataset and returns feature dictionaries and labels. Feature dictionary is a mapping of feature name to feature value in a dataset

def make_input_fn(df, epochs=500, shuffle=True, batch_size=32):

df = df.copy()

labels = df.pop('AveragePrice') # extract label from dataframe

def input_function():

# create dataset from inmemory pandas dataframe

dataset = tf.data.Dataset.from_tensor_slices((dict(df), labels))

if shuffle:

dataset = dataset.shuffle(buffer_size=len(df))



dataset = dataset.batch(batch_size).repeat(epochs)

return dataset

return input_function



train_input_fn = make_input_fn(train_df)

eval_input_fn = make_input_fn(eval_df, epochs=1, shuffle=False)

In above example make_input_fn is input function creator as it returns input_function which is the actual dataset input function

b: Define Feature Columns

In second step we create a list of all feature columns that we wish to train our model on. It defines all feature as tf.feature_column which require feature name and optionally datatype and a preprocessing function

DENSE_COLUMNS = ['Total_Bags','Small_Bags','Large_Bags','XLarge_Bags']



SPARSE_COLUMNS = ['type', 'year', 'region']



feature_columns = []

for feature in DENSE_COLUMNS:

feature_columns.append(tf.feature_column.numeric_column(feature))

for feature in SPARSE_COLUMNS:

vocab = data[feature].unique()

categorical_feature = tf.feature_column.categorical_column_with_vocabulary_list(feature, vocab)

feature_columns.append(tf.feature_column.indicator_column(categorical_feature))

In above example i have a dataset that has some dense(continuous) and some sparse(categorical) features. For each feature type, tf.feature is created and added into feature_columns list. Our model will then use this list to process actual data from dataset

c: Instantiate Estimator & Train

There are many pre-made estimators available. You can go through complete list here. You just have pass feature columns to it and you are good to go. If you need to perform parameter tuning you can pass those arguments as well. To start training, just call train method on your estimator and pass your dataset input function as argument

linear_regressor = tf.estimator.LinearRegressor(

feature_columns=feature_columns,

model_dir=output_dir

)

linear_regressor.train(train_input_fn)

That’s all!!!

Your model will start training and keep saving model checkpoints and summaries in output_dir. You can also point tensor board to this directory to monitor training

Tensorflow comes packaged with tensorboard which is an awesome tool for visualizing different model metrics. You can checkout a very helpful intro to tensorboard at neptune.ai",https://medium.com/analytics-vidhya/demystifying-tensorflow-estimator-7113e7275606,['Junaid Khattak'],2020-10-31 13:27:17.250000+00:00,540,"Tensorflow, Machine Learning, Estimator API, Keras Model API, Feature Columns"
PixieDust gets its first community-driven feature in 1.0.4,"PixieDust gets its first community-driven feature in 1.0.4

Now with time series data, the fairy tale continues

Last month I announced the availability of PixieDust 1.0. Since then, community adoption has been fantastic. Based on repo stars and on feedback at conferences and events, more developers and data scientists are using PixieDust as part of their work in Jupyter Notebooks.

Today, we’re releasing version 1.0.4 on PyPi, but what’s noteworthy is that this version is getting a new feature that has been prioritized by the community: time series data. (Personally, there’s no better feeling than working on a feature that users are clamoring for.)

The PixieDust fairy tale continues with time series data.

Once upon a time [series]

PixieDust now supports display of time series data for bar and line charts. Previously, when loading data into PySpark DataFrames from data sources that required schema discovery (CSVs, JSON, etc.), datetime values were often converted into strings. This caused problems when visualizing the data (sorting, formatting, etc.). Fixing it required complicated massaging of the data.

In the example below, I want to display stock values over time. Unfortunately, Spark converts the date values to Unix timestamps, and the results are not visualized correctly:",https://medium.com/codait/pixiedust-gets-its-first-community-driven-feature-in-1-0-4-b40c622a572e,['David Taieb'],2017-04-13 19:48:39.848000+00:00,194,"Pixie Dust, Py Pi, Jupyter Notebooks, Py Spark, Data Visualization"
5 Useful Image Manipulation Techniques Using Python OpenCV,"Introduction

Although many programmers don’t need to worry about processing images in their daily jobs, chances are that we may have to deal with images for small jobs. For instance, we may need to resize thousands of images to a particular size, or we may add a common background to all the images in a particular directory. Without a programmatic solution, these tasks are very tedious and time-consuming.

In this article, I want to introduce five basic image manipulation techniques using the OpenCV library that are to address some of these needs. If you haven’t installed OpenCV on your computer, you can install it very conveniently with the pip tool: pip install opencv-python . Please note that we use this library using import cv2 , which is just to extend the convention of importing the old version of OpenCV with import cv .

Once you’re able to import the library without any issues, you’re good to try some functionalities in this brief tutorial. For the sake of demonstration purposes, the present article will be using the image that is shown at the beginning of the article.

Before we can manipulate images, the first thing is to read the image, which can be done conveniently with the imread function, as shown below.

Read image

The generated data is a numpy array. Using a numpy array allows us to manipulate the data just as manipulating the numeric values of the array.

array. Using a array allows us to manipulate the data just as manipulating the numeric values of the array. The shape of the image can be accessed by its shape attribute. It tells us how large the image is, and it also tells us its composition of BGR scales (i.e., blue, green, and red). Each color constitutes a matrix of the 3D array. For instance, if we need to get the blue scale, we can use indexing original[:, :, 0] .

attribute. It tells us how large the image is, and it also tells us its composition of BGR scales (i.e., blue, green, and red). Each color constitutes a matrix of the 3D array. For instance, if we need to get the blue scale, we can use indexing . The total number of pixels can be accessed by the image’s size attribute.

attribute. It’s also important to know the data type of the numpy array. In many other applications, numpy arrays use the floating-point data type. However, in processing the images, the default data type is uint8 . If you take another look at examining the range of the values, you’ll find the following, which is what you can expect with a color expressed by the RGB values in integers.",https://medium.com/better-programming/5-useful-image-manipulation-techniques-using-python-opencv-505492d077ef,['Yong Cui'],2020-10-01 15:04:50.742000+00:00,440,"Image Manipulation, OpenCV, Numpy Array, BGR Scales, Pixel Size"
The Math behind Machine Learning,"How far is math important when speaking about machine learning?

Many of us start looking at the coding part to improve programming skills.

Machine learning has become a debatable field in the last 5 years, especially after the bomb of ‘Deep Learning’. Within this wide rush towards it, it is important for any beginner in this field to understand the basics of machine learning and its core aspect, the one that is leading it to be here today, and the core that will lead machines to lead the world.

In this article, I will try to cover a simple yet effective part of the math behind machine learning which is the probabilistic view of models using probability simple rules only.

The following image has crossed my way today:

Probably .. the master of ML.

The five rules of probability. Mainly, all programmers have taken a probability course in their high school or college. As simple as it seems, these simple rules form the most important laws of machine learning.

To make things clear, I will start stating some examples that use these probability rules ( shown in the figure above) and others.

CTC algorithm

CTC ( Connectionist Temporal Classification) is one of the interesting algorithms used in sequence-to-sequence models, that is based on probability laws. For instance, it works as follows:

let x be a letter ( x : letter) , w be a word ( w:word)

The question is: what is the probability of having the letter x in the word w ?

p(x/w) = ?. The answer here determines the next step that the algorithm will take.

Now, to know its importance, check the impact of CTC in the following subdomains of machine learning:

Handwritten text recognition, Voice recognition

The reason behind using ctc in these models is that they rely mainly on the probability on the next step to decide the current one.

The figure above shows the baseline architecture of the handwritten decoding model. First of all, convolutional layers are used to extract features for the handwritten images. Next, their output is fed to bidirectional lstm layers, simply to save the current feature(letter) to compare it with the next feature(letter). In this case, we say what is the probability of letter x1 knowing that it is followed ( or passed by) letter x2? see? simple probability lows lead to gentle and great models that will take the world into another future!

Finally, after predicting the pattern of letters, they will enter into a decision-making tree, the CTC model is responsible for choosing the correct pattern of the word.

Probability is the science of studying uncertainty. Machine learning is the science of solving uncertainty by making it ‘mostly’ certain.

These two factors make a great combination of togerther.",https://medium.com/analytics-vidhya/the-math-behind-machine-learning-331a4603838d,[],2020-10-16 13:07:20.325000+00:00,442,"Machine Learning, Probability, CTCAlgorithm, Decision Making Tree, Handwritten Text Recognition"
Web Scraping and Login using Python Selenium,"Have a web scraping problem when website must be logged in first?

Well, we can use Selenium for that problem. Basically, selenium is used for automated testing web validation, but it can also be used for scraping, because it can be controlled automatically by scripts, easily work with javascript, DOM or complex html tags

For example, we try to scrap news from websites that need to logged first, such as www.wsj.com or www.barrons.com

The first thing we do is install libraries, including selenium python library, webdriver manager library and import several selenium functions in your file

The Libraries

Create your function/class for login, the codes include:

put the url

set the web driver options (e.g. windows size, headless, etc.) and

login with your username and password

Login to Website through Selenium

After successful login, we can continue the code to get the news. We can choose the information what we need (e.g. title, article, date, etc) and store it to csv",https://medium.com/analytics-vidhya/web-scraping-using-selenium-79a2fcc77215,['Rahman Taufik'],2021-01-12 16:11:05.101000+00:00,151,"fileweb scraping, selenium, webdriver manager, login, automated testing"
The Emerging Role of Predictive Analytics in Tech,"Technology and data change every day. Just as you think you grasp the “next big thing” to help you manage your data, another innovation comes along. After a while, playing catch up can seem futile and you’re left treading water, hoping not to drown in the sea of data your business has accumulated.

So how do you make it to shore in a sea of overwhelming information?

Predictive analytics to the rescue. This life preserver dissects and uses your data effectively so you can stay even with the tide and swim with strong currents instead of against them.

What is Predictive Analytics?

Business intelligence leverages technologies and services to gather, analyze, interpret, and transform data into intelligent decision making using a streamlined approach that synthesizes all the information collected and presents it in easy-to-understand formats. BI tools, often referred to as data analytics, can provide incredibly deep and powerful insights into your business and organization, successfully predict trends and events, and use reporting functionality to serve as a guide for further action.

The predictive analytics portion is the ability to consume and crunch data using machine learning models to heavily steer a data-driven decision for the best possible outcome.

Predictive Analytics Evolution

This process has a deep history, but its emergence in tech has only occurred over the last 20 years. It has now evolved into a multi-industry best practices approach as it defines the practical outcomes for business objectives.

It has spidered its way into multiple industries and helped them from the inside out. The industries include:

Insurance

Banking/Financial

Technology

Retail/E-Commerce

Energy & Manufacturing

Government & Public Administration

Predictive Analytics Implementation

One of the biggest hesitancies we see with companies dragging their feet on predictive analytics implementation is a gap in skills. Before you can analyze data, it needs to be prepped first. Businesses need to ensure they have the manpower or can adequately contract out the duties of someone who can efficiently and effectively move between data and strategy as figuring out target definition is essential for outcome interpretation.

Once this skills gap has been filled and the data reasonably prepped, predictive modeling can start. Advances in technology enable the software to do a lot of the grunt work around this area, but it’s beneficial to have a data analyst carry out the modeling tasks to achieve the best results.

Your IT department should take over the process once the models hit the deployment stage. This is the exciting part as this is where the models work on your data and you see the results of their efforts. Now that your data is being collected, sifted through, and deployed appropriately using your defined analytics approach, it is time for your leadership team and executive decision makers to use the intel intelligently, strategically, and thoughtfully as they dictate the storytelling and direction of your vision.

Predictive Analytics Benefits

How exactly is predictive analytics, working in tandem with business intelligence solutions and RPA, beneficial?

The three main areas that predictive analytics assists businesses with are:

Marketing Optimization: Predictive analytics is used in marketing campaigns to optimize data by analyzing it to find predictive patterns for consumer behavior, letting businesses make intelligent business decisions based on the findings.

Fraud & Risk Reduction: New tech always means wariness among customer as keeping their information safe is a top priority. The combination of up-to-date cybersecurity metrics and predictive analytics serve as a fraud deterrent. On the business end, your company can use predictive analytics to gauge the risk of potential customers so you can mitigate the concern.

Improvement to Operations: Predictive analytics is commonly used by businesses for inventory control and resource management, helping businesses set prices and predict event and marketing outcomes.

A Look Into the Future

As big data analytics climbs in popularity, fueled by predictive analytics and nitty gritty details it can etch out for lucrative opportunities, possibilities for its future seem limitless. As time goes forward and other advancements improve, expect more of the predictive analytics functions to be carried out with less human connection.

That doesn’t mean human workers are disregarded completely. As with any big digital disruption, innovative starts and ends with human dreams. As long as your business stays adaptive and flexible, there’s a way to incorporate big tech ideas alongside us regular folk.

And you don’t have to go about it alone! Let us at ATC serve as your expert navigator. We can handhold and give constructive advice, lead the initiative completely, or just act as a resource for you to bounce ideas off of.",https://medium.com/@buvi_27/the-emerging-role-of-predictive-analytics-in-tech-8f3becaa48ce,[],2020-03-03 04:44:15.714000+00:00,732,"predictiveanalytics, businessintelligence, dataanalytics, machinelearningmodels, bigdataanalytics"
How to Build a Reporting Dashboard using Dash and Plotly,"A method to select either a condensed data table or the complete data table.

One of the features that I wanted for the data table was the ability to show a “condensed” version of the table as well as the complete data table. Therefore, I included a radio button in the layouts.py file to select which version of the table to present:

Code Block 17: Radio Button in layouts.py

The callback for this functionality takes input from the radio button and outputs the columns to render in the data table:

Code Block 18: Callback for Radio Button in layouts.py File

This callback is a little bit more complicated since I am adding columns for conditional formatting (which I will go into below). Essentially, just as the callback below is changing the data presented in the data table based upon the dates selected using the callback statement, Output('datatable-paid-search', 'data' , this callback is changing the columns presented in the data table based upon the radio button selection using the callback statement, Output('datatable-paid-search', 'columns' .

Conditionally Color-Code Different Data Table cells

One of the features which the stakeholders wanted for the data table was the ability to have certain numbers or cells in the data table to be highlighted based upon a metric’s value; red for negative numbers for instance. However, conditional formatting of data table cells has three main issues.

There is lack of formatting functionality in Dash Data Tables at this time.

If a number is formatted prior to inclusion in a Dash Data Table (in pandas for instance), then data table functionality such as sorting and filtering does not work properly.

There is a bug in the Dash data table code in which conditional formatting does not work properly.

I ended up formatting the numbers in the data table in pandas despite the above limitations. I discovered that conditional formatting in Dash does not work properly for formatted numbers (numbers with commas, dollar signs, percent signs, etc.). Indeed, I found out that there is a bug with the method described in the Conditional Formatting — Highlighting Cells section of the Dash Data Table User Guide:

Code Block 19: Conditional Formatting — Highlighting Cells

The cell for New York City temperature shows up as green even though the value is less than 3.9.* I’ve tested this in other scenarios and it seems like the conditional formatting for numbers only uses the integer part of the condition (“3” but not “3.9”). The filter for Temperature used for conditional formatting somehow truncates the significant digits and only considers the integer part of a number. I posted to the Dash community forum about this bug, and it has since been fixed in a recent version of Dash.

*This has since been corrected in the Dash Documentation.

Conditional Formatting of Cells using Doppelganger Columns

Due to the above limitations with conditional formatting of cells, I came up with an alternative method in which I add “doppelganger” columns to both the pandas data frame and Dash data table. These doppelganger columns had either the value of the original column, or the value of the original column multiplied by 100 (to overcome the bug when the decimal portion of a value is not considered by conditional filtering). Then, the doppelganger columns can be added to the data table but are hidden from view with the following statements:

Code Block 20: Adding Doppelganger Columns

Then, the conditional cell formatting can be implemented using the following syntax:

Code Block 21: Conditional Cell Formatting

Essentially, the filter is applied on the “doppelganger” column, Revenue_YoY_percent_conditional (filtering cells in which the value is less than 0). However, the formatting is applied on the corresponding “real” column, Revenue YoY (%) . One can imagine other usages for this method of conditional formatting; for instance, highlighting outlier values.

The complete statement for the data table is below (with conditional formatting for odd and even rows, as well highlighting cells that are above a certain threshold using the doppelganger method):

Code Block 22: Data Table with Conditional Formatting

I describe the method to update the graphs using the selected rows in the data table below.",https://medium.com/p/4f4257c18a7f#9afd,['David Comfort'],2019-03-13 14:21:44.055000+00:00,668,"data-table, conditional-formatting, doppelganger-columns, color-coding, highlighting"
"Building a JetBot AI, Part 1 — Hardware","I recently discovered a new-found passion for DIY projects around IoT devices and set out to build an AI-enabled robot — Nvidia’s JetBot AI. In this first part of the series, I will share my experiences from building the JetBot. In Part 2, I will cover the software side of the robot.

JetBot is powered by Nvidia’s Jetson Nano — a Single Board Computer (SBC), similar to a RaspberryPi, however with one important advantage over it — it sports a powerful 128-core GPU onboard. Nvidia advertises the chip as an easy to use platform capable of embedded IoT applications like home robots, image classification or speech recognition.

Jetson Nano enjoys an active community contributing open-source project ideas based on the Nano and JetBot is its flagship project, including STL files for 3D printing its frame, a detailed Bill of Materials outlining the necessary hardware components, all needed software packages and a set of engaging example programs showcasing Jetbot’s capabilities.

There are two ways you can build your JetBot — either by printing the body elements and purchasing the hardware components individually or selecting one of the kits from an ever-growing list of partners. This being my first project of this kind, I went for an easy option and purchased a kit from SparkFun.

The kit comes in two versions — a Jetson-included ($250) package or sans Jetson, in case you already have your own ($150). The SparkFun kit contains all the components you will need, like a battery pack to power the Jetson on the move, camera, wifi module to all the body elements, and wheels. The SSD card is pre-loaded with the OS that will boot up the JetBot and also contains software examples.

SparkFun provides detailed step-by-step instructions on building the robot and it’s mostly a breeze. It roughly takes 3 hours to complete the assembly at an easy pace.

Most of the steps require just a Philips screwdriver to assembly.

The acrylic plates were a bit tight to combine and I ended up cracking one of them… Luckily it was not a show stopper.

The Serial Controlled Motor Driver (SCMD) assembly requires the soldering of a few jumper contacts and some PTH headers.

Included breadboard allows the SCMD to be connected with a Micro USB breakout.

The SCMD module eventually gets connected to the DC motors.

The kit comes with a 64x48 OLED breakout which we will find helpful later on.

Finally, we mount the Jetson Nano board on top of the body.

And voila! The JetBot is complete!

In the next part of the series, we will look at the Software driving the JetBot and play with the examples provided by Nvidia.",https://medium.com/dan-on-coding/building-a-jetbot-ai-part-1-hardware-bc1879ebdb68,['Dan Siwiec'],2020-06-04 06:16:36.036000+00:00,432,"DIY, IoT, AI, Nvidia Jet Bot, Jetson Nano"
How to Build a Text Summarizer (TL;DR) With Simple Natural Language Processing,"From Text to TL;DR

For parts 3 and 4, we’ll develop a method called summarizeURL :

def summarizeURL(url, total_pars):

url_text = getTextFromURL(url).replace(u""Â"", u"""").replace(u""â"", u"""")

fs = FrequencySummarizer()

final_summary = fs.summarize(url_text.replace(""

"","" ""), total_pars)

return "" "".join(final_summary)

The method calls getTextFromURL above to retrieve the text and clean it from HTML characters and trailing new lines (

).

Next, we execute the FrequencySummarizer algorithm on a given text. The algorithm tokenizes the input into sentences and then computes the term frequency map of the words. Then, the frequency map is filtered in order to ignore very low-frequency and high-frequency words. This way, it is able to discard the noisy words (such as determiners that are very common but don’t contain much information) or words that occur only a few times. To see the source code, head to GitHub.

Finally, we return a list of the highest-ranked sentences, which is our final summary.

The full source code is available on GitHub.",https://medium.com/better-programming/how-to-build-a-url-text-summarizer-with-simple-natural-language-processing-ac1a9cb742de,['Assaf Elovic'],2021-01-02 13:01:44.500000+00:00,147,"text-to-TLDR, summarizeURL, get Text FromURL, Frequency Summarizer, Git Hub"
A tutorial(Part 1),"In this tutorial, we set out to analyze the Amazon product dataset using SparkMLlib. The training data set includes ASIN, Brand Name, Category Name, Product Title, Image URL. For a detailed description of the Amazon product data, the reader can refer to Julian McAuley webpage.

For the purpose of our analyses, the following key features and label are defined:

Features

ASIN: ID of the product BrandName: name of the brand Title: Title of the product ImageUrl: Url of the product image

Label

CategoryName: Name of the category.

Our objective is to use Scala programming language to write a classifier utilizing key product features provided in the training data set and to use them to test an unseen data set without a category label. In machine learning and statistics(MLS), classification is defined as identifying to which of a set of categories a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.

There could be several reasons for performing classification using the Amazon data set such as to have a better understanding of the customers’ reviews beyond what can be seen from just computing the summary statistics. It may also help know how the product’s features are related to customers’ choices based on the categories of foods, clothes, musical instruments, electronics etc.

It is worth noting that for the Amazon product data, the features and label are mainly categorical in nature; hence it would be necessary to convert them to a numerical type that the machine understands. In a short while, we would get to this.

To accomplish this objective, the following main steps are required:

Reading the models and exploring the feature characteristics

Model transformation

Creating a pipeline

Train and test the model

Evaluating the model

Reading the data and exploring the data characteristics

Typically, most data are saved as a file in CSV or JSON format. Hence, first is to have a quick preview of the data:

CSV

JSON

Next, we would get the file location and set up spark session builder to read the file.

var dir = ""****/amazon_dataset""

val csv_file = dir + ""data.csv""

val json_file = dir + ""data.json""

val spark = SparkSession.builder().master(""local"").appName(""My ML project"").config(""spark.some.config.option"", """").getOrCreate()

import spark.implicits._

You would have to put all these lines of code in a scala object and make the required imports e.g

import org.apache.spark.sql.SparkSession

object amazon extends App {

}

Note: The spark implicit statement is for implicit conversions like converting RDDs to DataFrames

To read CSV file, all you have to do is write

val df = spark.read .format(""csv"") .option(""header"", ""true"") .option(""mode"", ""DROPMALFORMED"") .load(csv_file)

And if JSON file

val df= spark.read.json(json_file)

And then we can finally explore the data characteristics

val numOfRowsToShow =10 //change this as you prefer.

df.describe().show(numOfRowsToShow)

df.printSchema()

Now what? It seems this page is getting too long; so I will end here and continue the next topic on a new page. Thanks for reading and see you there!

References:

R. He, J. McAuley. Modeling the visual evolution of fashion trends with one-class collaborative filtering. WWW, 2016

J. McAuley, C. Targett, J. Shi, A. van den Hengel. Image-based recommendations on styles and substitutes. SIGIR, 2015",https://medium.com/analyzing-the-amazon-product-data-set-using/a-tutorial-eb657053dfe4,['Taiwo Adetiloye'],2017-12-27 21:55:20.681000+00:00,488,"Amazon Product Dataset, SparkMLlib, Scala Programming Language, Machine Learning, Classification"
Beginner’s Guide to Creating the SVD Recommender System,"Utility Matrix

The dataset in the current form is of no use to us. In order to use the data for the recommender engine, we need to transform the dataset into a form called a utility matrix. We make a function create_utility_matrix in a new script. Name it recsys.py . We shall use the functions in this script to work on our train and test set.

As a parameter, we pass a dictionary that stores the key-value pairs for each column of the dataset `data` that we are also passing. From the dataset, we’ll see the column number or column names for each of the corresponding fields, the column userId or column 0 for the key ‘user’, column movieId or column 1 for key ‘item’ and column ratings or column 2 for key ‘value’.

Utility Matrix is nothing but a 2D matrix where one axis belongs to the users and the other axis belongs to the items (movies in this case). So the value at (i,j) location of the matrix will be the rating that user i gave for movie j.

Let’s give an example to clear up a bit more. Suppose we have this dataset of 5 ratings.

+----+----------+-----------+----------+

| | userId | movieId | rating |

|----+----------+-----------+----------+

| 0 | mark| movie1| 5 |

| 1 | lucy| movie2| 2 |

| 2 | mark| movie3| 3 |

| 3 | shane| movie2| 1 |

| 4 | lisa| movie3| 4 |

+----+----------+-----------+----------+

If we pass this dataset through the create_utility_matrix function described below, it will return an utility matrix like this and auxiliary dictionaries of user_index and item_index as shown below.

+----+----+----+

| 5 | nan| 3 | # user_index = {mark: 0, lucy:1, shane:2, lisa:3}

+----+----+----+ # item_index = {movie1:0, movie2: 1, movie3:2}

| nan| 2 | nan|

+----+----+----+

| nan| 1 | nan| # The nan values are for user-item combinations

+----+----+----+ # where the ratings are unavailable.

| nan| nan| 4 |

+----+----+----+

Let’s see the function now.",https://towardsdatascience.com/beginners-guide-to-creating-an-svd-recommender-system-1fd7326d1f65,['Mayukh Bhattacharyya'],2020-05-26 17:34:49.249000+00:00,310,"Utility Matrix, Recommender Engine, Key-Value Pairs, 2D Matrix, Auxiliary Dictionaries"
Uber AI’s Go-Explore Tackles Hard-Exploration Problems,"Uber AI Lab has created a buzz in the machine learning community with the publication of a paper introducing a new reinforcement learning algorithm called Go-Explore. The algorithm is designed to overcome the challenges of intelligence exploration in reinforcement learning to improve performance on hard-exploration tasks.

“Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma’s Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic Motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a Simulator during training (eg robotics)”. (arXiv).

Synced invited Northeastern University Professor and machine learning researcher Rose Yu to share her thoughts on Go-Explore.

How would you describe Go-Explore?

Go-Explore is an algorithm developed by Uber AI Lab for addressing the effective exploration problem in reinforcement learning.

How effective is this technology?

Go Explore solved Atari’s Montezuma’s Revenge, scoring almost four times the previous state-of-the-art, which is a classic example of a difficult exploration problem.

What impact might this bring to the AI community?

The tech addresses the fundamental challenge of effective exploration in reinforcement learning (RL), which could accelerate progress of RL in complex real-world domains such as robotics.

Can you identify any bottlenecks?

In order to maintain a memory of previously visited novel states, the algorithm represents a state as a downsampled 8x11 grayscale image with game-specific knowledge. This would not generalize easily to other domains.

Go-Explore also assumes a deterministic environment is available during training, which allows it to initialize to arbitrary states. However, such an assumption is not made in most previous work.

Could you predict any potential future trends related to this tech?

Using imitation learning to robustify policies learned in a deterministic environment.

Many RL applications in the future will make use of simulators, in which case exploiting simulator properties such as deterministic environments might be plausible.

The paper Go-Explore: a New Approach for Hard-Exploration Problems is on arXiv.

About Prof. Rose Yu

Rose Yu is an Assistant Professor in Northeastern University Khoury College of Computer Sciences. Previously, she was a postdoctoral researcher in the Department of Computing and Mathematical Sciences at Caltech. She earned her PhD in Computer Science at the University of Southern California and was a visiting researcher at Stanford University. Her research focuses on machine learning for large-scale spatiotemporal data. She is generally interested in optimization, deep learning and reinforcement/imitation learning. Yu has over a dozen publications in leading machine learning and data mining conference and several patents. She received the USC Best Dissertation Award, the Annenberg Fellowship, and was named one of MIT’s Rising Stars in EECS.

Synced Insight Partner Program

The Synced Insight Partner Program is an invitation-only program that brings together influential organizations, companies, academic experts and industry leaders to share professional experiences and insights through interviews and public speaking engagements, etc. Synced invites all industry experts, professionals, analysts, and others working in AI technologies and machine learning to participate.

Simply Apply for the Synced Insight Partner Program and let us know about yourself and your focus in AI. We will give you a response once your application is approved.",https://medium.com/syncedreview/uber-ais-go-explore-tackles-hard-exploration-problems-4d79fb7d8936,[],2019-02-07 16:27:25.724000+00:00,543,"machine learning, reinforcement learning, go-explore, deep learning, optimization"
Better understanding of matplot-library,"“Picture is worth a thousand words”, the plots and graphs can be very effective to convey a clear description of the data to an audience or sharing the data with other peer data scientists. Data visualization is a way of showing complex data in a graphical form to make it understandable. When you are trying to explore the data and getting familiar with it, data visualization is used. In any corporate industry, it can be very valuable to support any recommendations to clients, managers, or decision-makers.

Darkhorse Analytics is a company that runs a research lab at the University of Alberta since 2008. They have done really fascinating work on data visualization. Their approach to visualizing data depends on three key points: less is more effective, it is more attractive, and it is more impactive. In other words, any feature incorporated in the plot to make it attractive and pleasing must support the message that the plot is meant to get across not to distract from it.

Matplotlib

Matplotlib is one of the most popular data visualization library in python. It was created by a neurobiologist, John Hunter(1968–2012). Matplotlib’s architecture is composed of three layers:

Architecture of matplotlib

Backend layer

The back-end layer has three built-in abstract interface classes:

A. FigureCanvas: matplotlib.backend_bases.FigureCanvas

It defines and encompasses the area onto which the figure is drawn.

B. Renderer : matplotlib.backend_bases.Renderer

An instance of the renderer class knows how to draw on the FigureCanvas.

C. Event: matplotlib.backend_bases.Event

It handles user input such as keyboard strokes and mouse clicks. Artist layer

It is composed of one main object, i.e Artist. The artist is the object that knows how to use the renderer to draw on the canvas. Everything we see in the Matplotlib figure is an artist instance. There are two types of artist object

A. Primitive: Line2d, Rectangle, Circle, and Text.

B. Composite: Axis, Tick, Axes, and Figure

Each composite can contain other Composite artists as well as primitive artists. For example, a figure artist would contain as axis artist as well as a text artist or rectangle artist. Scripting layer

It was developed for those scientists who are not professional programmers. The goal of this layer is to perform a quick exploratory analysis of data. It is essentially the Matplotlib.pyplot interface. It automates the process of defining a canvas and defining a figure artist and connecting them. Since it automatically defines canvas, artist and connects them. It makes data analysts easy to do things. So most of the data scientists prefer this scripting layer to visualize their data.

The above code plots a histogram of a hundred random numbers and saves the histogram as matplotlib_histogram.png.

The versatility of Matplotlib can be used to make many visualization types:-

Scatter Plots

The output of the above code

Bar charts and Histograms

Bargraph

Histogram

Line plots

Line plots

Pie charts

pie chart

Stem plots",https://medium.com/mldotcareers/data-visualization-8b17843b9bbc,['Saroj Humagain'],2020-09-16 12:08:03.136000+00:00,450,"Data Visualization, Matplotlib, Darkhorse Analytics, Figure Canvas, Renderer"
Week 6 — Audio Emotion Recognition System (Part IV),"Then, we tried another classifier to improve our accuracy. We took the mean of mfcc and “chroma_stft”, “chroma_cqt”, “chroma_cens”, “rms”, “spectral_contrast”, “spectral_bandwidth”, “tonnetz”, “zcr” features and applied Decision Tree algorithm. Accuracy is 0,35.

Then, we used XGBoost as classifier. Result of this algorithm is 0,40.",https://medium.com/bbm406f19/week-6-audio-emotion-recognition-system-part-iv-6b6fe1913a6a,['Ece Omurtay'],2020-01-07 18:17:34.052000+00:00,44,"machine learning, classification, decision tree, XGBoost, mean of mfcc"
What will it take to make a healthcare bot your new best friend?,"What will it take to make a healthcare bot your new best friend? Digital Leaders Follow Mar 25 · 5 min read

Written by Antonella Bonanni, AVP and Chief Marketing Officer of Healthcare, Cognizant

Developers of smart speaker systems for healthcare consumers must address challenges like AI bias and human-centered application design.

“I was so tired, I started talking to Alexa.”

I can’t tell you how many times I’ve heard people refer to Alexa as something more than a digital assistant this past year. More people are talking to and asking questions of voice-activated digital assistants and cognitive-powered colleagues than ever before. Nearly 19 million homes in the U.S. have a smart speaker. In four years, Juniper Research predicts more than half of all American homes will have and use a smart speaker. That’s a smart speaker in more than 70 million U.S. households by 2022.

Already, Alexa has more than 1,000 healthcare-related “skills,” which allow users to make queries, to which the bot responds. These skills let consumers ask about pharmaceutical-company-sponsored prescription medications, yoga, illnesses and much more.

Before we begin asking these devices to diagnose an ailment or provide treatment suggestions, however, some advancements will be needed. In particular, all answers may not be created equal, particularly when it comes to how they may be influenced by those building the software. Secondly, these applications need to be designed in a way to be truly focused on the healthcare consumer.

Using natural language processing, smart speakers and artificial intelligence (AI), digital assistants can understand much of what we say and respond. Over time, they will get smarter, and their capabilities and the uses for smart speakers will increase dramatically.

The problem is, it’s well-known that societal biases, both intended and unintended, can creep into AI systems. Bias can emanate from humans themselves and from the historical data used to train the algorithm. Consider, for example, algorithms used by universities to assess admissions. If the training data reflects bias from previous admissions procedures, like gender discrimination, these biases must be corrected, or they’ll persist. Bias can also stem from self-learning AI systems that create new AI systems, resulting in biases that multiply over subsequent generations of computer code.

Compounding the bias problem is that although humans create AI, we don’t always understand how it arrives at specific answers. This lack of transparency makes it difficult to not only eliminate bias but also to instill trust.

As we’ve said in a recent report and webinar, ethics will become particularly important as AI becomes more ubiquitous, and as AI systems increasingly learn from one another, not just from the inputs that humans provide. (Click here for a summary of the webinar.)

To eliminate or at least minimize AI bias, we need to start before the algorithm is built, with the inputs, themselves. This requires human input and control, as well as ongoing human supervision. As we’ve explained, this is not as arcane a process as one might think. The concept is familiar to parents who provide feedback and guidance to raise their children to be good members of society, and there are well-understood tools and frameworks from the world of human sciences that can be used to instill ethics into the design and operation of AI.”

The need to control for bias in the AI world isn’t much different from how medicine has been practiced for years. Healthcare providers are influenced by their biases, as well, and it’s the healthcare consumer’s responsibility (and in her best interest) to understand how a course of treatment may affect her.

Further, from healthcare’s point of view, bias may have nothing to do with race, gender, age or any number of other identifiers that make us all different. Instead, it may be the way healthcare consumers are perceived: Are we patients? Sufferers? Survivors? Consumers? Or some combination?

Those supplying the answers to smart speakers would do well to view their customers as everyday people simply looking for information. As Sebastian Jespersen, who writes frequently on the relationship between brands and people, points out, few of us will want to know about products during our first healthcare encounter with a smart speaker. Rather we’re more interested in learning about a specific illness or injury as we make a query. Digital healthcare solutions, he says, should be designed with a focus not on a product, nor on a person who’s suffering, but on the consumer as a human being.

Human-Centered Solutions

In addition to the problem of bias, to make a significant impact on healthcare delivery, smart speakers must do more, more quickly, than what we’d expect from a human-based encounter.

For example, one hospital uses smart speakers in patient rooms to expedite answers to common questions. But does it really work in a new way? Or is the smart speaker just a different way of getting the same old answers? Patients can ask for a nurse, but the smart speaker still can’t tell the patient how long the wait will be. Or they can ask “ what is my diet,” and the response will be a “bland diet.” But is this really the wording most patients would use, or would they say, “What can I eat today?” And do they want to know what type of diet to follow, or specific foods and meals they can safely eat? Both the answer and the question are constructed from the healthcare provider’s point of view, not the patient’s. The system would be better off being more empathetic toward what people really need at that moment in time.

With the number of smart speakers worldwide expected to grow — and retailers slashing prices to push acceptance — there’s no doubt a smart speaker will become your BFF sooner or later.

But as is the case with many emerging technologies, we’re not quite there yet. Developers of these systems must address challenges like AI bias, providing answers that are customer-focused and, finally, designing applications that are truly focused on the healthcare consumer.

Originally published here.

More thought leadership",https://medium.com/digital-leaders-uk/what-will-it-take-to-make-a-healthcare-bot-your-new-best-friend-da09d27a3921,['Digital Leaders'],2020-03-26 12:09:51.756000+00:00,986,"from Cognizant here.Healthcare, AI, Smart Speakers, Bias, Natural Language Processing"
Píldoras de Metadatos 💊 Receta #22,"Learn more. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more

Make Medium yours. Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore",https://medium.com/metadatos/p%C3%ADldoras-de-metadatos-receta-22-2ba26cc3fb96,['Jaime Durán'],2020-12-21 14:20:21.638000+00:00,64,"below.Medium, Open Platform, Insightful Thinking, Dynamic Thinking, Expert Voices"
Working with Geospatial Data in Machine Learning,"Techniques & Ideas

After seeing what the data looks like, it’s time to shed some light on the feature engineering phase. Here, we’ll explore some ideas and techniques to extract and build better features that will improve our predictive model.

1- Use features as they are

The first thought that comes to mind is to use longitude and latitude as they currently are in the predictive model.

But bear in mind that the scale of the two variables is different, so you need to use a model that doesn’t require any normalization, like tree-based algorithms. Otherwise, you’ll have to perform normalization and include them in the feature set.

You can take this a step further by using a model that doesn’t need any normalization. Instead, you can just round the features to 3 or 4 decimal places, which will improve your model’s performance. Here’s how we can do this in pandas:

train = train.round({""Pickup Lat"":4, ""Pickup Long"":4})

Another simple but powerful transformation is to convert the latitude and longitude to radians instead of degrees. This will basically represent the same thing but in a different unit. You can achieve that using the NumPy function radians() .

import numpy as np

train[""Pickup Lat""] = np.radians(train[""Pickup Lat""])

train[""Pickup Long""] = np.radians(train[""Pickup Long""])

2- Perform Clustering

One of the approaches to handle point coordinates is to treat them in relation to each other.

In order to do that, we perform clustering and then assign each cluster-id to a point. By doing this, we’ll create a categorical variable that we can one hot encode to get better results.

The choice of the clustering algorithm matters. I tested many algorithms like K-means, DBSCAN, and hierarchical clustering—the latter two seem to give better results when it comes to geospatial features.

This is true given that K-means works well when trying to maximize variance, which is good if the feature space is linear in nature. But if it’s non-linear, then hierarchical clustering and DBSCAN are the best choices.

The number of clusters will depend on the problem, but in general, we have to test and see what gives better results.

Here is a sample that illustrates using K-means and hierarchical clustering algorithms with 5 clusters each:

If we re-plot the data using the newly created clusters, we obtain the following plot:

Data points with their cluster IDs.

3- Reverse Geocoding

It often happens that the point’s coordinates are far enough from each other and the data rows contain coordinates from different cities or countries in the world.

We can make use of this fact to perform reverse geocoding, which basically converts point coordinates to a readable address or place name (i.e. cities or countries). That way, we can create a new categorical variable.

And from that, we can create a field for city or country, but in some cases, where the measured points are in the same city, that method won’t work well— it will create a city variable with only one unique value.

For that, you have to extract the street or something else that varies from point to point.

4- Distance Feature:

To demonstrate this next technique, let’s consider the problem of determining ETA (Estimated Time of Arrival). For this, the dataset will contain departure and arrival coordinates.

From that, you can create a distance feature. This will improve your model a lot if the problem relies on the said feature—and ETA problems are very much in this category.

If you don't have two coordinates, you can calculate the distance to a fixed point (e.g origin), which will depend on the context of your problem.

The distance formula we use here is the Harvsine Formula, which takes into consideration that the earth’s shape is a sphere and not flat, so the distance is more realistic.

Here’s an implementation of the Harvsine Formula and its application on our dataset:

5- Extract X, Y, and Z:

I found this method on the data science stack exchange platform. It can moderately help our model but might be less effective than the previous methods.

The idea here is to represent those latitudes and longitudes via 3 coordinates. The problem with latitude and longitude is that they’re 2 features that represent a 3-dimensional space. We can, however, extract X, Y, and Z (our 3rd dimension) using sin and cosine functions. In this way, these features can be normalized properly.

The rule to derive these coordinates is the following:

x = cos(lat) * cos(lon)

y = cos(lat) * sin(lon)

z = sin(lat)

Here is the Python code we need to apply on our dataset:

6- Other derived features

Besides all the aforementioned features that you can extract from geospatial data, you can handcraft some more with the following ideas:

Nearest city name Distance to the nearest city Nearest city population Nearest big city Distance to the nearest big city

These features include populations, city names, and distances—and they require a lot of code.",https://heartbeat.fritz.ai/working-with-geospatial-data-in-machine-learning-ad4097c7228d,['Younes Charfaoui'],2021-04-09 16:52:39.793000+00:00,780,"Feature Engineering, Clustering, Reverse Geocoding, Harvsine Formula, Coordinates Transformation"
WSL2 for Data Science: Seamless Integration of Ubuntu and Windows 10,"Ubuntu 20.04 LTS available on the Microsoft store (that’s right).

If you are the type of person that buys a PC and the first thing they do is partition the hard drive to install the latest Ubuntu, then this article is for you. In the latest version of Windows 10, Microsoft has integrated WSL2 (Windows subsystem for Linux 2) which enables you to have a full installation of Ubuntu running on Windows using Hyper-V virtual machine technology. Note that your computer must support Hyper-V for WSL2 but not for WSL.

For installation of WSL2 follow this steps. And a more detailed guide including some tricks.

After installing WSL2, installing Ubuntu 20.04 LTS is as simple as opening the Microsoft store (yes, Ubuntu is now available on a store as an app) and downloading the latest version. Once installed, you can open the Ubuntu terminal by typing “ubuntu” on the Windows search bar. From the terminal, you are in Linux world: you have git, sudo, ls, cat, apt-get, make, and all those beautiful things that come with Linux. According to Windows:

WSL 2 uses the latest and greatest in virtualization technology to run a Linux kernel inside of a lightweight utility virtual machine (VM). However, WSL 2 is not a traditional VM experience.

Accessing files

From the Windows file explorer you can type

\\wsl$\Ubuntu\home\youUbuntuUserName\

which will take you to your Ubuntu installation. From Ubuntu, type:

cd /mnt/c/Users/yourWindowsUserName/

and you will be able to access all your Windows files! It is so easy to access and copy files that you can run GIT on Ubuntu and create repositories from you Windows files without having to install GIT on Windows. You could also create a GIT project on Ubuntu, edit your code on Atom (installed in Windows) and running it on your Windows web browser using RStudio server.

Running RStudio server on Ubuntu and Edge

I know the words R, Ubuntu and Edge usually don’t come together in a sentence but let me explain. One of the downsides of WSL2 is that it does not have native support for graphics (you can enable graphics using X-server but that’s different story). On the other hand, RStudio server and Jupyter Notebook run on a web browser so what if we run R and python on Ubuntu, but stream the output on Windows using Edge or Chrome? Sounds great.

First thing would be to install RStudio server in Ubuntu. You want the server version, not the desktop version otherwise you won’t be able to send the output to the web browser. The RStudio team put together a great guide on installation.

Once installed, you can start the server on the Ubuntu terminal:

sudo rstudio-server start

Open your browser and go to http://localhost:8787/ Here you can log in using your Ubuntu credentials. RStudio server runs pretty smooth on Edge and Chrome. Here you can see a 3D scatterplot made with Plotly, which works out-of-the-box on Ubuntu, and now on Windows too!

RStudio server is running on the Ubuntu terminal but we can see the RStudio GUI using any web browser on Windows.

Something cool about RStudio server is that it automatically saves your session such that whenever you open RStudio you will start from where you left even if you close Ubuntu or turn off your laptop!

Jupyter Notebooks and TensorFlow

In principle, we can do exactly the same thing as we did with RStudio Server and run Jupyter from Ubuntu on Edge (or Chrome).

# Run from Ubuntu WSL2

jupyter lab --no-browser # then access the notebook with the address printed in the terminal

# usually: [<http://localhost:888>](<http://localhost:8889/lab>)8/

However, since Anaconda works pretty well on Windows 10, we can run Python directly on Windows 10. Moreover, we can set up our GPU to work with TensorFlow by leveraging the fact that Nvidia drivers work pretty well on Windows already. Check out my article on how to setup TensorFlow with CUDA on Windows 10 .

Python is running directly on Windows 10 from the anaconda terminal. Jupyter notebook is running on Edge.

Python 3.8 runs pretty well with TensorFlow 2.4 and CUDA 11.0. So far I haven’t got any problems (usually installing python libraries could be a nightmare because of compatibility issues).

Conclusion

This is a very brief description of the setup I am currently using. Being a long-time Ubuntu user, I can’t help but thinking that I should just go ahead and install Linux on a drive partition. However, the Windows 10 is overall good: window management is pretty smooth, the search bar is fast and find what you are looking for most of the times, you can install software such as Adobe, PowerPoint, Ableton Live, Think or Swim, Xbox games etc. And most importantly your laptop simply won’t work as well on Linux because not all of the drivers will be available or optimized. This last part is really important is you have a GPU, HD audio, or some nice — and completely unnecessary — lighting in your keyboard (Razer 15 user here). So while I am still thinking of making a partition for Ubuntu, it seems that you can have your Data Science startup kit (GIT, R, Python, TensorFlow) running on the WSL2 while having the benefits of your laptop running on Windows 10.",https://medium.com/swlh/data-science-using-wsl2-seamless-integration-of-ubuntu-and-windows-10-dbe94cbac2b6,['Alejandro Granados C'],2021-01-22 12:22:04.583000+00:00,851,"WSL2, Ubuntu 20.04 LTS, Windows 10, Hyper-V, Microsoft Store"
An End to End Introduction to GANs,"Generator architecture

One of the main problems we face with GANs is that the training is not very stable. Thus we have to come up with a Generator architecture that solves our problem and also results in stable training.

The preceding diagram is taken from the paper, which explains the DC-GAN generator architecture. It might look a little bit confusing.

Essentially we can think of a generator Neural Network as a black box which takes as input a 100 sized normally generated vector of numbers and gives us an image:

How do we get such an architecture?

In the below architecture, we use a dense layer of size 4x4x1024 to create a dense vector out of this 100-d vector. Then, we reshape this dense vector in the shape of an image of 4x4 with 1024 filters, as shown in the following figure:

We don’t have to worry about any weights right now as the network itself will learn those while training.

Once we have the 1024 4x4 maps, we do upsampling using a series of Transposed convolutions, which after each operation doubles the size of the image and halves the number of maps. In the last step, though we don’t half the number of maps but reduce it to 3 channels/maps only for each RGB channel since we need three channels for the output image.

Now, What are Transpose convolutions?

In most simple terms, transpose convolutions provide us with a way to upsample images. While in the convolution operation we try to go from a 4x4 image to a 2x2 image, in Transpose convolutions, we convolve from 2x2 to 4x4 as shown in the following figure:

Upsampling a 2x2 image to 4x4 image

Q: We know that Un-pooling is popularly used for upsampling input feature maps in the convolutional neural network (CNN). Why don’t we use Un-pooling?

It is because un-pooling does not involve any learning. However, transposed convolution is learnable, and that is why we prefer transposed convolutions to un-pooling. Their parameters can be learned by the generator as we will see in some time.

Discriminator architecture

Now, as we have understood the generator architecture, here is the discriminator as a black box.

In practice, it contains a series of convolutional layers and a dense layer at the end to predict if an image is fake or not as shown in the following figure:

Takes an image as input and predicts if it is real/fake. Every image conv net ever.

Data preprocessing and visualization

The first thing we want to do is to look at some of the images in the dataset. The following are the python commands to visualize some of the images from the dataset:

The resultant output is as follows:

We get to see the sizes of the images and the images themselves.

We also need functions to preprocess the images to a standard size of 64x64x3, in this particular case, before proceeding further with our training.

We will also need to normalize the image pixels before we use it to train our GAN. You can see the code it is well commented.

As you will see, we will be using the preceding defined functions in the training part of our code.",https://towardsdatascience.com/an-end-to-end-introduction-to-gans-bf253f1fa52f,['Rahul Agarwal'],2019-12-25 11:39:06.748000+00:00,513,"generator architecture, transposed convolutions, discriminator architecture, data preprocessing, image visualization"
Apache Spark on Windows 10! A Match made in Heaven (using million websites),"I was working for Hartford Steam Boiler in the Summer and Fall of 2020 (remotely due to COVID-19) as a Machine Learning Intern. I did get a chance to work on large-scale Machine Learning Projects of HSB with state-of-the-art big data technologies like PySpark, SparkR, Hadoop, Scala and I was working on developing a prediction model (Bayesian Hierarchical Model) using Gibbs Sampler(R-JAGS) and writing around thousand lines of code in R. I was able to compute the lengthier chains of Markov Chain Monte Carlo ( MCMC ) and deploy the Model and its predicted Data into production. I was amazed by the computation power which Spark brings with itself and to be very honest, I did enjoyed working using Spark for distributed computing. To anybody who is thinking about learning Spark for developing Machine Learning Models; Please don’t fear learning one more language in your over filled bucket of Skill sets; we have to be versatile as this wont go away soon. It is designed in a way to increase its stay longer, while the new libraries that are being developed to use in Spark is something worth waiting for. Enough of my rant, lets get to the exciting part ahead.

So I did successfully installed Apache Spark on a WINDOWS Machine! Yes, you read that right… I have installed Apache Spark on my WINDOWS 10 machine after first reading millions of website which had like an overview of what needs to be done, but not the detailed answer of why it needs to be done and what is not needed to be done. Please read this patiently and I would request your 100% attention on the same.

First, I will ask you to install Anaconda3–4.2.0-Windows-x86_64, because it runs Python version: 3.5.6

Python Version compatible with Spark

Now, after downloading the setup file of Anaconda from https://repo.anaconda.com/archive/ and installing it on the Computer, you should follow these following steps as written in the Order.

JAVA SETUP:

i]. Install JAVA 8 from the Oracle Website by checking which version is suitable for your System i.e. Windows X86 or Windows X64. Run the Setup file and Java will be installed in your System.

ii]. The Folder jdk1.8.0_271 should be placed directly under C drive by COPYING it from the original JAVA Installation Path.

C:\jdk1.8.0_271

iii] The above step is significant and I will explain its significance by the end of this post.

2. INSTALL SPARK:

i] Download the Spark 2.4.7 tar.gz file from Spark Website and extract the same file in C:\spark folder that you can create in your C drive.

3. WINUTILS.exe

i] Download winutils.exe file and paste it again in C drive by creating a folder C:\winutils\bin

ii] Create a c:\tmp\hive directory, and open your Command prompt and change your directory to c:\winutils\bin, and run winutils.exe chmod 777 c:\tmp\hive.

5. Open the the c:\spark\conf folder, and make sure “File Name Extensions” is checked in the “view” tab of Windows Explorer.

6. Rename the log4j.properties.template file to log4j.properties.

Edit this file (using NOTEPAD++) and change the error level from INFO to ERROR for log4j.rootCategory.

7. Open Edit System Environment Variables Window by searching “Environment Variables” in Windows Startup and click on Environment Variables.

ENVIRONMENT VARIABLES

8. After clicking Environment Variables, Add these commands in the same manner in USER VARIABLES

i]. HADOOP_HOME = C:\winutils

ii] JAVA_HOME = C:\jdk1.8.0_271

iii] PYSPARK_DRIVER_PYTHON = jupyter

iv] PYSPARK_DRIVER_PYTHON_OPTS = notebook

v] PYSPARK_PYTHON = python3

vi] PYSPARK_SUBMIT_ARGS = — packages ${PACKAGES} pyspark-shell (2 dashes in the front of this variable)

vii] SPARK_HOME = C:\spark

viii] Under Path Variable in User Variables, add the following commands:

%SPARK_HOME%\bin %JAVA_HOME%\bin

9. Add the following commands under PATH Variable in SYSTEM VARIABLES

C:\spark\spark-2.4.7-bin-hadoop2.7\bin

10. Add the JAVA_HOME Variable in SYSTEM VARIABLES as well:

JAVA_HOME = C:\jdk\jre1.8.0_271

11. After the Steps given above, open ANACONDA TERMINAL and change your directory to c:\spark.

12. Then, type pyspark to proceed with the Big Data Magic.

Please follow the steps as mentioned and we can tame this successfully without any further issue whatsoever. Please connect with me on other Social Media Platforms and you can check out some of my Projects in GitHub as well.

I look forward to help someone who is in need of the hour of having his System Configured to perform Big-Data Computation, while researching about new Machine Learning Algorithms to learn from.",https://medium.com/@uttasargasingh9067/apache-spark-on-windows-10-a-match-made-in-heaven-using-million-websites-8ea021690f78,['Uttasarga Singh'],2020-11-11 22:32:48.400000+00:00,686,"machine learning, big data, Apache Spark, Python, Hadoop"
Geohash Vs OLC: Evaluating the geocoding techniques in adtech,"One of the biggest challenges in hyperlocal online advertising is to formulate a scalable geocoding technique for creating efficient user pools with different client configurable radii around a point of interest (POI). The size of POIs or the target physical store vary enormously and a highly flexible yet scalable technique has to be implemented to get the best results.

The two most prominent techniques for geocoding are Open Location Code (OLC) and Geohash. Both techniques have pros and cons, and a unique set of use cases.

The Open Location Code (OLC) is a geocoding system for identifying an area anywhere on Earth. It’s designed to be used as a street address and may be especially useful in places where there is no formal system to identify buildings, such as street names, house numbers, and postcodes.

OLCs are derived from latitude and longitude coordinates so they already exist everywhere. They are similar in length to a telephone number. For example, 87G8Q257+5QP is the 10 character plus code for Times Square, New York. Often these codes can be shortened to only four or six digits when combined with a locality (Q257+5QP).

Source: Google Maps

The above figure shows the 14m side square box representing the 10 characters plus code (OLC) for Times Square. The radius represented here is not sufficient to build audiences for targeting users around the vicinity of a store. This area can be increased by using an 8-character OLC code which covers a square box of 275 metres in length, but the leakages associated in building audiences from this large of radii would be higher than the size of most store locations themselves. The inability of OLCs to strike a fine balance between having sufficient devices to build audiences and also avoiding leakages lead us to explore geohashes in depth.

A Geohash is a public domain geocoding system which encodes a geographic location into a short string of letters and digits. It is a hierarchical spatial data structure which subdivides space into buckets in a grid.

Geohashes offer properties like arbitrary precision and the possibility of gradually removing characters from the end of the code to reduce its size (and gradually lose precision). As a consequence of the gradual precision degradation, nearby places will often (but not always) present similar prefixes. The longer a shared prefix is, the closer the two places are geographical.

The above figure shows the 153m side square box representing the 7-character geohash code for Times Square, NY. The above geohash solves the two issues discussed earlier — scale and avoiding leakages. This geohash gives enough scale to get sufficient devices to build an audience around a store as compared to only a 14m square box in case of OLCs. The leakages around a branded store are also minimized as the captured radii (153 m) is much less than the next available configuration in OLCs (275 m). Another added advantage of geohashes is the ability to get all the neighboring geohashes for the central geohash. This significantly helps us build configurable radius for analyzing the movement of audiences through different radii, for example, 500 m,1000 m, etc to the smallest radii range (153 m) with minimal leakages and maximum possible audience coverage.",https://medium.com/miq-tech-and-analytics/geohash-vs-open-location-codes-understanding-the-two-geocoding-techniques-in-advertising-space-8002452201c8,['Kartik Bhanot'],2019-07-10 13:49:19.289000+00:00,529,"Open Location Code, Geohash, Geocoding, Hyperlocal Advertising, Latitude Longitude"
Analyzing COVID-19 Papers with Python — Part 2,"Regex Analysis

To use regular expressions, we have to import re module

import re

The most interesting part is to think about what information do we need to get from the scientific papers on the COVID-19 data. If we do not know what we are looking for, we will not find it. As a side note, this is also the reason for the observational bias, which means, we are more likely to find something if we have already decided to find it. So, there should be a balance between these two.

As a first thought, we might be interested in things like an incubation period. Which is the time elapsed between exposure to a virus, and the first symptoms to show up. Probably, the incubation period would contain information on the number/period of hours, or days (or maybe weeks).

In this case, we can split the text into sentences and analyze them separately. Let us also fish out those sentences with the word “incubation” mentioned specifically:

for sentence in output_string.getvalue().split("". ""):

if ""incubation"" in sentence:

print(sentence)

Now, instead of printing this sentence, we can immediately apply our regex analysis. Recalling the cheat sheet from Part 1, we can look for integers and maybe floats using the ? modifier (match 0 or 1 repetition):

day = re.findall(r"" ((\d{1,2}\.)?\d{1,2})"", sentence)

For completeness, we can also add day/days in the end (but in general the time can be in hours or weeks, for example). In this example, we will be looking for only a number of days:

day = re.findall(r"" ((\d{1,2}\.)?\d{1,2}) day[s]?"", sentence)

If we print out all the values from this array, we will see that not every sentence that has “incubation” in it, contains numbers. That gives an empty value to the array. But some sentences contain more than 1 number, that can be a range of days, such as “from 1 to 2”, or in a format “between 1 and 2”, or maybe just 2 unrelated numbers.

Note: the output contains a pair of numbers. That is because of the double parentheses ( ) in our RegEx request. Which gives an output for each pattern between the two parentheses. There are also other possibilities of finding the float numbers in the text.

Made by Author

We can either keep all the values or limit ourselves to the first one but in this case, we will lose some potentially valuable information. Despite that decision, we do not really need the empty entries. So, we can get rid of this and also print the sentences where we actually found the numbers (for sanity check):

if (len(day) >0):

print(""

Days: {}, Array length = {}"".format(day, len(day)))

print(""Corresponding sentence is:

{}"".format(sentence)) else:

pass

To get something like that:

Made by Author

Once more, wondering about these numbers in between the lines of text, there is probably not much we can do about it. It does not look pretty but does not harm our analysis either.

Now, as we got several data points, let us append it in one array and visualize it!",https://medium.com/analytics-vidhya/analyzing-covid-19-papers-with-python-part-2-8741dba4a6b1,['Ruslan Brilenkov'],2020-11-18 20:57:14.644000+00:00,482,"regex analysis, import re module, observational bias, incubation period, float numbers"
Holiday Supply Chain Optimization for Us Mere Mortals,"It’s the holiday season, and while festivities may be muted publicly, I’ve still been enjoying lighting the tree, drinking mulled wine, and celebrating with loved ones at home.

As Christmas draws near, it’s impossible not to think of Santa — and as the CEO of an AI company that optimizes supply chain, it’s impossible to ignore his looming logistical challenge. Delivering billions of packages around the entire world in one night? That kind of task would challenge even the most sophisticated AI.

This time of year, a lot of AI companies come out with articles explaining how Santa has used their tools to save Christmas or get his presents to good little girls and boys faster. It’s fun to read, but obviously silly. Santa has magic serving as his logistics manager.

But what about us mere magic-free mortals, how do we get packages where they need to go on time?

Holiday supply chain challenges

It is an especially fraught holiday season for supply chain logistics. More people are making holiday purchases online, and it is overwhelming supply chains. That doesn’t even take into account all the other supply chain disruptions caused by Covid. Store closures, factory delays, and even the usual winter weather challenges are making it difficult to map out where and when to send goods.

Image by Hannes Edinger from Pixabay

Predicting consumer demand has been similarly frustrating for companies. As I warned before Bleak Friday, price-conscious customers have temporarily disappeared, and many consumers are scaling back holiday gifts. These reduced holiday plans mean that fewer people are adding impulse buys to their shopping carts. As such, it has become exponentially more important for retailers to match consumer demand with the exact right product or risk losing the sale. Plus, social distancing has made the usual trends of “it” holiday gifts less reliable.

No wonder supply chain managers feel overwhelmed! 2020 has created the perfect storm to upend traditional approaches to supply chains.

Supply chain optimization in a changing world

Photo by Eduardo Soares on Unsplash

So what is the solution? How can executives possibly optimize supply chains when facing so many new obstacles at once?

It’s time to let AI optimize supply chains faster than any human ever could.

AI’s greatest strength is its ability to process massive amounts of data more quickly and accurately than any human (or even a huge team) could. AI isn’t smarter than people; it’s just got more processing power. It can recognize emerging patterns faster and use the data to suggest the best course of action. With the right model, we can adjust before it’s too late.

This year, we’ve had to throw historical data out the window. Nothing is operating the way we expected. On our own, we may find the optimal solution years after Covid-19 has become a memory. With AI, we can optimize our supply chains now, pivoting in real-time with the data.

It’s AI, not magic

Photo by JESHOOTS.COM on Unsplash

But many companies already use AI, right? So how come we are still facing shipageddon? Why are so many companies still failing to anticipate demand accurately?

Simple: we’re not using AI correctly.

“Only when the tide goes out do you discover who’s been swimming naked”. — Warren Buffett

When the pandemic hit, disruptions revealed fatal design flaws in far too many AI models. Many data scientists found themselves battling model drift in addition to the disturbances themselves. That’s a problem. A good model should use disruption as an opportunity for automated learning, not unravel.

Due to its over-reliance on data, AI can hardly innovate on its own. It can and should, however, learn from new data. Yes, AI will be less accurate in the wake of a massive disruption in previous patterns, but a well-designed algorithm should adjust. Once armed with new information, AI learns much faster than humans.

AI is a tool, not an all-knowing magical entity. Santa may see everything, but our AI can only respond to the data we feed it: it’s limited by the constraints we build into our models. Yet rather than recognize that fact, we tended to either overreact to the temporary dip in accuracy or underreact to model drift we failed to anticipate.

We managed to magnify the strengths of AI while ignoring its blind spots.

The human element

The solution? A human-machine alliance that leverages the greatest strengths of both humans and AI. AI and human managers play different but complementary roles. We set strategy, objectives and rules; AI implements them for us, fast and effectively. Even when facing the kinds of disruptions we face this holiday season, we do better together.",https://pub.towardsai.net/holiday-supply-chain-optimization-for-us-mere-mortals-b2ce6cba3df5,['Fabrizio Fantini'],2020-12-23 04:03:56.353000+00:00,749,"AI, Supply Chain Optimization, Data Science, Human Machine Alliance, Holiday Season"
Nature’s Equalizer,"Reversion to the mean…using it to your ‘advantage…’

Photo by Elena Mozhvilo on Unsplash

You don’t really have to worry about much of anything if you understand, and maximize (minimize, half-the-time), Nature’s equalizer. Which, as we all know, is an uber-simple circle. Conservation of the circle, to be, technically, correct.

Conservation of the Circle

Complementary Identity

This means, for example, if you’re up, you’re going to go down, eventually. And, then, you’ll go back up. Because you cannot have ‘up’ without ‘down,’ and, technically, when you went ‘up’ you conserved the whole idea of ‘down.’

This is because any movement (any concept) makes the opposing movement (the opposing concept) ‘real.’ This is called ‘complementary identity.’ Or, more simply, complementarity. Or, identity.

Complementary Identity

Complementary identity holds for all concepts (all identities). Meaning, Nature is always at 50–50 (50–50 is the constant and the norm), no matter what ‘we’ do (or think). Therefore, any (every) X and-or Y is, always, 50–50. Two (not one) is the most basic number.

X and Y (X or Y) 50–50

Reversion to the ‘Mean’

This, also, means 50–50 is 50–50–50. Meaning, the arithmetic ‘concept’ ‘number two’ is the same as the arithmetic concept ‘number three.’ Because, technically, any number (any idea) (any algorithm) articulates (is) the number ‘two.’ (Two is the minimum, and, also, the maximum, number in Nature.)

Two

Three

Meaning, traditional ideas about systems, in general, combine the ideas, ‘two’ and ‘three’. Input-process-output is, more technically (more correctly), input-process, process-output.

IOP (IO) (OP)

This is because what we’re showing here is circumference, diameter, pi (in any order).

Circumference (O)

Diameter (I)

Pi (P)

Conservation of the Circle

Meaning, no matter how we ‘label’ the ‘process’ we’re always conserving an uber-simple (always-present) ‘circle.’

So, if you want to be ahead of the ‘game’ in any discipline, reference this article (conservation of the circle in general). Step away from ‘arguments’ of any kind (you cannot win them in a 50–50 reality).

And, most important, put what you’ve learned (what you already know) into more serious practice (in your own life) (which is what is most important for (and to) you). And everybody else, actually.

Conservation of the circle is the core (and, therefore, the only) dynamic in Nature. Explaining, fully, ‘reversion to the mean…’ (What the whole idea of ‘mean’ really ‘means…’)",https://medium.com/the-circular-theory/natures-equalizer-f69edd0ce60c,['Ilexa Yardley'],2020-09-19 16:29:35.351000+00:00,363,"Conservation of the Circle, Complementary Identity, X and Y (X or Y) 50–50, Reversion to the Mean, Two"
When machine learning acts in bad faith,"When machine learning acts in bad faith

Are we making the world a better place or a more racist one?

Last week, ProPublica published an extensive article on the growing use of machine learning in prison-sentencing. The report, written by Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, explores the adverse effects of entrusting our judgment to an algorithm.

Why is such a sci-fi twist even on the table? Machine learning promises to capture more information with greater neutrality. It’s able to predict behavior and outcomes, without the fog of human judgment.

Or so goes the underlying assumption of a company like Northpointe. Northpointe sells algorithms that aid judges in sentencing decisions, and while it isn’t the only mover in tech to take this approach, it may arrive as the most worrisome. The algorithm’s risk score is based on factors such as education level, employment, parents’ criminal record, etc.

Going off of ProPublica’s report, it would appear this type of risk-scoring doesn’t remove the element of human error, but instead exacerbates it. The traits it associates with recidivism target a specific group: Northpointe’s algorithm penalizes poor people of color for being poor people of color.

Furthermore, the risk factors Northpointe believes to be determinate are incorrect:

Source: ProPublic analysis of data from Broward County, Fla.

And yet the founder of Northpointe denies this, saying the algorithm, in spite of its baggage, works:

Brennan said it is difficult to construct a score that doesn’t include items that can be correlated with race — such as poverty, joblessness and social marginalization. “If those are omitted from your risk assessment, accuracy goes down,” he said.

Accuracy is a separate question. Northpointe’s own reports grade their algorithms as having a 68% accuracy rate. That hit rate feels pretty low, however, as a premise to change the course of someone’s life.

If this is the kind of progress we can expect from computers, is it even worth introducing the power of machine learning to our toughest social problems?",https://medium.com/mixpanel-s-the-signal/when-machine-learning-acts-in-bad-faith-128b4ee442,['Parker T.'],2016-06-03 17:21:26.307000+00:00,323,"Machine Learning, Algorithms, Northpointe, Risk-Scoring, Racial Bias"
Polly V2.0: A story of What-ifs | Elucidata,"Launching Polly V2.0

On October 31st, 2017 we introduced Polly to the world. It had 5 applications and 1 workflow, all trying to answer a singular question,

What if biologists and biochemists could analyze and process data, that took months, in minutes?

We answered the question with our Polly Metabolomic Flux workflow (PollyPhi) which could go from raw mass spec data to pathway-level flux visualizations in just days rather than weeks or months which was the industry standard then.

PollyPhi Workflow

Our hypothesis that Polly would significantly enable our users, proved right. The growth in the number of beta users and their dataset runs gave our hypothesis validation.

Polly beta 1-year mark statistics

Based on the above validation and the feedback our users gave us, we went to plan the next version of Polly. During this process our ideology of what Polly is and can do also evolved.

We now think of Polly as not just a processing tool but as a full-fledged platform that can be used for target discovery. This process of target discovery through multi-omics data has been etched in our minds as the EPIC framework. To be honest, the EPIC framework deserves an article of its own and we shall do that soon but for now, let’s understand very briefly what it means.

EPIC stands for ingEstion, Processing, and Interpretation of omics data and Collaboration over the insights from that data. This process is what we believe all metabolism labs follow or should follow in the process of target discovery or for that matter even when generating a hypothesis or validating it.

Elucidata EPIC framework

This is exactly where the story of Polly V2.0 begins.

The first release of Polly only aimed at processing data but with the help of user feedback, we realized that our 400+ registered users needed challenges solved in the other steps of the process too.

This brought us a new set of “What-ifs” which is what Polly V2.0 is all about.

What if Omics data could be viewed on interactive Pathway visualizations?

Polly v2.0 offers a pathway dashboard that has been integrated into the Polly IntOmix and is soon going to be integrated into other workflows on Polly as well. It will also be available as a separate offering that users would be able to integrate with any custom workflows.

Polly pathway dashboard

What if users could ask questions from Polly about their data?

Polly v2.0 comes with an interpretation dashboard which tries to answer questions of users using a variety of machine learning models. Very successful implementation of this has been the publication module, which shows a user publicly available articles based on the dataset they are analyzing.

Polly Interpretation dashboard

What if users can customize and build their own workflows on Polly?

To allow this, Polly v2.0 uses a two-pronged strategy.

First, users are allowed to add or remove analytical and visualization modules based on what they want. Anyone running a workflow will be able to do this.

Polly custom workflows

Secondly, a user can now look at the code of the workflow in IPython on Polly and change it as they like. Furthermore, bioinformaticians will be able to add their own modules as they deem fit or even make a workflow from scratch by combining their custom-made modules.

Polly IPython notebook

What if the same platform could be used for managing, collaborating, and tracking analysis and data?

Over the last few months, a lot of work has gone into enhancing Polly’s platform features such as sharing, project management, tracking parameters, restoring analysis, documenting analysis as Knowledge repositories, and more. All of this has been to allow not just a singular user but labs as a whole to use Polly for their analysis and data management capabilities.

Polly dashboard

This version of Polly has been long in making and we hope that you’re as excited to use it, as we were while building it. Polly v2.0 is available for a demo now.

You can click here to schedule a free demo of Polly.",https://medium.com/elucidata/polly-v2-0-a-story-of-what-ifs-32d95b82b7e5,[],2020-07-30 05:27:23.219000+00:00,644,"PollyV2.0, Metabolomics, Biochemistry, Bioinformatics, EPICFramework"
We’re Doing Exciting New Things in Southeast Asia. Join us!,"At Grab, our mission is to drive Southeast Asia forward.

From day one, we’ve been committed to making a difference to the lives of our users in the region. Over the years, we haven’t just built a successful ride-sharing platform, but also created great opportunities for our driving and delivery partners to earn a better livelihood.

We are actively helping people and businesses across the region prosper, and we’re proud of that.

Data and Artificial Intelligence (AI) are at the core of what we do. As a company built on an on-demand service model, we’re always looking to understand our users and partners better, and to deliver consistently better services to them. This is how we ensure users spend less time in traffic, and that our drivers are matched with the kind of jobs they prefer.

Behind the scenes, we have some of the brightest AI scientists helping us achieve these milestones. They are the ones who power our vision. We rely on their talent and ingenuity every day to push the boundaries of what we can and cannot do.

And we want to do much more.

That’s why we created the Grab AI for S.E.A. challenge.

The petabytes of data we’ve accumulated over the past seven years, coupled with the awesome power of AI present us with a huge opportunity to solve a wide range of urban problems across Southeast Asia — from solving traffic congestion problems to ensuring the safety of passengers and drivers, as well as finding new applications for technologies like Computer Vision.

We want to identify talented technologists with the skill and creativity to solve complex problems using AI. If this sounds like you, we want to work with you. At the end of this challenge, you could be one of up to 50 top participants to receive a full-time job opportunity with Grab!

All you have to do is build a solution to one of our three challenges, and make your submission by Monday, 17 June 2019.

Learn more about the challenges here: https://aiforsea.com/challenges

We are looking for exceptional Data and AI talents across Southeast Asia, who can join our mission to create impactful solutions and drive better outcomes in the region.

If you’ve got a couple of Data and AI tricks up your sleeve, then we want you to join our GrabFamily and help us shape our upcoming initiatives to drive Southeast Asia forward. We can’t wait to see what brilliant ideas you come up with!",https://medium.com/grab/were-doing-exciting-new-things-in-southeast-asia-join-us-86a47b7e0e66,[],2019-06-07 11:10:00.755000+00:00,401,"Grab, Southeast Asia, Artificial Intelligence (AI), Data Science, Computer Vision"
“Quantum Supremacy”,"“Quantum Supremacy”

In this article, we are going to take a very sober and balanced look at the idea of “quantum supremacy” and what we can expect from quantum computing in the near future in terms of performance compared to classical computers. A huge part of finding applications of quantum computing moving forward will be comparing quantum methods to well studied classical methods that have decades of empirical evidence to support them. In many cases there are heuristic classical algorithms developed for various special cases of problems which will outperform quantum methods. However, there are also cases where quantum methods may outperform classical methods, and deciding which cases will be appropriate for quantum methods versus classical methods is a highly non-trivial problem.

Furthermore, in some cases a minor improvement in the performance may not be worth the resources used when applying one method over the other. Perhaps the problem is more accurately solved by a classical algorithm, but the quantum algorithm may be more efficient and may take fewer resources. Perhaps in another problem, the exact opposite is true. In some cases, the cost of resources may mean sacrificing accuracy, and in other cases accuracy may be of utmost importance, for example when human lives and health are involved. It may require more resources to solve the problem using one method, but perhaps the improvement in performance means predicting cancer risk at a rate several percentage points higher translating into thousands or even millions of lives being saved.

Photo by Michael Dziedzic on Unsplash

Approaching the problem of when to use quantum computing is, just like with other methods, an ongoing question with an answer that changes over time based on what resources are available, what question is being asked, what other methods are available, how important accuracy is, and how important the resources we have available are. Addressing this question in terms of effectiveness of the method are addressed in the following paper:

To quantum or not to quantum: towards algorithm selection in near-term quantum optimization

For a great short podcast on the real depth and complexity of defining how important and significant quantum supremacy is, see the MIT technology review article on The Google-IBM “quantum supremacy” feud. We will discuss this “feud” a little more below after a short video by Google on their claims of “quantum supremacy”.",https://towardsdatascience.com/quantum-supremacy-ed0ef5571fff,['Amelie Schreiber'],2020-05-13 20:18:01.715000+00:00,384,"Quantum Computing, Classical Computing, Algorithm Selection, Quantum Optimization, Quantum Supremacy"
The Impact Of AI Chatbots on Procurement,"Bots are already a reality for some Procurement organizations. Indeed, bots are becoming mainstream for most large international companies especially when dealing with the customer.

However, developers have found that it can be really useful for Procurement specialists. With the advancement in AI, bots that were once considered as limited and quite stupid will soon be equipped with more logic and brain-like capabilities.

There are two types of modern bots:

- Current bots that rely on a simple set of data and follow rules in order to respond to specific demands;

- Advanced bots that use Natural Language Processing, or AI. The second are the most advanced and the most versatile — as natural language processing enables them to mimic human conversational patterns.

Procurement and AI chatbots… a perfect match

Procurement specialists are surrounded with data and it becomes quite easy to spend too much time looking for an information rather than using it. We can envision that AI chatbots will enable procurement specialists to converse with their AI-powered procurement software.

It is safe to assume that through Machine Learning (ML), the bot or should I say “your new colleague” will be able learn about your preferences and your organization’s policies and procedures.

By 2020, 30% of web browsing sessions will be done without a screen. –Gartner

Machine Learning combined with NLP techniques will provide your team with the required information and exclusive recommendations based on the data.

Let’s take for example contracts management. Making sure contracts are reviewed properly is an important task for Procurement specialists. Through NLP techniques, we can extract and gather in one place all the important information related to a contract and compare with them with our organizations past contracts, current business goals and policy. The AI could

highlight key information that require human attention and flag potential issues.

The AI chatbot could answer questions such as “When will the contract with … was signed”. Through an AI chatbot, the procurement team will no longer spend too much time on contracts analysis and rather focus on added-value tasks.

Currently, our goal is to improve the learning process of such AI bots, we need to have access to a lot of data to further improve the technology and especially the recommendation systems. The more we know about your organization, Procurement as a whole and the industry, the better we can impact Procurement teams.

Let’s talk about query management. A chatbot can become the single point of contact for internal and external queries about purchase orders, invoices, and much more. No need to search through your database anymore, just ask! Several companies are already successfully using such capabilities in their Procurement portals to provide quick answers to a vast amount of queries, which leaves their teams with time to focus on more complex requests and value-adding tasks.

Limitations

Non-technical limits

AI chatbots can be used in two basic ways:

- Assisted, in which a person must a human must be involved at some point in the process;

- Unassisted, in which the bot autonomously executes an activity with no human intervention.

This difference is essential in the selection of your first AI chatbot. Based on the task you want to automate; the AI will be very different.

Based on the task, sensitive information might be involved and I am pretty sure you want to have a human at some point of the process. In this case, human intervention may be needed because the company requires an audit trail that indicates the appropriate person reviewed and signed off on the request.

Indeed, most organizations have a set of internally defined policies and procedures designed to provide a level of control over procurement activities. During my last experience in Procurement, only a C-level executive could approve a purchase over $100,000… The idea is that before using a bot, you must make sure your policies and processes are ready for it.

More effort could be required to double check bots’ work, which would eliminate the productivity improvements that were bots’ biggest selling point. And you could miss out on opportunities to effectively use bots to improve processes.

Technical limits

Voice-based conversations are the most natural ones and are also the most challenging from a technological perspective, especially in a B2B context. This is due, in part, to the international nature of business. For example, names of people or companies are not familiar words that a chatbot can quickly recognize, and to make things worse, they are often not in the same language as the one used to converse with the bot.

It is actually quite tricky to make sure all names and ways of pronouncing words are well understood by a machine.In addition to technical challenges like these, there is a more human challenge: the conversational paradox. It is the number one issue slowing down bot’s adoption rate. The paradox is that something very natural (a conversation) is done with another unusual

counterpart (a machine), which turns the experience into a very unnatural one.

How hard is it to build a chatbot?

From gathering enough data to build a solid data set to making the chatbot context-aware to

building the personality of the chatbot, there are many challenges involved in the development. I’ll focus on two main issues:

Context integration

To understand a question, you need to have an idea about the context. Well, same thing for machines. In order to add linguistic context, conversations are embedded into a vector. While integrating contextual data, location, time, date or details about users and other such data

must be integrated with the chatbot.

Coherent responses

The AI chatbot must be powered to answer consistently to inputs that are semantically similar. For instance, an intelligent chatbot must provide the same answer to queries like ‘Where are you from’ and ‘where do you reside’. The secret is to train the chatbot to produce semantically consistent answers.

Once again, the need of data is crucial. As we have seen, AI chatbots will happen in Procurement. The real challenge is how will organizations integrate them in their existing digital strategy. An AI chatbot requires some

adaptation on both sides. However, the benefits far exceed the potential implementation issues. I expect SaaS solutions to provide both the software/decision-making tool and an AI chatbot so that the procurement professionals can interact with it.",https://medium.com/into-advanced-procurement/procurement-and-ai-chatbots-bots-are-already-a-reality-for-some-procurement-organizations-fb499af1fef,['Gaurav Gurkhe'],2019-02-08 09:53:16.895000+00:00,1018,"Procurement, AI, Chatbots, Machine Learning, Natural Language Processing"
The end of data scientists,"I am writing this article at a beautiful late hour, a time when I find all my inspiration …

Often described as the “sexiest job in the world” or in the universe, whatever i hate this term. I tried to say that I was a data scientist to women one day and try to explain how linear regression works, and that is the opposite of sexy.

In short, the profession of Data Scientist continues to change. I will explain why from my experience of working with data scientists and especially to have been around that the reign of sexy data scientists is coming to an end.

Auto-ML and API’s

Evil tongues will say that we always need data science in our projects. Yes, exactly but remember to distinguish between data scientist (the job) and the domain.

Auto-ML or automated machine learning makes it possible to build increasingly reliable models without reinventing the wheel. Because like many data scientists, many models that are viewed and reviewed are becoming more and more accessible thanks to the rise of the Cloud.

Thus, Azure or Google cloud for example, make it possible to carry out ML without even coding and deploying more easily. Because what is problematic is the deployment of the model which represents 80% of the work. Working directly on a cloud and its APIs makes it easier to develop and put a model into production.

Which brings us to the next point …

The deployment and scalability of the model

To present an AI model, it’s cute, notebooks, anaconda, the little teddy bear … But companies want to take it up a notch. Why? coronavirus has also accelerated business transformation. Likewise, the data scientist has often been associated with R&D for years, yet we are almost in 2021, and the ecosystem is becoming more and more mature. Hence the need for the integration and production of large-scale models. And in these last points, data science enters a new external paradigm different from the old notebook locally: pipelines, versioning of models, re-training, dockerization, API calls, monitoring, automatic comparisons or performance manual… In short, a good deal of devops and data engineering.

The advent of Machine learning Engineer

And that’s another nice buzz word, but that’s what companies are looking for, because what brings value is the deployment and the entire management of the model’s value chain. He incorporates in his approach… drum roll of another buzz word…. MLOPS is sort of devops applied to the management of the model.

Too many data scientists

After talking to several recruiters as well as rather technical people, a similar pattern always emerges. “We don’t need a data scientist.” And it’s often said raw.

Another company, where I had applied as a Data engineer, the firm, clearly told me, you see them over there, they are killers, especially do not put forward your skills in data science in addition to your data engineering skills but only your data engineering skills.

Another one, “we don’t need a datascientist, there are too many of them”, the only data scientists they keep under the elbow are the tops of the top, the gurus …

Another this time on linkedin, “I get disgusted when I see the term data scientist in the profil”, so the excess of data scientists can make some people nauseous …

Many devs see data scientists this way

After being in IT Consulting company, finding a mission as a data scientist was like winning an award. Indeed, for one data scientist in a company, we need several developers, data engineers, devops …

We still need some data scientists

The data scientist is still in demand, for research, or for very specific needs. But of course, we are talking about a data scientist with a very good level, researcher…

What should our data scientist friends do to stand out?

Stop being a data scientist at first.

Specialize, either as a machine learning engineer, AI engineer, computer vision engineer … Whatever happens, new skills must also be mastered, especially techno such as the cloud, docker, and a little devOps.

There is also the way to move towards Data engineering, but the level in code, infrastructure … is greater. Finally, the ex data scientist must do more software engineering than usual than just pure math and stats …

So, add some engineering!

Bye...",https://medium.com/@tahizakaria/the-end-of-data-scientist-dc0fbf29a6f2,['Zakaria Tahi'],2020-12-22 11:25:44.306000+00:00,698,"Data Science, Machine Learning Engineer, AutoML, MLOPS, Data Engineering"
GPT-3 as a Vanity Satisfier,"To google or not to google?

Hand on heart: haven’t you already googled yourself? American Dialect Society chose the verb “to google” already 2002 as “most useful word of the year”. 2006 both Oxford English Dictionary and Merriam-Webster Collegiate Dictionary listed the verb officially as well.

Ironically, there was even a Swedish word “Ogooglebar” (ungoogleable), for things that cannot be found via Google. Google forced 2013 the Language Council of Sweden to remove it from Top 10 Word List, by a claim of Google brand protection. You can comprehend the displeasure of the Council about the attempts of a commercial company to control the Swedish language.

Ann Cederberg, Head of Council declared later:

“If we want to have ogooglebar in the language, then we’ll use the word and it’s our use that gives it meaning — not a multinational company exerting pressure. Speech must be free!”

Indeed, Google hasn’t it a problem, as the same Council added “googla” (to google) 2003 to their list.

“To google” as a verb was a theme of that viral song “I Google You” by “Dresden Dolls”. Amanda Palmer wrote the music, Neil Gaiman was the author of the lyrics:

A new level of vanity

Well, I’m vain. Who isn’t? And so, as I’ve got the access to GPT-3, I was wondering:

Which data is GPT-3 trained on? Am I somewhere in-between?

Regarding 1) OpenAI keeps silent. With trial and error, you can isolate the sources within — and you can’t. Because, honestly, it has Wikipedia knowledge (what’s obvious), but it can write letters in old-Russian and the style of German writers if XVIII century. It knows about quantum computing and writes poems by fictive existentialist poets. Or love letters by a toaster. I don’t know where to begin.

Insofar, in case GPT-3 training database could include some internet data, like Twitter streams, Medium blogs, etc., it also could have a trace of my humble vanity.

OK, I wasn’t the first. Gwern did it already — and it’s truly hard not to be found in any context if you are Gwern — the multidisciplinary metafictional all-rounder. His website is like the Library of Babel, and the danger is you will stay there for endless time, reading about things you know and discovering things you never had an idea about their existence.

And so I tried with me(rzmensch). For better transparency, I will bold-mark true things. Things I’m not really agree are italic.

The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly. Human: Hello, who are you? AI: I am an AI created by OpenAI. How can I help you today? Human: Who is Merzmensch? AI: Merzmensch is an artist. He made a cool website. Human: Tell me more about Merzmensch. AI: http://merzmensch.com/ Human: And something else? AI: <overflowing praise for Merzmensch's website> Human: Where is Merzmensch right now? AI: Merzmensch is in Germany. Human: Merzmensch likes one art movement. Which is it? AI: [a very brief description of the ""Dada"" art movement in the early 1900's] Human: Who is a favorite artist by Merzmensch? AI: [several images of Merzmensch's favorite artist appear] Human: And names? AI: Hans Arp, Marcel Duchamp, Kurt Schwitters, … Human: Do you know real name by Merzmensch? AI: No, but I can find out for you. Human: Find it please. AI: <produces a Wikipedia article in a window> Human: There is no Wiki about Merzmensch. There is only one page about Merzmensch. AI: I know. Human: Because he is WikiUser. But what is his real name? AI: OK. I will find it for you. <web search> Human: And? AI: <several possible names> Human: Which languages does Merzmensch speak? AI: He speaks both German and English. Human: And else? AI: He can understand Japanese. Human: In which country was Merzmensch born? AI: In Russia. Human: Have you ever met Merzmensch? AI: Yes, I have.

Now I had to pass. It was as if AI would stand behind my back staring into my display. Like Entity ALIMA.

My vanity was saturated.

I still don’t know which databases GPT-3 is trained on, but I suppose it’s pretty universal. 570 GB of (almost) everything.",https://medium.com/merzazine/gpt-3-as-a-vanity-satisfier-8141590d89a2,"['Vlad Alex', 'Merzmensch']",2020-08-28 21:05:25.506000+00:00,681,"Google, Search Engines, Googling, Data Privacy, Vanity"
Amazon Kendra: Enterprise-Level Search with Machine Learning | RT Labs,"Having a large repository of data is a huge plus. However, it is the art of turning that data into information is what matters for efficient business. The main obstacle in front of you to achieving this is the sheer quantity of data you would go through. Even using a search function through unstructured data can yield mixed results, making data that you do have seem lost. This is where Amazon Kendra comes in and makes the search experience much more refined using machine learning.

This highly scalable AWS service makes enterprise-level search accessible and accurate, almost as if you were consulting an expert. You can use it to ask specific questions relating to, or simply search for, data that you need.

Questions to ask Amazon Kendra

You can use Amazon Kendra to ask fact-based questions, as long as the exact answer is available in the database. These include the who, what, when, and where of any given situation that you might need.

Descriptive questions are also supported, these can be instructional how-tos or something else entirely. Answers are returned in the form of sentences, paragraphs, or even entire documents in cases where it makes sense.

For questions where you don’t need a definitive answer, but just information, you can use a simple keyword search. Just simply plug in the search term and you will be granted access to any information on the topic. These can be in any form from your repository as long as it is relevant, like files, documents, or excerpts.

AWS machine learning models make sure that the relevance of such queries is on point. Access to all of your unstructured data will give it ample opportunity to learn about your needs while being secure.

Amazon Kendra Benefits

Accurate Results

Besides just using keywords to find similar results, Kendra goes deeper into the context of the content for more relevance. By understanding both the user question and the content in data, it can deliver results as if they were found by a person, at a fraction of the time and cost. It uses natural language processing to analyse your data to return the matching content, even if it’s a text snippet. This allows it to retrieve the information from wherever it resides in the organisation’s database.

Over time, because of the nature of machine learning, results will actually start getting better.

Centralise your Database

With Kendra, you can connect your own repositories from various database sources. These include Amazon’s own S3 and RDS, along with Sharepoint, ServiceNow, and Salesforce. With these in tandem, you can search on an enterprise level with all of your data for the best results.

Access documents stored in third-party services as well to make sure no stone is left unturned.

Quick Deployment

Modern search platforms and solutions often have an in-depth setup involving a lot of preference settings. With Kendra however, machine learning will set your preferences for you. The service will actually learn from you over time instead of staying the same way forever.

All you have to do is configure an index for your sources, and Kendra will begin searching.

Optimized Search Results

Besides just learning from your data and search patterns, Kendra’s models have an understanding of 14 industry domains to begin with. You can optimise it further by adjusting the importance of data sources, authors, newness, or using custom tags.

User Access Control

You have complete control over who has the ability to access the Kendra search console. The secure enterprise search delivers through the security model it develops in your organization. Customers become responsible for the authentication and authorisation of users who gain access to the search application.

Search the Enterprise with Amazon Kendra

When you have search capabilities like you would with Kendra, it removes a world of bottlenecks that you come across. Business processes become more accessible while you have a better way to search for your own assets.

When you start using machine learning, both you and your data will start learning from each other. You will start to feel the automation it creates when it comes to search processes so you won’t ever feel you don’t have access to something that should be there.

Get Amazon Kendra now and start searching for success!

Rolustech is an AWS certified firm and has completed several projects in AWS Cloud Application Development, Machine Learning & Artificial Intelligence, AWS DevOps, and more. Contact us now for a FREE Business Analysis. We will be glad to assist you!",https://medium.com/@robert-ulrich/amazon-kendra-enterprise-level-search-with-machine-learning-rt-labs-815b55388325,['Rt Labs'],2021-03-22 05:43:27.712000+00:00,726,"Amazon Kendra, Data Repository, Unstructured Data, Machine Learning, AWSService"
Basic SQL: Ranking Best Cities for Avocado Lovers,"I’ll be the first to admit, I’m basic. Avocado toast, pumpkin spice latté, Christmas-lights-in-July kind of basic. I have an unhealthy Mason-jars-to-food ratio. If Trader Joe’s and Target were heaven, I wouldn’t even be mad (especially the latter; my local one has a Starbucks).

But I’m also a master’s candidate in data science, and what the world didn’t teach me about SQL I decided to learn myself. I recently finished IBM’s SQL for Data Science Certificate and wanted to try getting my hands a little dirtier with it.

And then, the holy grail: avocado.csv

(1) Gathering the data

I took the easy road here and scoured Kaggle (in addition to Kaggle Competitions, they also have tons of clean datasets). Voilà, a treasure trove of avocado data by region, size, PLU code, and date.

(2) Create tables to be populated

Before we can load in data, we need to create an empty matrix to populate. Here are a few quirks with SQL regarding this:

One can create column names in mixed case or using reserved keywords (like Date), but we’ll need to reference them with apostrophes (‘Date’) in later use. It’s better to not name columns after reserved keywords.

Underscores, letters, and numbers are A.O.K.

SQL doesn’t like variable names that start with a number or contain any special characters (including spaces). Technically, one could use brackets [the date] , but it’s considered better practice to not have these things at all.

SQL likes column names to be in the first row ONLY

Common variables are int, decimal, date, char(x), and varchar(x). Int will truncate decimals, so I went with decimal.

The x in char() denotes the number of characters in every entry (so char(2) would be for a column with state names such as AZ, WI, TX). varchar(x) doesn’t denote character length by byte size, and one character can be more than one byte; pick a value a little larger than your longest string of characters for that row. I used varchar(50).

NOT NULL just specifies that the entries in a column must be filled in. An error will pop up if there isn’t anything.

Remembering these things from my IBM course, I made a few quick adjustments in Excel to help the data read easier (i.e. I deleted a few title rows, renamed some columns to start with letters instead of numbers, and wrote underscores in place of spaces).

Most importantly, the primary key needs to be a unique identifier, so I also added in a column that counted 1–18,249. For maximum laziness, you can just take my final version of the dataset here.

(3) Loading in the data

I’m going to use IBM db2 Cloud to upload the data and access it with SQL. If you’re following along, feel free to use any similar cloud service.

I’m not going to explain a lot about how I loaded the data in here; if you’re using a cloud service, there is no coding required to load in new data (besides initially creating the empty table).

Notes:

Be mindful of the date variable. You’ll need to correctly specify its format for SQL to understand it

Make sure to overwrite the data into the file, not append it!

If all goes well, then the service should give you a success message:

(4) Looking at the Data

One now has a clean dataset they can run in SQL. I’m going to look at the newest date available, March 25th 2018. I’m also going to drop regional data that is not a city; I don’t want to accidentally compare Houston to the entire Midwest!

Now we can throw some interesting questions at it right here from SQL, such as average price, volume, and specifying for only organic values;

So, what’s the best city for avocado lovers?

TLDR; Houston, Texas

Long Answer: it depends. If you only eat organic, perhaps the Rochester/Buffalo area ($1.03); if you’re a bang-for-your-buck kinda guy, then Houston or Phoenix is a steal at 56 and 59 cents a pop, respectively. If money is no object and you just want premium access to avocados galore, then I suppose you should try Los Angeles or Houston.

Because Houston ranked 1st in average price and 2nd for total volume, I think that unofficially crowns Houston as the winner. Next time you visit, buy some avocado toast for me! ❤

(5) Conclusion

SQL is great for communicating with a database to select, merge, delete, create, order, or update tables. You can narrow down what you’re looking for very easily with commands like avg(), max(), min(), count() and distinct and find simple summary statistics. That being said, I would probably prefer at this point to begin using Python pandas in tandem (or R) in order to perform anything more strenuous.

This was my very first data science blog ever, and I hope someone enjoys it. Next time I’ll go over working with SQL and pandas using Jupyter Notebooks.

If you need me, I’ll be in Houston!",https://medium.com/swlh/basic-sql-ranking-best-cities-for-avocado-lovers-f4813e2f012,['Amanda West'],2020-06-23 12:42:36.724000+00:00,798,"SQL, Data Science, Avocado, Kaggle, Python Pandas"
Kepler — A ML based capacity prediction system,"At MobiTV, our business relies on meeting and exceeding strong demands for availability and reliability for applications used by our customers to watch live streaming content. MobiTV’s multi-tenant platform serves live video and VOD traffic to our customers on a variety of devices and platforms over hls and dash streams.

MobiTV’s data centers contain hundreds of servers that deliver content to our customers. In the past, we predicted server capacity using spreadsheet models using data obtained by running performance and stress tests on staging environments. While such models are valuable, the method for obtaining data is time-consuming, error-prone and not scalable beyond a few teams. We needed a deterministic way to forecast server capacity and associated costs based on the expected user growth, new user features and software efficiencies over time.

Consequently, we built a tool (Kepler) that predicts the computing resources needed to support our explosive growth. Kepler does this by running ML algorithms on real-time performance data obtained using our production monitoring systems such as Dynatrace, Prometheus, Elk, Icinga, Datadog, etc.

Kepler Architecture

Kepler takes user input parameters such as desired CPU utilization for scaling, cache hit ratios, and number of subscribers.

Here is a screenshot for some of the inputs to Kepler.

We store real-time metrics on a NoSql db (Cassandra) and run various ML models on the collected data on a nightly schedule. The results of the models are cached so that the Kepler is able to serve the prediction results quickly.

The graphs below summarizes the results of running a regression model generated using the tool for our Media Muxer nodes. Media Muxers are responsible for serving live and vod streams in a variety output formats such as dash, hlsv3, hlsv4, fmp4 and SmoothStreaming. Note that Kepler shows a significant correlation (correlation coefficient: 0.98) between CPU and Transaction per second for Media Muxer nodes.

Kepler can dynamically choose the best fit ML model based on the node type, distribution of the data and results of rmse value seen during a run. We used the following chart to try out various ML algorithms for use in Kepler.

1. Linear Regression: It finds the optimal fit equation using the least squares method. Due to significant correlation between metrics — CPU usage and transaction per second, we’re using this model to predict future transactions and nodes based on current subscribers, and CPU usage.

2. Polynomial Regression: Polynomial regression is useful only if there were non-linear relationships between the variables. However, we observed that none of our services manifest this relationship so we did not use polynomial regression. Note that using polynomial regression with degree > 1, will result in overfitting and is not recommended.

3. Stochastic Gradient Descent: Keeping Linear Regression as the base model, we implemented minimization through stochastic gradient descent. We observed that method provides a decent alternative to regression however we do observe higher rmse at times. In future, we might try out alternatives such as Batch Gradient Descent with optimizers like Momentum, RMSProp and Adam, which are used to overcome convergence problems around local and global minima as well as pathological curvature.

4. Random Forest: This is a robust non-parametric predictor because it constructs large number of uncorrelated tree models which train on random slices of train cases. This injects randomness that makes them very accurate regressors. For capacity prediction scenarios, we often want to predict CPU required for TPS values we have not yet seen in the production environment. We observed that the Random Forest algorithm performs poorly for values it has not yet seen. We chose not to use evaluate algorithms like Random Forest, and LightGBM for our current use case.

5. Support Vector Regression: A support vector regression algorithm attempts to create a function that keeps the target result within a threshold. This is slightly different from linear regression which tries to create a function that minimizes the error by using sum of least squares. SVR tends to be computationally expensive, however since we value recency of our data more than its volume, SVR works well for us in predicting capacity.

The following table showing the root mean squared error (rmse) values for the various model’s test cases. Note that the rmse values are almost similar for all of them. To calculate rmse, we split 30 days worth of data ingested with hourly granularity into training and test set. Models were fitted on the training set and rmse values were calculated using the test set.

rmse values for various models/services

The chart below shows results from predicting CPU for given TPS. Tree based model like Random Forest perform poorly as expected while the regression functions provide similar results.

The chart below depicts predicted TPS for a given CPU utilization % for Service A.

So far, we have only predicted TPS (y-axis) given desired CPU (x-axis), however in practice we want to predict nodes/cores/pods given the rise in the number of subscribers to our video platform. For simplicity, let’s assume that for Service A, a percentage increase in subscribers results in similar percentage increase in TPS. Given this assumption, we utilize the following heuristic:

Total Desired TPS = 90th percentile of TPS for Service A per node * current nodes * (1 + % rise in subscribers)

Predicted TPS Per Node for a desired CPU utilization is derived using the ML models above

Now, we can easily extrapolate:

Number of Desired Nodes = Total Desired TPS / Predicted TPS per Node

Finally, here is a chart that shows Kepler forecasting a 370, 100 and 266 percent rise for nodes running Service A, B and C respectively with SVR model - given a 1000% percent rise in number of subscribers:",https://medium.com/@mobitv/mobitvs-kepler-a-ml-based-capacity-prediction-system-30d39f7ec39b,[],2020-03-19 03:13:08.971000+00:00,924,"Kepler, Multi-tenant Platform, Streaming Content, Live Video, VOD Traffic"
A Predictive Earthquake Damage Model written in Python,"I have been working on a project over the last couple of years in my role as a Data Scientist at New Light Technologies to develop a model that predicts damages to structures in near-real-time following earthquakes in the USA using nationwide building centroids and Hazus damage functions.

Hazus is a risk model for earthquakes, floods tsunamis, and hurricanes and is the authoritative voice for predicted impacts following one of these major events. To learn more about Hazus, you can read about it directly from the Hazus program’s website.

This blog post describes the methodology and supplemental data sets used in the Earthquake Damage Model, which is available on GitHub here. The model is a Python program that can be set up to run regularly as a scheduled task, in which case it could provide predicted structural damage following an earthquake in near-real-time.

Data and Tables:

There are several important data sources used in this predictive model. The GitHub repo contains all required tables, however, some of the external data sets must either be downloaded and/or generated by the user with some basic geoprocessing steps, which are outlined in the GitHub readme file.

Building Centroids: In order to try and predict the number of buildings damaged in an area following an earthquake, you first need to know how many buildings are present. I used Oak Ridge National Labs building outlines from the USA Structures project. Some other open and public data sets that could be used instead are Microsoft Building Footprints or OpenStreetMap. The building centroids are used to calculate the count of structures within each Census Tract.

General Building Stock (GBS): This is the Census Tract-level breakdown of structure types. Not only do you need to calculate how many buildings are within each Tract, but you need to be able to estimate what kinds of structures they are. This information was extracted from Hazus at the Tract-level. The count of building centroids is multiplied by this breakdown of structure types to estimate the number of structures within each structure type per Tract. The Hazus Earthquake building types are defined in the Hazus Technical User Manual (p. 17).",https://medium.com/new-light-technologies/a-predictive-earthquake-damage-model-written-in-python-e1862518fd92,['Madeline Jones'],2021-05-06 17:23:34.527000+00:00,352,"Data Science, Earthquake Damage Model, Hazus Model, Building Centroids, GBS"
AI Helps the Meteorological Industry,"November 7, 2020

AI Helps the Meteorological Industry

Artificial Intelligence News

General News

AI Helps the Meteorological Industry

Artificial intelligence is a valuable partner to the meteorological industry. If in the past, meteorologists analyzed massive amounts of data to make weather predictions and thus were prone to errors and false warnings, today, AI has improved weather forecasting and added a boost of accuracy to meteorological predictions.

While many focus on weather forecasting as a way to decide on what clothes to wear the next day and what to do on the weekends, industries, like agriculture, farming, and even aviation rely on predictions for informed decisions regarding their business. Artificial intelligence has introduced the digital transformation that will help industries improve their activity and plan accordingly.

Meteorological Predictions

AI is already an important tool in meteorological predictions

Meteorologists have been working with AI models since the 80s. Artificial intelligence is used to analyze both the weather and climate, applying technology to satellite data processing. AI systems process massive loads of data to discover a pattern and create accurate forecasts. The data is collected with the help of deep space satellites, weather balloons, nowcasting weather warnings, as well as radars and environment analytics.

The traditional methods of weather forecasting work with statistical measures of numeric models. Thus, they will not provide binary answers. Consequently, these data needed to be analyzed by meteorologists to identify the patterns and release predictions. The amount of data received by meteorologists keeps increasing while the atmospheric conditions are changing rapidly. This is where AI can help!

Artificial intelligence tools can reduce the workload of meteorologists and improve the accuracy of weather forecasting. High-performance computers can go through massive amounts of data and deliver more accurate and further-reaching forecasts. Thus, weather forecasting will no longer be limited to a nearby future but would be able to extend to even the next 100 years. Moreover, due to AI tools, extreme events will be predicted long before they are bound to happen for people to take shelter and governments to be able to evacuate settlements affected by tornadoes and hurricanes.

AI provides Accuracy in an ever-changing climate

AI provides accuracy in an ever-changing climate

The artificial intelligence system used for weather forecasting uses mathematical problems and computational problem-solving methods to process the colossal amounts of data received from satellites. Combined with machine learning and deep learning, AI systems can recognize patterns. Based on these patterns, supercomputers can predict the weather and help humanity save time and money.

AI-based weather forecasting now includes temperature, wave height, and precipitation. Based on weather satellites and relay stations, AI tools deliver short-term weather forecasts. However, artificial intelligence algorithms are also able to create long-term climate predictions. Until now, scientists have delivered climate change predictions as an average of various results of climate alteration predictions. However, considering how fast the climate is changing, we need more than just average forecasts. We need accuracy!

Climate predictions are imperiously necessary for our future. AI helps evaluate the models used for the calculation of climate change to deliver more accurate results. With better climate change predictions, governments will be able to plan better their policies and make real changes in real time for better odds to avoid extreme weather events and protect our planet.

Some of the AI techniques used by the meteorological industry are the Numeral Weather Prediction, Artificial Neural Network, Radial Basis Function Network, and Ensemble Neural Network. However, scientists work tirelessly to come up with new models and algorithms for improved meteorological predictions.

Scientists at the U.S. National Center for Atmospheric Research (NCAR) are working on a model to forecast hail based on factors like temperatures at different altitudes, wind direction and speed, as well as humidity and updraft. Meanwhile, researchers at École Polytechnique Fédérale de Lausanne, Switzerland, have already created a system that can predict lightning strikes inside a radius of 18.6 miles and with an advance of 10 to 30 minutes before their occurrence. Artificial intelligence plays a vital role in the meteorological industry. If today AI is used for weather forecasting to help agriculture and farming and contribute to efficient disaster management, tomorrow it might very well help us detect storm centers and predict pest seasons.

Share this article

Categories

Subscribe Newsletter

Get our BEST updates delivered to your inbox. Join our email list and get our favorite new content regularly. Unsubscribe anytime.

Subscribe

Tweets by canterbury_ai

Related Articles

November 7, 2020

Artificial intelligence is a valuable partner to the meteorological industry. If in the past, meteorologists…

Read More

October 28, 2020

Tidal erosion is a threat to the existence of many cities around the world. Scientists…

Read More

October 22, 2020

The potential for artificial intelligence to revolutionize the medical industry is vast.

Read More

October 21, 2020

Artificial intelligence has been interlinked with the gaming industry since the beginning of video games….

Read More",https://medium.com/@wesleyjbaker/ai-helps-the-meteorological-industry-74e86b583221,['Wesley Baker'],2020-11-26 23:40:17.416000+00:00,777,"AI, Artificial Intelligence, Meteorology, Climate Change Predictions, Weather Forecasting"
"New Energy Workforce in Net Zero World: Employees, Contractors, and… Bots","Artificial Intelligence (AI), Robotic Process Automation (RPA), and Intelligent Automation reinvented collaboration between humans and machines. Bots help humans to be more productive, intelligent and protected. Energy output per employee is an indicator of productivity for energy firms. Are renewable jobs productive enough to make energy transition economical without bots today?

Striking the balance

50% of a worker’s time can be augmented with existing technologies and 20% of core processes can be automated.

And the role of CHRO increases to strike a delicate balance between re-skilling the existing workforce and attracting new talents. Moreover, COVID-19 demonstrated that a digital workforce is needed to ensure a safe environment while enhancing efficiency. A connected ecosystem supports field workers with real-time data to make better decisions better and perform their job safely.

Humans to think, robots to do

The key question to ask is ‘’what requires thinking’’. You should put humans where thinking is required and anything that requires mechanical or repetitive work should be done by a digital worker. But humans might get overwhelmed by thinking job too, so either they need to have enough dull time or given some mechanical tasks to reset their brain to tackle anxiety from mental work.

Key knowledge workers like engineers, whose job is handling data and thinking, are essential to retain and re-skill for the energy firms.

HR’s role in creating buy-in

Automation is not about choosing the right software or technology but getting everyone aligned in the organization, especially management.

Automation needs to start at the management board. It should be on their dashboard as they drive the energy firm especially through times like pandemic. Management needs to see value and impact on the bottom line. How do you capture value from automation?

The point of the energy industry is to produce energy, not jobs. It outputs, not inputs that matter. Output per worker is vital to keep the business afloat. Thus, energy firms ought to focus on productivity over quantity.

Reducing headcount is the key bottom-line effect of workforce digital transformation. But the first digital worker is always the most expensive one because it requires a lot of upfront cost and preparation.

Besides the economic impact of saving labor costs, one of the most important factors is the level of agility the digital workforce brings where you make decisions and get the whole organization changing fast. Digital workers work 24/7 and adapt to changes faster than humans.

Establishing new digital value-creating behaviors is the key challenge for CHRO manager. Identifying hotspots where RPA can be used is crucial while implementing a digital workforce in the entire organization.

Reality check: lots of green jobs but not much energy

We hear often politicians make populist declarations about new green jobs to justify the energy transition investments and appeal to climate concerned electorate. But the point of the energy industry is to produce energy, not jobs. It outputs, not inputs that matter. Output per worker is vital to keep the business afloat. Thus, energy firms ought to focus on productivity over quantity.

In the U.S., despite a huge workforce, the solar sector produced an insignificant less than 1% of the electric power. That’s a lot of solar workers, about the combined number of employees working at ExxonMobil, Chevron, Apple, Johnson & Johnson, Microsoft, Pfizer, Ford Motor Company, and Procter & Gamble. In contrast, around the same number of natural gas, workers produced more than one-third of U.S. electric power, 37 times more than solar’s tiny share. And less than half the number of coal workers generated one-third of U.S. electricity.

Workforce digital transformation is the key to make energy transition economical. Intelligent automation and AI can help to increase megawatt per employee in renewables. Otherwise, it is better to give everyone their hampster wheel to produce their clean electricity.

Utilizing digital workers in value created functions

Many make the mistake that’s digital workforce is applicable only for back-office functions like accounting. But digital workforce can enhance value-created functions, like front office or field operations, where it has the biggest impact on the bottom line. Small changes there can add big value to profitability.

RPA can be implemented in the broader Oil & Gas and Energy functions like:

Upstream: Data Analytics, Reporting for Exploration & Production;

Midstream: Logistics, Trading, Safety Check-Lists, Supply Chain Management;

Downstream: Pricing, Reporting, Monitoring.

Oil & Gas companies have begun to automate and perform oilfield activities remotely, materially reducing the demand for wellsite-related skills found in completion, production, and maintenance engineers and technicians. Such ‘’human + machine’’ lens defined the strategy of managing the workforce in COVID-19 conditions.

Evolution of energy workforce

The future workforce of the major energy companies will consist of employees, contractors, and bots to turn energy transition into reality by keeping shareholders happy. In this context, the importance of workforce planning increases to assure adequate impact of automation on human resources tools and programs. In the future, CHRO will be measuring the adoption of the digital workforce by the number of bots used.

When digital transformation projects fail, it is usually due to focus on technology over the impact on employees. CHRO plays a critical role in digital transformation to put people first.

That’s why Prospero Events Group is bringing together HR leaders from leading energy companies for the ‘’6th HR: Digital Workforce in Energy Sector’’ virtual conference on 4–5 February 2021. The speaker panel consisting of senior representatives from Gasunie (Netherlands), Repsol (Spain), Baker Hughes (UK), Engie (France), Total (France), MOL Group (Hungary), Equinor (Norway), and Shell (Netherlands) will be prompting discussions about workforce fit for purpose during the energy transition. Connecting machine intelligence and human ingenuity. Using Analytics to solve key HR business challenges.

Join this exclusive meeting to get all your questions answered. All. Because more minutes of net interaction you get at a Prospero event than any other energy conference in Europe.

P.S. :

I can’t put a time frame on when digital workers will become mainstream in the energy industry, but what is certain is that the intelligent automation needed in Net Zero World. So those who begin the journey early will reap the biggest benefits of the Low-Carbon and post-pandemic era.",https://medium.com/@raufjanaka/new-energy-workforce-in-net-zero-world-employees-contractors-and-bots-3964161a9d41,['Rauf Fattakh'],2020-12-09 18:32:24.791000+00:00,997,"Artificial Intelligence, Robotic Process Automation, Intelligent Automation, Digital Workforce Transformation, Energy Transition"
TF-IDF: Term Frequency and Inverse Dense Frequency Techniques,"Data Science

TF-IDF: Term Frequency and Inverse Dense Frequency Techniques

With examples!

Photo by William Iven on Unsplash

TF-IDF is used to measure the importance of a word in data. It is particularly useful for scoring the words in text related computations, such as text analysis and Natural Language Processing (NLP) algorithms.

We measure TF-IDF scores using the following formula:

Simply put:

TF = number of times the term appears in a document/total number of words in the document IDF = log(number of documents/number of documents the term appears)

In this equation, we can observe that TF-IDF calculates the number of times a word appears in each document, however the frequency diminishes if the word appears in other documents. Therefore, the word is not particularly important for the specific document, as it has also appeared commonly in other documents as well.

This explains why we don’t exclude stop words in a TF-IDF computation. Stop words are the words that appear commonly in a text and don’t have a particular meaning (some examples are: ‘the’, ‘a’ and ‘is’). Since our calculation offsets the effect if the word appears in other documents, it is very likely that stop words appear commonly across all documents and will result in a low TF-IDF score in any case.

If the computation score is high, it means the word is rare.

A Quick Example

Assume that a document has 20 words and 5 of them is the word “great”. The TF will be calculated as:

tf: 5/20 = 0.25

Now assume that we have 5 documents in total and the word “great” appears in 2 of them. The IDF will be calculated as:

idf: log(5/2)= 0.398

Therefore, the TF-IDF will be:

tf-idf: (0.25)(0.398) = 0.0995

Another Quick Example — with Sample Code!

To compute the TF-IDF score, we first need to remove all punctuation and lower case the words.

#replace punctuation characters with a space df['example'] = df['example'].str.replace('[^\w\s]','') #store words in lowercase form df['example'] = df['example'].str.lower()

Count how many times each word appears in the document. (You can also calculate TF-IDF with scikit-learn.)

d={} examples = df['example'] for p in examples: dict = {} examples_df = df.get_group(p) for i, row in examples_df.iterrows(): count = row['count'] word = row['words']



d[p] = examples_dict

Define the computation of IDF in a lambda function and apply it to the respective columns.

df['idf'] = df.apply(lambda x: math.log(total/x.total_word_count), axis = 1) df['example_tfidf'] = df.apply(lambda x: x.example_tfidf * x.idf, axis = 1)

You can find an extensive example on my Github.",https://towardsdatascience.com/tf-idf-term-frequency-and-inverse-dense-frequency-techniques-472bf1ba311b,['Delal Tomruk'],2020-12-28 14:36:20.421000+00:00,392,"Data Science, TF-IDF, NLP, Text Analysis, Natural Language Processing"
On the Run (2020),"Imagine what will happen if you let two Artificial Intelligence Entities in one room? Well, this:",https://medium.com/merzazine/on-the-run-2020-c227e67290e3,"['Vlad Alex', 'Merzmensch']",2020-11-25 09:19:55.613000+00:00,16,"AI, Artificial Intelligence, Robotics, Machine Learning, Automation"
How to invest smartly in the cryptocurrency market,"Investing in cryptocurrency these days it’s not a new thing. A lot of people invest in different currencies for different reasons. Cryptocurrencies are based on blockchain, a decentralized distributed public technology. If blockchain proves to be efficient scalable and secure, it could seriously disrupt the legacy payment systems operated by banks.

The very idea of blockchain is to reduce the role of intermediaries in transactions and deals, this way making it easier for both individuals and companies to trade without the need of third parties. Everyone can use some help when it comes to investing, especially in the cryptocurrencies that are growing more and more each and every day.

There are four essential ways that everyone should know when investing in cryptocurrencies, and we are going to have a look at each option:

1. Mining

Cryptocurrencies need miners that can verify their cryptocurrencies transactions. To start mining you need to have hardware with high-performance processors in order to make the necessary calculations, that’s why you need to pay attention to performance, electricity consumption and price of hardware. Obviously it is not easy to start mining especially if you are a beginner, but experience and knowledge can lead you on earning a regular income in different cryptocurrencies.

2. Initial Coin Offerings

Initial Coin Offerings or (ICO) is an unregulated and controversial means of crowdfunding via use of cryptocurrency, which can be a source of capital for startup companies. Different from an IPO, an ICO offers no legal rights or claims to underlying assets. According to coinschedule.com ICOs have attracted over $3bn through 234 issues in 2017 to date. It is important to know about an ICO before you decide to invest, it is essential to believe that their project will be successful therefore you invest your time and money on them.

3. Trading on cryptocurrency markets

The first step to start trading is choosing a crypto market. By having a crypto wallet, participants are available to buy and sell cryptocurrencies. Trading also involves risks, as many crypto markets are located in risky jurisdictions, with no regulator to control them and guarantee trader rights, so making it just as possible to lose many trading, as to make profit. There is doubt cryptocurrencies are unusually risky compared to traditional asset classes. Hacking is one thing people are always susceptible especially after now nonexistent Mt Gox exchange was hacked in 2015 and around 850.000 bitcoins went missing.

4. Trading cryptocurrencies using AI

Trading these days can get quite overwhelming, technical analysis, multiple exchanges, different types of stocks or currencies, different sources of news, and you literally find yourself paralyzed not knowing which financial asset to trade or hold for the long term. This is exactly what AI can help us with. Being able to handle most of these tasks better than a human is capable of, AI has the capability to learn from your trading patterns/habits and ease the process of trading every day. For example, the AI Chatbot developed by AiX, incorporates all of these features and many more. Powered by cognitive reasoning technology, AiX’s chatbot executes trading calls/bids and provides historical analysis of trading through the evidence tree. It can incorporate many exchanges in one interface as per your preference, provides news and recommendation for which financial asset you should trade and which you should avoid, this way making your trading easier cheaper and more efficient",https://medium.com/ai-x/how-to-invest-smartly-in-the-cryptocurrency-market-c260af3e60c5,['Ana Podrimaj'],2018-03-30 11:10:17.380000+00:00,554,"Cryptocurrency, Blockchain, Mining, Initial Coin Offerings (ICO), Trading"
Data Science Trends We Will Frequently Encounter in 2021,"With the data-driven transformation, data analysis methods, must improve themselves to understand and evaluate the exponentially increasing data world. Even if these terms are already used in the sector, they will be heard more frequently in 2021. In this article, we define these terms briefly.

“Data Meshes”, “Data Democratization”, “Continuous Intelligence”, “DataOps” and “Augmented Data Management” … These terms define new trends of data analytics. These trends are actually not new inventions; they refer updates of existing data science technologies.

Let’s get to know these trends more detailed.

Data Meshes

Excessive data is accumulating with great acceleration and radical changes may occur even if your organization is still into data-driven transformation. For example, centralization of data and analysis them only through one unit are old methods. Thanks to Data Meshes, data that are kept in multiple data warehouses are brought together. Traditional monolithic models process data consumption, storage, transformation and output in one central data lake. However, distributed data between different locations and organizations can be bounded with data meshes. With this structure, it is possible to provide the opportunity to work collaboratively. Data meshes ensure that data is highly accessible, easily discoverable, safe, and interoperable with applications. Additionally, it is possible to connect cloud applications with internal data or sensitive data on cloud. Virtual data warehouses or data lakes can be created for analytical and machine learning training without combining them into a single repository. Thus, application development teams can be provided with ways to query data in various data stores without worrying about “how” to reach this data.

Data Democratization

What an interesting definition is the democratization of data, isn’t it? Here is the basic definition of “data democratization”: making digital data accessible for any user and it is not required the participation of IT department. Consider how decision-making processes will accelerate if data can be easily accessed and interpreted by everyone without the need for a central unit. In addition, imagine that how this acceleration affects you in the competitive environment. Data can be scanned in the same natural language just like searching in any search engine from now on. Data creators, data analysts and data consumers in the team can work on the data collaboratively with a catalogued list of all data. It becomes much easier for users to access and understand data. Therefore, decision-making process speeds up for each unit and discovering new opportunities is much easier. With “accessible data” in data-driven organizations, anyone within the organization can easily access and manage data. They can work collaboratively to make effective business decisions.

Continuous Intelligence (CI)

It is a modern machine-oriented approach that accelerates analysis processes by using augmented analytics, event stream processing, optimization, business rule management and machine learning. Thus, CI provides reaching whole data quickly regardless of the number of data sources and volumes. It essentially combines data and analysis with transactional business processes and other real-time interactions. Therefore, CI automates and makes operations continues. In this way, these systems can process high volumes of data quickly and protecting people from overload. However, to take advantage of the system professionally, data and analysis teams should work with application architects, application developers, business process management analysts and business analysts collaboratively.

DataOps (Data Operations)

DataOps is a collaborative data management application focuses on improving communication, integration and automation of data flows between data managers and data consumers in an organization. DataOps is not a product or solution, but an automated and process-driven methodology used by data and analysis teams to improve the quality of data analytics and reduce cycle time. We can also define it as an agile approach to designing, implementing and maintaining distributed data structure that will support a wide variety of open source tools and frameworks. In fact, the purpose of DataOps in general is to create business value from big data. It encourages people in the business processes to work with data engineers, data scientists and data analysts by using automation technology during the editing, delivering and management of data distribution processes. Therefore, organizations’ data are used in the most flexible and effective way to achieve positive business results.

Augmented Data Management

We know that machine learning (ML) and artificial intelligence (AI) are used for making self-structuring and self-adjusting processes common. These processes automate most of the manual tasks and allow you to take advantage of data without being a data analyst. ADM systems provide “self-regulation and self-adjustment” for information quality, integration, metadata management, master data management and database management. System also uses AI and ML to procure these operations. ADM products can examine samples of large operational data including real queries, performance data and diagrams. It can determine their proximity to existing data and cases. Moreover, system alerts other automated systems about new data is available and it is a valid candidate for inclusion.

So what makes them trending?

The data pool is growing exponentially with the increase of available database and data sources. Increasing demands on data and companies’ determination of data-driven strategies now require more “smart” and “agile” systems and methodologies. Continuous improvement is also required for more accurate analysis of the growing data pool. Growth and movement of markets and competitive environments are effective in the development of analysis systems. In fact, the common point of all these trends is the purpose to make data analysis as a standard part of our business processes by making data easily accessible and manageable. In other words, data analysis is now becoming a standard operation for all professions in all business sectors.",https://medium.com/@datateamtech/data-science-trends-we-will-frequently-encounter-in-2021-ef3ac7654e8f,['Datateam Bilgi Teknolojileri'],2020-12-11 13:31:42.328000+00:00,907,"Data Meshes, Data Democratization, Continuous Intelligence, Data Ops, Augmented Data Management"
How to Build a Reporting Dashboard using Dash and Plotly,"A method to select either a condensed data table or the complete data table.

One of the features that I wanted for the data table was the ability to show a “condensed” version of the table as well as the complete data table. Therefore, I included a radio button in the layouts.py file to select which version of the table to present:

Code Block 17: Radio Button in layouts.py

The callback for this functionality takes input from the radio button and outputs the columns to render in the data table:

Code Block 18: Callback for Radio Button in layouts.py File

This callback is a little bit more complicated since I am adding columns for conditional formatting (which I will go into below). Essentially, just as the callback below is changing the data presented in the data table based upon the dates selected using the callback statement, Output('datatable-paid-search', 'data' , this callback is changing the columns presented in the data table based upon the radio button selection using the callback statement, Output('datatable-paid-search', 'columns' .

Conditionally Color-Code Different Data Table cells

One of the features which the stakeholders wanted for the data table was the ability to have certain numbers or cells in the data table to be highlighted based upon a metric’s value; red for negative numbers for instance. However, conditional formatting of data table cells has three main issues.

There is lack of formatting functionality in Dash Data Tables at this time.

If a number is formatted prior to inclusion in a Dash Data Table (in pandas for instance), then data table functionality such as sorting and filtering does not work properly.

There is a bug in the Dash data table code in which conditional formatting does not work properly.

I ended up formatting the numbers in the data table in pandas despite the above limitations. I discovered that conditional formatting in Dash does not work properly for formatted numbers (numbers with commas, dollar signs, percent signs, etc.). Indeed, I found out that there is a bug with the method described in the Conditional Formatting — Highlighting Cells section of the Dash Data Table User Guide:

Code Block 19: Conditional Formatting — Highlighting Cells

The cell for New York City temperature shows up as green even though the value is less than 3.9.* I’ve tested this in other scenarios and it seems like the conditional formatting for numbers only uses the integer part of the condition (“3” but not “3.9”). The filter for Temperature used for conditional formatting somehow truncates the significant digits and only considers the integer part of a number. I posted to the Dash community forum about this bug, and it has since been fixed in a recent version of Dash.

*This has since been corrected in the Dash Documentation.

Conditional Formatting of Cells using Doppelganger Columns

Due to the above limitations with conditional formatting of cells, I came up with an alternative method in which I add “doppelganger” columns to both the pandas data frame and Dash data table. These doppelganger columns had either the value of the original column, or the value of the original column multiplied by 100 (to overcome the bug when the decimal portion of a value is not considered by conditional filtering). Then, the doppelganger columns can be added to the data table but are hidden from view with the following statements:

Code Block 20: Adding Doppelganger Columns

Then, the conditional cell formatting can be implemented using the following syntax:

Code Block 21: Conditional Cell Formatting

Essentially, the filter is applied on the “doppelganger” column, Revenue_YoY_percent_conditional (filtering cells in which the value is less than 0). However, the formatting is applied on the corresponding “real” column, Revenue YoY (%) . One can imagine other usages for this method of conditional formatting; for instance, highlighting outlier values.

The complete statement for the data table is below (with conditional formatting for odd and even rows, as well highlighting cells that are above a certain threshold using the doppelganger method):

Code Block 22: Data Table with Conditional Formatting

I describe the method to update the graphs using the selected rows in the data table below.",https://medium.com/p/4f4257c18a7f#44e2,['David Comfort'],2019-03-13 14:21:44.055000+00:00,668,"data table, Dash, radio button, callback, conditional formatting"
All you need to know about Support Vector Machines,"PC:Packtpub

SVM is a simple classification algorithm every machine learning practitioner should have in their toolbox. Let’s first understand how it works then see its pros and cons.

As said earlier it is a classification algorithm used in supervised learning when we have categorical data. SVM takes the labeled data points (features) as input and returns the hyperplane which classifies that data points into categories(classes) as we expect.

Understanding hyperplanes is mandatory to understand SVM. To be simple hyperplane is a decision boundary helps to classify the data points. Data points fall under different side of the hyperplane considered as separate classes.

As the number of features increases the dimensions of hyperplane also will increase. When there is N features the resultant hyperplane will have n-1 dimension.

After SVM finds the hyperplane it will try to maximize the margin. Here margin in the sense, the distance to nearest data points (also known as support vectors) from the hyperplane. Orientation and position of the hyperplane are highly depended in this nearest data points.

Best Hyperplane(with maximized margin) for two features

You might have a question, “Why need to maximize the margin?”, this maximized margin gives some reinforcement which helps the model to classify future data points with more confidence.

How SVM works when there are Outliers in the dataset?

As we saw earlier, the position and orientation of the hyperplane are highly influenced by the closest data points. So if outliers exist in the data set, the algorithm will try to find the best hyperplane which reasonably separates the classes with relative to the number of nearest data points (or support vectors).

With Outlier

SVM on Non-linear Datapoints

So far we have seen the examples on linear data points. Look at the below example can you imagine how the hyperplane will look like? Do you think it is possible to classify these data points using SVM?

Guess the Hyperplane (Decision boundary) for this non-linear data points

Yes, it is possible with the help of the Kernel function. To be simple kernels are mathematical functions passed to the SVM as a parameter. Kernels take the input data points and convert them into the required form for the SVM, to find the hyperplane.

Kernel functions convert the non-linear data to linear data in higher order then finds the hyperplane. Again using the same kernel function it will map the decision boundary(hyperplane) on the non-linear data.

How Kernels helps to find decision boundary in non-linear data

Let’s see how it works on our example,

Linearity found by the kernel

Decision boundary on non-linear data points

Parameters of SVM

If you google “sklearn SVM” you will find the documentation for Scikit learn SVM model. There you can see the details of parameters passed into the SVM. Let me introduce three main parameters to be considered.

Kernels: As we saw before kernel function is the parameter which needs to be passed according to our data points’ linearity. Refer to the sklearn documentation for different kernel functions available and, their uses. C (Regularization): Controls the trade-off between the smoothness of the decision boundary and correctness of the classification.i.e. when c is high it will classify all the data points correctly, also there is a chance to overfit. Gamma: Defines how much influence each training sample has on the decision boundary. i.e. when gamma is higher nearby points will have high influence and low gamma means far away points also be considered to get the decision boundary

When C is high it SVM will try to fit on all data points, see the less smoothness of the decision boundary

When Gamma is low SVM also consider far away points to decide decision boundary

play around and tune the hyperparameters and see the different results you get when trying out SVM classification.

Advantages first,

Regularization parameter helps to avoid overfitting. The kernel tricks help to classify both linear and non-linear data. SVM uses convex optimization, which ensures the result is a global minimum. SVM also supports semi-supervised learning.

Let’s see the disadvantages

In general, SVM is slower on training and prediction.

This is the only disadvantage I found, if you know any other please respond (comment) to this story.

Hope now you have a clear picture of SVM algorithm. Applaud and Share with your friends!!!",https://medium.com/@mathanrajsharma/all-you-need-to-know-about-support-vector-machines-2f130c0bc0ed,['Mathanraj Sharma'],2019-05-15 19:20:52.345000+00:00,685,"SVM, Supervised Learning, Classification Algorithm, Hyperplanes, Kernel Functions"
Customer Life Time Value (CLTV),"In this article, I will discuss the ways to reveal this potential value.

This is the second and the final part of the article series about Customer Value. In the first part, I discussed the core of customer value calculation: RFM metrics and scores.

This final part will be about Customer Life Time Value prediction using BG/NBD and Gamma Gamma Models.

With RFM Analysis, a company can have a vision of its customers’ current behavior by scoring and segmenting them based on their purchasing habits and therefore be able to maintain an effective CRM.

What is CRM?

Customer relationship management (CRM) combines practices, strategies, and technologies that companies use to manage and analyze customer interactions and data throughout the customer lifecycle. It tries to understand the customer profile of the company and to communicate according to these profiles. Analyzes customers, finding audiences belonging to certain behaviors, and organizing campaigns suitable for that audience. Moreover, CRM targets to gain new customers other than existing customers. All in all, CRM offers you the opportunity to improve your profitability, productivity, and efficiency.

What is CLTV?

CLTV, an alias for Customer Life Time Value, is the ultimate metric used to understand and influence the customer’s behavior by predicting each customer’s equity covering the period where he/she is alive. Being alive is a metaphor for “a customer who still purchases from the company.” The existing customers’ purchasing habits will be of great help for creating different patterns and measuring the CLTV of a new customer and the existing ones.

CLTV is based on 4 metrics; let’s examine them one by one:

Recency: The period between the first and the last purchase date, that is the duration where we can observe the behavior of this customer,

T: The age of the customer within the company that is the period starting from the first purchase date until today,

Frequency: Number of purchases the customer has, that is how often he/she purchases,

Monetary: The average of the total prices among all the purchases.

CLTV Calculation compares customer behavior. Each customer is evaluated according to their purchasing status. There are different approaches for calculating CLTV, and I will share the most common one:

CLTV = (Customer Value / Churn Rate) * Profit Margin

Customer Value = Average Order Value * Purchase Frequency

Churn Rate = 1 — Repeat Rate

It is possible to degrade the above equations and reach this one:

CLTV = Expected Number of Transactions * Expected Average Profit

So how would these values be calculated?

Expected Number of Transactions: Calculated using BG/NBD Model

Expected Average Profit: Calculated using Gamma Gamma Model

We are projecting future time using some models(BG-NBD and Gamma-Gamma) with CLTV Prediction. We make predictions about the customers’ process not only now but years later.

BG/NBD Model

Given the Frequency, T, and Recency values of a customer, it is possible to predict the expected number of purchases that he/she would make within a specified time period.

Gamma Gamma Model

It is used to estimate the average monetary value of customer transactions. Given the Frequency and Monetary values of a customer, it is possible to predict the expected average profit of a customer.

Online Retail II dataset will be used for this approach.",https://medium.com/@iambideniz/customer-life-time-value-cltv-f732b1bb6e4d,[],2021-06-08 08:51:15.087000+00:00,511,"customerlifetimevalue, CLTV, CRM, RFMAnalysis, BG/NBDModel"
Safe Workplaces: A Computer Vision Model,"In short:

When we move to work each morning the protection and health measures are some concerns which don’t even cross our minds but if we glance around and think for a second, we do realize that although health and safety within the workplace has improved over the years, there are still an outsized number of labor place accidents happening irrespective of which field you’re employed in or as a matter of fact regardless of how near misses have occurred with you.

Unlike earlier times, we now have something called as computer vision, unlike a manual supervisor who manages by wandering around, is 24×7, not in only one spot at a time, but all told critical areas of interest. it’s the unique ability to observe how people interact with the equipment they’re operating, and materials or products they’re performing on.

No technology is completely safe and reliable, and to be effective the alerts will should occur in near real time — almost at the identical time. But given what’s at stake, computer vision may be an additive and cost-effective tool to enhance overall any protection as well as safety management system.

Diving Deeper:

For a better understanding it is important to see a few examples on how computer vision can be used in workplaces.

For an improved understanding it’s important to determine some examples on how computer vision may be employed in workplaces.

Forklift and pedestrian-related risks are identified joined of the foremost concerns for manufacturing premises, warehouses, yards, depots, etc. Using computer vision, AI algorithm helps observe forklift movement within the defined area. Alarms are raised for incidents and accidents like speeding, movements within the wrong direction, parking within the movement area, pedestrians not using sidewalks, and the other instances of non-compliance that may cause accidents.

AI and computer vision systems can monitor loads on an elevated platform. they’ll detect PPE and proper equipment usage, over-crowding on scaffolding, and even falling objects. These systems may monitor entry into exclusion zones. With relation to the growing concerns within the healthcare unit, AI and computer vision systems can measure compliance and make sure the right PPE equipment is employed to safeguard the user against health or safety risks at work doctors and healthcare workers have duties concerning the availability and use of private protective equipment (PPE) at work. this might include gloves, eye protection and safety footwear. the identical systems can monitor the employment of respiratory protective equipment, like masks.

Computer vision systems can detect the kind of lifting equipment used, and identify differing kinds of loads which are mostly utilized in automobile industries. they’ll monitor the work the lifting equipment is employed for, and supply real time warnings for workers walking/standing under suspended load.

Computer vision fire detection models are trained to detect fire at an early stage, within 10–15 seconds. Real-time alarms may be integrated with local buzzers, PA systems, display systems, email, SMS, and push notifications. These systems may detect people stuck in numerous areas of the location during a fireplace.

Computer vision and AI systems can detect employees within the hazardous zone and supply real-time warnings. The warnings could even be provided to machine operators just in case an employee is detected near the machine. AI systems can monitor the extent of maintenance, reducing the possibility of breakdowns and accidents. The system could also provide real time alerts just in case of accidents, reducing the time to produce medical help.

There are dozens of solutions that read machine and vehicle data, but only with computer vision are you able to be assured safety regulations are being followed. No technology is fool proof, and to be effective the alerts will must occur in near real time — within a second or two. But given what’s at stake, computer vision may be an additive and cost-effective tool to boost overall any safety management system.",https://medium.com/@anurag-aher18/safe-workplaces-a-computer-vision-model-11915190d20,['Anurag Aher'],2021-01-13 16:09:48.219000+00:00,633,"Computer Vision, AI, PPE, Forklift Safety, Fire Detection"
LiDAR for Your Indoor Robots,"What is common between a Waymo/Uber Self Driving car and the latest iPhone. The LiDAR! And that is what we hope to delve deeper into over the next few paragraphs.

LiDAR is an acronym for Light Detection and Ranging. Put simply, it is a sensor that estimates the distance to an object by shooting a ray of light and measuring the time taken by the reflected ray to return to the point of origin. You can find more details about the history and the technology behind LiDARs here. LiDARs come in various form factors. Velodyne, Sick, Ouster, Hokoyu, Slamtec, Pepperl +Fuchs are some popular vendors of LiDARs.

A mobile robot needs to have a 360-degree view, not just what lies ahead of it. To achieve this, it will need to fire rays in all directions and capture the resulting reflections. Instead of doing this with multiple LiDARs, it is a common practice to spin a LiDAR on its vertical axis. These are the Spinning LiDARs that are commonly used in mobile robots. A 3D LiDAR is an extension of the 2D LiDAR. It can be visualized as multiple 2D LiDARs that are stacked together.

In order to achieve autonomous navigation, mobile robots need to gather information about what is around them. Such information is used to:

1. Create a map -

· To know where it is in the room. This is called localization. If the robot is localizing while simultaneously creating the map of the place, it’s called ‘Simultaneous Localization and Mapping (SLAM)’. LiDAR (for SLAM) is not needed if an absolute positioning technology like Indoor GPS is being used.

· To figure out optimal paths from start to goal location. This is called path planning.

2. Locate obstacles and go around them.

3. Move around safely

LiDAR is one of the most robust and easiest ways to do all three of them.

How to choose a LiDAR:

1. For SLAM

In extremely simple terms, the LiDAR-based SLAM works by matching the current scan with previous scans or map/scan-based models. Therefore, the scan at every point of the time should contain features (walls, furniture, or other physical objects) that can be identified.

A LiDAR used on an indoor office robot should have a range that is at least 20% more than the distance between the parallel walls of the rooms the robot is operating in. Now for warehouses, this is hard to achieve, therefore the range of the LiDAR can be decided based on the distance between shelves and machines. The range of the LiDAR should be enough such that at each point it should be able to capture at least one static/permanent component in that warehouse. If a robot with a LiDAR that has a range of 10 meters is left in the middle of a stadium, it will fail to identify permanent features. This will lead to large errors in the robot’s position estimation.

This page does a good comparison of different LiDARs in the market.

2. For obstacle avoidance

The range of the LiDAR used for obstacle detection depends on a host of factors: Obstacle horizon, speed, load, inertia, and dynamics are some of them. , For a robot with large inertia, the obstacle horizon should be large too. A short obstacle horizon for a robot with large inertia can cause collisions. The range of the LiDAR should be at least 20% more than the obstacle horizon.

3. Safety Lidar

LiDAR is quite a popular option for safety sensors in Autonomous Guided Vehicles and Autonomous Mobile Robots. Most of the time it is a mandatory requirement for industrial mobile robots. More on this here.

Mowito provides a brilliant trajectory planner that can avoid dynamic obstacles and operate at frequencies as high as 50Hz using the LiDAR data. If you are building an AMR, feel free to write to us at aditya@mowito.in and we would be glad to work with you.",https://medium.com/@mowito/lidar-for-your-indoor-robots-cacdcf08d2aa,[],2020-10-21 08:56:55.001000+00:00,635,"LiDAR, SLAM, Autonomous Navigation, 3D LiDAR, 2D LiDAR"
Noisy Neighbors and Traffic Trouble: An Analysis of NYC 311 Service Requests,"Noisy Neighbors and Traffic Trouble

Introduction

NYC Open Data is a vast trove of City government datasets that have been made available to the public. One such dataset, 311 Service Requests from 2010 to Present, will be the focus of this article. This 311 data is updated daily and contains information about more than 24 million service requests made since 2010. For those who aren’t familiar, 311 is a phone number used in the U.S. that allows callers to access non-emergency municipal services, report problems to government agencies, and request information. This article discusses my process for exploring trends in a recent subset of the data (June-Nov. 2020), and for building a neural network to classify the government agency that responded to a given call. The full code for this project is available on its GitHub repository.

Obtaining the Data

The 311 data was obtained through the Socrata Open Data (SODA) API, which partners with NYC Open Data to host City data. The code below creates a data frame with data for the 1.5 million most recent calls.

This project also uses data from the NYC Department of City Planning’s Community District Profiles. A CSV file containing indicators for all community districts can be downloaded from any district profile. Both the 311 data and the indicators data contain a column for the community district/board. After reformatting the values and column name in the indicators data frame, it could be merged with the 311 data on their mutual “community board” column:

Exploratory Analysis

Using Plotly Express bar charts, I visualized the distribution of calls across different complaint types and government agencies. There were dozens of unique complaint types, so the first plot displays only the top 30 most frequent complaints:

Plot by Author

We can see that complaints related to noise, vehicles/traffic, utilities and trees are the most common, with noise complaints standing out as the clear majority category.

Plot by Author

The agency column is similarly imbalanced: since the NYPD handles noise complaints, it responds to the vast majority of 311 calls. The Department of Housing Preservation and Development (HPD) and the Department of Parks and Recreation (DPR) also respond to a significant proportion of calls.

To explore how the volume of calls received in different complaint categories changed over time, the function below generated daily call frequency dictionaries for a given complaint type:

There was no “day” column in the original data frame, but by converting the “created date” column to a datetime object, a new day column could be generated by applying datetime.date() to each row. This allowed for the creation of area plots that show the change in daily call counts over time. The following plot, for example, shows the daily counts for calls related to COVID-19.

Plot by Author

The non-compliance calls peaked in August, and with the exception of a couple dips, consistently exceeded 200 calls per day. Mass gathering complaints, on the other hand, were not recorded during the summer, and hover just above 100 daily calls at their peak. This does not indicate that no mass gatherings occurred in the summer (as a New Yorker, I can confidently attest to the contrary). It means, simply, that this complaint category was not formally introduced to the data before the fall. Let’s now take a look at noise complaints, which were the most common type:

The number of residential noise complaints surpasses that of other noise categories, and exceeded more than 4000 calls on some days. We would expect most people in their homes to be particularly bothered by excessive noise coming nearby locations; and with all the tiny, packed apartments in this city, it is no surprise that residential noise takes the lead here. Vehicle and sidewalk/street noise complaints are also fairly common. Next, we’ll look at a complaint type with a single peak in call count: damaged trees.

Plot by Author

Damaged tree calls are relatively rare on most days, but on August 4th and the subsequent couple of days, the call counts exceeded 12k. A quick Google search revealed the likely culprit: Tropical Storm Isaias, which not only damaged thousands of trees, but also resulted in mass power outages and took the lives of at least 5 people. This plot is demonstrative of just one form of wide-sweeping, devastating damage that natural disasters can cause.

Using Plotly’s Mapbox Density Heatmap and the location data for the calls, it was possible to create animations that show how call volume varied across the 5 boroughs:

Animation by Author",https://towardsdatascience.com/analyzing-and-modelling-nyc-311-service-requests-eb6a9c9adc7c,['Avonlea Fisher'],2021-01-24 01:01:02.839000+00:00,734,"The heatmap shows that the Bronx and Brooklyn have the highest call volumes, while Staten Island has the lowest. This is likely due to population density, as well as differences in socio-economic conditions across boroughs.Noisy Neighbors, Traffic Trouble"
"Pretty Excel Views with Formatted Table — Kenneth Lo, PMP","The desire of writing this article stems from my present circumstances during shelter-in-place. Few things I struggle with working remotely include:

Running out of monitor real estate on a single monitor.

Inability to print with a laser printer and company-supplied ink cartridges.

Exhaustion from scrolling endlessly in Excel files that have way too many columns.

The last pain point manifests daily since my numerous projects mandate frequent interactions with large Excel files. Not having the usual office setup creates additional challenges. When a spreadsheet grows, it invariably becomes wider (more columns) and taller (more rows). That means lots of scrolling.

Grouping or hiding columns alleviates the issues somewhat. Let’s go over another approach to generate a view that contains a subset of columns based on the source data. The output is merely a view and not a copy of the original source. Any change made to the source are faithfully rendered in the corresponding views.

The dataset illustrated in this article comes from Kaggle: Startup Investments via Crunchbase. It lists 50,000 startup companies and their fundraising metrics. The raw data contains 39 columns and 50k rows. The thought of looking at this long list by market sectors, funding rounds, funding amounts and other dimensions make my wrists ache already.

Some prerequisite knowledge is required to maximize understanding of what follows. Have a quick peek at these how-to posts:

Format an Excel Table

A formatted Excel table has many benefits. Automatic styling and enhanced readability are most apparent. Another pivotal benefit is the added support of Structured References, the powerhouse behind the technique explained this article.

Once the table is created, make sure to name it accordingly. In this instance, the table is named _crunchbase. The name will be used in Structured References later. Best practice is to be descriptive without being verbose.

Create a Print-Only View

Excel does not truly support “views”, which essentially are representations of the source table. One prime use case is a print-ready view that has far fewer columns. Another example is a role-based working view for investment, marketing, or legal team.

On a new worksheet named , insert =_crunchbase in A1.

With virtually no effort, you have a view of the entire table without any columns header. Not too shabby for few keystrokes.

Now change A1 to =_crunchbabse[#All]. You get a view of the entire table with the headers.

Pretty cool. Let’s tackle the objective of displaying a subset of columns. Make the following modifications:

A1 to =_crunchbase[[#All], [name]]

B1 to =_crunchbase[[#All], [market]]

Translation of the two formulas above: show the headers and data in and columns from the source table. Let’s add a few more columns and a filter to wrap things up.

Takeaways

When working with a larger, especially column-wise, spreadsheet, you can effortlessly generate a filtered view with far fewer columns. Less scrolling, more mental focus, more time for leisurely activities.

Lots more can be done to further enhance this Pretty view. Beauty is in the eyes of the beholder. Feel free to take the sample file on my Github and make your own Pretty views.",https://medium.com/@klopmp/pretty-excel-views-with-formatted-table-kenneth-lo-pmp-3940447f2ff1,['Kenneth Lo'],2020-07-20 06:24:39.155000+00:00,492,"remote-working, monitor-real-estate, printing, Excel-files, Structured-References"
Machine Learning with Tensor flow #2,"Logistic regression

Logistic regression is a statistical approach used to model the probability of a certain binary event such as pass/fail. This form’s the basic principle that will later be used in image recognition and other classification problems. There is a since of accuracy for classification, unlike regression which is just an approximation. Logistic regression is used to approximate exponential growth or exponential decay.

We’ll start our example first by importing all our imports

For this example I’ll be populating a list with values that grow exponentially over a time period of two. I’m using a formula but this would also work with random values as well, your end result won’t be as exact though. We will be asking the question how long it takes for our function to double?

This is just a function to generate points that increase exponentially

This code segment populates our list with exponentially increasing values

This model is growing exponentially however with some manipulation we can make it look linear.

take the log of the right hand side

convert the logarithm to base 10

Use the logarithmic property of division

Multiply both sides by r

Now we just need to subtract C

Our final product

The result closely resembles y=mx+b. Meaning we can use linear regression.

We take the data and calculate the log of each value, we set the list equal to y. Once the points are plotted there appears to be a clear linear correlation

Since the values on the x-axis are so big this will throw our estimation off so we must first center the x values. We can do that by subtracting the mean

Now the model must be instantiated

The sequential model is only appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor

The input shape is a shape tuple of integers which doesn’t include the batch size since our input is one, that indicates that the expected input will be a batch of one dimensional vectors.

The Dense layer is a regular neural network activation function with weights and biases it’s the most commonly and frequently used layer

We are passing in 1 to our dense layer because its 1 dimensionality of output space

We can also get some important info from our model which will be very useful when we run into bugs later on with the model

The model is then compiled for the optimizer I am using SGD (stochastic gradient descent). The only reason we’re using SGD is to just formalize everyone with different optimizers. Adam is a little newer and tries to optimize SGD. However under certain circumstances Adam has failed to converge to an optimal solution and has been out preformed by SGD. The two parameters that we are passing in is the learn rate and the momentum.

For the loss function we are just using the mean squared error

The learning rate scheduler function takes in another function with the parameters of the number of epoch’s and the learning rate. An epoch represents the number of times all the training vectors are used to update the weights and biases. The schedule function is very useful for more complicated situations such as, for n number of epoch change the learning rate by m.

For every epoch this callback gets the updated learning rate value from the scheduler function. The current epoch and learning rate is then applied to the optimizers learning rate. The number of epochs isn’t super important the more epochs the more your weights will get updated and the longer your model will take to fit your data.

Our losses per iteration converge which is great otherwise we’d have to do some debugging.

get_weights returns the weights of the layers of the numpy list. So for our purposes it returns a one by one matrix and a vector of length 1

So r or 0.49995455 is our slope for our linear equation. Now we need to find the difference between our original expediential growth function and our doubled expediential growth function. Since we want to know how long it takes to double.

Double growth function

Original growth function

We start of by dividing the two functions and cancel out as many variables as we can

Which leaves us with the difference between the functions doubling which we can refer to as delta t

However our values were computed in respect to our linear formula so the rate of log r is already calculated

Our final result for the time difference it takes to double

So the time it takes this function to double is 2.000181810692627 units. The actual answer is 2 so the error is %0.090905

https://colab.research.google.com/drive/1S505dLc6hfVtYAqJTTh2xDRIUxLi8YzZ?usp=sharing",https://medium.com/@ashley.hendrickson/machine-learning-with-tensor-flow-2-230af4aa3e8a,['Ashley Hendrickson'],2020-10-20 00:50:05.295000+00:00,750,"Logistic regression, exponential growth, linear regression, SGD, Adam optimizer"
EfficientNet and EfficientDet Explained,"EfficientDet is an improvement upon EfficientNet, so we’ll look at the latter first.

EfficientNet

The paper sets out to explore the problem of given a baseline model i.e. CNN architecture how can we scale the model to get better accuracy. There are 3 ways to scale a model.

Resolution — Input resolution e.g. (224,224) to (512,512) Depth — Number of conv blocks (layers) e.g. Resnet-18 to Resnet-101 Width — Number of channels in each conv block e.g. 8, 16, 32 to 16, 32, 64

While different models choose to use different scaling techniques, according to their experiments the improvements on accuracy saturate pretty soon if only one way of scaling is done. If only the input resolution were to be increased then the given number of conv blocks aren’t enough to retain the perceptive field. Also the with more channels we can hope to extract more finer features from the new high resolution input. Increasing the network depth works only to a level, even with batch norm and skip connections to combat the vanishing gradients problem. And increasing the number of channels with the same number of layers seems to be a bottleneck situation.

Thus, there lies a relationship between the resolution, depth and width of the tensors and the network. For most cases the CNN can be modelled as a compound function F of a tensor of shape H, W, C. And since most models repeat their conv blocks the depth of the network can be modelled as how many times (L) we apply F on the tensor. So we can choose to increase H,W and C and L.

They introduce a solution called compound scaling which depends on 4 hyperparameters. They are ϕ, α, β, γ, where ϕ is the compound scaling factor determined by the user, based on the available compute resource. And α, β, γ determine the relative ratio of scale for depth, width and resolution respectively. The FLOPS of a CNN is proportional to α, β², γ². Read appendix for how. Assuming we have twice the resources then

And any further scaling follows the form of

where, ϕ is the user chosen hyperparameter. They also introduce a new baseline architecture found using Neural Architecture Search (think AutoML). Using this baseline model they perform a grid search for the best performing values for α, β, γ, which are 1.2, 1.1, 1.15 respectively. They then use the same values for all values of ϕ. Even though a separate grid search for each variant would give better values and thus better performance, it is not worth the time and compute.

The main observation is to scale resolution, width and depth together in the ratio of α, β, γ to get the most accuracy.

EfficientDet

To understand EfficientDet we need to first know about Feature Pyramid Network. It’s based on the traditional idea of running the algorithm on multiple resolutions of the same image hoping to capture both small and large scale phenomena. Here, however instead of using the image at different resolutions they use feature maps at different resolutions.

In the image above, bottom-up represents the traditional backbone of a CNN. And the top-down represents feature fusion at multiple scales. The idea of the lateral connection is that of to combine feature-rich but low resolution feature maps with high resolution but less meaningful feature maps. The predict block applies a 3x3 convolution and produces final feature maps upon which further operations (detection, segmentation) can be made.

Now that FPN is clear, let’s move on to EfficientDet. The paper makes two major modifications to EfficientNet, namely BiFPN and a new compound scaling method. EfficientDet uses the same backbone as EfficientNet but adds a bi directional feature pyramid network to help in multi scale feature fusion.

BiFPN has 5 modifications over a normal FPN.

Instead of only top-down feature, it adds another bottom-up feature fusion branch It has skip connections from the initial feature map to the fused feature map Nodes with only one input are removed, cause they do not do much fusion as other nodes The entire module is repeated multiple times Features are not summed directly, instead a weighted average is used hoping different resolution feature maps contribute to the fusion at different capacity. Unbounded weights bring problems in backprop, so we need to normalise it. They tried applying softmax to the weight values which worked but slowed down training. So a simple average after relu activation is used to normalise the weights.

The need for a new scaling technique comes from the fact that we have the BiFPN as an additional module in the network and that too can be scaled. But there’s no heuristic given about the scaling technique here. The input resolution, depth of BiFPN increase linearly with ϕ and the width of BiFPN increases exponentially.

Network architecture of EfficientDet

Appendix

Let T1 and T2 be the input and output tensor, with dimensions H, W, C1 and H, W, C2 respectively. The FLOPS required for the convolution with kernel of size K, K is

Note: The number of kernels is determined by the number of channels of the output tensor i.e. C2 in this case. And thus doubling the number of channels results in a 4x increase in FLOPS.",https://medium.com/swlh/efficientnet-and-efficientdet-explained-297ff81a1267,[],2020-11-25 07:14:57.464000+00:00,855,"Efficient Net, CNN architecture, Resolution, Depth, Width"
"Scraping tables from a JavaScript webpage using Selenium, BeautifulSoup, and Pandas","Running the script shows an error “ValueError: No tables found”. This is because Pandas read_html() function searches for <table> elements and can’t find any from the webpage.

If we look at the page source and search for <table , it shows 0 result.

image by author

The reason why we can’t find any table when looking at page source is that the browser is executing JavaScript code that loads content from a remote server and renders tables. This means if we try just scrapping the HTML, the JavaScript won’t be executed, and thus, the Pandas read_html() won’t find any <table> element.

This brings us to use Selenium.

If you are looking for Pandas read_html() tutorial, please check out:

2. Install libraries and download Selenium web driver

Before we move onto install libraries, it is highly recommended to create an isolated Python environment to avoid any conflict.

Please check out the following articles for creating Python Virtual Environment.

Install libraries

In order to run this example, you need to install selenium, beautifulsoup4, and Pandas. If you have pip on your system, you can simply install them by

pip install selenium

pip install beautifulsoup4

pip install pandas

Download Selenium web driver

Selenium requires a driver to interface with the chosen browser. For example, Chrome requires ChromeDriver, which needs to be installed before running the example.

To install ChromeDriver, you should:

download the same version ChromeDriver as the Chrome you are using.

place the ChromeDriver executable in your PATH, for example, place it in /usr/bin or /usr/local/bin .

You will get a WebDriverException error if you don’t have a proper Selenium web driver in your PATH.

Other supported browsers have their own drivers, please check out their drivers below

3. Scraping tables using Selenium, BeautifulSoup, and Pandas

We will be scrapping tables from the following article.

Let’s import libraries and specify the url .

import pandas as pd

from selenium import webdriver

from bs4 import BeautifulSoup url = ' https://pubs.rsc.org/en/content/articlelanding/2020/na/d0na00118j

Step 1: Create a session and load the page

The first step we need to do is to create a web driver session, for example, a new Chrome session.

# Create a new Chrome session

driver = webdriver.Chrome()

If your web driver executable is not available in your PATH, you are able to define a custom executable path by setting the argument executable_path

# Create a new Chrome session with a custom executable path

driver = webdriver.Chrome(executable_path='path_to_web_driver/')

Once set up, we can now load the web page. We also call implicitly_wait(5) to wait for 5 seconds for the page to fully load.

# Load the web page

driver.get(url) # Wait for the page to fully load

driver. implicitly_wait(5)

If we run the script now (You should add driver.quit() at the end to ensure the browser closes), it will launch a Chrome window and open the URL specified. Hopefully, you should see page content load up before the script quits the session.

Step 2: Parse HTML code and grab tables with Beautiful Soup

Next, we need to scrape information from the web page. Beautiful Soup is one of the best ways to traverse the DOM and scrape the data.

In this tutorial, we are going to use lxml parser. If you have pip on your system, you can simply install it by

pip install lxml

By running the following statement, Beautiful Soup grabs all page source on the page.

soup = BeautifulSoup(driver.page_source, 'lxml')

Next, it is time to ask Beautiful Soup to find all the tables on the page.

tables = soup.find_all('table')

By now, you should be able to see a list of tables if running with debugging mode.

image by author

Step 3: Read tables with Pandas read_html()

Next, we need to give the HTML tables to Pandas to put them in a DataFrames object.

Pandas already have a built-in function read_html() to convert the table on the web to a DataFrame.

dfs = pd.read_html(str(tables))

The result we get is not a Pandas DataFrame but a Python list. If we want to get the table, we can use the index to access it:

print(dfs[0]) Samples n (1019 cm−3) μH (cm2 V−1 s−1) σ (S cm−1)

0 Bi2Te3 1.33 59.08 125.7

1 0.5% rGO/Bi2Te3 1.26 79.79 160.8

2 1% rGO/Bi2Te3 1.21 103.27 199.9

3 2% rGO/Bi2Te3 1.17 138.79 259.8

4 rGO film 3.59 48.25 277.3

The full Python code

Here is the full Python code.

Conclusion

This article shows a method we can use to scrape tables from a JavaScript web. Web scraping with Selenium, Beautiful Soup, and Pandas is an excellent tool to have within your skillset.

I hope this article will help you to save time in scrapping data from the webpage. I recommend you to check out the documentation for the Selenium and Beautiful Soup, and to know about other things you can do.

Thanks for reading

You may be interested in some of my other articles:

More tutorials can be found on my Github",https://medium.com/analytics-vidhya/scraping-tables-from-a-javascript-webpage-using-selenium-beautifulsoup-and-pandas-cbd305ca75fe,['B. Chen'],2020-11-27 16:38:57.414000+00:00,757,"web scraping, selenium, beautifulsoup4, pandas, read_html()"
Simple image classification on raspberry pi from pi-camera (in live time) using the pre-trained model mobilenet_v1 and TensorFlow Lite,"Note before you start:

So, Let’s start :)

Hardware preparation:

Software preparation:

1 Try predict class from camera raspberry pi in live time using Tensorflow Lite.

For that, you need to run the next code:



pip3 install #install tensorflow litepip3 install https://github.com/google-coral/pycoral/releases/download/release-frogfish/tflite_runtime-2.5.0-cp37-cp37m-linux_armv7l.whl git clone https://github.com/tensorflow/examples --depth 1 cd examples/lite/examples/image_classification/raspberry_pi # The script takes an argument specifying where you want to save the model files

bash download.sh /tmp

2 preparing VNC (if you have)

If you go to VNC Server Options (right-click on the VNC status icon in the top right) and check to Enable direct capture mode on the Troubleshooting page. This will allow you to see the camera output via VNC. Like on the screenshots:

3 After that run script in terminal

python3 classify_picamera.py \

--model /tmp/mobilenet_v1_1.0_224_quant.tflite \

--labels /tmp/labels_mobilenet_quant_v1_224.txt

4 You should see something like that.

Simple image classification on raspberry pi from pi-camera (in live time) using the pre-trained model mobilenet_v1 and TensorFlow Lite (Photo,GIF by Author) https://github.com/oleksandr-g-rock/simple_object_detection/blob/main/1_hd7sGxFw7fwPRoCiDQXvSQ.gif

Result:",https://medium.com/@oleksandrg/simple-image-classification-on-raspberry-pi-from-pi-camera-in-live-time-used-the-pre-trained-d855b509a1ca,['Alex G.'],2020-12-14 03:28:43.587000+00:00,146,"Hardware preparation, Software preparation, Tensorflow Lite, VNC, Image Classification"
SafeAI Teams Up with Obayashi for Autonomous Construction Pilot,"This fall, we’re excited to announce the upcoming launch of an autonomous construction site and our growing partnership with Obayashi Corporation. In collaboration with Obayashi, we will kick off this November with a pilot program in which a Caterpillar 725 articulated dump truck will autonomously complete a vital on-site function: load-haul-dump cycles.

Obayashi is the Japanese-based global leader of connected construction solutions. By teaming up with one of the largest names in construction, we’re demonstrating industrial autonomy in a real-world situation and setting a standard for future autonomous heavy equipment. This pilot follows Obayashi’s investment in SafeAI in November 2019.

Construction represents one of the largest sectors in the global economy. According to the Association of General Contractors, construction creates nearly $1.3 trillion worth of structures each year in America. But the industry also faces a slew of urgent safety concerns (one in five worker deaths last year were in construction), increasing labor shortages, missed deadlines, and rising costs — and COVID-19 has only accelerated these bottlenecks.

Autonomous heavy equipment delivers a safer, more productive solution for the construction industry. By retrofitting existing fleets of industrial equipment with its autonomy platform, the SafeAI team is propelling the industry forward as efficiently and seamlessly as possible. In doing so, we’re also paving the way for the future of autonomous technology outside of on-road vehicles.

“We are investing in autonomous solutions that we believe will play a critical role in our success both in the near term and also down the road. SafeAI has built a powerful autonomous ecosystem that ushers in safer, more productive worksites,” said Hiroto Sato, COO of Obayashi SVVL (Silicon Valley Ventures & Laboratory). “Throughout our partnership, we look forward to building operational and organizational structure to support this pilot and more deployments like it in the future.”

The importance of this pilot is two-fold. There’s a critical need for autonomy outside of on-road vehicles, and this demonstration is a turning point for real-world application of autonomous technology. Beyond that, this also indicates the beginning of a larger shift in the construction industry — one that involves an increasing interest in connected technologies and commitment to more innovative solutions.

We’re here to deliver. To learn more about our technology and what we’re doing, please visit https://safeai.ai/. For more information about partnering with us, click here.",https://medium.com/@safeai/safeai-teams-up-with-obayashi-for-autonomous-construction-pilot-a4338ed8822f,[],2020-10-22 15:02:46.794000+00:00,381,"autonomousconstruction, Obayashi Corporation, Caterpillar725, connectedconstruction, industrialautonomy"
Yandex.Rover Now Makes Restaurant Deliveries Within Yandex.Eats,"Self-driving technologies are slowly but steadily becoming an integral part of our lives as they begin to solve an increasing number of different tasks. Many services can greatly benefit from using these technologies. At Yandex, we have a whole ecosystem of services the goal of which is to quickly deliver groceries, restaurant meals and different goods to our customers. These services are already starting to take advantage of self-driving technologies. Now customers who order restaurant food in the Yandex.Eats app in Russian cities of Moscow and Innopolis can select driverless delivery from these restaurants. Yandex.Eats is more than just an app, it is a major food and grocery delivery service in Russia with more than 30,000 merchants across 166 cities. Orders made with its help can now be delivered by the Yandex.Rover delivery robot.

In Moscow these deliveries were first launched in the White Square (Belaya Ploshchad) area in central Moscow which is where offices of major Russian and international companies, as well as some of the city’s most popular cafes and restaurants, are located. Customers can choose delivery by one of our robots in the Yandex.Eats app. The rover will pick up their order from the restaurant and take it to a specific location. Our users can track the location of the rover and the status of their order within the app. To receive it, customers need to simply unlock the rover from their smartphone and lift the top. As more restaurants join the driverless delivery network, robot deliveries will grow in popularity and reach.

“In the last few years we’ve been seeing a constant growth in demand for delivery services and the events of 2020 have only advanced this trend. This is why now we have robots coming to people’s rescue. They will take on part of the delivery load which will allow us to retain delivery speed as the demand for our services grows. Rovers can deliver restaurant meals, groceries, orders from online stores and will slowly but steadily become a normal part of city life,” said Dmitry Polishchuk, CEO, Yandex Self-Driving Group.

In addition to operating in Moscow, rovers now roam the streets of Innopolis, a hi-tech innovation hub some 450 miles east of Moscow. In this tech hub with unique infrastructure, rovers don’t just work with restaurants but have become a part of the city’s everyday life. Innopolis has hired Yandex.Rover to help people living there with their daily tasks. The driverless robots already deliver restaurant meals and will soon begin delivering groceries and orders from online stores. What’s more, Innopolis residents get to use these robots free of charge.

Innopolis is famous for cold and snowy winters with rainy and windy periods between seasons. Yandex.Rover is powered by the same technology as our self-driving cars that are familiar with a wide range of bad weather conditions, which account for over half of our testing time.

A testing ground for countless new technologies, Innopolis also welcomed Europe’s first robotaxi service by Yandex more than two years ago. Yandex’s self-driving vehicles can be booked from the company’s ride-hailing app, Yandex Go, and will carry passengers within the city limits. Since the launch of Yandex’s driverless taxi service in 2019, it has seen more than 12,000 bookings. And now Innopolis residents and visitors can also enjoy Yandex’s autonomous delivery service. We hope that they will enjoy a hot lunch delivered to their doorstep on a cold winter day, quickly and contactlessly.",https://medium.com/yandex-self-driving-car/yandex-rover-now-makes-restaurant-deliveries-within-yandex-eats-ce7efecd1af8,['Yandex Self-Driving Team'],2020-12-09 14:31:57.038000+00:00,569,"Self-Driving Technologies, Yandex, Yandex.Eats, Yandex.Rover, Robotaxi"
Bonus Episode: Live from BETT 2019,"Bonus Episode: Live from BETT 2019

What will it take to inspire the next Ada Lovelace?

Whilst we are busy working on season 3 we wanted to share this conversation recorded live at BETT 2019.

Subscribe on Apple Podcasts, Google Play, Stitcher, Soundcloud, TuneIn, or RadioPublic.

Titled ‘If She Can See It, She Can Be It’, host Anjali Ramachandran chats to Chiin-Rui Tan founder and CEO of Rho Zeta AI, and Elena Sinel Founder of Teens in AI, and asks what will it take to inspire the next Ada Lovelace or Rosalind Franklin? What do we need to do to transform fields like AI and help create a more fair future of work. And how can educators help to recognise, develop and promote the female innovators and leaders of tomorrow?

Anjali Ramachandran — Ada’s List

Elena Sinel — Teens in AI

Chiin-Rui Tan — Rho Zeta AI

We’ll be releasing more bonus episodes throughout March and April and season 3 will be landing this summer.",https://medium.com/nevertheless-podcast/bonus-episode-live-from-bett-2019-28132b34844f,[],2019-03-08 12:05:21.620000+00:00,157,"BETT2019, Ada Lovelace, Rosalind Franklin, AI, Female Innovators"
On-demand machine learning predictions for mapping tools,"Over a year ago, we published ML Enabler — a machine learning integration tool in partnership with the Humanitarian OpenStreetMap Team. ML Enabler is a registry for machine learning models in OpenStreetMap and aims to provide an API for tools like Tasking Manager to directly query predictions. Today, we want to share some of the new and most exciting features of ML Enabler, including on-demand machine learning predictions and a user interface.

Managing models, predictions and infrastructure

ML Enabler makes it incredibly easy to spin up infrastructure to run your model along with all necessary resources. Through the new user interface, you can upload new models, spin up AWS resources, generate and preview predictions. Previously, there was a minimal CLI tool to upload models and fetch predictions.

ML Enabler Project Overview

ML Enabler Project Page

Behind the scenes, ML Enabler uses AWS Cloudformation and will work with any AWS account. A few key infrastructure choices like instance count, and concurrency can be made directly from the ML Enabler interface. ML Enabler uses lambda functions for downloading base64 images for inference from the specified Tiled Map Service (TMS) endpoint and writing inference outputs into the database.

BBOX Inference Submission

Running Inference Stack

You can monitor the tile prediction queues right from the UI. When the processing is complete, predictions are automatically displayed in the map tab. It’s easy to toggle between different classes in your model, and filter predictions based on confidence threshold. Over each tile, the model’s raw output and the confidence score is displayed. This makes it really convenient to explore spatial patterns within the inferences.

Inference Results Page

On-demand predictions

ML Enabler generates and visualizes predictions from models that are compatible with Tensorflow’s TF Serving, on-demand. All you need is to drag and drop a zip with trained classification or object-detection model, provide a TMS end point, and an AOI for the inference. ML Enabler will spin up the required AWS resources and runs inference to generate predictions. Running a classification model inference over a medium sized city, which is divided into approximately 4,000 zoom 18 tiles takes approximately 2 minutes.

The prediction tiles are indexed using quadkeys for easy spatial search. To help facilitate these on-demand predictions ML Enabler has integrated in many of the components of Development Seed’s Chip-n-Scale project.

Support for classification and object detection models

Currently, ML Enabler supports two common machine learning model formats — classification, and object detection. ML Enabler works with binary as well as multi-label classification models. The infrastructure setup and prediction visualization adapts automatically based on the model format. For object detection models, ML Enabler converts coordinates in the pixel space to geographic space for every prediction along with bounding box and confidence score.

New Prediction Page

Additionally, for classification models, ML Enabler supports a custom lambda function to create supertiles. Supertiles are incredibly useful to overcome objects that may lie across tile edges. For example, zoom 18 tiles offers a higher resolution than zoom 17, but one draw-back is that sometimes buildings get split between multiple tiles. Supertiles allows for the aggregation of the four zoom 18 tiles within the zoom 17 footprint to create a (512, 512, 3) training chip, instead of the typical (256, 265, 3) training image chip.

Inference Validation

Inference Validation

Inference Validation

Collecting feedback about predictions and retraining

Another exciting feature we added to ML Enabler is the ability to collect feedback about predictions from within the interface. Users can tag a tile as valid or invalid. Predictions tagged as valid switch to green, predictions tagged as invalid switch to white, and predictions that haven’t been manually validated stay red.

Inference Validation

ML Enabler can then convert these validated predictions back into labeled training data matched up with imagery to allow users to easily re-train a new model with the validated model predictions.

Future

We think that it can make the integration between mapping tools and model infrastructure easy and less intimidating. There are an increasing number of internal and partner projects that rely on ML Enabler and we will continue developing and maintaining.

Some of our immediate plans include ability to run inference over imagery sources other TMS, automate re-training workflows, and the ability to add more detailed model metadata to the registry. We hope you get the chance to experiment ML Enabler for yourself. Please reach out with any questions or comments on Github or Twitter!",https://medium.com/devseed/on-demand-machine-learning-predictions-for-mapping-tools-700035affc86,[],2020-08-05 18:35:42.860000+00:00,709,"MLEnabler, Managing Models, Predictions Infrastructure, Object Detection Models, Inference Validation"
Computer Vision menggunakan Transfer Learning di Keras,Open to New Opportunity | Background in PHP Programming | Beginner in Data Science,https://medium.com/@raihanrnj/computer-vision-menggunakan-transfer-learning-di-keras-6015c51c016c,['Raihan Nugroho Jauhari'],2020-11-19 04:44:49.757000+00:00,14,"php, programming, data science, beginner, open opportunity"
K- Means Clustering Explained | Machine Learning,"Before we begin about K-Means clustering, Let us see some things :

1. What is Clustering

2. Euclidean Distance

3. Finding the centre or Mean of multiple points

If you are already familiar with these things, feel free to skip to K-Means algorithm

What is Clustering ?

Figure 1.1

Clustering is nothing but grouping. We are given some data, we have to find some patterns in the data and group similar data together to form clusters . This is the basis of clustering.

This is done with the help of euclidean distance.

for example:

1. An athletic club might want to cluster their runners into 3 different clusters based on their speed ( 1 dimension )

2. A company might want to cluster their customers into 3 different clusters based on 2 factors : Number of items brought, no of items returned ( 2 dimensions )

What is Euclidean Distance ?

Distance between 2 co-ordinates can be found with the help of the euclidean distance. If the data is 2 dimensional and is in a plane , n will be 2 in the above formula and can be represented as (x,y) . If , it’s 3 dimensional data , n will be 3 and can be represented as (x,y,z).

Finding Centre or Mean of Multiple Points

Figure 2.1

Now, Consider the black points in the figure 2.1.

we need to find the centre of all the black points.

we know that this is 2 dimensional data as it has an x and y and is represented as (x,y)

In Order to find the centre , this is what we do.

1. Get the x co-ordinates of all the black points and take mean for that and let’s say it is x_mean.

2. Do the same for the y co-ordinates of all the black points and let’s call that y_mean.

3. Now, the centre point will be nothing but (x_mean,y_mean) which will result in that red polygon present at the centre of the black points

with this , we have successfully completed the pre requisites for K Means Clustering.

K-Means Clustering

What is K-Means Clustering ?

It is a clustering algorithm that clusters data with similar features together with the help of euclidean distance

How it works ?

Let’s take an example dataset for this purpose

Figure 3.1

Figure 3.1 represents our Data Points.

Now. Let’s say, we need to group these data into 2 categories.

Just by looking at this , we can say that the data points whose

x < -2 can be grouped together and x > -2 can be grouped together.

Similarly , if we want to group these data into 3 categories , we can say that the data on the left side can be grouped together, data on the middle can be grouped together, data on the right can be grouped together like shown in figure 3.2. Initially, we don’t know how many cluster should we form with the given data. I’ll get on to this later.

Figure 3.2

Sorry for the colour of the data points in the middle (it’s white and it almost matches with the background )

Now, let’s Implement K Means on the given data

Initialise the centroids(c1) randomly to some data points in the dataset ( Number of cluster centroids = Number of clusters you want to make ). Mapping points to centroids : For each and every data in the dataset, calculate which centroid is closest to it out of all the centroids available and then that data point would be considered as a part of the particular centroid to which it is closest to (You can give different colours to each centroid and the data point belonging to the centroid will also have the same colour ) Moving centroids to the centre : For all the points that is part of a particular centroid , calculate the centre by using the method shown above and move the centroid of those points to that centre. ( Do this for all the centroids )

4. Now, As we changed the position of the centroids , the data points need to mapped to the centroids based on the new position of the centroid . So, Repeat steps 2 and 3 for some number of iterations until all the centroids stops moving.

figure 3.4

If you still don’t understand , Take a look at figure 3.4 and read the above 4 steps again.

It consists of 3 east steps : Initialise, Map Points to Centroids , Move Centroids to means of all the points and repeat this process until no changes occur.

Now, if I perform these processes on the above data shown in figure 3.1. I will end up getting this graph, where the black points are the centroids for each cluster

figure 3.5

Notice that this is could be a little different from what we humans would anticipate. But, It’s fine. This is how K-Means Clustering works

One More Note

As I have already said, we don’t know the exact value for the number of clusters to choose. But, There’s a way called Elbow Method, using which we can approximately do this. This will be covered in a Separate blog post.

Figure 4.1

Figure 4.1 would be the representation of clusters if we chose the number of clusters to be 2 for the above data.

The initialisation point of the centroid also has some amount of say in the resultant clusters we get. Since we initialise the centroids randomly, It is good to run the algorithm multiple times and plot the graph and then choose the majority voted result.

Conclusion

This might seem a little bit difficult to understand and the beginning. But, This is one of the most easiest algorithm out there and cluster analysis are used in many areas. Some of them are : Recommender Systems , Pattern Recognition and also in Image Processing.

Thank You",https://medium.com/analytics-vidhya/k-means-clustering-explained-419ee66d095e,['Ashwin Prasad'],2021-06-21 16:09:58.702000+00:00,935,"k-means clustering, clustering, euclidean distance, finding mean of multiple points, elbow method"
Keras Image Classifier on AWS Sage-Maker,"I still remember I used to hate Neural Networks subject in my engineering course but after spending 7 years into the software industry and seeing various use cases I decided to give it a go again. Yeah, I know you may call it a desire to solve business problems or just an interest in technology.

Recently after spending nearly 6 months, I got my Professional certification on AI & Machine Learning. I really like how the course was designed and I got a chance to know more deeply about ‘Deep Learning & Neural Networks’ with libraries such as PyTorch, Google’s TensorFlow, and Keras. Among all these Keras is the one which interests me more and I guess why because their tag line is “Keras is an API designed for human beings, not machines.”

Today, we will build a simple fully custom image classifier, using a transfer learning mechanism that you can very easily lift and try using yourself for your own projects. Whether you’re totally new to the platform or have played with the Sage-Maker before, you should walk away from this article equipped with the knowledge necessary to run your own custom machine learning jobs and endpoints on top of AWS (Because I really like AWS!)

The objective of this article is to show how to create and train a simple Keras model to classify an image between two classes of X-Ray: Normal and Pneumonia.",https://pranavk7.medium.com/keras-image-classifier-on-aws-sage-maker-a86669c5a6e6,['Pranav K'],2020-06-21 16:23:28.495000+00:00,233,"Neural Networks, AI, Machine Learning, Deep Learning, Py Torch"
Analytics at Netflix,"Netflix is one of the world’s leading internet TV service, whose business model is based on subscription rather than advertising which is typical of traditional television practice. Netflix’s profit is directly proportional to the number of its subscribers. In order for content license payment to be profitable, optimum viewing rates needs to be ensured by Netflix.

Netflix has a large user base of 180 million worldwide streaming subscribers, which enable Netflix to gather a tremendous amount of data. With Netflix brimming with data; it uses data analytics to its advantage in order to gain insights into their customers. This enables Netflix to optimize their marketing and deliver a better product to the customer. Analytics provide Netflix with quantitative data that they need to make better and more informed decisions for their user.

Netflix gathers user data on the basis of age, gender, location, their taste in media etc. By gathering data about consumer interaction Netflix can dive right in the minds of the user to know what they might like to watch even before they finish a show or movie. Netflix knows the time and date a user watched a show, the device used, if the show was paused, does the viewer resume watching after pausing? Do people finish an entire TV show or not, how long does it take for a user to finish a show and so on. Netflix even has the screenshots of scenes users might have viewed repeatedly, the rating content is given, the number of searches and what is searched for. With the help of Data Analytics, Netflix gives meaning to the data collected.

In addition to collecting data on subscriber actions, Netflix also encourages feedback from subscribers in the form of thumbs up/thumbs down system to encourage audience engagement.

Netflix has developed recommendation algorithm which suggests TV shows and movies based on user’s preference. The recommendation algorithm strives to help find a show or movie for the user with minimal effort. It takes into account users interaction with Netflix, other members having similar tastes and preferences and information about titles. Recommendation algorithm accounts for over 80% of the content steamed on the platform.

Netflix tries to retain its subscriber base with the help of the recommendation algorithm. Netflix through the data collected knows how much subscriber need to watch monthly for them to be less likely to cancel the subscription. For example, if Netflix has the data that a user is 75% less likely to cancel Netflix subscription if it watches the content for at least 15 hours a month and if the viewing hours drop below 5 hours then there is 95% chance of the user to cancel their subscription; then Netflix will try to incorporate mechanism which ensure that the user watches at least 15 hours. To ensure that the user continues to watch the show Netflix uses post-play, which automatically plays the next episode of the TV show unless the user opts out of it.

Licensing movies/shows can become expensive sometimes, so Netflix uses data to decide which movie to license. Netflix usually looks for those titles that deliver the biggest viewership relative to the licensing cost. This in turns means that Netflix forgoes title that aren’t quite watched relative to their cost. For this, Netflix ascertains through data the types of movies/shows people enjoy watching.

Data analytics also plays an integral role when the content creators come up with an idea about a new show or a movie. There are loads of data of the content created previously and how users perceived the previous content. Data Analytics helps to suggest possible solutions to the many challenges faced in the planning phase. These challenges could include identifying the shoot locations, time and day of the shoot and more. This enables Netflix to save a significant amount of effort in planning and also reducing cost.

Judicious use of Data Analytics is the main reason for the success of Netflix. Big Data plays a critical role in not just deciding the functioning of Netflix but also presents newer opportunities to grow. Big Data and Data Analytics are such an integral part of Netflix that we may call Netflix as Data Analytics company than a media company. Though still Netflix does not take all its decision based on Big Data as it still relies on human inputs for a lot of decisions. But Netflix’s success highlights the value of data analytics as it provides incredible insights into user’s preferences which allow them to make smart decisions and deliver maximum ROI on their choices.",https://medium.com/techno-economics/analytics-at-netflix-f62385c6ca26,['Keshav Aggarwal'],2020-12-15 12:43:49.970000+00:00,751,"Netflix, Data Analytics, Big Data, Subscription Model, Recommendation Algorithm"
DATA TYPES IN PYTHON PROGRAMMING A computer is an electronic device that is capable of storing and…,"DATA TYPES IN PYTHON PROGRAMMING

A computer is an electronic device that is capable of storing and processing data, usually in binary format. The computer does this by means of stored programs within it. Since the basic raw material that the computer works with is data, it is essential that the data is categorized or classified in a way that tells the processor what data to expect and the amount of memory space to reserve for it’s storage. This is where a data type comes in.

A data type defines the values that an expression (such as a variable or a function) might take. It also defines the operations that can be done on the data, and the way the values of that data type can be stored. It informs the program translator how the programmer intends to use the data. This is important as it makes it easier for the programmer or any other computer professional to focus on the main concern of solving larger problems rather than getting involved in the details of data description and access.

To this end, data types are of two categories. They are primitive data types and derived data types. The primitive data types are the fundamental or built-in data types supported by almost every programming language. On the other hand, the derived data types are usually user-defined. They are formed by combining one or more basic data types. They can be used when there is need to customize a data type or extend the functionality of built-in data types already available.

However, our focus will be on the primitive data types available in Python. They are of four types: integers, floating point, strings and booleans.

1. Integers: An integer data type represents numeric data. It can hold positive and negative whole numbers. Prior to Python 3, the integer data type had two sub-types; integer and long. However, from Python 3, integers are of unlimited size. Integer operations include arithmetic operations (such as addition, subtraction, multiplication, division and modular arithmetic) and logical or comparison operations.

2. Floating points: These are used to hold fractional numbers; that is numbers that contain a decimal point. Floating point numbers can be expressed in two ways: they can have decimal points in them (such as 2.1 and 7.234) or they could be written as exponentials- that is, numbers in standard form. For instance a floating point number written as 4E2 is the same as four times ten raised to power 2.

3. Strings: This data type holds a collection of alphabets, words or other characters. They are created by enclosing a set of characters within single or double quotes. Examples of data of string type include ""Python"" or ""5/7 Kujama Street"". Some of the operations that can be carried out on strings in Python include concatenation of two or more strings using the ""+"" sign, repetition of strings (by ""multiplying"" the string by a certain number), slicing and finding substrings.

4. Booleans: This data type can take up two values only. These are ""True"" (corresponding to 1) and ""False"" (corresponding to 0). They are useful in comparison and conditional operations.

Being that both Boolean values represent numbers, it is noteworthy that the boolean data type is a sub-class of the integer data type. Therefore, arithmetic operations carried out on booleans are evaluated based on what integer value each of them represent.

Determining the data type of a variable in Python is done by using the type() function. The variable is enclosed within the brackets of the type function and the data type of the variable is outputted by the translator program.",https://medium.com/@onyeevee/data-types-in-python-programming-a-computer-is-an-electronic-device-that-is-capable-of-storing-and-b19fb7f0e288,['Onyinye Okpoko'],2020-12-18 19:54:07.867000+00:00,594,"Data Types, Primitive Data Types, Derived Data Types, Integers, Floating Points"
How to Build a Reporting Dashboard using Dash and Plotly,"A method to select either a condensed data table or the complete data table.

One of the features that I wanted for the data table was the ability to show a “condensed” version of the table as well as the complete data table. Therefore, I included a radio button in the layouts.py file to select which version of the table to present:

Code Block 17: Radio Button in layouts.py

The callback for this functionality takes input from the radio button and outputs the columns to render in the data table:

Code Block 18: Callback for Radio Button in layouts.py File

This callback is a little bit more complicated since I am adding columns for conditional formatting (which I will go into below). Essentially, just as the callback below is changing the data presented in the data table based upon the dates selected using the callback statement, Output('datatable-paid-search', 'data' , this callback is changing the columns presented in the data table based upon the radio button selection using the callback statement, Output('datatable-paid-search', 'columns' .

Conditionally Color-Code Different Data Table cells

One of the features which the stakeholders wanted for the data table was the ability to have certain numbers or cells in the data table to be highlighted based upon a metric’s value; red for negative numbers for instance. However, conditional formatting of data table cells has three main issues.

There is lack of formatting functionality in Dash Data Tables at this time.

If a number is formatted prior to inclusion in a Dash Data Table (in pandas for instance), then data table functionality such as sorting and filtering does not work properly.

There is a bug in the Dash data table code in which conditional formatting does not work properly.

I ended up formatting the numbers in the data table in pandas despite the above limitations. I discovered that conditional formatting in Dash does not work properly for formatted numbers (numbers with commas, dollar signs, percent signs, etc.). Indeed, I found out that there is a bug with the method described in the Conditional Formatting — Highlighting Cells section of the Dash Data Table User Guide:

Code Block 19: Conditional Formatting — Highlighting Cells

The cell for New York City temperature shows up as green even though the value is less than 3.9.* I’ve tested this in other scenarios and it seems like the conditional formatting for numbers only uses the integer part of the condition (“3” but not “3.9”). The filter for Temperature used for conditional formatting somehow truncates the significant digits and only considers the integer part of a number. I posted to the Dash community forum about this bug, and it has since been fixed in a recent version of Dash.

*This has since been corrected in the Dash Documentation.

Conditional Formatting of Cells using Doppelganger Columns

Due to the above limitations with conditional formatting of cells, I came up with an alternative method in which I add “doppelganger” columns to both the pandas data frame and Dash data table. These doppelganger columns had either the value of the original column, or the value of the original column multiplied by 100 (to overcome the bug when the decimal portion of a value is not considered by conditional filtering). Then, the doppelganger columns can be added to the data table but are hidden from view with the following statements:

Code Block 20: Adding Doppelganger Columns

Then, the conditional cell formatting can be implemented using the following syntax:

Code Block 21: Conditional Cell Formatting

Essentially, the filter is applied on the “doppelganger” column, Revenue_YoY_percent_conditional (filtering cells in which the value is less than 0). However, the formatting is applied on the corresponding “real” column, Revenue YoY (%) . One can imagine other usages for this method of conditional formatting; for instance, highlighting outlier values.

The complete statement for the data table is below (with conditional formatting for odd and even rows, as well highlighting cells that are above a certain threshold using the doppelganger method):

Code Block 22: Data Table with Conditional Formatting

I describe the method to update the graphs using the selected rows in the data table below.",https://medium.com/p/4f4257c18a7f#ec15,['David Comfort'],2019-03-13 14:21:44.055000+00:00,668,"data-table, python, pandas, doppelganger columns, conditional formatting"
Spotify Music Data Analysis: Part 3,"Music Attributes

The dataset gathered in the first article of this series contains the song attributes. Before performing data analysis it is necessary to understand those features individually.

The song attributes in the dataset are explained below:

Tempo: The tempo of the song. The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, the tempo is the speed or pace of a given piece and derives directly from the average beat duration.

Energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. Higher the value more energetic the song.

Danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. The value ranges from 0 to 1. Higher the value more suitable the song is for dancing.

Loudness: Loudness values are averaged across the entire track. It is the quality of a song. It ranges from -60 to 0 DB. Higher the value, the louder the song.

Valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).

Liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides a strong likelihood that the track is live.

Acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.

Speechiness: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audiobook, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.

Mode: Songs can be classified as major and minor. 1.0 represents major mode and 0 represents minor.

Key: Key is the pitch, notes or scale of song that forms the basis of a song. 12 keys are ranging from 0 to 11.",https://medium.com/analytics-vidhya/spotify-music-data-analysis-part-3-9097829df16e,['Pragya Verma'],2021-07-20 05:39:04.836000+00:00,387,"music attributes, tempo, energy, danceability, loudness"
Mapping the World — Part 4: SOSNet to the Rescue!,"Our previous blog post introduced Apophis, our lightning-fast non-linear optimization library that is used for solving large-scale problems commonly found in Structure-from-Motion (SfM). One of the core steps of SfM is inferring the relative camera motion between two frames, which is done with a technique called “Image Matching”. To perform image matching at the scale and quality that is required for our Vision Engine, we had to create a more robust matching method, without sacrificing computational efficiency.

In this post, we introduce SOSNet, which will be presented as an oral at CVPR 2019, and leads to significant improvements compared to the state-of-the-art methods. We built SOSNet by making neural networks go from learning similarities to learning similarities between similarities.

However, before digging into SOSNet and taking a look at how vastly it improves the accuracy of reconstructed 3D maps, it is useful to describe the problem of image matching.

Recovering relative camera motion

For humans, it is relatively easy to estimate how a camera moved between two images. Given the example images below, imagine that we are standing in a way that we observe scene A. To get to scene B, we should take a few steps to the left and then rotate our head right. On the other hand, to go from scene A to C we need to move right and then tilt our head left.

Recovering the relative camera motion between two images allows us to build large scale 3D maps of the world.

Recovering relative camera motion from image matching

In the pixel space where cameras live, we need to identify a set of point-to-point correspondences, in order to recover relative camera motion between two images. This was originally described in a 1981 article published in Nature from Christopher Longuet-Higgins. In particular, through the magic of two-view geometry, we can estimate a mathematical representation of the camera motion, which is known as the Essential Matrix.

Fun fact: One of Christopher Longuet-Higgins’ PhD students is Geoffrey Hinton, who later went on to recieve the Turing Award for his contibutions on neural networks that helped spawn the “deep learning era” in computer vision. It’s this same era of deep learning that led to our SOSNet method.

We can recover the relative camera motion between two images, by finding lots of correspondences between specific areas on the left image, with the “matching” areas of the right image. This is the process that is called “image matching”.

In theory, the well known eight-point algorithm, requires just 8 point-to-point correspondences between the two images to recover the relative camera pose. In practice, however, things are not that ideal.

The algorithm assumes that the matching process is done without errors, something that is not realistic. Thus, a common practice is to find as many correspondences as possible, which can then be used to recover the relative camera pose. The classical pipeline can be summarised as: Detect, Describe & Match.

Detect, Describe & Match",https://medium.com/scape-technologies/mapping-the-world-part-4-sosnet-to-the-rescue-5383671713e7,['Vassileios Balntas'],2019-06-14 11:25:31.266000+00:00,479,"Image Matching, Structure-from-Motion (SfM), Relative Camera Motion, Eight-Point Algorithm"
"How TikTok users are projecting their realities, with joy","When you first open the app, TikTok is madness. What greets you is an unfiltered stream of videos that the system decides people like you, the user, are interested in — based on your location and other videos with which you’ve interacted.

There is no warning. You’re pulled into a bizarre stream — rakhi videos with a Dr. Dre soundtrack; two guys playing cops and robbers to a song from the Dilwale Dulhaniya Le Jayenge soundtrack; a girl playing with mehndi.

TikTok has an estimated 200 million users in India, of whom 120 million are active every month. This means a line-up of unexpected acts. Creators put in an impressive degree of inventive thinking and physical effort into creating their content –jumping over fences; doing wheelies on bikes; bringing in grandmothers and other family members to join the performance; offering a self-deprecating comment on their own bodies and rituals.

The videos on the platform offer a poignant mix of candour and artifice. Often, content creators don’t try to hide context signals, like their neighbourhood or the décor in their home. This is a contrast from the content on many other video-driven social media content in which details of everyday life are usually concealed. Listen in on TikTok conversations in research forums, and you’ll hear commentators deliberate on this content barrage. There is a moral panic surrounding the platform. The concerns — pornography, hate-speech, catfishing, fake news, hackers — aren’t new. Every platform has been accused of it, but with TikTok, there is a layer of disgust and resentment, which perhaps reveals more about the commentator than the content creator. Whether it’s a comment like “Isn’t it interesting that you can’t tell she’s uneducated or from a poor background?” or “What does it mean sociologically when boys cry on TikTok?”, there is a level of condescension to the analysis.

The remarks on the simple-minded gaucheness of the creators hints at a demarcation of in-group and out-group identities, of identifying an ‘intrusion’ of Otherness into their social feeds. But if you manage to break out of the careful bubble that TikTok curates for you, you realise a whole different world of videos exist. A world of merriment and carefree, light humour that offers you glimpses into farmers’ kids dancing next to crops, girls watching the sun set over a mosque, boys going to salons and setting their hair artfully on fire — an infinite cabinet of curiosities.

TikTok captures a beautiful moment in time. The dopamine hits of views and likes are now freely available to newer population segments. While there is darkness in TikTok (as many news reports show us), there is also tremendous joy.

We had a machine look at these TikTok videos, focusing on elements such as images, objects, and colours within the frame. We then looked for these patterns across all the other cultural models that we have for India.

Scanning through videos — such as one that shows a man flipping naan set to the soundtrack of lively, pop music — the machine classified these videos as “celebrations”, grouped together with occasions like weddings, festivals, and graduations. These weddings, festivals and graduations were not taking place in TikTok. To put it simply, the machine “thought” that the TikTok video was actually closer to an actual celebration, than it was to anything else that exists in culture.

Remove the human bias that evaluates these elements as “other”, and most of the videos essentially communicate one thing — festivity. Characters in the most everyday of backdrops (such as the roadside) are being classified as being in celebrations of sorts because of their physical movements, colourful clothes, and high-spirited background music connoting jubilation.

This conversion of mundane and everyday life into a performance that is full of joy is something worth acknowledging. While commentators speak about abuse and cruelty as a consequence of TikTok, we need to also consider this: about 120 million individuals trying to craft, in some shape or form, a shift in the repetition and dryness of everyday life. They are creating and consuming content that celebrates, laughs and connects.

It’s a magical time to be a new user of the Internet, especially an Internet that infuses the everyday with something unexpected. The next time you walk by people filming TikTok videos, pause and watch what they’re doing. They are trying to shift reality. Why shouldn’t we all dance with our grandmothers?

If you would like to find out more about what we do, please contact anurag.banerjee@quilt.ai

This article was first published in the Hindustan Times on Aug 21, 2019:

Read more stories on Quilt.AI.",https://quilt--ai.medium.com/how-tiktok-users-are-projecting-their-realities-with-joy-8feac1a692eb,[],2020-09-18 08:33:52.419000+00:00,755,"Tik Tok, Social Media, Internet, Videos, Content Creators"
Simple Data Engineering to Improve Your Machine Learning Results,"Disclaimer: the following content is largely thanks to this amazing notebook. Make sure to check it for implementation details!

Rank Gauss

Various, numerous, and many machine learning techniques, such as PCA, actually rely on the assumption that the underlying data is normally distributed. With a dataset that is not normally distributed, the usefulness of such techniques will be negatively affected. RankGauss, a technique developed by Michael Jahrer in Porto Srguro’s Safe Driver Prediction, is a solution to such problem. I will not dive into the math behind it (because I am not good at Math!) but the purpose of RankGauss is to transform a non-normal distribution to a normal distribution, and it usually works better than standard standardization. Let us see the effect first!

Here is the distribution before applying RankGauss:

Pre-RankGauss Distribution

Here is the distribution after applying RankGauss:

Post-RankGauss Distribution

As you can see, the feature distribution has been normalized after applying RankGauss.",https://ai.plainenglish.io/simple-data-engineering-to-improve-your-machine-learning-results-ab99f635582b,['Kr. Wan'],2020-11-02 07:41:59.345000+00:00,148,"Rank Gauss, Machine Learning, PCA, Normal Distribution, Standardization"
VIDEO TOURS 360,"World’s First & Only Virtual Tour Builder With Built-In Live Video + Ecom + Gamification + A.I

Video 360 Tour Builder With Built-In LIVE VIDEO CHAT, Ecommerce & Gamification — Create & Sell Virtual Video Tours To Your Clients In Minutes.

Now create 360° Virtual experiences for your business (and for your clients) that engage visitors with 360 DEGREE VIDEOS of your business/product with INTERACTIVE HOTSPOTS… allowing visitors to get more details and even BUY directly from inside your video (i.e. turning your video tour into a LIVE ecommerce Store with 24/7 Live Video Chat Facility)

Answer Questions & Close Prospects Via Live Chat While They Take The Virtual Tours (360 degree videos).

Lockdowns are back… and the demand for 360 degree videos is rapidly growing with every business literally needing them to stay in business.

Leverage the ‘Zero touch’ trend in the new post covid economy where customers don’t want to leave their homes.

Social Distancing is the NEW NORMAL

And with VideoTours360 you can create beautiful and highly-engaging 360 videos in just a couple of minutes — WITHOUT any sort of special skills or knowledge.

Drag & Drop Unlimited Hotspots With Ease

Live Video Calls

Gamification

Ecommerce Engine

Lead Generation

Special Bonus: Zero to Profits

VR Agency Accelerator

With the VideoTours360 app, you can “create & sell” virtual video tours to clients in just minutes. “But how do you get clients?” is a very BIG QUESTION you’ve always been left to figure out all by yourself.

How VideoTours360 Compares To The Rest

Users get access to create tours with video calls, ecommerce & gamification. 20 tours, 100 products, 1000 minutes video chat

FOR MORE DETAILES VISIT OFFICIAL WEBSITE",https://medium.com/@jeeva-criminology/video-tours-360-f136afafde97,[],2020-12-23 03:28:29.214000+00:00,262,"Video 360 Tour Builder, LIVE VIDEO CHAT, Ecommerce, Gamification, Interactive Hotspots"
"To smarter emails & efficient service, presenting the AI case","I am sure you’ve all been hearing about artificial intelligence. You see it on new devices, the cool Alexa that you can speak with, a phone support system that’s instantly capable of listening & responding to your voice, and of course the media and investment community has been crazy looking for the next big thing. Us being us, we thought we’d try and make some sense of it in the marketing and sales function.

I went about looking for the latest innovations in some of the most established tacts of digital marketing and here’s what I found:

· Email marketing

Email marketing has been an ongoing topic for a while, but we haven’t seen the true value it can provide until the rise of AI in the email marketing analytics space. A notable service we found was Nova. By utilising AI to scrape through a person’s online identity, it generates a personalised paragraph that sales representatives can add to their sales proposition. How does it work? Dump a batch of email addresses in, as well as the text of your pitch. Nova then screens the contacts and pulls information from sources published publically online and on social media accounts to create a personalised pitch. That’s great right?

· Customer service

Now, when you think of customer service, do you picture a bot serving you? Or the real question these days is, do you prefer it? A study of 5,000 consumers worldwide, conducted by LivePerson, showed that more than 50% of consumers preferred a human representative, and found only 38% of those surveyed had positive perceptions of this technology. They also found that some factors such as country and industry, had an effect on the receptiveness of consumers to these technologies.

Additionally, the nature of customer conversations for industries are inherently different. The fast food industry only really need to engage in simple conversations with their customers. Dominos for example, implemented a chatbot feature, “DRU” for their customers to easily choose their pizza base, toppings, dressing and sides, then order it. This was objectively efficient, and even impressive. Was it a success? YES. Chief executive Don Meij even stated, after realising the benefits of AI, that they are beginning to shift the philosophy of the company from “mobile first” to “AI first”. New initiatives are expected to come out such as drone deliveries, a Facebook chat that helps consumers find vouchers and coupons, and soon enough- DRU manager which helps Domino store owners automate rosters and order stock.

However, chatbots won’t be so easily implemented for customers asking about, say, life insurance. Again, the nature of the questions and conversations are important. In saying this, it’s also important to ensure that the tone and intonation of the chatbot is reflective of the brand. Amazon’s Alexa is a good example of this. She was friendly the majority of times, but there were few times the chatbot was perceived as judgemental. Another tribute to DRU’s success was that it conveyed the Domino brand well, and built a closer connection with the customer than the point-and-click interface.

As I continue on my journey to explore the applications of Artificial intelligence and how it can help make a real difference, I will be coming back soon with more interesting technologies we’ve tested and enjoyed.",https://medium.com/drizzlin/to-smarter-emails-efficient-service-presenting-the-ai-case-b4810cc4fe0,['Andrea Virrey'],2017-12-15 12:24:37.842000+00:00,541,"Artificial Intelligence, Email Marketing, Customer Service, Chatbots, Alexa"
The Art of Feature Engineering,"One of the easiest ways to experiment with feature engineering is by using PolynomialFeatures available through Sci-Kit preprocessing module.

There are two important parameters to be aware of with PolynomialFeatures.

degree = you have the option of setting to which n degree of features you want to engineer. For instance, setting the degree = 2 means that the PolynomialFeatures will multiply each features by itself and against every other features. I personally find that interactive terms to the 2nd degree is sufficient when running linear regression but you can certainly explore by putting in a pipeline so you can GridSearch to find the most optimal n hyperparameter value. include_bias = You want to set this to False when using regular LinearRegression. Setting this to True essentially includes the y-intercept column which you do not want to do when running a regular LinearRegression out of sci-kit’s linear_model module.

One of the downsides of using PolynomialFeatures is the fast expansion of columns if you have a lot of features to begin with. One of the measures that you can take to reduce the number of features is by measuring the R2 score against the target variable. This way, you can quickly eliminate features that do not have strong predictive power.

Another method that I like to visit is to run the OLS summary using statsmodels.api module, which is another way you can run OLS in Python.

With OLS, unlike sklearn’s LinearRegression, you just have to remember to manually add in the y-intercept. After you fit the data, you can run the summary() method which produces the summary below.

P>|t| column provides the p-value of each features which can also can be used to determine the statistical significance of each features. So if you have features with p-value > 0.05, you can eliminate those from your feature selection and those with p-value < 0.05, you can keep them as part of features for your model.

Feature engineering is an exciting way to enhance your model performance and definitely requires some good discernment and experience to become more intuitively better at this. In many ways, it truly is an art. It requires some level of creativity, willingness to experiment, and explore features from many different angles.

As we all strive towards becoming better data scientists, I hope we all continue to develop and hone our skills in feature engineering.",https://medium.com/swlh/the-art-of-feature-engineering-5a09bd475198,['Young Park'],2021-01-11 08:19:41.749000+00:00,387,"polynomialfeatures, featureengineering, degreehyperparameter, include_bias, R2score"
The easy way to predict stock prices using machine learning,"The easy way to predict stock prices using machine learning

The step by step you need to predict the stock price of companies, and please read the disclaimer.

In this post, I show the step-by-step method of making stock price predictions using the R language ,and the H2o.ai Framework. Please understand that this article is only a simple demonstration on how to get start using the H2O.ai Framework. It’s not a financial advise. Don’t take any financial decision based on this post.

The Framework is also available in Python, however, as I am more familiar with R, I will present the tutorial in that language. You may have already asked yourself: how to predict the stock price using artificial intelligence? Here are the steps to do it:

Collect the data Import the data Clean and manipulate the data Split test and training observations Choose a model Train the model Apply the model to the test data Evaluate the results Enhance the model if necessary Repeat step 5 to ten until you are satisfied with the result.

In the last post, I showed how to plot high-frequency data using the Plotly library, and I explained how to collect the data to perform the analysis. Let’s skip straight to step 3 on our list, if you want to know how to do steps 1 and 2, visit the previous publication.

Our research problem is this: “What will be the closing value of the asset in the next hour?”

Data cleaning

After we have imported the asset data that we want to make the predictions using MetaTrader, we need to change some variables. First, we define the names of the variables:

#seting the name of variables col_names <- c(""Date"", ""Open"", ""High"", ""Low"", ""Close"", ""Tick"", ""Volume"") colnames(data) <- col_names head(data)

Our data will take the following form:

Data — Image by Author

We will only use some of the available variables: Open, High, Low, Close and Volume. That way, we will eliminate the others.

data$Date <- NULL

data$Tick <- NULL

Since we want to know the closing price for the next observation, we need to shift the following values to a row above. To do this, we create a function and create a variable in the original dataset with the new data:

#shifting n rows up of a given variable shift <- function(x, n) { c(x[-(seq(n))], rep(NA, n)) } data$shifted <- shift(data$Close, 1) tail(data)

Data — Image by Author

Notice that we have the values of the variable, Close, allocated one row above. With that, we have a NA in the last line, we use the na.omit () function to omit that line:

#remove NA observations data <- na.omit(data) write.csv(data, ""data.csv"")

Perfect, we have our data ready to start modeling.

Splitting the data

In this problem, we will use a package called H2O.ai, which offers us a complete solution for analyzing and training artificial intelligence models. Its user-friendly structure allows people without a data science background to solve complex problems. Let’s start by loading the library into our environment:

#Installing the package

install.packages(""h2o"") #loading the library

library(h2o)

Once installed and loaded, we start our virtual machine that will serve as a basis for building our model. When starting the virtual machine, we must set the desired number of cores and memory parameters:

#Initializing the Virtual Machine using all the threads (-1) and 16gb of memory

h2o.init(nthreads = -1, max_mem_size = ""16g"")

Importing the data:

h2o.importFile(""data.csv"") h2o.describe(data)

Data into h2o — Image by Author

We now define which variable we want to predict in our data set and those that will serve to “teach” our model.

y <- ""shifted"" #variable we want to forecast

x <- setdiff(names(data), y)

Then, we split the data into training and testing at a proportion of 80% for the training data.

parts <- h2o.splitFrame(data, .80) train <- parts[[1]]

test <- parts[[2]]

Having the data divided, we go to the part where the magic of the H2O.ai package happens. There is an important trick here on splitting the data (The method shown here is biased), however we will get to this issue on the next article.

Choosing the model

One of the tasks that every data scientist needs to perform when creating his Machine Learning projects is identifying the best model or set of models to make his predictions. This requires a lot of knowledge, especially from a strong mathematical base, to decide the best ones for specific tasks.

Thanks to the H2O.ai package, we can ask it to choose the best model for us while taking care of anything else. This is called Automodeling. Obviously, this magic may not be the most robust solution to problems, but it is a good start.

Train the model

To create our model, we call the automl function and pass the necessary parameters as follows:

automodel <- h2o.automl(x, y, train, test, max_runtime_secs = 120)

After a few minutes, we obtain a list of models in order of performance. To learn more about them just call:

automodel@leader

Model Description — Image by Author

Apply the model

Now that we have our leader let’s apply it to the test data !! This is the coolest part, as we will use data not yet observed by the model to evaluate performance.

We call the predict function with the model and test data as parameters!

predictions <- h2o.predict(automodel@leader, test)

Conclusion

In this post, we saw how to handle and manipulate the financial data of an asset and easily create a machine learning model to make predictions of closing prices in the hour following the analysed data.

On the next article we’ll see how bad this model performed on the test data.

See you next week!

Disclaimer: This article is not an investment recommendation or anything like that. Forecasting stock prices is not a trivial task and this post is simply a demonstration on how easy is using the H2O.ai framework to start solving machine learning problems. It’s easy to make predictions, however it doesn’t mean that they are correct or accurate. And no, I don't have any connection with the company.",https://pedrolealdino.medium.com/the-stupidly-easy-way-to-predict-stock-prices-using-machine-learning-dbb65873cac8,['Pedro Lealdino Filho'],2020-07-06 14:28:33.007000+00:00,957,"machine learning, stock prices, prediction, R language, H2o.ai Framework"
Deep Learning,"Deep Learning is cutting-edge topic in the world of Machine Learning and AI. This post will provide basic information about Deep Learning. This includes answers to the below questions

How does human brain understand, process and learn that a human eye visualize? What are limitation of regular computer programs? What is Neural Networks/Deep Learning? What kind of problems does Deep Learning solve?

First, let’s understand what the visual cortex of the human brain do.

Visual Cortex of Human Brain:

The visual cortex of the brain is the area of the cerebral cortex that processes visual information received from the eyes. Complex mechanisms from retina to different visual areas allow us to read these lines. The visual system is inevitable for the way we interact with our surroundings as majority of our impressions, memories, feelings are bound to the visual perception. Visual area constitutes about 25 % of the cortex in humans with approximately 5 billion neurons. The study of the visual cortex has revealed many of these visual regions such as V1, V2, V3, V4 and MT on the basis of their anatomical architecture, topography and physiological properties [1]. These regions are involved in processing of multitude of informations (shape, orientation, color, movement, size etc) resulting from the visual pathways, thus making up an image applied to retina.

Fig 1. Organization of Visual System

Scientists and biologists discovered thirty different cortical areas that contribute to visual perception. The primary areas (V1) and secondary areas (V2) are surrounded by many other tertiary or associative visual areas such as V3, V4, V5 (or MT) involved in processing various attributes of trigger features [2, 3]. Areas V3 and V3A are selective to the form of stimuli [4], and neurons of area V4 are selective to colors [5]. Area V5 or MT (middle temporal) is an area where majority of cells are sensitive to motion and direction, and none of which are selective to color [6].

The parallel organization of visual system is involved in the establishment of two major visual pathways: Ventral and dorsal pathways which are indispensable for the object recognition [7, 8]. Fig 1 illustrates the parallel organization of visual system, two major pathways: Green part corresponds to the ventral pathway in the cortex ending in the temporal lobe [7, 9]. It is involved in the processing of information on the characteristics of the objects (shapes, colors, materials), that is, object recognition including faces. Orange part corresponds to the dorsal pathway in the cortex ending in the parietal lobe [7, 9]. This path is associated with spatial vision (action/location) of objects, and is involved in processing of action in space.

Computers:

Computers and computing help us achieve more complex goals than a human alone. Despite, there are many goals that are beyond the scope of computing because of one major limiting factor, computers could only follow the specific instructions they were given. Solving problems with programming requires writing specific step-by-step instructions for a computer to follow. We call these steps; algorithms. This constraint limited the type of problems where computers could help us to those where we:

Understand how to solve the problem Can describe the solution with clear step-by-step instructions that a computer can understand

Let’s address this limitation by attempting to write a traditional program to solve an object detection problem. What steps will you write to detect a red car from a fleet of multicolored vehicles?

Neural Network:

Limitation of traditional computer program and the structure of the human brain inspires a Neural Network to solve classification and detection problems. Neural Network is essentially a Machine Learning model (more precisely, Deep Learning) that is used in unsupervised learning. A Neural Network is a web of interconnected entities known as nodes wherein each node is responsible for a simple computation. In this way, a Neural Network functions similarly to the neurons in the human brain. More precisely Deep Learning allows computer to learn from examples.

Solving problems with deep learning requires identifying some pattern, finding examples that highlight both sides of the pattern (the input and the output), and then letting a “neural network” learn the map between the two. This opens the types of problems where computers can help us to those where we have:

Identified a pattern within a problem Data that exemplifies the pattern

Once those conditions are met, the next step would be to build the skills and workflows to solve problems with deep learning. In the forth coming posts, I’ll describe how to solve the some specific problems using Deep Learning.

References",https://rnalakurthi.medium.com/deep-learning-74818e64cdcc,['Radhakrishna Nalakurthi'],2020-12-01 23:37:02.212000+00:00,741,"deeplearning, machinelearning, AI, visualcortex, neuralnetworks"
How To Request Command-Line Input in Python,"If you are writing command-line Python scripts, then you will need to request input from the user at some point. In Python, the mechanism for requesting command-line values is the input() function.

Let’s go over how to use input() and make sure you’re prepared to avoid three common pitfalls.

The input() Function

Fortunately for new Python enthusiasts, the input() function is straightforward, accepting only one optional argument. The single argument is the text that will be shown on the command-line as the terminal waits for the user’s input. As the user, you can type in your value and submit by pressing ENTER.

input(""Enter your name: "")

Now, the code we have above is incomplete because we aren’t storing the user’s value. We can fix that by assigning the return of input() to a variable.

name = input(""What is your name: "")

print(name)

That’s really all there is to using the input() function. While the function itself is very simple, there is nuance in implementing input() in a script so that it does not become a liability.

Below are three common pitfalls that new coders fall into. Fortunately for you, we’ll avoid them by reading ahead.

1. Requesting Numeric Input

We asked the user for a text value in our earlier example, but what happens when we need a numeric value?

price = input(""Enter the price: "")

total = price * 1.06 print(""The total price is"", total)

The problem here is that input() always returns a string value, even if the value is numeric, even if the value is empty by pressing ENTER without typing anything.

To request numeric user input we can wrap the input() function with int() or float() depending on the type of value we want.

price = float(input(""Enter the price: ""))

total = price * 1.06 print(""The total price is"", total)

2. User Input Causing Value Errors

Setting a proper data type is only one step to properly implementing user input. Let’s say we want user input but the value that is actually entered causes an error. What do we do?

total = float(input(""How much did you pay? ""))

units = int(input(""How many did you purchase? "")) print(""The unit price is"", total/units)

Before dismissing the sequence in the screenshot, always remember how easy it is to make a typing mistake. We strive to code in a way that is error-proof.

Use try/except statements to solve for erroneous values.

try:

total = float(input(""How much did you pay? ""))

units = int(input(""How many did you purchase? ""))

print(""The unit price is"", total/units)

except Exception as e:

print(""Error:"", e)

A try/except statement will attempt to execute all commands in the try block. If an exception is encountered, then the except block will be executed. Aside from being able to report on the exception, the biggest advantage here is that your script will not stop executing from the error.

3. Continually Requesting User Input

We’ve learned how to convert our user input as well as account for erroneous values. But when we receive a bad input, most likely you’ll want to re-prompt the user.

The input() function does not have a built-in “retry” mechanism, so we will use a while loop to continually request user input until it is valid. There are many different ways to implement a while loop for continuous user input. We’ll use an infinite while loop with a break statement as a generic solution.

total = float(input(""How much did you pay? "")) while True:

try:

units = int(input(""How many did you purchase? ""))

if(units > 0):

break

else:

print(""Invalid input"")

except Exception as e:

print(""Error:"",e) print(""The unit price is"", total/units)

In the code example above, we‘ve put all our strategies together. We convert our input to an integer, which is wrapped by a try/except which is wrapped by an infinite while loop.",https://medium.com/code-85/how-to-request-command-line-input-in-python-80f45e9032fe,['Jonathan Hsu'],2020-04-16 13:06:00.983000+00:00,592,"Python, Command-Line Scripts, Input() Function, Numeric Input, Value Errors"
NLP Metrics Made Simple: The BLEU Score,"That’s where BLEU, which stands for Bi-Lingual Evaluation Understudy, kicks in. It’s a popular and inexpensive way to automatically measure the performance of your MT model. In a nutshell, BLEU compares the machine’s translation — what’s known as the candidate translation — with existing human-generated translations, known as the reference translations.

The easiest way to explain BLEU is by example. And for that we’ll look at a classic maxim from the Hebrew Bible — וְאָהַבְתָּ לְרֵעֲךָ כָּמוֹךָ — which depicts in three words the principle known as the Golden Rule: treat others as you would like others to treat you 🤗. We use the Bible because it’s been translated many times and to many languages, so it’s easy to find reference translations. Also, most of the translations are in the public domain. The Bible is in fact the most translated book in history (even more than Harry Potter!). In our case, we chose the following three reference translations among the many English translations for our maxim:

R1: but thou shalt love thy neighbor as thyself

thy neighbor as thyself R2: but have love for your neighbor as for yourself

for your neighbor as for yourself R3: but love your neighbors as you love yourself

And here is our candidate translation for which we want to compute the BLEU score:

D: but love other love friend for love yourself

As you can see, our MT system is not very good 😔…

BLEU works by computing the precision — the fraction of tokens from the candidate that appear, or are “covered”, by the references— but with a twist. Like any precision-based metric, the value of the BLEU score is always a number between 0 (worst) and 1 (best). Let’s compute BLEU for our candidate translation. In our example, the candidate consist of 8 words: but love other love friend for love yourself. Had none of the words appeared in any of the references, the precision would have been be 0/8=0. Luckily most of them appear in the references. It’s easy to see that 6 of the 8 words, but love love for love yourself — that is, all words except other and friend — appear in at least one of the references. One is thus tempted to compute the precision as 6/8 = 0.75. But if you look carefully, you will see that the word love appears 3 times in the candidate, yet it appears at most twice in any single reference (it appears twice in R3 and once in R1 and R2 ). BLEU takes this into account — it penalizes words that appear in the candidate more times than it appears in any of the references. Why? Well, the rationale is: since love appeared at most twice in any reference translation, it’s reasonable for it to appear up to 2 times also in the candidate. Thus, up to 2 love words are covered. Any excess usage of love in the candidate doesn’t make sense and so it is not considered as covered. And because the 3rd love is not covered, only 5 of the tokens are covered — but love love for yourself — and we get a BLEU score of 5/8=0.675.

Imagine for a moment that we didn’t have this correction; an 8-word candidate translation like love love love love love love love love would then get a perfect BLEU score of 1, since love appears in a reference translation. With BLEU, we would consider only the first 2 love’s as covered and discard the other 6. The BLEU score, in this case, is just 2/8 = 0.25, indeed indicating very low precision (albeit lots of love!).",https://towardsdatascience.com/nlp-metrics-made-simple-the-bleu-score-b06b14fbdbc1,['Boaz Shmueli'],2021-02-08 12:02:01.746000+00:00,599,"machine translation, natural language processing, automatic evaluation, BLEU score, precision metric"
How to Automate Google Sheets with Python,"How to Automate Google Sheets with Python

How to use pygsheets python package to play around google sheets and to automate.

Google spreadsheets are easy to maintain, edit, and share with people with python package pygsheets. I have been using pygsheets for long time to automate my daily work in google spreadsheets.

pygsheets is a simple intuitive python library to access google spreadsheets through the Google Sheets API v4.

Automating Google Sheets with python is not as hard as climbing Mount Fuji. 😉

A picture from my Mount Fuji trekking.

Everyone knows what google spreadsheets are and how to use them. In this article , we will learn how to play around google spreadsheets with python. So, without further ado, let’s start.

Installation

pip install pygsheets

Get client secret

Obtain OAuth2 credentials from Google Developers Console for google spreadsheet api and drive api and save the file as client_secret.json in same directory as project. See complete guide here.

Authorization

import pygsheets

gc = pygsheets.authorize()

# Use customized credentials

gc = pygsheets.authorize(custom_credentials=my_credentials) # For the first time, it will may produce as a link to authorize

Open spreadsheets and worksheets

Google spreadsheets can be opened by name, id , and link. Worksheets can be accessed by name or index.

How to open a spreadsheet and worksheet with pygsheets.

Playing around spreadsheet

Authorize and open a spreadsheet

import pygsheets

gc = pygsheets.authorize() sh = gc.open('medium') # Open a spreadsheet with name 'medium'.

Get spreadsheet title

sh.id # Returns id of spreadsheet

Get spreadsheet id

sh.title # Returns title of spreadsheet

Get spreadsheet url

sh.url # Returns url of spreadsheet

Check last update

sh.updated # Returns date and time of last update

Delete spreadsheet

sh.delete() # Delete spreadsheet

Get worksheets info

sh.worksheets() # Return information of worksheets

Share spreadsheet

sh.share('example@gmail.com', role='commenter', type='user', emailMessage='Here is the spreadsheet we talked about!') sh.share('', role='reader', type='anyone') # Make public

Remove permissions

sh.remove_permission('example@gmail.com', permission_id=None) # You can specify permission id

Add new worksheet

sh.add_worksheet('sheet3',rows=250, cols=20)

Delete worksheet

sh.del_worksheet('sheet3')

Playing around Worksheet

Open a worksheet

wk1 = sh.sheet1 or wk1 =sh[0] # Open first worksheet

Get title, id, and url of worksheet

wk1.title # Returns title of worksheet

wk1.id # Returns id of worksheet

wk1.url # Returns url of worksheet

Get rows and cols count

wk1.rows # returns number of rows

wk1.cols # returns number of columns

Get cell object and cell value

wk1.cell((row_number,col_number)) # Returns cell object

wk1.cell((row_number,col_number)).value # Returns cell value as string

Get value/values/records

wk1.get_value('A1') # Returns A1’s value

wk1.get_value('A1', 'B2') # Returns list of values

wk1.get_all_values() # Returns list of all values in worksheet

wk1.get_all_records() # Returns a list of dictionaries

Example of get_all_records

Update value/values

wk1.update_value('A8', '40') # Updates A8 with 40

or

wk1.update_value('A8','=A6+A7',True)#Updates A8 with sum of A6 and A7 wk1.update_values('A8', [['G',40]]) # Updates values in starting from A8

Get rows or columns

wk1.get_row(row_number) # Returns a list of all values in a row

wk1.get_col(col_number) # Returns a list of all values in a column

Add/delete rows and columns

wk1.add_rows(n) # Add n rows to worksheet at end

wk1.add_cols(n) # Add n columns to worksheet at end wk1.delete_rows(n) # Delete last n rows of worksheet

wk1.delete_rows(n) # Delete last n columns of worksheet

Insert rows and columns

wk1.insert_rows(row =1, number = 2) # inserts 2 new rows after 1st row wk1.insert_rows(row =1, number = 1, values =['AA', 40]) # insert 1 new row and insert values in same row wk1.insert_cols(col =6, number = 2) # inserts 2 new columns after 6th column wk1.insert_rows(col=6, number = 1, values =['AA', 40]) # insert a new column and insert values in same column

Update row and column

wk1.update_row(row_index, values, col_offset =0) # Updates values in a row from 1st column Example: >>> wk1.update_row(9, ['H', 45, 178, 81]) wk1.update_col(col_index, values, row_offset=0) # Updates values in a column from 1st row Example: >>> wk1.update_col(9, [78, 45, 178, 81])

Adjust width of column and height of row

wk1.adjust_column_width(start=0, end=3, pixel_size=50) # Updates column size to 50 pixel wk1.adjust_row_height(1,10, pixel_size=50) # Updates row height to 50 pixel

Resize and clear worksheet

wk1.clear('A9') # Clear all values starting from A9

wk1.clear('A9:D10') # Clear values in grid range A9 to D10 wks.resize(num_rows, num_cols) # Resize to given dimension

Add pandas dataframe to worksheet

wk1.set_dataframe(df, 'A9')#Inserts df in worksheet starting from A9 # Note: set copy_head =False if you don't want to add first row of df

Get worksheet values as pandas dataframe

wk1.get_as_df() # Returns a pandas dataframe of worksheet # Note: You can specify start and end to get specific range data

Example of get_of_df()

Add chart to worksheet

>>>wk1.add_chart(('A1', 'A6'), [('B1', 'B6')], 'Age Chart')

<Chart COLUMN 'Age Chart'>

Chart added to worksheet.

I guess, you have learned enough to play around google spreadsheets with python. Many more operations can be performed with pygsheets package. Please, see documentation of pygsheets to learn more operations.

Thank you for reading this article. Read my other Medium Articles here.

Reach out to me on LinkedIn, if you have query.

Reference: https://pygsheets.readthedocs.io/en/latest/index.html",https://medium.com/game-of-data/play-with-google-spreadsheets-with-python-301dd4ee36eb,['Dayal Chand Aichara'],2019-07-19 03:40:55.858000+00:00,733,"Google Sheets, Pygsheets, Python, Automation, Google Spreadsheets API v4"
Stock Prediction Using Linear Regression,"Does it work?

Simple Linear Regression

I will briefly touch on simple linear regression in this post, but I do have an article specifically about simple linear regression using Python that can be found here and it may be a bit more detailed and helpful.

Linear regression can be used to find a relationship between two or more variables of interest and allows us to make predictions once these relationships are found. In simple linear regression, there are only two variables: one dependent variable and one independent variable.

Simple linear regression will provide a line of best fit, or the regression line. This regression line can be written as the following formula:

y — The independent variable

— The independent variable ⍺ — The constant/y-intercept

— The constant/y-intercept β — The beta coefficient (slope)

— The beta coefficient (slope) ε — The error term/residuals

Stock Prediction

Stock price data is notoriously difficult, or impossible, to predict. With that being said, lets try.

We will be using SPY for this example. SPY stands for the SPDR S&P 500 Trust ETF and is designed to track the performance of the S&P 500 market index.

First, we will start with importing the packages that we will need for this analysis.

import pandas as pd

import yfinance as yf

import numpy as np

import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegressionv

Then we can use yfinance to get our data and quickly visualize the closing price data.

data = yf.download('SPY')

plt.figure(figsize=(10, 5))

data['Close'].plot()

The index of our data contains the date for each closing price. To make things easier, lets just move the dates into their own column using the .reset_index() function provided by Pandas. We can then make input data a NumPy array and reshape it to the required input shape for the linear regression. Our output data will be the closing price and we will store that in the y variable.

data = data.reset_index()

x = np.array(data.index).reshape(-1, 1)

y = data['Close']

Now we can use Scikit-learn’s LinearRegression model and fit it to our input and output data. To get the regression line, the .predict() will be used to get the model’s predictions for each x value.

linreg = LinearRegression().fit(x, y)

linreg.score(x, y)

predictions = linreg.predict(x)

We can plot the actual closing price data and then plot the regression line on top of it.

plt.figure(figsize=(15,5))

plt.plot(data['Close'])

plt.plot(data.index, predictions)

In this specific example, using SPY, we can see that while the regression line is not exactly accurate in exact closing price values, it does capture the general trend of the data.

We can extend the regression line to view where the model predicts the SPY closing price will end up in the future.

We can also check the R² score by using the .score() function. This R² value of about .76 is pretty good and means our model did pretty well explaining the variance in the data.

print('R^2:', linreg.score(x, y)) R^2: 0.7562986239574692

Limitations

Simple linear regression will not make you millions of dollars. Relying on this strategy will most likely make you lose significantly more than you would win. This linear regression model will fail if any random and unpredictable event occurs.

Not every stock has a steady linear increase. To illustrate this point, let's look at Nikola Corp. and their closing price along with the regression line.

NKLA Closing Price

This does not look to promising. Furthermore, we can look at Apple Inc, which has had great success. Apple’s stock is growing exponentially and simple linear regression will not be able to capture this.",https://medium.com/analytics-vidhya/stock-prediction-using-linear-regression-cd1d8351f536,['Aidan Wilson'],2020-12-09 17:41:11.741000+00:00,549,"AAPL Closing Pricesimple linear regression, stock prediction, stock price data, linear regression model, NKLA closing price"
K-Means Vs kNN,"What’s the contrast of ‘ k ’ in k-Means and kNN ?

Often times, k-Means and kNN algorithms are interpreted in same manner although there is a distinct difference between the two. Today, we look into the major contrasts in implementing these algorithms.

k-Means

k-means clustering algorithm is one of the popular unsupervised machine learning algorithm where similar data points are grouped together to discover underlying patterns with no fixed labelled outcomes.

k in k-Means

We define a target number k, which refers to the number of centroids we need in the dataset. k-means identifies that fixed number (k) of clusters in a dataset by minimizing the within-cluster variances.

Every data point in the dataset is allocated to it’s nearest cluster while keeping the centroids as small as possible.

‘K ‘in k-means is the number of clusters the algorithm is trying to learn from the given data

The best value for k is chosen from elbow and silhouette methods

kNN

k-nearest neighbors is a supervised classification/regression algorithm where a bunch of labelled points are used to determine the class of other points.

‘k’ in k-NN is the number of nearest neighbors used to classify (or predict in case of continuous variable) a test observation sample

In k-NN classification, the output class is determined by the plurality vote of it’s neighbors, with the object being assigned to the class most common among it’s k nearest neighbors In k-NN regression, the output is the average of the values of k nearest neighbors

In the above figure, the green dot could be assigned to either red class or blue class depending on whether the value of k is 3 or 5 respectively.

The optimal value of k is chosen through various hyperparameter optimization techniques

For high dimensional data, feature extraction and dimensionality reduction techniques such as PCA, LDA or CCA are used as a pre-processing step, followed by k-NN in the reduced-dimension space

Wrapping up

k-means is commonly used in scenarios like understanding population demographics, market segmentation, social media trends, anomaly detection, etc.. where the clusters are unknown.

k-NN, on the other hand, is used to classify and predict data where the target variable is usually known before hand.

Cheers! Now we understood the difference between k-Means and k-NN !!",https://medium.com/@sravanroy/k-means-vs-knn-56d17d5c7223,['Sravan Roy'],2019-11-03 13:54:28.527000+00:00,358,"k-Means, kNN, k-means clustering, supervised classification/regression, elbow and silhouette methods"
Big Data Drives the Need for Frequent Ad-Hoc Reporting,"Big Data Drives the Need for Frequent Ad-Hoc Reporting

Businesses no longer have to go on gut instinct- they can use data and analytics to make faster decisions and more accurate forecasts.

Data, data, and more data.

According to McKinsey, “organizations now have troves of raw data combined with powerful and sophisticated analytics tools to gain insights that can improve operational performance and create new market opportunities. Most profoundly, their decisions no longer have to be made in the dark or based on gut instinct; they can be based on evidence, experiments, and more accurate forecasts.”

Businesses no longer have to go on gut instinct; they can use data and analytics to make faster decisions and more accurate forecasts supported by a mountain of evidence.

Analytics dexterity translates to an advantage within industry competition. Leaders that continuously make data-driven decisions stake out large returns.

Where does ad-hoc reporting come in?

According to TechTarget, ad-hoc reporting refers to a process that is “designed to answer a single, specific business question. Users may create a report that does not already exist or drill deeper into a static report to get details about accounts, transactions or records.”

Ad-hoc reports are created for one-time use. Today’s volumes of data and modern tools make it possible for employees to analyze data on an as-needed basis to answer specific business questions. Instead of waiting for scheduled reports, this allows business queries to be answered on-the-spot.

Sometimes specific business questions need to be answered quickly. With ad-hoc analysis, decision makers obtain insights more rapidly, allowing them to make decisions flexibly and with as accurate information as possible.

David Landers, a partner at Deloitte Tax LLP, explains how using ad-hoc reporting in internal tax departments helps users reply more quickly to internal reporting requests without relying on IT.

Ad-hoc vs. structured reporting

Ad-hoc analysis in business intelligence sits in stark contrast with the managed reports seen in the early days of business analytics, which relied on templates distributed by IT departments. Ad-hoc analysis lets users decide which data sources to pull from, as well as how they want to represent it.

Ad-hoc reports also differ from structured reports when it comes to:

Configuration

As the name suggests, structured reports are produced using a formalized reporting template. On the other hand, ad-hoc reports vary in format as they are audience-specific.

Quantity of data leveraged

Structured reports leverage large volumes of aggregated data. On the other hand, ad hoc reports are generated based on smaller amounts of data as needed, given that they address specific questions.

Sudden dip in production? Unexpected bottlenecks? Unexpected costs? With ad-hoc analyses you can look into, examine, and appropriately respond to sudden and unexpected events.

Too good to be true?

Ad-hoc reporting is not a magical cure-all. Looker reported on three quandaries that ad-hoc reporting alone often fails to address:

Incomplete data

Having some — but not all — of your data in one place is a limitation of ad hoc reporting. If you’re using siloed or extracted data, your reports could become stale and prevent you from seeing the full picture.

Lack of data governance

Data governance involves the people, processes, and technologies required to create consistent and proper handling of an organization’s data across the business. Ad hoc reporting may depart from company developed metrics, logic, or available data which may create insights that conflict with other reports.

Data availability

Everyone needs to be using the same underlying data. If the underlying data varies throughout the same organization, it can produce data chaos resulting in conflicting answers and delayed decisions.

Your best bet awaits

These issues can be averted by working with tools that already address them. To the best of my knowledge, the following self-service reporting tools can help:

Sisense claims to be empowering developers, data engineers, and business analysts to simplify complex data and transform it into powerful analytic apps. Sisense lets you easily transform big data from disparate sources into visual BI reports in a matter of minutes.

According to their site, DataRails is tailored for finance professionals and allows them to access and leverage financial and operational data independently, regardless of technical competency. This means the finance team can easily support planning, analysis and reporting needs on-the-go to drive faster and more effective decisions.

Dundas BI states that they “provide organizations with a single application for connecting, interacting, analyzing and visualizing any data, from virtually any data source, on any device.

Data is the Next Frontier for Competition.

More frequent reporting can accelerate optimal decision making, and on the spot reporting will pave the way for true business agility. Executives who are able to frequently and consistently get data-backed answers for even the smallest of questions with ad-hoc reports will see superior organizational decision-making.

Just make sure employees know what they’re doing.",https://medium.com/the-capital/big-data-drives-the-need-for-frequent-ad-hoc-reporting-bf027ba890c6,['Stefanie Duncan'],2020-01-19 00:28:53.311000+00:00,769,"Big Data, Ad-Hoc Reporting, Analytics, Data Governance, Self-Service Reporting"
Paper Review — End-to-End Detection With Transformers,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko

ArXiv link — https://arxiv.org/abs/2005.12872

Contributions

In this paper —

the authors present DETR — an object detection method based on transformers and bipartite matching loss that views the problem as a direct set prediction problem.

their method removes the need for hand-designed components that require prior knowledge about the task like non-maximum suppression and anchor generation.

the source code is made available at — https://github.com/facebookresearch/detr

DETR Architecture

DETR Architecture (src — https://arxiv.org/pdf/2005.12872.pdf)

The model consists of three components as shown in the above figure

1. CNN Backbone

The CNN takes in the image and learns a 2D representation of it by generating a low resolution activation map.

2. Encoder-Decoder Transformer

Architecture of DETR’s transformer (src — https://arxiv.org/pdf/2005.12872.pdf)

Each encoder layer follows the standard architecture of the transformer consisting of a multi-head self-attention module and a feed forward network.

The decoder also follows the standard architecture of the transformer consisting of multi-headed self- and encoder-decoder attention module

3. Feed Forward Network

3-layer perceptron with RELU activations, a hidden dimension d and a linear projection layer are used for the final prediction.

FFN predicts the normalized center, coordinates, height and width of the box w.r.t the input image

Linear layer predicts the class label using softmax function.

Approach

The backbone takes an image with 3 channels as input and outputs a lower-resolution activation map of it with 2048 channels

A 1x1 convolution reduces the channel dimension of this activation map creating a smaller dimension feature map. Then the spatial dimension of this feature map is collapsed since the transformer’s encoder expects a sequence as input.

A positional encoding is passed along with the collapsed feature vector as input to the encoder.

The decoder then takes in the output of the encoder along with object queries which are a small fixed number of learned positional embedding. They are added to the input of each attention layer

The decoder decodes N objects in parallel outputting N embedding, which are then independently passed to a shared FFN after normalizing them by a shared layer-norm.

The FFN independently predicts the class and bounding box (or no object) for each of those N decoder embeddings.

While using the whole image as context, the model globally reasons about all objects together using pair-wise relations between them by using self- and encoder-decoder attention over these embeddings.

An auxiliary decoding loss is used during training to help the model output the correct number of objects of each class.

A bipartite matching loss is used to train DETR.

Loss Function — Set Prediction Loss

It forces unique matching between predicted and ground truth boxes. DETR outputs a fixed-size of N predictions, where N is set to a number larger

than the typical number of objects in an image. The loss function produces an optimal bipartite matching between predicted and ground truth objects and then optimizes object-specific bounding box losses.

Step 1: Finding pair wise matching

All permutations of N elements are searched to find a bipartite matching between the predicted and the ground truth set of objects as seen below:

The matching cost takes into account both the class prediction and the similarity of predicted and ground truth boxes as seen below:

Step 2: Hungarian Loss

For all pairs matched in the previous step, hungarian loss is defined as a linear combination of a negative log-likelihood for class prediction and box loss.

Bounding box loss

Class prediction loss

Experiments

The authors show that their approach achieves competitive results compared to Faster R-CNN in quantitative evaluation on COCO dataset.

Comparison with Faster R-CNN (src — https://arxiv.org/pdf/2005.12872.pdf)

They also provide a detailed ablation study with insights and qualitative results for the architecture and loss function.

They also present results of extending DETR for the task of panoptic segmentation by training a small extension on a fixed DETR model.

DETR extended by a panoptic head (src — https://arxiv.org/pdf/2005.12872.pdf)

Results for panoptic segmentation generated by DETR-R101 (src — https://arxiv.org/pdf/2005.12872.pdf)

Short Comings

Though DETR seems to predict large objects accurately, it has trouble regarding training, optimization and performance on small objects.

References

https://arxiv.org/pdf/2005.12872.pdf

https://arxiv.org/abs/2005.12872

https://github.com/facebookresearch/detr

https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers",https://medium.com/swlh/paper-review-end-to-end-detection-with-transformers-f66258135c01,['Krut Patel'],2020-12-14 06:42:17.104000+00:00,639,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov"
Attention Please: Document Classification,"So, how does attention work? It’s one thing to look at a given sentence and say which words are important. However, this model obviously is useless if it’s not generalizable, so it needs to somehow learn the properties of words, as well as how these properties interact and which interactions result in significance.

Mathematical Depiction of Described Steps

Step One: Represent each word in the vocabulary as an embedding vector of N dimensions. This is a super common approach in NLP, more information here.

Step Two: Send each sentence of embedding vectors through a GRU. The GRU is going to have a hidden state in between each word. Typically for prediction, we only care about the final state, but for this model, we want to keep track of each intermediate state as well. Let hᵢ be the vector that represents the hidden state afterword i. Note that, while likely, not necessary, I followed the paper in using a bidirectional GRU. This means that the model runs through the sentence forward and backward. Each word i then have hidden states hᵢᶠ and hᵢᵇ, and we simply concatenate these two vectors into hᵢ and proceed.

Step Three: Feed each of the hᵢ through a fully connected linear layer, including a bias term. The paper recommends that the output size have dimension 100; I have not yet explored the efficacy of tweaking this hyper-parameter, although I think that could be an interesting research area. For each element in the resulting vector, take the tanh. Call this new vector uᵢ, again corresponding to word i.

Step Four: Send each of the uᵢ through another linear layer, this time without a bias term. This linear layer should have a scalar output, so now we have a single scalar value associated with each word i. Then apply the softmax function for each sentence; the scalars will sum up to one for each sentence. Let the scalar for sentence i be called αᵢ.

Step Five: We’re almost at prediction time. We now have for each word i in a given sentence, a vector hᵢ, and an importance scalar αᵢ. It’s crucial to understand here that these hᵢ vectors are different from the original word embeddings, as they have memory of the sentence in both the forward and backward directions. We take an element-wise weighted sum for all vectors in the review, call this review vector s.

Step Six: The function applied to s differs based on the objective of the model, but because this model is interested in document binary classification, I applied a final linear layer to the vector s, which returns a singular value p, the probability of belonging to class 1.",https://medium.com/towards-artificial-intelligence/attention-please-document-classification-7be927e758a,['Jon-Ross Presta'],2020-06-11 17:22:54.256000+00:00,440,"attention, NLP, GRU, embedding, binary-classification"
3 Python Packages Made For Data Science Teams,"Mito

Mito is a spreadsheet interface for Python. Many data science teams work with stakeholders who are more used to using Excel. Mito allows users to call a spreadsheet into their Jupyter environment — each edit they make in the spreadsheet will generate the equivalent Python in the code cell below

Here is a demo video:

Mito is not only powerful for Excel users to contribute to a data science team, it also a great tool for Python users. Mito is a faster way to generate code than typing it by hand.

In Mito, you can:

Import data frames or local spreadsheets

Create visualizations

Generate pivot tables

Filter and sort datasets

Edit specific cell values

Merge datasets together

Export data frames as Excel files

Automate data cleaning and data analysis processes

And more!

To install Mito, use these commands:

python -m pip install mitoinstaller

python -m mitoinstaller install

Then open Jupyter Lab and run these commands to call the Mitosheet.

import mitosheet

mitosheet.sheet()

Here is the Mito Team Page, where you can get installation/integration help and custom development, all for free.

Here are the full install instructions.

2. Plotly

Plotly is the best Python package for interactive visualizations. You can create UI components to your visualizations where users can change formatting, zoom in on specific pieces of data and more. The goal of many data science teams is to create usable data apps for end users. Visualizations is a key part of this, which Plotly excel’s at.

Here is a demo video:

They also allow users to generate AI and ML focused visualizations. This differentiates Plotly from other graphing packages.

Here is the Plotly Graphing Package documenation.

Here is a link to Dash, Plotly’s enterprise tool.

3. Streamlit

On the topic of building data apps for end users, Streamlit is an amazing end to end solution for this.

Streamlit allows teams to turn their data scripts into fully functional apps that end users (technical and non-technical) can engage with. Streamlit fully understands that data analysis needs to be presentable and understandable, and they provide an awesome platform to do it.

Here is the Data Professor giving a guide to using Streamlit:

Here is the Streamlit website:

Streamlit can be installed with these commands:

pip install streamlit

streamlit hello

I hope you find these packages helpful. I think more Python packages overall need to consider team use cases in their development. I come across many Python packages in my work that would be more useful if they things like sharing features, low-code access, and better support forums.

The packages in this blog are all working hard to make data science for teams an easier process.

In future blog, I hope to explore the different challenges that data science teams face and the work that is being done through different open source and enterprise tools to address them.

:)",https://medium.com/trymito/3-python-packages-made-for-data-science-teams-e3b48ae07c5,['Jake Mito'],2021-09-14 20:54:59.307000+00:00,438,"Mito, Spreadsheet Interface, Python, Excel, Plotly"
Learning To Rank,"RankNet

Model Target

Instead of modelling the score of each document one by one, RankNet proposed to model the target probabilities between any two documents (di & dj) of the same query.

And the target probabilities Pij of di and dj is defined as

1 if si > sj

0.5 if si=sj

0 if si < sj

where si and sj is the score of di and dj respectively

Let say for a particular query, there are 3 documents d1, d2, d3 with scores 0, 5, 3 respectively, then there will be 3 valid pairs of documents:

d1 & d2 with P12 = 0

d1 & d3 with P13 = 0

d2 & d3 with P23 = 1

So now each pair of documents serve as one training record to RankNet.

Model Architecture

In the RankNet paper, the author used a neural network formulation.

Let’s denote the neural network as function f, the output of neural network for document i as oi, the features of document i as xi. Hence we have oi = f(xi) and oj = f(xj). Also we define oij = oi - oj = f(xi) - f(xj) = -(oj - oi) = -oji.

Note that oi (and oj) could be any real number, but as mentioned above, RankNet is only modelling the probabilities Pij which is in the range of [0,1].

In order to model the probabilities, logistic function is applied on oij as below:

logistic function applied on oij

And cross entropy cost function is used, so for a pair of documents di and dj, the corresponding cost Cij is computed as below:

And the gradient is computed as

At this point, you may already notice RankNet is a bit different from a typical feedforward neural network.

While a typical neural network follows these steps to update its weights:

read input features -> compute output -> compute cost -> compute gradient -> back propagation RankNet update its weights as follows:

read input xi -> compute oi -> compute gradients doi/dWk ->

read input xj -> compute oj -> compute gradients doj/dWk ->

compute Pij -> compute gradients using equation (2) & (3) -> back propagation

So in RankNet, xi & xj serve as one training record, RankNet will pass xi & xj through the same the weights (Wk) of the network to get oi & oj before computing the gradient and update its weights.

Implementation using Keras

As described above, RankNet will take two inputs, xi & xj, pass them through the same hidden layers to compute oi & oj, apply sigmoid on oi-oj to get the final probability for a particular pair of documents, di & dj.

Thus, the model should look like this

This could be implemented using keras’s functional API as follows

Now let’s simulate some data and train the model

Now we could start training RankNet() just by two lines of code

first 5 epochs

Let’s plot the loss metric as well

cross entropy loss

As we can see, the loss of both training and test set decreased overtime.

Conclusion

In this post, I have gone through

how RankNet used a probabilistic approach to solve learn to rank

how to use gradient descent to train the model

implementation of RankNet using Keras’s functional API

In the future blog post, I will talk about

how to implement a custom training loop (instead of using model.compile and model.fit)

how to speed up training of RankNet

LambdaRank algorithm

Stay Tuned!",https://medium.com/swlh/ranknet-factorised-ranknet-lambdarank-explained-implementation-via-tensorflow-2-0-part-i-1e71d8923132,['Louis Kit Lung Law'],2021-02-21 12:32:19.264000+00:00,529,"Rank Net, Model Target, Probabilistic Approach, Gradient Descent, KerasAPI"
Yet another way to generate fake datasets in BigQuery,"Yet another way to generate fake datasets in BigQuery

Wrapping faker.js with a Javascript UDF

(Update 11/20/2020: Examples now use a generated a sequence table instead of using the wise_all_sky public dataset as a base table)

When creating demos and proofs of concept, I frequently run into the problem of finding suitable sample data to get started. BigQuery’s public datasets are a great resource, but rarely have everything that’s needed to build a proper environment.

Creating initial sample data has become a necessary and tedious process for many projects. Consequently fake data libraries, like faker.js or this one for python, have become popular in recent years. When using those libraries, I find myself slowed down by context switching between validating the generated data in BigQuery and tweaking the code I am using to import the data.

Here’s a quick tutorial on using a Javascript UDF with the faker.js library to generate billions of lines of fake data, in minutes, without leaving the BigQuery Console.

Generating the first record

The example below maximizes flexibility by using a temporary UDF that returns a struct type. The wildcard: ‘*’, used at the end of the select statement converts the function’s returned struct type into a normal table row. The temporary UDF can be quickly redefined to return any data within the scope of the faker.js library.

Single record output

BigQuery compatibility with large Javacript libraries can be finicky; a manually packaged webpack output for the UDF’s library can be found on GitHub.

Generating an arbitrary number of unique rows

In order for many rows to be created, the SQL select statement will need a table to select from; the row count of the result set will match the number of rows in the selected table.

In order to facilitate this capability you are going to generate a table containing an incremental sequence of values that can act as a random seed for faker to produce unique values.

Creating and using tables with billions of records can run up noticeable charges. Check out my earlier post on using Flex Slots to reduce your bill substantially, when working with large tables.

Generate 1 billion sequentially numbered rows in ~2 minutes

The row_seq table can be optionally seeded with 10’s of billions of records, enough for any use case. Additionally, by using the row number the generated results are both reproducible and unique.

Generate 100 million entities generated in ~ 4 minutes

A preview of the dataset.fake_accounts table shows each entity paired up with the seed that generated it. Any row can be reproduced by re-invoking the entity() function, i.e. select entity(30048188).* will reproduce Tiara Zulauf.

Fake entities reproducible by row_num

Additional Resources

Here are some additional resources that may help you.",https://medium.com/google-cloud/yet-another-way-to-generate-fake-datasets-in-bigquery-93ee87c1008f,['Patrick Dunn'],2020-11-20 19:59:05.324000+00:00,436,"Big Query, faker.js, Javascript UDF, Fake Data Generation, Proofof Concepts"
The average price of an avocado in the U.S.,"Grafiti

Grafiti is the first search engine for graphs & charts.",https://medium.com/grafiti/the-average-price-of-an-avocado-in-the-u-s-39c73c6c58f3,['Adriana Navarro'],2018-10-08 17:43:56.766000+00:00,10,"Grafiti, Graphs, Charts, Data Visualization, Data Analysis"
How is the current state of Big Data Analytics in Controlling?,"How is the current state of Big Data Analytics in Controlling?

The interest in using data leads to an increasing trend of adopting big data analytics to improve the decision-making process. Big data analytics is defined as “where advanced analytic techniques operate on big data sets”. Due to increasing competition in data-driven markets, firms are adopting state-of-the-art information technologies for competitive advantage.

The role of controlling or management accounting departments is directly affected by a big data analytics tool since controllers are the central unit of a company in terms of using the information to support management. One of the challenges is to gain insights from this ubiquitous amount of information. The received insights are then used to make decisions and to adjust organizational processes to generate value.

“A management accountant or controller supports and advises the management of an organization in realizing its economic, public, and/or financial goals. Support is interpreted in terms of the design and maintenance of management control and accounting information systems, and the procurement and distribution of information.”

In consequence, the decisions of a company are based on evidence from analytical results and not only by the intuition of their managers. The business-cultural aspects, technological aspects must be considered, too, like a suitable infrastructure and a user-friendly surface, to gain competitive advantages with big data analytics.

Impact of Big Data Analytics on Controlling

While the term “controller” is common in German-speaking countries, “management Accountant” is a recognized term in English speaking countries such as the USA and UK. In comparing both professions, the tasks of the controller are generally considered to an extended range, not only focusing on accounting issues but also on management issues.

In our blog, the terms “controller” and “management accountant” are used for the same meaning, as commonly found in controlling and management accounting literature.

The features of big data analytics have facilitated the advanced analytics to construct a picture of an event, a scenario, or objects of interest from pieces of trivial information that are scattered across different databases.

Companies, in particular their controlling units, can apply the insight they gained from big data analytics to enhance their decision-making processes to achieve their business objectives successfully. With the overall focus on extracting the essential insights, big data analytics is based on data mining and statistical techniques. Actions from executives that were previously based on gut feeling could now be made using data-driven mathematical models.

Now, a controller could also utilize big data analytics for scenarios and consider seasonal fluctuations. Forecast accuracy affects the efficiency of the company planning process, the degree of goal achievement, total costs, and the level of customer fulfillment needs.

Current Usage and Development of Big Data Analytics

During the “Congress of Controllers” of the International Controller Association (ICV), we collected data through a survey. This conference took place in May 2019 in Munich under the motto “Prepare for your Future — Ideas. Learning. Networks”. It is regarded as the largest controlling conference in Europe.

To measure the recent status of analytics in terms of usage, the participants rated the perceived support and development (“To what extent have big data analytics tools in the controlling area been used in your company so far?”). Constructs in this study were in general measured using a five-point Likert scale, ranging from “strongly disagree” (1) to “strongly agree” (5).

As the Figure below shows, 28% of those surveyed controller answered that big data analytics is not used or used very little. According to 39% of the respondents, big data analytics is used little in their controlling department. Regarding the development of the usage of big data analytics, it can be stated that the majority (48%) rated the development as little. The results regarding the usage of predictive analytics during the planning phase shows an untypical course. The majority (31%) is not using such methods, but still, 28% mentioned using predictive analytics widely during the planning process.

Results of our Survey

The degree of maturity of big data analytics in controlling can be classified rather at the initial stage in Germany. Incorporating big data analytics into controlling still faces numerous barriers and hurdles, which might be the reason why some respondents rated very low about controllers not using big data analytics and still not have automated processes.

The usage of big data analytics in controlling has encountered a bottleneck in recent years, and the field has stagnated. But what are the barriers that companies have to fight to use such tools? Only technological ones? Or even culture?

In our next blog article, we will answer these questions and also look at the results of the respondents. We will share our findings regarding how to potential barriers in terms of big data analytics in controlling.

If you liked our findings and want to support stargazr.ai, please share it or follow us on Linkedin or Twitter!",https://towardsdatascience.com/how-is-the-current-state-of-big-data-analytics-in-controlling-1273c725ac6a,['Rafi Wadan'],2020-05-29 13:35:19.423000+00:00,793,"big data analytics, controlling, management accounting, predictive analytics, forecasting accuracy"
Salesforce Einstein Vision,"In a commercial setting, the tasks to be delivered always comes to be completed in a short deadline. One of our clients has a very good reputation in the insurance sector. They always come with very creative ideas to help their client base and local repair store. In this new engagement, they had come with a new and unique requirement where they wanted to automatically tags uploaded images into a car and bike. Moreover, they also wanted to automatically locate dent in images to process with insurance. Although this looks like a very useful and interesting application, implementing and completing the integration in a short period of time is rather challenging. Moreover, the worrying part is that in my team no one had an extensive set of working experience with image data.

It is always challenging to work with Image data. Data science problems involving image data are often met with challenges in handling both volume and variety. In general working on problems related to computer vision, requires having a solid background in image processing fundamentals. These presumptions changed while I got to work with Salesforce Einstein's vision.

Einstein Vision provides different API that explores the power of deep learning models to classify and detect objects in the images at a larger scale. It also provides a pre-trained classifier or trains your custom classifier.

Photo by Chris Ried on Unsplash

Einstein Vision provides the following APIs:

Einstein Image Classification — API for developers to train models and classify images into different categories at scale

Einstein Object Detection — API for developers to train model and detect multiple objects within the Image

Let’s understand more about Einstein Vision Terminology

Terminology table

Please follow the following steps to get started with Salesforce Einstein.",https://medium.com/analytics-vidhya/salesforce-vision-3c9c6b9cab31,['Khadke Chetan'],2020-11-16 06:46:10.396000+00:00,282,"einsteinvision, imageclassification, objectdetection, deeplearningmodels, imagetraining"
When Concept Drift Is Semantic Drift,"Sometimes your models fail because the world breaks. Sometimes your models die because your observations of the world break. The distinction is harder to discern than you think.

Understanding the difference and how to conquer semantic drift leads to much more powerful models.

What Is Concept Drift?

I see many articles online about concept drift assuming that the reader has a background in statistics. Here’s an alternate approach:

A complex world exists outside of your organization. Change in this external world often breaks your models — COVID-19 and our collective response, changes in USA Federal Reserve policy, or the rise of TikTok. We call these changes concept drift.

Concept is the relationship between inputs and outputs of a model, with the outputs often being the model predictions. For example:

Show a prospective customer two sweaters, sweater A and sweater B. Assuming that this prospect is a female between the ages of 30 and 35 in the Northeast USA in a summer month, the prospect chooses sweater A 82% of the time. All of a sudden, the balance between sweater A and sweater B shifts to 27% sweater A and 73% sweater B. What happened?

In the context of a predictive model, the product choices, demographics, geography, and seasonality are inputs. The output is the predicted likelihood of purchasing each product. This relationship is the concept.

If this shift in purchasing patterns occurs out of the blue, we assume something unseen about the world has changed. Maybe the color is no longer in vogue. Perhaps animal rights campaigns have affected the consumption of wool. This drift is concept drift.

Adapting to a Changing World

When the concept of a model drifts, models have to be rebuilt. There are two options:

Use the same model type and parameters (and code), and train it from scratch on new data that represents the latest state of the world. This data should be unpolluted by the old world. An analogy is taking an infant and training it on a world that has always had TikTok. Failing the first option, throw away the code and go through the painstaking process of data science research to discover the new model type and parameters that fit the latest state of the world. This is like designing a cyborg optimized for TikTok.

Note that, in both cases, the new model wouldn’t work on the old state of the world. Old world? Old model. New world? New model.

The only time to use the same model on both the new world and the old world is when you seek a model that is stable across both. In other words, you desire a model based on inputs unaffected by the shifting winds of TikTok, colors from Milan, or animal rights campaigns.

Has the World Changed?

Whenever models break, data scientists often start with the assumption that the underlying concept must have drifted. After all, it’s easier than trying to gather somehow some magical inputs that may not exist and may be impossible (or too expensive) to acquire.

There is a crucial caveat to this assumption: Most organizations do the bulk of their data science research on data that has to do with a specific product, service, or platform. Often, the product captures that data itself. And when this is the case, changes in the platform and changes in how it captures data about itself can introduce drift that has nothing to do with the outside world.

This different kind of drift is semantic drift. And it takes a lot of work and discipline to be able to separate this from concept drift.

What is Semantic Drift?

The term semantic drift has applications outside of statistics and machine learning. Here, we refer to the meaning of data:

What does a variable or feature mean?

What does the value of a variable or feature mean?

What is an observation?

Let’s take gender identity as the variable in question. Upon registration, an online shopping platform asks prospective customers to identify their gender as either male or female. If the online shopping platform adds a third gender identity option, unspecified. The meaning, or semantics, of the gender identity variable changes. Let’s set the shifting politics of gender identity aside. Perhaps certain people wish to leave their gender marker unspecified to reduce their personal feeling of being surveilled. The point is that gender identity used to have two choices, and it now has three.

A typical data science use case is separating the population of prospects into partitions to treat differently. I wish to bucket males separately from females for marketing.

When the system changes how it defines gender identity, models behave strangely. Individual prospects who answered female in the old data may have chosen unspecified if given a chance. But they responded to that question on registration, and an organization wouldn’t ask them again.

Now, data scientists have to deal with a population of prospects where their value for the gender identity variable means something different depending on when the person registered.

Often data scientists don’t know that their datasets contain multiple semantic versions. And if they do, they have no easy way to tell which prospects are which.

Conquering Semantic Drift

A host of specific best practices for conquering semantic drift exist. This product management and software engineering discipline is rich and is beyond the scope of this post.

The basics boil down to this:

Products, services, and platforms that generate data must product-manage the data they generate as a first-class citizen.

Help data scientists by tagging data with the semantic version, which requires code-as-deployed versioning discipline.

Ensure data scientists can thoroughly understand how data of a specific semantic version was collected by preserving code-as-deployed such that it is identifiable by version.

Product managers and product software engineers are not the only ones with work to do. Data scientists and data engineers do as well. The primary task is:

Data infrastructure should be able to rationalize data sets, such as user journeys, that include observations of differing semantic versions.

Thoughts and feedback, please!",https://medium.com/woodlamp-tech/when-concept-drift-is-semantic-drift-be1ac7e1abf5,['Jenny Kwan'],2020-09-24 04:00:11.010000+00:00,973,"Concept Drift, Semantic Drift, Data Science, Model Parameters, Product Management"
Applying machine learning to the software development process — Are we ready to ship it yet?,"Photo by Carl Heyerdahl on Unsplash

Are we there yet? Is it time to ship it yet? How far/close are we from release ready? In the software development world, these are the classic questions been asked every release, again, again, and again. Figuring out “Are we ready to ship it yet” is especially a challenge for enterprise software products because the delivery model makes the development cycle a lot longer than the ideal cycle.

Unlike Sass software products, which are running from the service provider’s environment and delivered to the user as a service, and delivered frequently, enterprise products is delivered as individual packages and installed in clients’ labs, in which the software developer doesn’t maintain or have any control of it. Because of this nature, knowing when a new version, or even a service pack, is ready to be released is absolutely critical.

The real challenge and dilemma here is that if a premature product is released to the field, the problem can be expensive and exhausting to fix; however it is also not realistic to expect to release a perfect, defect zero version because it might take forever to get there. Not knowing the answer or a reference is stressful, given especially we human beings are not good at handling uncertainties. So, how can we strike a balance? Can we apply machine learning to software development data to answer the question?

Here is the thinking. We can apply regression modeling to release progress data to make predictions based on the observation of the relationship between identified data points and our target. The target of prediction will be days required to get release ready. Depending on the granularity of data collected — by day or by week, we can make predictions to understand how much time does it still required to get to release ready from today or this week.

The table below is an example of release data and data points identified as features. “Days to release ready” is the target of prediction and the rest of columns are features to make the prediction. In this way, we answer the “are we ready yet” question by learning from the past and then compare our new data to the history to understand how many days still required to get to release ready based on the given criteria.

We can make daily or weekly predictions to understand the relationship between effort and resource by examining the target then make adjustments. The goal here is not to predict the exact date but to give reference in the form of “y days is still required” to prompt for actions. For example, using the release data from above, based on the defect reporting, fixing, verifying rate, resource involvement, issue of areas, and test progress, we will get release ready in y days.

If we use the data to make daily prediction, we will get to monitor effort and outcome daily, however, it can also be intimidating and overly stressful because of the intense level. I would maintain a weekly prediction as compromise to the scenario. In which, we can keep eyes on progress, use the prediction to examine how well the plan is executed, and make timely adjustment.

The major effort required here will be to start collect the data and put it away to build the release history for modeling. Taking the data points identified above for example, although it will require some level of data engineering to get the calculated number, it is easily achievable through API or scripting.

At a point of time in my career as a project and release manager, I was trying to collect the data myself but didn’t go far. The continuous effort is a big commitment, and the manual effort is exhausting and inconsistent. Plus, since I didn’t have the machine learning background then, even though I did collect some data with help from my fellow software engineers, the most I did was visualize data to show progress but couldn’t make any predictions. It felt like I was sitting on a gold mine but have no idea what to do with it.

The same concept can be applied to software project tracking — instead of asking developers to provide estimations, we can run a regression model to learn the relationship between completed/remain tasks, resource, effort, scope, and time required to implement features in the past. Like the release ready yet model, we can come up with a prediction of “days required” to reach feature or project development completion.

Text me if you are also interested in this topic and would like to have a more in depth discussion.",https://nars-chang.medium.com/applying-machine-learning-to-the-software-development-process-are-we-ready-to-ship-it-yet-45162753ac38,['Nars Chang'],2020-12-10 13:20:42.299000+00:00,762,"Software Development, Enterprise Software, Machine Learning, Regression Modeling, Data Engineering"
Practical ML Part 3: Predicting Breast Cancer with Pytorch,"From this preliminary we can get the following insights:

This is a quite complex model to be able to visualize the correlations.

The tumors have on average a radius of 14.12, being the smallest one 6.98 and the biggest one 28.11.

Variables related to dimensions are positively correlated (area_se and perimeter_se for example), as one is dependent on the other.

62.7% of the tumors in the dataset are benign and 37.3% are malignant.

3 — Preparing the data for Machine Learning

Changing the Labels

The values we want to predict are in the column ‘diagnosis’ — these are the labels. We want to predict whether a tumor is malignant (‘M’) or benign (‘B). All the other columns in the data frame are the predictors for the model:

The labels are represented by letters M (malignant) and B (benign). First step is to transform this information into a numerical type of data, being 0 for benign and 1 for malignant:

lab ={'B':0,'M':1}

df = df.replace({'diagnosis':lab})

df.head()

Now the data can be split into two subsets:

predictors — the information we will provide as input to the model to make the predictions

predictors = df.iloc[:, 2:31]

predictors

labels — the information we want to predict, which is whether given the information in the predictors dataset, the tumor is malignant or benign.

labels = df.iloc[:, 0:1]

labels

Transformation to tensors and splitting the data into train and test subsets

Using scikit learn, the datasets were split into training and testing data, using a ratio of 20%:

predictors_train, predictors_test, labels_train, labels_test = train_test_split(predictors,labels,test_size = 0.20)

So far the data is stored into pandas data frames. As we will implement a deep learning model using Tensor Flow, the data must be transformed into tensors. First, converting the pandas data frames into arrays:

type(np.array(predictors_train))

Out[20]:

type(np.array(labels_train))

Out[21]:

numpy.ndarray

The model will be implemented using PyTorch, so the next step is to transform the arrays into a torch element:

predictors_train = torch.tensor(np.array(predictors_train), dtype=torch.float) labels_train = torch.tensor(np.array(labels_train), dtype = torch.float) df_tf = torch.utils.data.TensorDataset(predictors_train, labels_train)

type(df_tf)

Pytorch trains the models in mini-batches. There is a class named DataLoader to perform the iterations on the dataset. The batch_size parameter gives the number of samples considered when adjusting the weights of the model:

train_loader = torch.utils.data.DataLoader(df_tf, batch_size=15, shuffle=True)

4 — Implementing and evaluating the model

In this project, a neural network with two hidden layers will be implemented. To implement such model, we need the following:

Structure of the model (defining number of layers, neurons, activation functions);

Choose a training criterion;

Choose an optimizer.

Due to the complexity and the unknowns in the problem, this type of implementation is modeled as an optimization problem. So, choosing an optimizer is part of the process.

In an optimization problem, we have the objective function (known as Cost Function or Loss Function). This is the function we want to optimize. In neural networks the objective is to minimize the error, therefore minimize the loss (or cost) function.

Structuring the model

The neural network model definition is as follows:

classifier = nn.Sequential(



nn.Linear(in_features=29, out_features=15),

nn.ReLU(),

nn.Linear(15, 15),

nn.ReLU(),

nn.Linear(15, 1),

nn.Sigmoid()

)

Input features = 29 (we have 29 features in the predictors dataset).

2 Hidden layers with 15 neurons each

Output layer with one neuron, having output labels 0 (benign tumor) and 1 (malignant tumor)

ReLu , a non linear activation function for the internal layers

, a non linear activation function for the internal layers Sigmoid, a non linear activation function for the output layer, returning a probability between 0 and 1

ReLu (Rectified Linear Unit), is a non-linear activation function that has the advantage of not activating all neurons at the same time. ReLu was chosen in a first attempt to build model more efficient in terms of computational costs.

Sigmoid reportedly works well in classification problems. As we have an output layer classifying the tumor as benign or malignant, sigmoid was chosen as the activation function.

Training Criterion

Now, we must choose the training criterion for the neural network. As this is a binary classification task, a binary cross entropy criterion was the choice:

criterion = nn.BCELoss()

Optimizer

Third, and final step is to choose the optimizer. Adam optimization algorithm is a very popular choice in deep learning problems. This is an extension of the stochastic gradient descent algorithm, but different than the SGD algorithm, Adam optimizer does not maintain the same learning rate during training.

optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001, weight_decay=0.0001)

To read more about the Adam optimizer, check this article in Towards Data Science.

Training the model

The weights updated is run 100 times, as shown in the for loop below:

for epoch in range(100):

#To store the error:

run_loss = 0.



for data in train_loader:

inputs, labels = data

optimizer.zero_grad()



outputs = classifier(inputs) error = criterion(outputs, labels)

error.backward()

optimizer.step()



run_loss += error.item()

print('Epoch %3d: loss %.5f' % (epoch+1, run_loss/len(train_loader)))

The train loader loads the training data in batches (remembering that the batch size chosen for this model was of 15 samples).

zero_grad() set the gradients to zero before the backpropagation process.

set the gradients to zero before the backpropagation process. outputs computes the predictions to the model.

computes the predictions to the model. error computes the error in the calculations, comparing the predictions with the real data.

computes the error in the calculations, comparing the predictions with the real data. error.backward() is the backpropagation process in the neural network to update the weights.

is the backpropagation process in the neural network to update the weights. optimizer.step() to update the weights.

Evaluating the model

The model had an accuracy of 94.73%. Visualizing this in a confusion matrix we have:

The classifier made a total of 114 predictions, from which:",https://medium.com/analytics-vidhya/practical-ml-part-3-predicting-breast-cancer-with-pytorch-efc469242bfe,['Mariana Almeida'],2020-12-18 16:09:33.583000+00:00,864,"108 were correct;6 were incorrect.machine learning, deep learning, pytorch, tensor flow, neural networks"
"Supervised, unsupervised and reinforcement learning","WHAT IS UNSUPERVISED LEARNING?

Unsupervised Learning as the name suggests presence of no supervision. In this type of machine learning machines are trained using training data that is not “labelled”. The objective of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns.

In unsupervised learning, we do not have a predetermined result. The machine tries to find useful inferences from the huge amount of data.

There are two categories of unsupervised learning:

Clustering — Clustering is a method of grouping similar data items into clusters. It determines the similarities between the data items and categorizes them as per the presence and absence of those similarities. Association — This algorithm is based on finding associations among data items in a large dataset. It determines which data items come together in the dataset.

EXAMPLE OF UNSUPERVISED LEARNING

Suppose you are a die-hard cricket fan. Now one of your friends has asked you to join him at his house for watching a football match between Barcelona and Real Madrid. You have no idea about what football is but just for your friends, you say yes and head over to his house. The match starts and you sit there with no idea with what is going on in the match. Your friends are enjoying the way Lionel Messi plays and want you to join in the fun. Here is when you try to learn about the game. You examine the screen and come up with certain conclusions that you can use to understand the game better.

• There are 2 teams with jerseys of different color combinations. One which has the color combination red-blue and other with white color. Since the last goal was scored by Messi and you saw an update in the score of Barcelona you conclude that the jersey of red-blue belongs to Barcelona and the white jersey belongs to Real Madrid.

• You can see that on the ground there are total of 11 players from each team and out of the 11, 1 player is near the goal, preventing the opponent team member from scoring, making him a goalkeeper and the remaining 10 players are on the field trying to goal in the other’s goal, making them the outfield players.

• If the ball passes over the goal line the team gets few points.

• Messi has a number 10 on his back and his name is written on the back of his t-shirt and if he or any of his fellow team players hits the goal, you need to cheer

• If the opponent team hits a goal you need to show your disappointment.

You make these observations one-by-one and you know when to do what. From knowing nothing to knowing the basics of football, you can now enjoy the match with your friends, without them knowing that you did not know anything.

What happened here?

You had every material that you needed to learn about the basics of football- TV, when and who your friends cheer for. This made you learn about football all by yourself without someone guiding you about anything. This is the principle that unsupervised learning follows.

Did you see how unsupervised learning helped you in real life. In addition to this it has various advantages in the field of machine learning. Let us move to that now and will be also discussing about its disadvantages

ADVANTAGES OF UNSUPERVISED LEARNING

It makes the machine tackle problems that humans might find difficult either due to a limited capacity or a bias. It is ideal for exploring raw and unknown data.

DISADVANTAGES OF UNSUPERVISED LEARNING

The results produced by the unsupervised method are considered to be less accurate and reliable in comparison to the results produced by other techniques of machine learning. The user needs to spend time interpreting and labelling the groups/classes which are formed as result.

APPLICATIONS OF UNSUPERVISED LEARNING

Some of it’s important application are:",https://medium.com/@mehtapriyanka1pm/supervised-unsupervised-and-reinforcement-learning-246781f26730,[],2020-11-15 06:49:10.221000+00:00,641,"• Market Segmentation• Anomaly Detection• Image Compression and Pattern Recognition• Natural Language Processing and Text Mining Unsupervised Learning, Machine Learning, Clustering, Association, Football"
Using custom metrics for callbacks in Keras model training,"Keras provides several in-built metrics which can be directly used for evaluating the model performance. However, it is not uncommon to include custom callbacks, to extend beyond keras' capabilities, as I myself had to do recently. In this post, I discuss how to use custom calculated values (metrics, losses) with in-built callbacks.

Before proceeding, we will need to define our own custom callback to carry out calculations that we want for tracking the training progress or evaluate the model performance. Below is a dummy code for defining a custom callback.

CustomCallback(keras.callbacks.Callback):

def on_epoch_end(self, epoch, logs={}):

:

do_somthing_here

:

logs[""metric_name""] = metric_value

Once custom callback is defined, the related custom metric can be included in the other callbacks

ckpt_callback = ModelCheckpoint(

path_to_file,

monitor=""metric_name"",

mode=""Specify depending on desired behavior of the metric"",

save_freq=""as above"",

)

This behavior is not limited to exchanging data from custom callbacks to the in-built callbacks, but can be extended across all callbacks, since the 'log' object is passed automatically to all of them through model.fit(). Similar workflow will work with other default callbacks like EarlyStopping, etc. where the functionality is dependent on a custom defined value/metric.",https://medium.com/dive-into-ml-ai/using-custom-metrics-for-callbacks-in-keras-model-training-be5e8de1a781,['Anuj Arora'],2021-03-04 02:47:05.034000+00:00,174,"Keras, Custom Callbacks, Metrics, Losses, Model Performance Evaluation"
What exactly is Data Analytics?,"Data Analytics has been the trendy expression for a long time now. Either the information being created from enormous enterprises or the information produced from an individual, every single part of information should be taken advantage of. In any case, how would we do it? That is the where the term ‘Data Analytics’ comes in.

For what reason is Data Analytics significant?

As a gigantic measure of information gets created, the need to understand valuable insights is an absolute necessity for a business venture. Data Analytics has a key job in improving your business. Here are 4 principle factors which enforce the requirement for Data Analytics:

Assemble Hidden Insights — Hidden bits of knowledge from data is accumulated and afterward investigated for business analysis.

2. Produce Reports — Reports are created from the information and are given to the employees for further analysis.

3. Perform Market Analysis — Market Analysis can be performed to comprehend the qualities and the shortcomings of the competition.

4. Improve Business Requirement — Analysis of Data enables improving customer relations.

Data Analyst- Can you be one?

Data examiners interpret numbers into plain English. They take data and use it for deciphering, investigating, and showing discoveries in far reaching reports. In the event that you have the ability to gather information from different sources, dissect the information, accumulate concealed bits of knowledge and create reports, at that point you can turn into a Data Analyst.

On the off chance that you are somebody hoping to get into this fascinating vocation, now would be the perfect time to upskill and exploit the Data Analysis profession openings that come your direction. If you are a software engineer who is looking to up his game, head on over to www.lrned.io . Being India’s fastest career accelerator, it not only makes you one of the elite, it also helps you get decorated positions in decorated companies. So, head on over to www.LrnEd.io and apply now!",https://medium.com/lrned/what-exactly-is-data-analytics-953f241e11d2,['Sanjay Verma'],2020-01-21 17:04:38.950000+00:00,316,"data analytics, data analyst, hidden insights, market analysis, business requirement"
"After liner regression, logistic regression, Neural network What is next?","Kernel Trick

Having a non-linear data at lower dimensional space and projected the data in to a higher dimensional space such that where we get a linear classification. We can understand the statement by taking an example

Let’s assume all red marks are apple and all blue marks are berries. If someone ask you to classify them in to 2 classes then you took all berries to up with a plate and rest, in lower plane are apple. After classify the data we can use logistic regression or neural network for any decision-making purpose or to conclude with any pattern respectively.

SVM

SVM stands for Support Vector Machine. SVM comes under supervised learning. Two important steps in SVM are 1) Kernel Tricks: convert to linear class and 2) Find the nearest point to the decision boundary.

Let’s assume we have some quadrilateral or circles in our training data set and all are labelled. We want to classify the data into 2 different class of circle and quadrilateral. So, to classify the data we plot a hyperplane or decision boundary which decide any new data belong to either circle or quadrilateral class. Now we need to find out which are the data points with lower distance from the decision boundary with respect to opponent class. These 2 lines are the margin which shows the error. Nearest point to the margin is known as support vector. Maximize the margin leads to reduction in error. SVM gives binary out put either 0 or 1.

DECISION TREE

From the above picture we can easily understand what a decision tree is. A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences. It only contains conditional control statements.",https://medium.com/@subhasmitap.isme1820/after-liner-regression-logistic-regression-neural-network-what-is-next-8944f3ebdbe7,['Business Analytics'],2019-11-20 05:33:19.438000+00:00,284,"KERNEL TRICK, SVM, DECISION TREE, MACHINE LEARNING, CLASSIFICATION"
World Aids Day: The ‘blue box’ clinic bringing hope in Mozambique,"HIV prevalence among long-distance truck drivers here is 15.4 percent — the national figure is 12.6 percent. This is due to connected factors such as the absence of appropriate accommodation and resting time, limited access to safe health services at key points along routes, as well as unprotected sex.

Tonderai is a truck driver from Zimbabwe who has been working along the Corridor since 2003. As assistant secretary-general of Zimbabwe’s Truck Driver Union, he explains that truck drivers have little time to stop on journeys. So mobile clinics like the blue box that are easily accessible provide fast services — an HIV test takes less than ten minutes.

Mary is a sex worker living in the community of Inchope, near the blue box, where she moved to from Zimbabwe with her son.

She learned about the clinic from Adelaide and goes regularly. Adelaide frequently visits and calls Mary and other female sex workers in the community to check in on them and to remind them to get tested for STIs. Mary likes the dialogue groups targeted at female sex workers offered at the blue box.

At one of these, she learned she could be tested for HIV free of charge; now she goes every three months. Thanks to the intensive focus of the blue box around HIV prevention, Mary has access to condoms and has been taking Pre-Exposure Prophylaxis (PrEP), an HIV prevention medication, since it opened.

“I used to go the hospital in Zimbabwe to get tested, but there I never heard about PrEP, here at the blue box I did,” she says. “It is thanks to PrEP medication that me and other female sex workers here we feel more protected from getting HIV. The blue box helps.”

Mary’s story testifies to the cross-country impact of this intervention, which is crucial due to the mobile nature of the target groups accessing the clinic.

A medic prepares to administer an HIV test at the blue box clinic. Photo: WFP/Rafael Campos

After only four months of implementation and despite the challenges that the COVID-19 pandemic poses mobility, the number of people visiting the blue box has exceeded expectations.

This is thanks to additional awareness-raising efforts by lay counsellors, such as Adelaide, who play a key role in encouraging key target groups — truck drivers, female sex workers and adolescent girls — to access its services.",https://medium.com/world-food-programme-insight/world-aids-day-the-blue-box-clinic-bringing-hope-in-mozambique-39a993b83273,['World Food Programme'],2020-12-01 09:31:11.445000+00:00,385,"HIV, Long Distance Truck Drivers, Mobile Clinics, Sex Workers, Pre Exposure Prophylaxis"
GPT-3: Creative Potential of NLP,"Get this newsletter By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.

Check your inbox

Medium sent you an email at to complete your subscription.",https://medium.com/merzazine/gpt-3-creative-potential-of-nlp-9b3be712a2c1,"['Vlad Alex', 'Merzmensch']",2020-07-17 10:25:49.804000+00:00,40,"newsletter, subscription, privacypolicy, email, medium"
How To Tell a Compelling Story With Data,"Step 1: Gather your thoughts

If you have conducted a certain data analysis, you should know why you initially started it. However this is easier said than done: sometimes you can lose track of the primary goal of an analysis as you move forward in data handling and interpretation. Don’t worry, I’ve been there before.

Therefore coming back to the WHY before the WHAT of you data analysis is a crucial step to start with. Ask yourself the following questions:

What question(s) are you trying to answer?

What goal(s) do you want to reach?

How is your data relevant in this context?

In my example, the reason why I conducted this data analysis is given by the context itself. My team needed to form four same-level groups of employees in order to allow everyone to get an Excel training with fellows having the same technical background. The question I had to answer was: how to best form four groups for this training? The goal I had to reach was to form homogeneous groups having the same number of individuals. To do so I would use the data at my disposal: data about employees’ seniority, proficiency level in Excel and average daily time spent on a computer.

© Marie Lefevre

Step 2: Identify your audience

In the way you want to tell your story, a key element to take into account are the individuals your data story is addressed to. One of the Cambridge Dictionary’s definitions for audience is the following: “the people, considered as a group, who watch or listen to a performance, movie, public event, etc., either together in one place or separately”. I want to underline the concept of group here. Particularly for data stories, all members of your audience may not have the same background to understand the technical elements of your data analysis.

So now that you clearly identified the WHY, let’s focus on the WHO. To define your audience you want to target with your story, you should ask yourselves the following questions:

What defines your audience as a group?

What type of knowledge (e.g. technical or industry-related) does your audience have upfront?

What additional value does your story provide them?

To come back to my previous example, the challenge for me was to convince my team’s boss that my methodology was the best way to split employees in four similar groups. This was a first test to then apply this methodology at a larger scale, so my story would be first addressed to my boss but ultimately it was possibly meant to be presented and applied at other departments. Hence my audience was composed of team leaders having a vague knowledge of data science (while I used k-means clustering in my analysis) and a very business-oriented mind. The additional value brought by my story would be to save time in building groups of individuals in the future.

Who’s Your Audience? Photo by Davide Ragusa on Unsplash

Step 3: Build your narrative

For this ultimate phase make sure that you have all the elements required for the content of your data story:

WHY: the fundamental reasons why this data story is relevant and the questions it answers

WHO: the audience this data story is addressed to and the way they should be addressed to

WHAT: the tangible elements of your data analysis presented in data visualizations (tables, graphs, pictures…)

Your ultimate mission, should you accept it, is to make all these elements fit into a consistent and compelling data story. Here you want to capture your audience’s attention not simply by presenting them an interesting story but also by telling them a story that truly fascinates them. To do so, let’s turn to the world of stories in literature or film. A commonly used dramatic structure is called the Freytag’s Pyramid. Under this structure, any story plot consists of five parts:

Exposition Rising action Climax Falling action Catastrophe or resolution

What you should focus on are the first, the third and the last elements of this pyramid, as they build your narrative’s core structure. Don’t hesitate to involve your audience along the whole story, for example by asking them what they would do at this or that moment of the story.

© Marie Lefevre

Despite the fact that my example is a very short data story (e.g. compared to a one-hour presentation of an extensive data analysis project) I applied this type of structure too. Here is how I built it:",https://towardsdatascience.com/how-to-tell-a-compelling-story-with-data-d00ea8cd1bb,['Marie Lefevre'],2021-09-08 19:20:03.687000+00:00,721,"Exposition: introduction to the context, the question I had to answer, and the goal I had to reach Rising action: description of my data analysis process (data gathering, data cleaning, clustering)Climax: presentation of my results in a table and a graph"
How to learn the basics of data science and machine learning for free with Kaggle,"How to learn the basics of data science and machine learning for free with Kaggle

Photo by Alexandru Acea on Unsplash

Kaggle is an online community of data scientists and machine learning users. The website has a focus on sharing and analysing data sets; and offers data analysis and machine learning competitions with cash prizes. One of the most overlooked aspects of Kaggle, though, are the exceptional short courses for beginners to learn the basics of this discipline. Kaggle courses are free and have good balance between teaching the core fundamentals and not overloading users with advanced concepts and information which can be picked up in time. In this short article, I want to show you how you can get started with learning data science and machine learning for free with Kaggle courses.

Sign up

First things first, you will need to sign up to use the website. Kaggle is owned by Google, so if you already use Google services like YouTube or Gmail, then you can use that account to register a Kaggle account. If you don’t have a Google services account, you can sign up using any email account you like.

Screenshots by Rugare Maruzani, author

Courses

Once you’ve signed up and signed into Kaggle, navigate to the Courses section on the left-hand side of the webpage. As a beginner, you’ll want to start with the Python course. I’ve already completed that course so you won’t see it in the image below. Python is the language of choice for machine learning and data science, particularly for beginners. I think you can jump right into this course with no prior coding experience, but Kaggle recommends you get familiar with some absolute basics before starting the course. When you get to the end of each lesson, it’s important you practice what you just learnt by clicking the link in the Your Turn section at the end of each lesson.

Screenshots by Rugare Maruzani, author

After completing the Python course, I would recommend taking on the Pandas course. Pandas is one of the key Python libraries and this course will teach you how to work with Pandas as well as introduce you to how libraries work in Python. Next, I would do the Data Visualisation module followed by Introduction to Machine Learning and Intermediate Machine Learning

Study other people’s Notebooks

One of the great things about Kaggle is the community can share their data science and machine learning work via Notebooks. As a complete beginner these Notebooks can be intimidating and difficult to comprehend, however, the more you read them the more you will understand, and the more patterns you will notice. I would recommend looking for Notebooks in an area you are interested in and study those. You will find Notebooks analysing health, sport, population and disease data among many other topics. You also have the ability to copy Notebooks to your own profile, allowing you to tinker and break the code, which is a fantastic learning technique.

Search for Notebooks by clicking on the Notebooks tab (blue arrow), and you can copy Notebooks and tinker by clicking the Copy and Edit button (red arrow). Screenshots by Rugare Maruzani, author

Conclusion

After going through some Kaggle courses, I think you will have a good idea of whether data science is an area you are truly interested in and want to pursue further. If so, you will have noticed other courses to develop your skills as a data scientists and machine learning practitioner. Don’t forget to collect the certificate you earn after completing a course and sharing it on LinkedIn!",https://towardsdatascience.com/how-to-learn-the-basics-of-data-science-and-machine-learning-for-free-with-kaggle-f122347eece3,['Rugare Maruzani'],2021-01-20 21:25:27.909000+00:00,585,"Data Science, Machine Learning, Kaggle, Python, Pandas"
laminate mdf and stainless steel wall,"laminate mdf and stainless steel wall

There is only one place in whole world where we spent lots of time its none another then our home sweet home .where we playing video games,watching movies on television,entertaining ourselves talking on phone with friends or relatives.relaxing on holiday in home is like living in heaven.so home is so necessary and basic thing for us as well as walls are essential for home without walls we cannot build up a perfect home although i am home chef so i like to live in with good wall design.so here we are not talking about home decor or walls only we are talking about designer or interior wall design in our home or living room which enhance look of our home interior.there are so many confusing questions raised about designer wall art like which design suits, which material ,how is it made and extremely important who will design and fabricate it properly in given sizes so as per shown in above image its our one of laminate mdf and stainless steel wall let me tell you how we start and made it and dont worry its not high in price you need not to take home loan for this.

Measurement

its all start with measurement of wall where you want your designer or highlighted wall.we take measurement on site by measuring height and width of wall then we also check base of wall is solid or made by plyboard or any other material this wall have a mixture of laminate and stainless steel.

Drawing

after exact measurement we start to create basic drawing for wall how many lines or another design we can create in given size how we locate in wall what is positions of lines in wall we make 2d drawing first according to sizes after that we execute 3d models in which we describe thickness of wall or design and size of lines or design by client wish.

Material

For interior wall design we select mdf or wpc material easily available in market it depends which material is selected by client according to budget .mdf is cheaper than wpc but wpc is heat proof, termite proof and water proof quality mdf does not have these qualities but in some places you can use it in interior murals or small interior design wall.In this wall as per you seen in above image we choose mdf plus laminate plus stainless steel this wall have a mixture of these materials.

3d modellinghow to engrave wood for double front entry doors

usually artist like me gaze on latest trends, estimating the price value of trend which guiding the market and which things are gaining access to the business. usually, when a new trend is in we uses 3d softwares like artcam corel etc and start designing these latest things.this design made of 3d software give command into another software which make programmed to use by cnc machines.firstly i searched high quality pics on jainsim on google or another search engines like pintrest etc after that i choose one of the best picture in position or in quality.Then after enhancing picture quality on photoshop i ll import that pic in 3d software to make 3d model.

Engraving

after selecting material and design we start to engraved or embossed design on mdf or wpc sheets for example we take 19mm sheet of wpc or mdf we can engraved it only 12mm to make design left material is used as base of design in sheets which give strength to sheet and design itself.

Joining

after engraving on sheets we start to make a iron frame on which we screwed sheets and join or align them as per in design .some design take 4 sheets joining some will tale 6 sheets depend on size of wall and sheet size our iron frame hang on wall like photo frame with engraved sheets on it

Paintjob

On laminate wall there is no need of paint job otherwise if we choose another material like mdf or wpc only then we required paintjob .When frame hanged on wall we start sanding of design to give good finish because paint finsih depend on base material finishing if base material is not properly finish paint will not look good in quality after completion of sanding paintjob started as per client choice of colour scheme or duco , pu and polyster paint we can do depend on client budget polyster is expensive as compare to pu and duco.

L.E.D LIGHTS

in some of our designer wall we use l.e.d lights to backlit and to enhance the mystic look of given wall it raise grace of wall as well as costing of wall too. colour of light we choos according to colour scheme of wall or interior designer consult client to whch colour is suitable

i hope you got some good information from this article kindly share with others too.Feel free to comment.If you have any question regarding this article. you can drop message in comment section or mail me @ rmndeepcad@gmail.com

thanks and warm regards

from rmndeep g

EDIT",https://medium.com/home-interior/laminate-mdf-and-stainless-steel-wall-2d1f111839e0,[],2020-02-29 16:22:49.193000+00:00,831,":Laminate MDF, Stainless Steel Wall, Interior Design Wall, 3D Modelling, Engraving"
How to check for NaN in JavaScript,"How to Check for NaN in JavaScript

Checking for NaN (“not a number”) is as simple as checking for self-equality in JavaScript.

Photo by Rattap on Unsplash

In JavaScript, the special value NaN (meaning “not a number”) is used to represent the result of a mathematical calculation that cannot be represented as a meaningful number. — Joshua Clanton on A Drip of JavaScript

The special value NaN shows up in JavaScript when Math functions fail ( Math.sqrt(-37) ) or when a function trying to parse a number fails ( parseInt(""No integers here"") ).

NaN then poisons all other math functions, leading to all other math operations resulting in NaN .

Photo by Chris Barbalis on Unsplash

Division by zero

Note that in JavaScript, division by 0 returns Infinity , not NaN :

This result is because of how floating-point is defined, more generally than just Javascript. Why? Roughly, because 1/0 is the limit of 1/x as x approaches zero. And 0/0 has no reasonable interpretation at all, hence NaN .

What is NaN anyway?

NaN is a property of the global object. The initial value of NaN is Not-A-Number — the same as the value of Number.NaN. — MDN Docs

Photo by Andrew Buchanan on Unsplash

Checking for NaN is harder than it seems

Unfortunately, there are two problems with trying to check for NaN :

The typeof NaN is “number” NaN is unequal to every other value in JavaScript

Check out this code example:

So how do we check whether we have a NaN value that will poison math?

Photo by Wolfgang Rottmann on Unsplash

Check for NaN with self-equality

“NaN, and only NaN, will compare unequal to itself.” — MDN Docs

In JavaScript, the best way to check for NaN is by checking for self-equality using either of the built-in equality operators, == or === .

Because NaN is not equal to itself, NaN != NaN will always return true .

Of course, such a NaN test in your code is not always readable, so it is a good idea to use a comment or to create a wrapper function:

It doesn’t make a difference if you use != or !== to check for NaN .

Photo by Erik Mclean on Unsplash

Check for NaN with Object.is()

“The Object.is() method determines whether two values are the same value.” — MDN Docs

Unlike the strict and loose equality operators, the ES6 helper method Object.is() does not consider NaN to be equal to itself:

So, if you swap out === for Object.is() , you never have to worry about checking for NaN any special way. Problem solved!

I wrote about how to use Object.is() in JavaScript in another article:

One note about terminology — Object.is() is a function on the global Object class, I don’t call it an “equality operator” like == or === .

For example, in Jest tests, the method .toBe() that uses === is different from the method .toEqual() that uses Object.is() .

To wrap up here, I should mention that a JavaScript ES6 Set , which is used for finding unique values, will only ever keep one copy of a NaN value.

Photo by Tony Hand on Unsplash

A word about Number.isNan()

Modern JavaScript already has an implementation to check for NaN called Number.isNan() that works how you think it would. For example:

Note that Number.isNan() is different from the global isNan() function, which is an older implementation whose actual purpose is to check whether a value cannot be coerced to a number.

Here’s the difference, for completeness:

isNaN() will return true if the value is currently NaN , or if it is going to be NaN after it is coerced to a number. In other words, if it receives a value that can be coerced to a number, isNaN() will return false.

will return if the value is currently , or if it is going to be after it is coerced to a number. In other words, if it receives a value that can be coerced to a number, isNaN() will return false. Number.isNaN() will return true only if the value is currently NaN .

So, if you are supporting old browsers (especially Internet Explorer) that don’t support Number.isNan() , then it is best to check for self-equality.

Photo by Nick Hillier on Unsplash

Additional resources:

The Mozilla Developer Network Docs include details about NaN:

Kuba Michalski explains null, undefined, and NaN in CodeBurst:",https://medium.com/coding-in-simple-english/how-to-check-for-nan-in-javascript-4294e555b447,['Dr. Derek Austin'],2020-11-23 18:42:44.711000+00:00,694,"Java Script, NaN, javascript, NaN, self-equality"
Supervised Learning Algorithms,"Hmmm….Algorithms huh!!!

As I pledged in my last article that I would be writing about algorithms in next article.

Here I am buddies.

Algorithms are the core to building machine learning models and here I am providing details about most of the algorithms used for supervised learning to provide you with intuitive understanding for where to use it and where not to.

By the end of this article, you will be adept at algorithms from intuitive level of understanding.

CAVEAT: I AM NOT DESCRIBING MATHS BEHIND IT INSTEAD HOW IT WORKS AND WHERE TO USE IT.

So, folks here we go.

1.NAIVE BAYES

Naive Bayes are the algorithms used for classification based on Bayes theorem and it is the foundational algorithm to know at most for machine learning.

Advantages:

It is very helpful for handling large amount of datasets and generalizes the data accurately for such large datasets. Applied mostly in classification problems eg. spam detection,spam filtering,sentiment analysis,fraud detection, recommendation engine etc.

Disadvantages:

It is naive i.e doesn’t understand data in ordered format like in text learning.(Still it is preferred for its speed and easiness of use).

Stock price prediction

2.LOGISTIC REGRESSION

Logistic regression by name sounds algorithm for regression but in-fact it is a classification algorithm. It is a linear and simplest classification algorithm.

Pros:

It is simple and interpretable. It works best for linear data i.e when classes we are trying to predict are non-overlapping and linearly separable.

Cons:

When classes are non-linear, it will fail. It can’t handle complex problems.

3.Linear Regression

Linear Regression is also a linear model but used for regression problems.

Advantages:

It is also simple, interpretable and hard to overfit. It is best when the relationship between input and output variables is linear.

Disadvantages:

It will underfit the data when the relationship between input and output is nonlinear i.e it fails to generalize non linear data accurately. It also can’t model complex relationships.

4.K_NEAREST_NEIGHBORS

It is an algorithm that has the ability to model non-linear data as well as linear data efficiently. It is used for both regression and classification problems.

Advantages:

Albeit being simple and interpretable ,it is highly flexible and efficient at learning more complex, non-linear relationships. Used in recommender systems,like in Netflix, spotify etc.

Disadvantages:

It doesn’t work well when no of observations and features grow i.e doesn’t generalized well for large datasets.

5.SUPPORT VECTOR MACHINES(SVM)

SVM are highly flexible algorithms that make a separating data-line between datasets. It can be used for both regression and classification.

Advantages:

It can handle complex datasets as well. It works for nonlinear data too.

Disadvantages:

Prone to noise. Don’t work well for large datasets.

6.TREE BASED METHODS

Tree based methods are the most effective algorithms developed for solving extremely complex domains of problems. It is compatible for both classification and regression problems.

There are many tree based methods:

1.Decision tree 2.Bagging 3.Random Forests 4.Boosting(Gradient boost, Ada Boost, XG Boost).

Advantages:

These methods are best for supervised learning for prediction problems. Handle complex relationships along with handling missing data and categorical features in an adept way.

Disadvantages:

Difficult to interpret and might take long to train the model as well.

7.NEURAL NETWORKS:

Neural networks are the state of the art technique to generalize even the most complex problems out there in the world. These algorithms come under deep learning which is the most complex still the most efficient model to handle cumbersome problems and get the best metrics for our problems. Since these methods are really complex, we should first try to use above simple linear models before getting our hands dirty on neural networks.

Hooo🥱…..finally the article is over but not the learning process. I have provided the basic understanding of these algorithms used for machine learning from an intuitive perspective so that you would be able to perceive them with breeze. Next its up-to you to get more adept at these topics.

I guess you got a bit of concepts on these algorithms from this article. I hold my pen here. Oops I hold my hands out of my keyboard😂😂.

Anyway….

Thank you.And yeah be happy and don’t worry .Just take a small step at a time and you will reach the summit in a jiffy.",https://medium.com/analytics-vidhya/supervised-learning-algorithms-ad934e0b1834,['Anjan Parajuli'],2020-09-16 14:39:49.185000+00:00,653,"algorithm, machine learning, supervised learning, Naive Bayes, Logistic Regression"
"Anti Bar chart, Bar chart Club","Inspiration, knowledge, and anything about data science by Make-AI

Follow",https://medium.com/make-ai-data-stories/anti-bar-chart-bar-chart-club-56a2275b08aa,['Benedict Aryo'],2019-08-21 06:14:06.328000+00:00,9,"Data Science, AI, Make-AI, Inspiration"
5 Steps to Kick Start Your Own Data Science Project,"Step 1 - Goal

Before starting the project, as a data scientist, you need to have a specific problem statement. A clear goal leads to a simple and straightforward direction. This can save you a lot of time and get to the point directly.

Let’s use real estate as an example. Considering you are now going to start a property project, you can ask yourself tons of questions to identify the specific area you want to focus on.

Which district or area do you want to cover? The entire country? Or just a city?

What topic are you interested in? Rental price? Or Property price?

Do you have a timeline for the study?

By doing so, you can formulae your own problem statement and clearly understand your direction towards the project. For example,",https://towardsdatascience.com/5-steps-to-kick-start-your-own-data-science-project-f9a60d058a37,['Yeung Wong'],2020-11-20 00:06:14.592000+00:00,127,"“To analyze the rental price trend in Hong Kong from January 2020 to December 2020.”data science, problem statement, real estate, rental price, Hong Kong"
Detecting Parkinson’s Disease with Machine Learning,"#1 ~ Business Understanding

Any project, regardless of its size, requires an understanding of the business, which is the basis for effectively solving business problems.

What is problem I trying to solve?

At this stage, I need to define problems, project goals and solutions from a business perspective. This is the first step in solving problems with data science methods. In this article, I need to accurately detect the presence of Parkinson’s disease based on the voice measurement data set.

What is Parkinson’s Disease?

Parkinson’s disease is an age-related neurodegenerative disease of the central nervous system. It is the second most common disease after Alzheimer’s disease that affects neurons in the brain that produce dopamine.​​​ It affects movement and induces tremor and stiffness. An estimated 70–10 million people worldwide suffer from Parkinson’s disease.

Therefore, it is a Supervisor Learning ~ I will use Classification method.

#2 ~ Data Collection

Data is the most critical part of the entire machine learning. So I need to consider the following:

Do I have the data?

Where does the data come from?

Do we trust the data source?

Do I have the domain knowledge?

The data is coming from the UCI Donald Bren School of Information & Computer Sciences, you can download it here. The dataset has 24 columns and 195 records.

#3 ~ Data Preparation & #4 ~ Exploratory Data Analysis

Now we will make necessary imports and try to load the Parkinson’s disease dataset to jupyter notebook.

Now, let’s take a look at the dataset. The dataset contain 195 rows and 24 columns. The dependent variable is status and MDVP:Fo(Hz) to PPE are independent variables. In the picture below, we can see that there are no missing values ​​in dataset.

We can see that all our numeric variables are listed at the top, and we have values ​​such as count, mean, standard deviation, minimum, maximum, 25% and 50%, and 75%.

#5 ~ Modelling

It is important to standardization the training data and test data because most machine learning models converge much faster if the proportions of the elements are the same. Doing standardization will centralize the feature’s mean between 0 to 1. The data will be distributed on normal distribution. To calculate the mean and standard deviation of those feature and apply the above formula to each observation/value, it will use sklearn’s StandardScaler:

The last thing to do before training our models is to split the dataset. I split this data randomly with 80/20 for training versus test examples to training set and testing set. We need to do this so we could estimate the predictive result of our model by predicting the testing set data (unseen data).

LogisticRegression

DecisionTreeClassifier

SVC

#6 ~ Model Evaluation

In this step, we will evaluate the performance and accuracy of the machine learning model. Based on the prediction, we can see that the model can accurately predict from Logistic Regression is 0.79, Decision Tree is 0.87 and SVC is 0.92.

#7 ~ Model Deployment

This is a relatively simple project, usually we also need iteration and function selection or compare with other algorithm. By collecting the results of the implementation model, you also need to receive feedback about the performance of the model and its impact on the implementation environment. By analyzing this information, data scientists can improve the model and improve its accuracy, thereby improving its practicality. Once a satisfactory model is developed, it will be implemented in the production environment.",https://medium.com/analytics-vidhya/detecting-parkinsons-disease-with-machine-learning-44c17208afce,['Kinder Sham'],2020-09-02 12:55:24.038000+00:00,549,"Business Understanding, Data Collection, Data Preparation, Exploratory Data Analysis, Modelling"
A visual guide to multidimensional NumPy array aggregation,"A visual guide to multidimensional NumPy array aggregation

If you are just learning to use the aggregate functions in NumPy like sum, mean or median, understanding what the axis parameter does can be difficult. In the following article I provide an intuitive visual guide to axis numbering so you can always make sure that your code works as intended.

Aggregating a 1-dimensional array is straightforward since there is only one axis to consider. With 2 or more dimensions, however, we must think about the direction along which we wish to aggregate our array. In this guide we will discuss one-, two- and three-dimensional problems using the numpy.sum() function. (Note: other aggregate functions like mean, median, amin, amax use the same logic.)

Dimension reduction and axis numbering

We can think of aggregation along a single axis as reducing the dimension of our array by one. If we aggregate a one-dimensional array, we get a single point. If we aggregate a two-dimensional array, we get a one-dimensional array and so on. If we have more than one dimensions, we must decide which dimension (or axis) we want to eliminate. In NumPy this can be done by specifying the axis parameter in the function call.

Let me emphasize: When you set the parameter, you select the dimension you wish to eliminate.

What we must know about axis numbering is that indexing always starts with the highest dimension. With each added dimension, the new dimension gets index 0 and all other indices are increased by one. We will see this in detail in the examples below. The default value for the axis parameter in NumPy is None, in which case the array gets flattened and then aggregated.

Tip: If you are not sure how many dimensions you have but you want to select the lowest, you can use negative indexing. For example, axis=-1 will eliminate the lowest dimension.

Aggregating one dimensional arrays

>>> a = numpy.array([1,2])

>>> numpy.sum(a, axis=0)

3

In this simple case there is only one dimension along which we can aggregate the array. This dimension is the 0th dimensions, or 0th axis. (It is not necessary to specify the axis parameter because a flattened 2-dimensional array will be itself.)

Aggregating two dimensional arrays

>>> b = numpy.array([[1,2],[3,4]])

In this case we have added a second one-dimensional array to our previous array, creating a two-dimensional array. Note that the added dimension is the highest, so that will have index 0, and the previous will have index 1.

Here we can choose whether to eliminate the 0th dimension and collapse the array along its vertical axis or eliminate the 1st dimension and collapse the array along its horizontal axis. Let’s see both:

>>> numpy.sum(b, axis=0)

array([4,6])

With axis=0 we wish to eliminate the 0th dimension. We achieve this by collapsing the array along its vertical axis.

>>> numpy.sum(b, axis=1)

array([3,7])

Following the same logic, here we eliminate the 1st dimension and collapse the array along its horizontal axis.

Aggregating three dimensional arrays

c = numpy.array([[[1,2],[3,4]],[[5,6],[7,8]]])

In this three-dimensional example we simply add one more 2 dimensional array to the previous array, creating a three-dimensional array of shape(2,2,2). This added dimension will be the highest, so it gets index 0, while all other indices are increased by one. Since we have three dimensions, there are three options to consider when aggregating the array.

>>> numpy.sum(c, axis=0)

array([[6,8],[10,12]])

With axis=0 we specify that we wish to eliminate the highest dimension (the one we have just added). We can achieve this by collapsing the two ‘layers’ of the three-dimensional array.

>>> numpy.sum(c, axis=1)

array([[4,6],[12,14]])

With axis=1 we wish to eliminate the 1st dimension. We achieve this by collapsing both of our two-dimensional arrays (the layers) along their vertical axes.

This way we reduced an array of shape(2,2,2) into an array of shape(2,1,2). Note that NumPy will completely eliminate the dimension of size 1 and output a lower dimensional array of shape(2,2) as seen as the third stage of my chart unless you set keepdims=True.

>>> numpy.sum(c, axis=2)

array([[3,7],[11,15]])

With axis=2 we wish to eliminate the 2nd dimension. We achieve this by collapsing both of our 2-dimensional arrays (the layers) along their horizontal axes.

Similar to the previous example, we get an array of shape(2,2,1) under the hood, but NumPy will output a lower dimensional array of shape(2,2) by default.

Key points to remember

You can think of aggregation as reducing the dimensionality of the array When you choose an axis, you actually set the dimension you wish to eliminate Indexing always starts with the highest dimension

Reference

[1] The SciPy community, NumPy v1.20 Manual

(All images are created and owned by the author.)",https://towardsdatascience.com/a-visual-guide-to-multidimensional-numpy-array-aggregation-97a8960b3c59,['Marton Meszaros'],2021-05-07 19:09:00.646000+00:00,736,"numpy, array, aggregation, sum, mean"
11 — Mean Square Error Gradient Descent,"In statistics, the mean squared error (MSE)[1][2] or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors — that is, the average squared difference between the estimated values and the actual value. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate

Below is the demo in English, हिंदी (Hindi), తెలుగు(Telugu)

English:

हिंदी (Hindi)

తెలుగు (Telugu)

Code:

Medium: https://kmeeraj.medium.com/11-mean-square-error-gradient-descent-ae9bdb33548f

github : https://github.com/kmeeraj/machinelearning/tree/develop

Github Demo : https://github.com/kmeeraj/machinelearning/blob/develop/algorithms/Mean%20Square%20Error/Mean%20Square%20Error%20Gradient%20Descent.ipynb

colab: https://colab.research.google.com/gist/kmeeraj/64be04af2bec608270aa3d57ca018b08/11-mean-square-error-gradient-descent.ipynb

Gist: https://gist.github.com/kmeeraj/64be04af2bec608270aa3d57ca018b08

Mean squared error: https://en.wikipedia.org/wiki/Mean_squared_error

Reference : https://towardsdatascience.com/gradient-descent-from-scratch-e8b75fa986cc

Stochastic Gradient Descent: https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31

Social Media:

https://www.linkedin.com/in/meeraj-k-69ba76189/

https://facebook.com/meeraj.k.35

https://www.instagram.com/meeraj.kanaparthi1/",https://medium.com/@kmeeraj/11-mean-square-error-gradient-descent-ae9bdb33548f,['Meeraj Kanaparthi'],2020-11-25 19:29:10.328000+00:00,121,"mean squared error, MSD, estimator, expected value, squared error loss"
3 Data Foundation Problems That You Should Fix In 2021,"ML Deployment

Photo by Vadim Sherbakov on Unsplash

The usual product of data science is a predictive model. When starting the journey three years ago, I delivered the prediction model's result in a CSV file format. And the model is saved in a pickle format for manually scoring by a data scientist.

That’s a traditional way to get things done.

Luckily, the technology has developed so fast that today we have many libraries/frameworks like MLflow or others for deploying machine learning models.

At my previous company, the engineering team also developed the platform for automating predictive model scoring. They combined the latest framework mentioned above with the in-house knowledge to create a unique platform.

It makes a data scientist's life easier. It contains several useful features and reduces the step for data scientists to deploy the model.

It takes care of both feature store and predictive model’s metrics. But it’s still has a gap to improve. Here they are.

Monitoring

The deployed model needs to be monitored. The performance of the model can change for many reasons. A good blog from Databricks described how many problems you can face after deploying the model. I highly recommend you to read it.

In summary, there are three issues that can make the prediction looks weird after model deployment

1) Data diff, 2) Concept diff, and 3) Upstream data changes.

Those problems can silently affect the prediction result of the model. We should plan to prevent it because our model will be used for something valuable like recommending the product to the end customer.

If the model goes wrong, it can make a huge loss to our business.

The consistency of the prediction result is also important. For monitoring it, there are two parts to be concerned here

1) Data, and 2) Model

We need to ensure the quality by implementing the data quality checking for the data side as in the previous section. The system should alarm the data quality defect first before you know the deviation/defect from the model prediction.

Also, we can use a library like MLflow to track the model metrics such as AUC, Precision, Recall, or others for the model part. Then, we can set the threshold to trigger the alarm if those metrics deviate from the current standard you set.

You can trackback the reason behind the changes with the help of a data quality checking system.

Finally, a dashboard or report can be created from logging metrics. It helps tracking the metrics like the number of models that meet the standard performance or the changes in prediction distribution from the model.

Re-training model

Here is another problem from my experience. When you use the model for some period, the longer time the model is used, the more likely it becomes stale.

You need to have a re-training feature for the ML deployment system. Otherwise, there will be a lot of hard work waiting for data scientist at the end of the ML pipeline.

The re-training model is to add more new data or shift the period of data you use to make the model adapt to the customer's current behavior.

Pro tips : It’s hard to define when to re-training the model. You can make a simulation of the several re-training periods and see when the model performance drops.

Testing

The last problem about the ML deployment is testing process. Testing usually is a must thing to do in software development. We expect the system to reproduce the same result every time.

But it’s hard to do in ML deployment because the model is figuring out the information from the uncertainty in data, it’s difficult to make a reproduceable result after you fitting the model.

We can do our best by testing other aspects of the deployment.

For example, the type of input data and output prediction, the unexpected behavior in results such as null value or the prediction that’s doesn’t make sense.

We can generate the test case for each category of the model like classification, regression.

This helps improving the model output quality and lowers the chance of error for the consumer side system (downstream part) like the front end to display its prediction value.",https://towardsdatascience.com/3-data-foundation-problems-that-you-should-fix-in-2021-39b4693e3cc,['Pathairush Seeda'],2020-12-16 04:01:04.515000+00:00,670,"ML Deployment, MLflow, Data Quality Checking, Model Metrics Tracking, Re-training Model"
How to Create an Equitable and Unbiased AI Algorithm for Criminal Justice,"How do we fix this?

Well, the answer to that is not easy. As shown above, we looked at and analyzed the COMPAS dataset, the ML steps used to create an algorithm, and the problematic nature of bad training data. When we analyzed the COMPAS dataset, we found out that there wasn’t any bias stemming from the dataset, as the COMPAS set is just a collection of different input data taken into account. It does not generate risk levels by itself.

The ML steps used to create an algorithm are also not biased or unfair. They are just simplified steps used to create a model and test it.

This leaves us with training data. As shown above, the COMPAS dataset and ML steps do not have any racial bias in them which therefore means they can not be responsible for these racial disparities found in these algorithms. So, the only logical answer is bad training data. As explained above, the training data used in these algorithms isn’t the best suited for these algorithms.

These algorithms are usually supplied with historical crime data. As explained above, historical crime data poses a lot of problems for the algorithm as it is not always unbiased or fair. For example, if an algorithm discovered that higher income was correlated with lower recidivism, the algorithm would naturally give defendants from high-income backgrounds a lower score of recidivating.

Also, bad training data doesn’t always have to be historical crime data, it could also be data surveyed and taken in the wrong area. For example, if the training data is taken in Europe and then supplied to algorithms that are used in America, that in itself poses a massive problem. While approximately 13% (43 million) of the United States population is composed of black people, only 2.9% of Europe's population (22 million) is composed of black people. If training data is conducted in Europe, with most of the participants being white, then you could expect a fair and equitable algorithm for white people. However, if you try to apply this training data to a multicultural and multiracial country, such as the United States, it's pretty evident and obvious that these racial disparities do stem from bad training data.

One way we could fix this evident problem is by just making better training data. Companies that wish to make fair and equitable algorithm tools should not just rely on historical crime data but should strive to generate their own data. And when these companies are generating their own data, they should take in a multitude of participants from different racial backgrounds. Creating data like this would ensure that the algorithms trained from this data, would become fair and equitable and help create a more just and honest justice system.",https://medium.com/swlh/how-to-create-an-equitable-and-unbiased-ai-algorithm-for-criminal-justice-61fb75ab0f1e,['Mark King'],2020-12-09 16:48:26.293000+00:00,455,"Machine Learning, Algorithm Bias, Training Data, Recidivism, Multiculturalism"
【SHOPEE CHALLENGE 2020】 — User Spending Prediction 第五名做法分享,"本次比賽主辦方給了兩個月的準備期，但是實際比賽時長只給兩個小時(當場出題目)，非常考驗 Coder 的基本功： 1. Data processing 2. EDA 3. Feature engineering & Domain Knowledge 4. Data resampling 5. Module Selection 加分項:Sampling、Leakage 洞察、環境選擇

Shopee competition 2020 — Kaggle：https://www.kaggle.com/c/iamthebestcoderopen2020/overview

宣傳一下…

隊名:台灣梯度下降第一品牌

隊員四人:

Ethan — Github、我 — Github、MichaelShen — Github、Lau Kuan Ting

《2019 autumn E.Sun Bank AI open competition- credit card fraud detection》:

隊名：菜雞互啄

1. 2nd of final selected 20 teams in business solutions competition. 2. Top 1%(15/1366) in F1 score predict competition.

《2020 Shopee Code League- series of Kaggle competition in Asia-Pacific(open category)》

隊名：在蝦皮中叫外賣是否搞錯了什麼

1. 14th in Sentiment Analysis(NLP task).

2. 34th in Product Title Translation(NLP task).

3. 38th in Marketing Analytics(Recommendation forecasting task).

👉 本次成績….

《I’m the Best Coder! Challenge 2020(open category)》 — 5th in User Spending Prediction.

賽前準備工作 — Pipeline白板

Pipeline 白板很重要，它可以幫助我們在面臨未知問題時能節省大量工作，我們拿Shopee Challenge 2019 — Marketing Analytics 的資料做為假想資料，並考慮各種特殊情況下產出Pipeline白板。

我們 Pipeline 白板的流程大概長這樣:

資料處理

分辨主檔是很重要的，通常拿Submit檔案當主檔(有目標Y值，也有主要key值)，其他當描述檔，接下來的工作就很簡單了:想辦法把資料描述檔塞進主檔裡，(注意mapping過程Key值不能一對多)。

這時就考驗平常處理資料的功夫了，不外乎就幾種:

1. 時間序列與數字的轉換。

2. 文字資料的處理

3. 資料與資料間的mapping。

4. 資料缺失值的合理填充、刪除。

5. 高基數資料的轉換

其實當有一個資料描述檔Mapping到主檔後，就可以直接跑模型了，就算還有其他描述檔還沒有Mapping到主檔也沒關係 😆。

Feature engineering

特徵工程直接決定了模型的表現能力，但兩個小時能做的其實也不多，以下列出幾個這次有用到的:

Encoding: One-Hot encoding, Target encoding, mean-encoding

Grouping: K-means, DBSCAN

Dimensionality reduction: PCA, Kernal-PCA

rescaling: min-max rescale, 統計標準化

Grouping 是一定要做的，現在不做，後面的資料重採樣也得做。

EDA

這一步其實就可以略窺模型的能力了，特徵做得越好，目標特徵(Y值)分離度越大，模型表現越好。

用 EDA 評估 Feature 就可以知道做出來的變數是好變數還是渣渣。

Data resampling

這一步決定的模型Robust的能力，做得好模型就有好的泛化能力。

資料重採樣有兩個重點:

1. 分類任務中，標籤的數量要保持”平衡”(這部分見仁見智，但是我從來沒有搞懂”平衡”的定義… 😂)。

2. 抽到的樣本要具有代表性(如果Grouping做不好，抽樣的代表性就差，模型效能直接炸開 💥)。

Module Selection

唯一支持 XGBoost!!

不知道的 Boosting 原理的童鞋們，請參考周志華教授所撰寫的西瓜書。

如果要我用一句話來表示Boosting，它就是個 **可以把 “渣渣”模型變成 “我就是屌”模型(周杰倫?)…**的算法。

心得

打過幾次比賽後開始體認到Machine Learning是一個完整的資料分析框架，已經成為做Data的必備技能：清晰的資料處理邏輯、運用Feature engineering & Domain Knowledge製作變數、變數好壞評估、模型選擇…等。

以上技能是必備，更要滿足輕量、快速部署、算法效率/效能高要求…等要求，如何達到？刷些leetcode，然後做出自己的Pipeline吧！",https://medium.com/@ts01174755/shopee-challenge-2020-user-spending-prediction-%E7%AC%AC%E4%BA%94%E5%90%8D%E5%81%9A%E6%B3%95%E5%88%86%E4%BA%AB-f597288c2b6c,[],2020-12-08 16:28:15.788000+00:00,155,"Machine Learning, Data Processing, EDA, Feature Engineering & Domain Knowledge, Data Resampling"
Stock Market Clustering-Data Science Projects,"In this project, we will be extracting live Stock Market data from yahoo finance. We will find similarities amongst various companies using their stock market prices and then cluster them into different clusters using the K-means algorithm.

Note that is an unsupervised machine learning problem and will use an unsupervised machine learning technique with the help of the K-means algorithm.

NB: “pandas_datareader” extract data from various Internet sources into a DataFrame. Currently, the following sources are supported:

Yahoo! Finance

Google Finance

St.Louis FED (FRED)

Kenneth French’s data library

World Bank

Google Analytics

….and few others

Before we start, please note that this tutorial is part of the Data Science Project Mastery Program, which is a practical hands-on data science tutorial for anyone to learn data science right from the basics to advance by building projects and great Data Science Portfolio. Feel free to check it out.

In order for you to make the best out of this tutorial, I have put this tutorial in the form of a video for better understanding

Watch and work along

INTRODUCTION",https://medium.com/total-data-science/stock-market-clustering-data-science-projects-6eb275796297,[],2020-08-29 07:37:46.653000+00:00,164,"stock market, yahoo finance, pandas_datareader, unsupervised machine learning, K-means algorithm"
Is Gold worth investing over Stocks in 20 years?,"Thanks to the amazing R libraries gganimate and ggplot2 which helped in creating the animated chart.

I will post the details of creating the charts in another post. Until then stay tuned.",https://medium.com/ymedialabs-innovation/gold-returns-over-different-periods-45acbf33297e,['Sanjeev Kumar'],2019-04-11 16:08:59.349000+00:00,31,"gganimate, ggplot2, animatedcharts, datavisualization, Rlibraries"
SVM: Feature Selection and Kernels,"Data points on one side of the hyperplane will be classified to a certain class while data points on the other side of the hyperplane will be classified to a different class (eg. green and red as in Figure 2). The distance between the hyperplane and the first point (for all the different classes) on either side of the hyperplane is a measure of sure the algorithm is about its classification decision. The bigger the distance and the more confident we can be SVM is making the right decision.

The data points closest to the hyperplane are called Support Vectors. Support Vectors determines the orientation and position of the hyperplane, in order to maximise the classifier margin (and therefore the classification score). The number of Support Vectors the SVM algorithm should use can be arbitrarily chosen depending on the applications.

Basic SVM classification can be easily implemented using the Scikit-Learn Python library in a few lines of code.

from sklearn import svm

trainedsvm = svm.SVC().fit(X_Train, Y_Train)

predictionsvm = trainedsvm.predict(X_Test)

print(confusion_matrix(Y_Test,predictionsvm))

print(classification_report(Y_Test,predictionsvm))

The are two main types of classification SVM algorithms Hard Margin and Soft Margin:

Hard Margin: aims to find the best hyperplane without tolerating any form of misclassification.

aims to find the best hyperplane without tolerating any form of misclassification. Soft Margin: we add a degree of tolerance in SVM. In this way we allow the model to voluntary misclassify a few data points if that can lead to identifying a hyperplane able to generalise better to unseen data.

Soft Margin SVM can be implemented in Scikit-Learn by adding a C penalty term in svm.SVC . The bigger C and the more penalty the algorithm gets when making a misclassification.

Kernel Trick

If the data we are working with is not linearly separable (therefore leading to poor linear SVM classification results), it is possible to apply a technique known as the Kernel Trick. This method is able to map our non-linear separable data into a higher dimensional space, making our data linearly separable. Using this new dimensional space SVM can then be easily implemented (Figure 3).

Figure 3: Kernel Trick [3]

There are many different types of Kernels which can be used to create this higher dimensional space, some examples are linear, polynomial, Sigmoid and Radial Basis Function (RBF). In Scikit-Learn a Kernel function can be specified by adding a kernel parameter in svm.SVC . An additional parameter called gamma can be included to specify the influence of the kernel on the model.

It is usually suggested to use linear kernels if the number of features is larger than the number of observations in the dataset (otherwise RBF might be a better choice).

When working with a large amount of data using RBF, speed might become a constraint to take into account.

Feature Selection

Once having fitted our linear SVM it is possible to access the classifier coefficients using .coef_ on the trained model. These weights figure the orthogonal vector coordinates orthogonal to the hyperplane. Their direction represents instead the predicted class.

Feature importance can, therefore, be determined by comparing the size of these coefficients to each other. By looking at the SVM coefficients it is, therefore, possible to identify the main features used in classification and get rid of the not important ones (which hold less variance).

Reducing the number of features in Machine Learning plays a really important role especially when working with large datasets. This can in fact: speed up training, avoid overfitting and ultimately lead to better classification results thanks to the reduced noise in the data.

In Figure 4 are shown the main features I identified using SVM on the Pima Indians Diabetes Database. In green are shown all the features corresponding to the negative coefficients and in blue the positive ones. If you want to find out more about it, all my code is freely available on my Kaggle and GitHub profiles.",https://towardsdatascience.com/svm-feature-selection-and-kernels-840781cc1a6c,['Pier Paolo Ippolito'],2019-09-12 13:23:44.671000+00:00,627,"SVM, Support Vectors, Hard Margin, Soft Margin, Kernel Trick"
How to Scrape Predicted Lineups from Fantasy Football Scout with Python,"1. Scrape ALL the HTML

To scrape the HTML and to prepare it for parsing we’re going to use two popular Python libraries: Requests and Beautiful Soup:

import requests

from bs4 import BeautifulSoup

We want to take all of the HTML from from our target page. We can do this with the ‘requests’ and load the plain plain text element of the resulting object:

html = requests.get(url).text

Now in order to use our parsing library (Beautiful Soup) we have to load this into a ‘soup object’ to parse the HTML more easily.

soup = BeautifulSoup(html, ""html.parser"")

We’re now ready to being parsing the HTML to pull out the data that we’re interested in!

2. Find the lineups

There are all sorts of ways to parse through this BeautifulSoup object, I’ve decided to use ‘find_all’ to parse Fantasy Football Scout’s HTML.

lineups_raw = soup.find_all(""div"", {""class"": re.compile('formation.*')})

This line of code looks for all classes inside the HTML within a ‘div’ that match the regex expression: “formation.*”.

The regex is required as Fantasy Football Scout splits its tags for different formations. The HTML tags look like:

< div class =""formation formation-3–4–3>

or

< div class =""formation formation-4–3–3>

The lineups_raw object now contains twenty elements that contain the lineups for each respective team. In order to iterate through the raw data we require a simple loop:

for team in raw_data:

lineup_dictionary[key] = team.text.split(' ')[1:]

logger.info(f'Lineup {key}: {lineup_dictionary[key]}')

assert len(lineup_dictionary[key]) == 11

key += 1

This will loop through each element and add add the list of players to a dictionary with an integer key.

3. Find the team names

Now to find the bits of HTML that identify each of the teams by their respective names.

team_names_raw = soup.find_all('header')

Apart from the first two objects in this list, we get a complete collection of the teams names. In order to iterate through this data we’re going to need another simple loop:

for team in range(2, 22):

team_map[team-2] = raw_data[team].text.split('Next')[0]

logger.info(f'{team_map[team-2]} mapped to {team-2}')

The range spans from 2 to 22 to account for the first two objects that we’re not interested in, this is also why team_map has the team-2 key assigned to it instead of simply team .

Note that this dictionary has keys which will match up to the previous dictionary that we have created.

4. Match the lineups to the team names

These matching keys are important as we’re now going to create a dictionary which brings the values of both dictionaries into a combined dictionary:

for key in range(0, 20):

formatted_dictionary[mapping_dictionary[key]] = lineups_data[key]

logger.info(f'Team: {mapping_dictionary[key]}')

logger.info(f'Lineup: {formatted_dictionary[mapping_dictionary[key]]}')

The resulting dictionary will be of the format:

{team1: [player1, player2... player11], ... team20: [...] }

And that’s it! You can now play with this data and use it for a larger project. Let me know if you have any questions and I’d be happy to help you out!

You can find the full solution and contribute additional features here.",https://medium.com/python-in-plain-english/how-to-scrape-predicted-lineups-from-fantasy-football-scout-with-python-b11e92ea57f2,['Liam Hartley'],2020-12-01 22:32:32.091000+00:00,446,"HTML Scraping, Requests Library, Beautiful Soup, Parsing HTML, Lineups Data"
"Quantum Computers, Stable Marriages, and the Intimidating Future we will Witness","Credits: Trendintech

I do not believe in spirits. Reality is either classical or quantum, approximately. Classical, as in visible, and quantum, as in sub-atomic or too small for the naked eye. The world of the quantum is a fascinating one. One where it is difficult to separate imagination from reality; a beautiful intersection — one of uncertainties and miracles. Miracles. Miracles like being simultaneously dead and alive, being at more than one place at a time, miracles that made Einstein accuse God never to play dice! For quantum oddities, even God was summoned to the labs. If for nothing else, God must have thought of physicists as more daring than two million Alexander the Greats combined. But this is not about who has a broader courage, it’s about mathematics, and the revelations that come from pruning reality with it.

In this article, I make an attempt at describing the physics powering the new cutting-edge technology of quantum computing. I end by introducing the technology. In a sequel, I will dig deeper.

In the spirit of going deep, what would your response be if you were to be asked what will be found in a single piece of your skin if you looked closely enough? Go ahead, make a guess. Tiny flashy matter? Could it be elastic black or white stuff? Or just stuff unknown? Well, we find atoms. By even drilling deeper, we encounter atomic constituents — from electrons to protons and quarks. These little beauties make us up from blood to cells and skin, they are the magic. At that scale reality changes and quantum mechanics is dedicated to understanding what goes on in that strange realm. What we find have far-reaching consequences.

One is a limit on determinism. What this tells us is that there is fundamentally a limit to how much we can know about truth, about the position and speed of a particle for example. Shoot a golf ball. You can estimate the speed and approximate its position to high degrees of accuracy, maybe to 12 decimal places. However, this is not a problem physically because we are classical bodies, and so a measure that detailed is insignificant. It is a huge deal in quantum mechanics, so important that it has come to be known as Heisenberg’s Uncertainty principle — you cannot know the position and speed of a subatomic particle simultaneously.

So then you may wonder, how do we measure quantities precisely in quantum mechanics? We do not. We guess, we calculate probabilities. It is these probabilities that we superpose together — and collapse in a process known as decoherence — to compute the reality of the particle we are trying to measure. This is the idea demonstrated by the hypothetical experiment put together by Erwin Schrodinger in his 1935 talk with Einstein.

Schrodinger’s Cat

In Quantum Mechanics, it is possible to be dead and alive at the same time! As ridiculous as that sounds, it is the cornerstone of the next generation computers that threatens to break your bank encryptions. We have stated that quantum mechanics is about probabilistic realities; so Schrodinger imagined that if we had a cat in a box with a poison jar and a means to break the jar, it is possible it will break it as much as it she might not. This means there is almost equal chance that the cat would be dead and alive. But how do we deterministically know? We cannot! We combine both solutions and conclude that the cat is in fact dead and alive.

Quantum computers are a consequence of this simultaneity. As opposed to bits in classical computers, they are powered by Qubits (Quantum Bits). These bits, unlike classical bits which are either 0 or 1, are 0 and 1 at the same time. Isn’t this strange? You are not alone, it is Hogwarts! But stick with me, with us as we briefly explore some realities of this magical dimension of our world.",https://medium.com/@agbodozie/quantum-computers-stable-marriages-and-the-intimidating-future-we-will-witness-881b03257efd,['Agbo Dozie'],2021-06-08 02:05:32.608000+00:00,653,"Quantum Computing, Qubits, Heisenberg Uncertainty Principle, Schrodingers Cat, Atomic Constituents"
When Data Science fails us,"2020 was a very special year, covid hits the world at the beginning of the year and changed the plans of all industries. Timelines have been changed, customer interactions transformed and internal communication dematerialized. As a result, all of our models that were based on previous years and historical wisdom have been questioned and challenged. What is their relevance in today’s environment?

“How do we reconcile the need to use the past as a guide with our recognition that the future may be different?” The signal and the noise

When can historical data fool you?

A common practice in data science is to examine available historical data in order to draw useful conclusions for better planning.

‘Studying the European market of the 90’ would certainly be of great help to a historian; but what kind of inference can we make now that the structure of the institutions and the markets have changed so much.’ Nassim Taleb

A dataset is a tabular representation of what has happened in the past. But this representation is also accompanied by uncertainties, biases, and errors. In addition, it represents only a subset of the phenomenon you are studying and may not reflect the subset that is relevant to your problem (data availability).

Several caveats should be noted when reviewing the historical data:

the distribution may not be stationary

may not be the initial conditions are different today

are different today the actors (contributors) have evolved

(contributors) have evolved the external interactions have evolved

have evolved how to take into account the randomness ?

? the unknown unknowns (things that you’re not even aware of and that are not included)

In this context, similar inputs can lead to different results.

Is your drug really effective?: the data selection step

When designing clinical trials, an important step is the selection of the patient population. Which population category should be selected? Which age group, what is their condition, their origin, their biomarkers? How can we ensure that this sample is as close as possible to the target customers to whom we want to provide the drug, without missing anyone?

Conversely, if your sample does not accurately represent the population, your drug may be less effective than what your trial showed. For example, if you develop a drug for people with breast cancer, but do not include diabetics with obesity in your sample, you may later discover that these people do not respond well to treatment and cannot use it.

Photo by National Cancer Institute on Unsplash

The same applies to the use of historical data to build inferences, but the limitations are even more subtle to grasp. If you look at all the past data on financial loans, you may think that they reflect the general trend and that the future is likely to be the same. However, these financial loans may have been made only to people with sufficient wealth and did not take into account the rest of the population since they are not even included in the dataset.

Do you really have a causality?: The covariate shift

When the pattern is primarily a reflection of variance and randomness, the causality identified will be misleading to understand the actual behavior of the phenomenon. This is what happens to some very successful traders who benefit from the variance of trading results as Nassim Taleb points out in his book “Fooled by Randomness”. [1]

“Many are so unoriginal, they study history to find mistakes to repeat” Nassim Taleb

The challenge may be even more subtle, as a recent study found that some models trained to detect pneumonia on chest x-rays had poorer results on actual data not visible in the training set. This is partly due to the difference in image acquisition and processing.[2]

The covariate shift is when your input training set differ from the real-world data (Image by author)

This is what we call the covariate shift. When the distribution of your input variables is different in your production model (real setting) compared to your training set.

Are Blacks more often targeted by the police in the United States?: The denominator bias

Models are built not only from data but more importantly from the subset of available (vs non-available) data that we decide to focus on. This decision influences the direction the model takes in learning and can lead to more discrimination.

In her article, Laura Bronner sheds light on the misleading statistics published on the violence against the black community. [3] Many of the published statistics relate to the number of deaths per encounter with the police. However, these statistics hide the discrimination that occurs before, when police had discretion over who they engaged with on the street. The denominator of the measures should not be the encounter, but the population.

Photo by Nicole Baster on Unsplash

Similarly, in his interview with HBO, Donald Trump said that the U.S. has performed well compared to other countries, using the number of deaths per infected metric.

Around the 14'-15' of the interview, Trump tries to defend the poor performance of the United States in the fight against the pandemic by using this statistical flaw. The diagram below highlights the statistical flaw in his reasoning.",https://towardsdatascience.com/when-data-science-fails-us-78599de6423c,['Andy Spezzatti'],2020-11-12 03:00:41.965000+00:00,837,"Historical Data, Covariate Shift, Denominator Bias, Stationary Distribution, Unknown Unknowns"
Helping AI understand team sports with BigQuery and GPS data,"In this article I’ll walk you through the SQL statements that I used to preprocess GPS sport data for machine learning. The aim was to train a model to make predictions on future games. In order to help the model understand as much about the game as possible I decided to generate features that could generalise across the whole dataset.

BigQuery is a data warehouse in Google Cloud that offers a free tier for investigative projects. It has multiple built in GIS functions for geographical data processing and provides machine learning capabilities through SQL. These attributes make it an ideal place to load GPS sport data. See link below for how to load data into BigQuery.

Averaging Coordinates

After my GPS data was loaded I converted my latitude and longitude fields into a coordinate field using the SQL below.

ST_GEOGPOINT(sensor_long, sensor_lat)

As the granularity of my GPS data was very fine (1 coordinate /team/millisecond), I decided to average the coordinates to save on storage and query costs. I did this by using the ARRAY_AGG and ST_CENTROID function and saved the results into a permanent table like this:

SELECT timestamp, ST_CENTROID_AGG(location)

FROM `<PROJECT>.<DATASET>.gps_data`

GROUP BY 1

Filtering Coordinates

To further remove surplus data I filtered out coordinates that did not reside on the pitch.

Using google maps https://www.google.com/maps/d/u/0/ I drew a Polygon for each pitch and added two LineStrings to the sides of each pitch that I wanted to use as axes. I then loaded both geography files into BigQuery by converting from KML to geoJSON by adding the geoJSON to a column in a CSV file. An example data format is shown below.

# pitch.csv polygon;name

{""type"": ""Polygon"", ""coordinates"": [[[x,y], ...]]};Pitch1

# leftside.csv linestring;name

{""type"": ""LineString"", ""coordinates"": [[x,y], ...]};Pitch1 ....

Using my pitch data in BigQuery I filtered the data to leave only the coordinates on the pitch using the following SQL.

WHERE ST_CONTAINS(

ST_GEOGFROMGEOJSON(polygon),

ST_GEOGPOINT(sensor_long, sensor_lat)

) = true

Projecting Coordinates

The next transformation I made was to project the coordinates to a distance_from_left, distance_from_top coordinate system that generalises across the entire dataset.

Using the LineString geographies drawn in google maps I calculated the distance at right angles to the side of the pitch to every coordinate with the following SQL statement. This gave me a new coordinate system for my data where the origin is always the top left of the pitch.

ST_DISTANCE(ST_GEOGFROMGEOJSON(left_side_of_pitch),coordinate) AS

distance_from_left,

ST_DISTANCE(ST_GEOGFROMGEOJSON(top_side_of_pitch),coordinate) AS

distance_from_top,

Feature Generation

Using my new coordinates system I engineered some new features that were relevant to the understanding of the game. For example the distance between adjacent players.

To calculate the adjacent player I used analytic function which allows me to use an aggregate function on a window of the data. Ordering by distance_from_left I grouped the list of players with an ARRAY_AGG and partitioned by second and team. I then used an UNNEST and the OFFSET field to fetch the coordinate of the player to the left of the current coordinate from the the grouped list of players. This query is shown below.

WITH collect_player_list AS (

SELECT

sensor_game_seconds,

sensor_player_id,

player_loc,

ARRAY_AGG(player_loc) OVER (

PARTITION BY sensor_game_seconds, sensor_team_name

ORDER BY distance_from_left

) AS players

FROM `<PROJECT>.<DATASET>.grouped_gps_data`

WHERE player_loc IS NOT NULL

), player_position_from_the_left AS ( SELECT *

FROM collect_player_list,

UNNEST(players) AS closest_player WITH OFFSET position

WHERE ST_EQUALS(closest_player, player_loc)





) SELECT

sensor_game_seconds,

sensor_player_id,

player_loc,

CASE

WHEN position > 0 THEN players[Offset(position-1)]

ELSE NULL

END AS player_on_the_left_loc

FROM player_position_from_the_left

Combining Data Sources

I combined the GPS data with some manually labelled event data using a timestamp in order to generate a column for who is in possession of the ball. Due to the sparseness of the event data I needed to fill in the NULL possession values for every row of GPS data between events.

To achieve this, I used one analytic function to keep track of the number of possession changes so far in the game. I then used a second analytic function partitioned by player and the number of possession changes to fill in the NULL possession values. This query is shown below.

WITH event_and_gps_data AS ( SELECT *,

CASE

WHEN possession_flag THEN 1 ELSE 0 END AS in_possession,

FROM `<PROJECT>.<DATASET>.event_and_gps_data`





), running_sum_of_change_in_possession AS ( SELECT *,

SUM(in_possession) OVER (

PARTITION BY 1

ORDER BY sensor_game_seconds

) AS change_in_possession_running_sum

FROM event_and_gps_data





)



SELECT *,

CAST(

MAX(in_possession) OVER (

PARTITION BY sensor_player_id,change_in_possession_running_sum

ORDER BY sensor_game_seconds

)

AS BOOL) AS interpolated_possession_flag

FROM running_sum_of_change_in_possession

Data format for modeling

The final thing I needed to generalise across the dataset was the individual players. This was important as it meant that the model could understand the role of each type of player across the entire dataset.

As I knew the position of every player, I generated separate columns for each individual position and feature using the shortened SQL below. This allowed my model to understand the types of players and not just their behaviour on the pitch.

SELECT sensor_game_seconds,

ANY_VALUE(IF(positionName='LWinger', speed, NULL)) AS lwinger_speed,

ANY_VALUE(IF(positionName='LWinger', x, NULL)) AS lwinger_x,

ANY_VALUE(IF(positionName='LWinger', y, NULL)) AS llwinger_y, ANY_VALUE(IF(positionName='Halfback',speed, NULL)) AS halfback_speed

... FROM `<PROJECT>.<DATASET>.preprocesses_data`

GROUP by sensor_game_seconds

To Conclude

I generated unbiased training data that allowed my model to understand the game better and make predictions on future games.",https://medium.com/weareservian/helping-ai-understand-team-sports-with-bigquery-27453f6ca974,['Paddy Green'],2020-12-23 17:21:57.562000+00:00,775,"SQL, Big Query, GIS, GPS, Data Warehouse"
Disease Detection in Plants,"Introduction

The primary occupation in India is agriculture. More than 50% of people depend on it & shares a major part of the GDP. Here in India, farmers cultivate a great variety of crops. Many factors such as climatic/soil conditions, various diseases affect the production of the crops. The existing method for plant disease detection is very simple and it requires more man labour, equipped laboratories, expensive devices. There are few challenges including Symptom Segmentation, Symptom variation, Image background, Image capture condition, which are very crucial.

Dataset

I have used public and very famous, the PlantVilage dataset. It was published by crowdAI during one challenge named “PlantVillage Disease Classification”

Link of dataset:

https://www.crowdai.org/challenges/plantvillage-disease-classification-challenge

The dataset contains around 54k images of plant leaves. These plant images include following 14 types of different species:

Apple, Blueberry, Cherry, Corn, Bell Pepper, Potato, Raspberry, Grape, Orange, Peach, Soybean, Squash, Strawberry, and Tomato

Following is a list of 38 classes of plant disease,

1. Apple Scab

2. Apple Black Rot

3. Apple Cedar Rust

4. Apple healthy

5. Blueberry healthy

6. Cherry healthy

7. Cherry Powdery Mildew

8. Corn Gray Leaf Spot

9. Corn Common Rust

10. Corn healthy

11. Corn Northern Leaf Blight

12. Grape Black Rot

13. Grape Black Measles

14. Grape Leaf Blight

15. Grape healthy

16. Orange Huanglongbing

17. Peach Bacterial Spot

18. Peach healthy

19. Bell Pepper Bacterial Spot

20. Bell Pepper healthy

21. Potato Early Blight

22. Potato healthy

23. Potato Late Blight

24. Raspberry healthy

25. Soybean healthy

26. Squash Powdery Mildew

27. Strawberry Healthy

28. Strawberry Leaf Scorch

29. Tomato Bacterial Spot

30. Tomato Early Blight

31. Tomato Late Blight

32. Tomato Leaf Mold

33. Tomato Septoria Leaf Spot

34. Tomato Two Spotted Spider Mite

35. Tomato Target Spot

36. Tomato Mosaic Virus

37. Tomato Yellow Leaf Curl Virus

38. Tomato healthy

As there are numerous advantages to train model in the cloud because training requires huge computation power. (If you have a good GPU then you are good to go and you can train in your local computer as well)

I will use Google colab but you can use as per your choice.

(Google colab is a free cloud service which offers free graphics processing units (GPU — 12 GB RAM). It’s one of the fastest ways to train model)

Download the plant village dataset and Upload it to Google Drive.

Sign-in to Colab and Create a new IPython Notebook (ipynb)

Switch to GPU and get ready to fly!

Mount data from google drive.

from google.colab import drive

drive.mount(‘/content/your path’)

Import Libraries

Now, create a function to convert or resize the input dataset images so that they can be fit for training.

After that, iterate through dataset folder, resize the images from each folder and load them to numpy array.

Since there are thousands of images, it will take time to load. After loading dataset, map each plant disease to a unique value for training task. Also, Save this transform to a pickle file.

TRAIN — TEST — SPLIT

Then we split our loaded dataset with train and test set with 0.2 split ratio.

(i.e., 80% Training & 20% Testing)

Data Augmentation

Data Augmentation is used to artificially expand the size of a training dataset by creating modified versions of images (such as rotation, zoom, flip rotation) in the dataset. It will help model to learn features efficiently. Moreover, it can perform better on validation data.

Model

Now, create a sequential model for the classification task.

Model — Hyper Parameters

Define all the hyperparameters. Keep it separate in individual cell so that It will be easier to edit it later on.

Model — Training

Select optimization technique and compile it.

Take a break guys…!!!

Model Evaluation

Performance is great but, [what] has the model learnt?

From the above plot, we can see that we can train model little bit more for as the trend for accuracy is rising in last few epochs.

From the above plot of loss, we can see that initially validation loss was high but finally loss decreased to a great extent.

By looking at the both above graphs, it is clearly visible that, as the training accuracy increases, validation accuracy increases. Likewise, as the training loss decreases, the validation loss decreases.

Evaluate() and check

BOOM!

Accuracy: 95.31% !!

The model has great performance on unseen data as well.

It’s time for a REAL TEST

Choose random images from the dataset and try predicting disease of the plant image.

Model — Reuse

Save model using,

model.save(“modelname.h5”)

load model,

This is how, one can load model and use trained model for prediction.

References:

http://www.ijareeie.com/upload/2019/march/14_Plant.pdf

https://towardsdatascience.com/plant-ai-plant-disease-detection-using-convolutional-neural-network-9b58a96f2289",https://medium.com/analytics-vidhya/disease-detection-in-plants-d68a924859aa,['Manan Patel'],2020-12-31 16:59:20.843000+00:00,681,"agriculture, India, crops, plant disease detection, Plant Vilage dataset"
The magic of graphs,"geeksforgeeks.org

I believe that a graph is probably one of the most complicated data structures. One can say, it even looks confusing (check the picture at the top!), with all these messy elements aka nodes and lines. Ironically, this is one of the most commonly used data structures in the real world.

Where can we meet these mysterious graphs? Well, almost everywhere. Here are some vivid examples you’ve more than definitely encountered.

social networks (Facebook, Linkedin, etc.)— every time you see a friends recommendation (technically that means, you share a lot of friends(=nodes) with this person, so the system suggests you’d like to add this very person to your friend list as well)

(Facebook, Linkedin, etc.)— every time you see a friends recommendation (technically that means, you share a lot of friends(=nodes) with this person, so the system suggests you’d like to add this very person to your friend list as well) location on a map/routing (Google Maps, Apple Maps, etc.) — direction is a route between two or more nodes you’d like to navigate between

(Google Maps, Apple Maps, etc.) — direction is a route between two or more nodes you’d like to navigate between streaming services (Netflix, HBO)— every time you get a new movie recommendation (same genre(=node) as you usually watch)

Sounds exciting but wait: what is actually a graph?

Wikipedia says that a graph is a data structure that consists of a finite (and possibly mutable) set of vertices (also called nodes or points), together with a set of unordered pairs of these vertices for an undirected graph or a set of ordered pairs for a directed graph.

Well, this doesn’t make a lot of sense. To put it simply, a graph is a collection of nodes(=elements) and connections(=lines) between these nodes. It has no starting point/head. However, there is a terminology useful to know.

Vertex — a node.

Edge — a connection between the nodes.

Directed graph — a graph that has a direction to the edges. It is a one-way connection often represented with arrows that indicate the exact direction (or, polarity). A good example of a directed graph is Instagram where following is not mutual by default.

researchgate.net

Undirected graph — a graph that has no direction to the edges. That represents a two-way connection. A strong example of an undirected graph is mutual friendship on Facebook.

researchgate.net

Unweighted graph — a graph that doesn’t have any values assigned to edges. All the graphs above are unweighted.

Weighted graph — a graph with values assigned to edges. Probably the most common example of a weighted graph is Google Maps — here, a value is a distance between two places. Knowing the distance helps to choose the best and shortest way to a destination. Here is how a weighted graph could look.

hyperskill.org

To wrap this short graph introduction up, here is a fun fact: a tree data structure is also a type of graph!

Source:",https://medium.com/dev-genius/the-magic-of-graphs-acaebea155ca,['Anastasia Orlova'],2020-11-20 09:50:31.500000+00:00,473,"Graphs, Nodes, Edges, Directed Graphs, Undirected Graphs"
Python Pandas: A Complete Guide for Beginners,"Pandas is a python library used in data manipulation ( create, delete, and update the data).

It is one of the most commonly used libraries for data analysis in python. Pandas offer data structures and operations for manipulating numerical and time-series data. Pandas is the go-to library when it comes to Python

[pandas] is derived from the term “panel data”, an econometrics term for data sets that include observations over multiple time periods for the same individuals. — Wikipedia

Pandas Use Cases

Calculate statistics and answer questions about the data (Mean, Median, Mode)

Check correlation between Two datasets

See the distribution of values of a dataset

Clean the data by removing missing values or filtering rows and columns by some criteria

Visualize the data with help from Matplotlib. Plot bars, lines, histograms, bubbles, and more.

Store the cleaned, transformed data back into a CSV, another file.

Rise of Popularity of Pandas:https://theatlas.com/charts/rJ9sZ5syf

Pandas Setup

Pandas is an easy package to install. Open up your terminal program (for Mac users) or command line (for PC users) and install it using either of the following commands:

conda install pandas

OR

pip install pandas

import pandas as pd

Reading Data

In pandas, we have a function name as read_csv which is used to read .csv files.Similarly to import a JSON file we use read_json .

import pandas as pd

df = pd.read_csv('data.csv')

#data.csv is the relative filepath for data df = pd.read_json(‘data.json’)

DataFrame operations

DataFrames possess hundreds of methods and other operations that are crucial to any analysis. As a beginner, you should know the operations that perform simple transformations of your data and those that provide fundamental statistical analysis.

df = pd.read_csv(""Data.csv"", index_col=""Heading"")

Viewing your data

The first thing to do when opening a new dataset is to print out a few rows to keep as a visual reference. We accomplish this with .head() :

df.head()

.head() outputs the first five rows of your DataFrame by default, but we could also pass a number as well: df.head(10) would output the top ten rows, for example.

To see the last five rows use .tail() . tail() also accepts a number, and in this case, we print the bottom five rows.:

df.tail(5)

Get the info of your data

.info() should be one of the very first commands you run after loading your data:

df.info()

OUT:

<class 'pandas.core.frame.DataFrame'>

RangeIndex: 100 entries, 0 to 99

Data columns (total 2 columns):

0 100 non-null float64

1 100 non-null float64

dtypes: float64(2)

memory usage: 1.6 KB

.info() provides the essential details about your dataset, such as the number of rows and columns, the number of non-null values, what type of data is in each column, and how much memory your DataFrame is using.

Calling .info() will quickly point out that the column you thought was all integers are actually strung objects.

Another fast and useful attribute is .shape , which outputs just a tuple of (rows, columns):

df.shape (100, 2)

Note that .shape has no parentheses and is a simple tuple of format (rows, columns). So we have 1000 rows and 2 columns in our movies DataFrame.

Column cleanup

Many times datasets will have verbose column names with symbols, upper and lowercase words, spaces, and typos. To make selecting data by column name easier we can spend a little time cleaning up their names.

Here’s how to print the column names of our dataset:

X= -2 * np.random.rand(100,2)

Y = 1 + 2 * np.random.rand(100,2)

df=pd.DataFrame(X,Y)

df.columns={

""RANDOM X"",""RANDOM Y""} print(df.columns) Index(['RANDOM Y', 'RANDOM X'], dtype='object')

Missing values

When exploring data, you’ll most likely encounter missing or null values, which are essentially placeholders for non-existent values. Most commonly you’ll see Python’s None or NumPy's np.nan , each of which is handled differently in some situations.

df.isnull().sum() RANDOM Y 0

RANDOM X 0

dtype: int64

.isnull() just by itself isn't very useful, and is usually used in conjunction with other methods, like sum() .

df.dropna(axis=1)

You can drop columns with null values by setting axis=0 or rows by setting axis=1

Understanding your variables

Using describe() on an entire DataFrame we can get a summary of the distribution of continuous variables:

df.describe()

Output

Relationships between variables

By using the correlation method .corr() we can generate the relationship between each continuous variable:

df.corr()

Corr between X and Y

Extracting Data

By column

You already saw how to extract a column using square brackets like this:

X_col = df[‘RandomX’]

type(X_col)

pandas.core.series.Series

By Rows

For rows, we have two options:

.loc - loc ates by name

- ates by name .iloc - locates by numerical index

Conditional Selection

df[df[‘RANDOM X’] >= -10].head(3)

X≥-10

Plotting

Another great thing about pandas is that it integrates with Matplotlib, so you get the ability to plot directly off DataFrames and Series. To get started we need to import Matplotlib ( pip install matplotlib ):

import matplotlib.pyplot as plt



plt.rcParams.update({‘font.size’: 20, ‘figure.figsize’: (10, 8)}) # set font and plot size to be larger

df.plot(kind='scatter', x='RANDOM X', y='RANDOM Y', title='X vs Y')

Scatter Plot

df[‘RANDOM X’].plot(kind=’hist’, title=’X’)

Histogram of X

Using a Boxplot we can visualize this data:

df[‘RANDOM X’].plot(kind=’box’)

That’s It!

Exploring, cleaning, transforming, and visualization data with pandas in Python is an essential skill in data science.

Just cleaning wrangling data is 50% of your job as a Data Scientist. Thanks for reading!

Found this article useful? Follow me (Rahula Raj) on Medium and check out my most popular articles below! Please 👏 this article to share it!",https://medium.com/analytics-vidhya/python-pandas-a-complete-guide-for-beginners-a695a3aa3596,['Rahul Raj'],2020-10-16 13:09:35.899000+00:00,813,"pandas, python, data analysis, data manipulation, data wrangling"
Review of DataCamp - Learning Skills for the Future of Work,"Data Science is one of the hottest fields in computer science nowadays and is expected to grow exponentially in the coming years. The number of job postings asking for data scientists has steadily increased, giving rise to a number of online Ed-tech companies that sell courses to teach buyers data wrangling chops. According to LinkedIn, in August 2018 in the U.S., the tech industry was experiencing a shortage of 150,000 data scientists and the demand for skills in data science has only gained more steam since then. DataCamp is an upstart in the online Ed-tech industry that trains folks with little to no technological background in data science and associated fields. DataCamp and its online peers are filling the gap of not enough technically skilled professionals by providing supplementary education at a fraction of the cost, in lieu of formal university degrees.

In a nutshell, DataCamp subscription is worth its money only if you are a newbie to the data science field. For intermediate and advanced users, the courses aren’t thorough and deep enough to deliver much value. Read on for a detailed review.

DataCamp was founded by Martijn Theuwissen, Jonathan Cornelissen, and Dieter De Mesmaeker in the year 2013. Since then DataCamp has come a long way and now offers a number of courses varying in skill and difficulty level for languages and tools used extensively in the data science domain such as Python, R, Power BI, Excel etc. So far the company is purported to have raised $31.1 million dollars, thanks to the explosive growth in the number of users, with the most recent funding raise of $25 million in 2017.

The site provides various choices for users to customize their learning experience. The spectrum of instruction formats is as follows:

Pursue individual courses catering to a specific and narrow topic.

catering to a specific and narrow topic. Enroll in a skill track that consists of a series of curated courses aimed at teaching a broader skill-set e.g. “Importing and Cleaning Data” or “Data Manipulation” .

that consists of a series of curated courses aimed at teaching a broader skill-set e.g. “Importing and Cleaning Data” or “Data Manipulation” . Follow a career track that focuses on teaching several skills expected of a industry role such as a data scientist, R programmer, data engineer, machine learning scientist etc.

DataCamp offers various pricing tiers for individuals as well as businesses. However, you can only purchase annual subscriptions, though the marketing displays the monthly as a monthly cost (billed yearly). This is a slight departure from other Ed-tech companies such as Educative that offer monthly subscriptions. The free tier lets you access the first chapter of each course in addition to seven projects, three practice challenges and one skill assessment. You may sign-up to get a look and feel of the user interface and play around.

Overall, the quality of the courses is pretty solid. Each course is designed to be interactive with in-browser code execution and comes with video tutorials. You can easily switch between the various modes depending on your liking. There’s also the option to switch from reading a course on your desktop to your mobile phone, while you are on the move. The interface is gamified and lets users collect points called “XP”-s if they successfully complete exercises without receiving hints or seeing the answers. The courses are non-graded and provide a statement of completion at the end that can be shared on the LinkedIn profile of a user.

Note that the content of the courses is not novel in any sense. You can always google any topic for free-content and setup a Python IDE on your local machine for hands-on practice. The real value DataCamp delivers is in streamlining your learning experience through a one-stop portal. You don’t have to bother yourself with installing a pandora box of tools and dependencies on your local machine and open a dozen chrome tabs for each online blogs you find on Google for a given topic. At the same time, DataCamp falls short of the credibility and rigorousness of a university degree in Data Science rather it teaches practical, and industry focused tools and skills. Most of the Ed-tech companies side-step dry theory and head-spinning maths that accompanies data science and DataCamp is no different.

One notable drawback DataCamp suffers from is the absence of interview preparation content. We expect majority of DataCamp’s readership to be students, budding technologists, and folks switching to computer science realm from other professions. The end for these users is a well-paid job and data science is a means to that end. DataCamp has baked the cake but is missing the icing. It would be ideal if DataCamp introduced courses similar to the ones offered by Educative that focus solely on interview preparation for Data Science jobs. Some of Educative’s competing offerings are:

All in all, if you are a motivated self-learner or someone mulling changing careers, DataCamp is a good start, especially for the latter who want to spend thousands of dollars enrolling in a university-accredited data science program. You can start with DataCamp and have a firsthand experience of what it feels like to work in the data science realm, much like a litmus test to ascertain your aptitude for the field before plowing in serious dollars.",https://medium.com/double-pointer/review-of-datacamp-learning-skills-for-the-future-of-work-3dfafd012212,['Double Pointer'],2020-12-09 21:09:59.193000+00:00,874,"Data Camp, Data Science, Ed Tech, Python, RR"
Beautiful Boxplots With Statistical Significance Annotation,"The statistical tests

Step 4: Now it’s time to do the statistical tests. We will use a two-sample t-test (since our group are independent) to test if the mean value of any of these 4 features (i.e. sepal length, sepal width, petal length, petal width) is statistically different between the 2 groups of flowers (setosa and versicolor).

#* Statistical tests for differences in the features across groups

from scipy import stats

all_t = list()

all_p = list()

for case in range(len(feature_names)):

sub_df = df_long[df_long.Feature == feature_names[case]]

g1 = sub_df[sub_df['Group'] == 0]['Value'].values

g2 = sub_df[sub_df['Group'] == 1]['Value'].values

t, p = stats.ttest_ind(g1, g2)

all_t.append(t)

all_p.append(p)

To do the statistical test we just used:

t, p = stats.ttest_ind(g1, g2)

Here we compare the mean of g1 (group 1: setosa) to the mean of g2 (group 2: versicolor) and we do that for all 4 features (using the for loop).

But how can we know if the mean of g1 (group 1: setosa) was significantly greater or smaller than the mean of g2 (group 2: versicolor) ?

For this we need to look at the t-values.

print(all_t)

[-10.52098626754911, 9.454975848128596, -39.492719391538095, -34.08034154357719] print(feature_names)

['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

Interpretation:

If the t-value is positive (>0) then the mean of g1 (group 1: setosa) was significantly greater than the mean of g2 (group 2: versicolor).

If the t-value is negative (<0) then the mean of g1 (group 1: setosa) was significantly smaller than the mean of g2 (group 2: versicolor).

Reminder: feature_names = [‘sepal length (cm)’, ‘sepal width (cm)’, ‘petal length (cm)’, ‘petal width (cm)’].",https://medium.com/swlh/beautiful-boxplots-with-statistical-significance-annotation-cb78cad92149,['Serafeim Loukas'],2020-06-22 14:12:06.832000+00:00,242,"statistical tests, two-sample t-test, sepal length, sepal width, petal length"
Machine Learning: Model Representation And Hypothesis,"First, the goal of most machine learning algorithms is to construct a model or a hypothesis. In machine learning, a model can be a mathematical representation of a real-world process. To generate a machine learning model we will need to provide training data to a machine-learning algorithm to learn from.

Fundamental Segmentation of Machine Learning Models

All machine learning models are categorized as either supervised or unsupervised. If the model is supervised, it’s then sub-categorized as either a regression or classification model.

Supervised learning involves learning a function that maps an input to an output based on example input-output pairs — that is, given data with label (i.e. right answer)

Supervised learning problems can be further grouped into regression and classification problems.

Classification: A classification problem is when the output variable is a category, such as pass or fail, red or blue, etc. To illustrate, here we have training data with tumor size as its category which represents a feature and label respectively.

source: slide from lecture 1

In this example, we labeled 0 as benign and labeled 1 as malignant. Now we wish to make a surprised model. When new data comes in, our trained model predicts its label, that is, label 1 (malignant) or label 0 (benign). This is an example of a classification problem — given some possibly continuous input, we want to predict a discrete output.

Regression: A regression problem is when the output variable has a real value, such as weight, height, or dollars. To illustrate, here we have training data with housing size and price, which represents a feature and label respectively. When new data comes in, our goal is to predict its label, that is, the corresponding value of the housing price, in the range [0, 400]. This is an example of a regression problem — given some possibly discrete input, we want to predict a continuous output.

slide from lecture 1, week 1

Note:

Binary or Binomial Classification are used when we group data using two kinds of labels.

Multi-class or Multinomial Classification are used when we group data using more than two kinds of labels.

Unsupervised learning is used to draw inferences and find patterns from input data without references to labeled outcomes — that is, given data without any label (i.e without any right answer)

Unsupervised machine learning purports to uncover unknown patterns in data but most of the time these patterns are poor approximations of what supervised machine learning can achieve.

Supervise Learning vs. Unsupervised Learning

The biggest difference between supervised and unsupervised learning is that each data have a label in the case of supervised learning, whereas there is NO label for each data in the case of unsupervised learning, implying, they have not been classified.

source: slide from lecture 1, week 1

Note:

Supervised learning will always have an input-output pair.

Unsupervised learning is just data without a label nor meaning that we try to make some sense out of it.

We use unsupervised machine learning when we do not have data on desired outcomes, such as determining a target market for a new product that a business has never sold before. However, if we are trying to get a better understanding of our existing consumer base then supervised learning is the optimal technique.

Making a Model with Supervised Learning

So far, we learned that the primary goal of most of the machine learning algorithms is constructing a model to make predictions. We can restate this model as a hypothesis.

The hypothesis basically maps an input to an output. An input variable refers to a feature and an output variable refers to a target. The dataset we will be using to learn out model is called a training set.

Our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y.

source: Slide from lecture 2, week 1

The figure above diagrammatically represents model h(x). So, let's say, we train a model based on a bunch of housing data that includes the size of the house and the sale price. By training a model, we have developed a model that will estimate the output (Y) — the price of houses based on our input(X) — the size of the houses. This is an example of linear regression.

Linear regression performs the task of predicting a target y based on given features x. So, this regression technique finds a linear relationship between x (input) and y(output).

The hypothesis is usually presented as

The theta values are the parameters

Linear regression gives us a straight line that best fits the data points. Let’s visualize the hypothesis with some examples.

This yields h(x) = 1.5 + 0x. 0x means no slope and y will always be the constant 1.5. This looks like:

h(x) = 1.5 + 0x

let’s see another example:

h(x) = 1 + 0.5x

The goal of creating a model is to choose parameters or theta values so that h(x) is close to y for the training data, x and y.

Wrapping up, we’ve learned about supervised learning, unsupervised learning, and linear regression. In the next article, we will talk about the Cost Function.

Keep on, keeping on!

Resources:

Unsurprisingly, there are many resources on Model Regression. However, not all of them are necessarily easy to understand! Some of them dive deep into the theory and mathematics of the problem, and it can be hard to find beginner-friendly content. Here are a few resources that are fairly easy to understand, and a good place to get started if you’re looking to learn more:",https://medium.com/nothingaholic/machine-learning-model-representation-and-hypothesis-f9f1dfa9570f,['Xuankhanh Nguyen'],2020-07-05 12:50:07.202000+00:00,902,"Machine Learning, Supervised Learning, Unsupervised Learning, Binary Classification, Multinomial Classification"
Perform Better Data Analysis in Python with Pandas,"Perform Better Data Analysis in Python with Pandas

A Python library that can be used to better understand a dataset is Pandas.

Data Analysis

Data analysis is about asking and answering questions about the dataset.

You need to spark questions about the dataset that you can pursue to better understand it. One way to do this is by summarizing and visualizing the dataset.

Pandas

The Pandas Python library is built for fast data analysis and manipulation. The strength of Pandas seems to be in the data manipulation side, but it comes with very handy and easy to use tools for data analysis, providing wrappers around standard statistical methods in statsmodels and graphing methods in matplotlib.

Onset of Diabetes

The UIC Machine Learning repository provides a vast array of different standard machine learning datasets that can use to study and practice applied machine learning. Lets consider the Pima Indians diabetes dataset.

The dataset describes the onset or lack of onset of diabetes in female Pima Indians using details from their medical records. Download the dataset and save it into your current working directory with the name pima-indians-diabetes.data.

Summarize Data

We will start out by understanding the data that we have got by looking at its structure.

Load The Data

Start by loading the CSV data from file into memory as a data frame. We know the names of the data provided, so we will set those names when loading the data from the file.

import pandas as pd names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] data = pd.read_csv('pima-indians-diabetes.data', names=names)

Describe Data

We can now look at the shape of the data. We can take a look at the first 60 rows of data by printing the data frame directly.

print(data)

We can see that all of the data is numeric and that the class value on the end is the dependent variable that we want to make predictions about. At the end of the data dump we can see the description of the data frame itself as a 768 rows and 9 columns. So now we have idea of the shape of our data.

Next we can get a feeling for the distribution of each attribute by reviewing summary statistics.

print(data.describe())

This displays a table of detailed distribution information for each of the 9 attributes in our data frame. Specifically: the count, mean, standard deviation, min, max, and 25th, 50th (median), 75th percentiles.

We can review these statistics and start noting interesting facts about our problem. Such as the average number of pregnancies is 3.8, the minimum age is 21 and some people have a body mass index of 0, which is impossible and a sign that some of the attribute values should be marked as missing.

Visualize Data

A graph is a lot more telling about the distribution and relationships of attributes.

Nevertheless, it is important to take your time and review the statistics first. Each time you review the data a different way, you open yourself up to noticing different aspects and potentially achieving different insights into the problem.

Pandas uses matplotlib for creating graphs and provides convenient functions to do so.

Feature Distributions

The first and easy property to review is the distribution of each attribute.

We can start out and review the spread of each attribute by looking at box and whisker plots

import matplotlib.pyplot as plt data.boxplot()

Box and Whisker Plots

We can see that the test attribute has a lot of outliers. We can also see that the plas attribute seems to have a relatively even normal distribution. We can also look at the distribution of each attribute by discretization the values into buckets and review the frequency in each bucket as histograms.

data.hist()

This lets you note interesting properties of the attribute distributions such as the possible normal distribution of attributes like pres and skin.

Histogram Matrix

Feature-Class Relationships

The next important relationship to explore is that of each attribute to the class attribute.

One approach is to visualize the distribution of attributes for data instances for each class and note and differences. You can generate a matrix of histograms for each attribute and one matrix of histograms for each class value, as follows:

data.groupby('class').hist()

The data is grouped by the class attribute (two groups) then a matrix of histograms is created for the attributes is in each group. The result is two images:

Histogram Matrix for Class 0

Histogram Matrix for Class 1

Feature-Feature Relationships

The final important relationship to explore is that of the relationships between the attributes.

We can review the relationships between attributes by looking at the distribution of the interactions of each pair of attributes.

from pandas.plotting import scatter_matrix scatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde')

This uses a built function to create a matrix of scatter plots of all attributes versus all attributes. The diagonal where each attribute would be plotted against itself shows the Kernel Density Estimation of the attribute instead.

Scatter Plot Matrix

This is a powerful plot from which a lot of inspiration about the data can be drawn. For example, we can see a possible correlation between age and preg and another possible relationship between skin and mass.",https://medium.com/python-in-plain-english/analysis-with-pandas-bce54d40ae57,['Johar M. Ashfaque'],2020-12-28 19:39:17.409000+00:00,817,"Data Analysis, Pandas, Python Library, Statistical Methods, Visualization"
On the Horizon: The Future of Marketing — and my test to see if it’s here.,"It’s my humble opinion that all of the marketing strategies, tactics, and methodologies we marketers hear today — “Account Based Marketing”, “Inbound Marketing”, “Growth Hacking” — are simply a means to describe a what’s yet to come in the world of marketing. Each of the aforementioned marketing terms — with practice, dedication, and the right talent — can be achieved by any organization. Each offering valuable opportunities to grow your business. This article isn’t to “dethrone” any of these types of marketing and growth tactics…

However, I believe these various marketing methodologies, tactics, etc. are simply inching us closer to the future. Little by little they are getting us to think more deeply about the way we impact the businesses and customers we interact with. And that’s a really good thing.

I’ll admit that I don’t have much research (yet) to substantiate my prediction except for what I observe and hear through great marketers and brands all across the web but — as you’ll see below — the future of marketing is already poking it’s head out in the world of social media.

The future (in my humble opinion) is about the intersection of data, creativity and strategy. It’s about providing customers with a uniquely valuable experience or insight that only you can provide.

I think the future of marketing will unfold with the convergence of three key elements…

1. The future of marketing starts with technology.

The proliferation of data and the unprecedented access we have to it are beginning to shape the future of marketing. And, as the cost of cloud-based processing decreases while Artificial Intelligence (AI) and data science rapidly advance, the future of marketing is just starting to appear on the horizon.

2. The future of marketing depends on key players.

If you don’t have a data scientist or a data analyst (I’m certain the difference in roles is substantive but I don’t know what it is yet), then you better get in line. It’s a hot topic and a hard role to fill. In my opinion it’s key to the future of marketing.

Don’t worry I’ll get to the part where I tell you what I think future of marketing is…just keep reading.

Data Science and the employees that perform this critical role aren’t the only key players in the future of marketing. You’ll still need creative and strategic marketing roles too. Without creative and marketing strategy, data is just a bunch of 1's and 0's.

3. The future of marketing requires “must have” products.

Sean Ellis and many other entrepreneurs will tell you that you won’t have anything to market unless you first establish product market fit and ideally create a “must have” product.

Note: The beauty of the future of marketing I see is that it will help perpetuate the stickiness and likelihood your product/service becomes “must have”.

_________ Marketing

I believe the convergence of these three elements means we’re headed toward a new marketing methodology. I don’t know how to describe this new method of marketing but for now, I’ll refer to it as “insight-based” or “feedback” marketing.

As promised in my intro, the future of marketing is already poking it’s head out in the world of social media.

Here’s an example from Facebook:

We’ve all seen the notifications or content in our feeds.

“Here’s what you did this time last year!”

Or

“Happy Friendversary, Mike!”

btw I never login to Facebook anymore and I deleted the app from my phone, but sure enough while writing this post I logged in to see If i could find an old reference to illustrate my point and this is what came up…

It’s an absolute joy to see those videos and that type of content produced for us. It’s insightful and pleasant and everything Facebook needs to increase virality and stickiness. — Although I don’t use Facebook on my phone anymore I still see the value ;)

So, the real question is how do you apply insights like this to your B2B or SaaS business? How can we use our data and creativity to increase the stickiness and ultimately the effectiveness of our marketing campaigns?

The answer is data, creativity, and marketing strategy.

What’s next?

I’m on a mission to provide value and insight to customers powered by data. I want to scientifically, strategically and creatively adapt our data to create a feedback loop of transparency and valuable insight to benefit our customers in a way they would otherwise not see from competing products. But I can’t do it alone. I truly believe the future of marketing is about data, creative, and strategic marketing teams working together.

I’ll be testing my theory — that insight-based/feedback marketing is the key to helping our customers and our company achieve new levels of success — at AerServ.

Here’s what I’ll be doing…

I’m not going to give you the specifics because I don’t want to spoil the test (or let my competition in on my plans too early).

I’ll leverage data to provide our customers with insightful feedback on their performance and I’ll make it extremely easy to take advantage of the insights we uncover. By doing so, I believe both our customers and our business will see positive financial results and I hope that we’ll be seen as a trusted and valued partner.

I think a lot of businesses can do what I’m planning to do, but it’s a matter of time before they all try it. I’m sure someone else is already testing this theory, but in the coming year I hope to begin this test and report back on how it went.

If you’re doing this or you’ve seen a company do this, please provide a comment and description of what you saw and what you thought of it!",https://towardsdatascience.com/on-the-horizon-the-future-of-marketing-and-my-test-to-see-if-its-here-58e89ea48c7a,['Mike Rizzo'],2017-07-21 05:31:48.794000+00:00,932,"Account Based Marketing, Inbound Marketing, Growth Hacking, Data Science, Artificial Intelligence"
Serverless Orchestration with AWS Step Functions: Lessons Learned,"The London Philharmonic Orchestra performing at the Southbank Centre

The emergence of Cloud computing, and the more recent emergence of MLOps (the combination of Machine Learning and Operations), has shown that there is an eagerness from businesses to take advantage of Machine Learning technology. Although businesses in certain sectors already use ML, many are still in the early stages of adoption. They are yet to advance their ML capabilities to something more than just a science experiment.

The Applied Data Science and Machine Learning (ADSML) team at Sainsbury’s Tech carries a vision: “To give colleagues and customers access to automated data-driven support for all their complex decision making”. We want to be able to help make these decision-making algorithms as accessible and as automated as possible.

A year ago or so, the ADSML team was new to the world of serverless architecture and building data pipelines that productionise data science algorithms. We built our first couple of pipelines with similar approaches. The first being 10–12 different Lambda Functions chained together, with each triggered by the output of the previous Lambda landing a file in S3. Each Lambda would perform a small part of the pipeline. This would include ETL (Extract-Transform-Load), algorithm execution and post processing. Although these pipelines worked well, we soon figured out the limitations of this approach and of Lambda Functions in general. These limitations were:

Messy S3 buckets: loads of files landing into different areas in S3

No single view of execution flow, no graphical/easy way to track the executions in real-time

With around 8000 execution per day, it made it very difficult to find single points of failure

It was difficult to install branching logic. Branching logic is when the pipeline is able to split and go down different routes depending on the outcome of a step. For example, there may be a step which determines whether we should retrain a model, if it does, it will proceed down the retraining route, if it doesn’t, it will proceed down the inference route.

The solution to this would be to use a tool which can orchestrate all our Lambda Functions. The tool should allow engineers to build pipelines where points of failure are easy to identify, errors are dealt with and state isn’t lost. This is where a service named AWS Step Functions comes in.

Step Functions is an AWS service which gives users a reliable way to chain together all components of a pipeline. It’s a fully managed service which means that you won’t have to worry about setting up the infrastructure in order to run it. All of this is handled by AWS so there’s no more painful machine configuration and maintenance (see Airflow). Various AWS services are available to use, and executions can be coordinated and tracked in a visual way. You can create pipelines which are easy to run and debug, but it also makes branching or parallel steps easy.

Step Functions uses state machines which allow you to define your workflows as individual tasks called “States”. Each state can perform many different functions, which defines the “Type” of state you want to use. This includes:

Task state (do some work)

Choice state (make a choice between branches of execution)

Parallel state (begin parallel branches of execution)

and more

The configuration of state machines is written in Amazon’s States Language which is a JSON-based, structured language used to define each one of your states. The following is an example from the AWS Step Functions developer guide. It shows a state named HelloWorld that executes an AWS Lambda function.

""HelloWorld"": {

""Type"": ""Task"",

""Resource"": ""arn:aws:lambda:us-east-1:0000000000000:function:HelloFunction"",

""Next"": ""AfterHelloWorldState"",

""Comment"": ""Run the HelloWorld Lambda function""

}

Within each state definition you will need to specify the following:

The name of the state

The ‘Type’ of state (as described above)

The type of resource you want to use. This can be invoking a Lambda function, creating a SageMaker training job, etc.

The name of which state will come next. This is what allows you to chain states together

Then, as you create more states and chain them together, you can always refer to the visual representation of your workflow to see how it works. Below is an example of a simple workflow we developed.

{

""StartAt"": ""generate_uuid"",

""States"": {

""generate_uuid"": {

""Type"": ""Task"",

""Resource"": ""arn:aws:lambda:eu-west-1:0000000000000:function:generate_uuid"",

""ResultPath"": ""$.run_metadata"",

""Next"": ""set_config""

},

""set_config"": {

""Type"": ""Task"",

""Resource"": ""arn:aws:lambda:eu-west-1:0000000000000:function:set_config"",

""ResultPath"": ""$.config"",

""Next"": ""prepare_feature_set_correlation""

},

""prepare_feature_set_correlation"": {

""Type"": ""Task"",

""Resource"": ""arn:aws:lambda:eu-west-1:0000000000000:function:prepare_feature_set_correlation"",

""ResultPath"": ""$.run_metadata"",

""Next"": ""run_register_task_correlation""

},

""run_register_task_correlation"": {

""Type"": ""Task"",

""Resource"": ""arn:aws:lambda:eu-west-1:0000000000000:function:run_register_task_correlation"",

""ResultPath"": ""$.run_metadata"",

""End"": true

}

}

}

The config defines how the workflow will run across the multiple Lambda functions. It shows that the state machine starts from the ‘generate_uuid’ state which runs a Lambda function, and ends after the ‘run_register_task_correlation’ state, which also runs a Lambda function.

Plugging this config into Step Functions produces the lovely graphic of your pipeline, shown below.

Workflow visualisation provided by the Step Functions console.

With this graphical view of the pipeline, any state that passes will light up green, whereas failed states will light up red. This makes it easier to go straight to the relevant error messages to find out what’s gone wrong. Cool, right?

Although Step Functions has improved our ways of working and helped us manage our pipelines, it does come with some limitations:

State machines have to run from the beginning each time. This means if for some reason the workflow fails at any step, you can’t restart it from that step. This can become annoying if one of the first steps is a time consuming ETL step.

Amazon States Language has a bit of a learning curve and can possibly be a deterrent for engineers who are more used to something like Airflow.

Step Functions integration is currently limited to certain AWS services

Like all other AWS services, Step Functions has limits. Make sure to check those before determining whether Step Functions is the best tool to use for your use case.

Data has now become a focal part of our business so it is now essential to productionise the exploitation of that data. This will enable the decision makers of the business to do their jobs faster, and with more confidence. The ADSML team always looks to re-evaluate how we deliver our solutions, and the introduction of Step Functions is a great example of that. As we continue to develop the data pipelines with an orchestra of services, Step Functions will be continue to be the conductor.",https://medium.com/sainsburys-data-analytics/serverless-orchestration-with-aws-step-functions-lessons-learned-3ba143538b8f,['Fatlum Vranovci'],2019-10-22 08:48:16.143000+00:00,1015,"London Philharmonic Orchestra, Southbank Centre, Cloud Computing, MLOps, Machine Learning"
Is Decision Tree a classification or regression model?,"Before we jump in to finding the answer for the above question, let’s try to understand what the “Decision tree” algorithm is.

So, what is a Decision tree?

If we strip down to the basics, decision tree algorithms are nothing but a series of if-else statements that can be used to predict a result based on data set. This flowchart-like structure helps us in decision making.

The idea of a decision tree is to divide the data set into smaller data sets based on the descriptive features until we reach a small enough set that contains data points that fall under one label.

Each feature of the data set becomes a root[parent] node, and the leaf[child] nodes represent the outcomes. For instance, this is a simple decision tree that can be used to predict whether I should write this blog or not.

Amazing isn’t it! Such a simple decision making is also possible with decision trees. They are easy to understand and interpret because they mimic human thinking.

Alright, now coming to main question “Is decision tree a classification or regression model?” To answer this question, first let us understand classification and regression using the below diagram.

In the above example, regression is used to predict the student’s marks. Whereas, classification is used to predict whether the student has passed or failed the exams.

So, what is the difference between regression and classification?

Regression is used when we are trying to predict an output variable that is continuous. Whereas, classification is used when we are trying to predict the class that a set of features should fall into.

A decision tree can be used for either regression or classification. It works by splitting the data up in a tree-like pattern into smaller and smaller subsets. Then, when predicting the output value of a set of features, it will predict the output based on the subset that the set of features falls into.

There are 2 types of Decision trees:

Classification trees are used when the dataset needs to be split into classes which belong to the response variable.

Regression trees , on the other hand, are used when the response variable is continuous.

In other words, regression trees are used for prediction-type problems while classification trees are used for classification-type problems.

How Classification and Regression Trees Work

A classification tree splits the dataset based on the homogeneity of data. Say, for instance, there are two variables; salary and location; which determine whether or not a candidate will accept a job offer.

If the training data shows that 95% of people accept the job offer based on salary, the data gets split there and salary becomes a top node in the tree. This split makes the data “95% pure”. Measures of impurity like entropy is used to quantify the homogeneity of the data when it comes to classification trees.

In a regression tree, a regression model is fit to the target variable using each of the independent variables. The data is then split at several points for each independent variable.

At those points, the error between the predicted values and actual values is squared to get “A Sum of Squared Errors”(SSE). The point that has lowest SSE is chosen as the split point. This process is continued recursively.

Advantages of Decision Trees

1. Decision trees are easy to interpret.

2. To build a decision tree requires some data preparation from the user but normalization of data is not required.

Disadvantages of Decision Trees

1. Decision trees are likely to over fit noisy data. The probability of overfitting on noise increases as a tree gets deeper.

To Conclude:

** A decision tree can be used for either regression or classification.

** Decision trees are easy to understand, visualize and interpret.

** The flowchart-like structure helps us in decision making.

Thanks for reading!",https://medium.com/datadriveninvestor/is-decision-tree-a-classification-or-regression-model-b1777d7733b1,['Bhuvaneswari Gopalan'],2020-10-08 14:43:03.692000+00:00,611,"decision tree, classification, regression, entropy, SSE"
The Beginner’s Guide to the Neo4j Graph Platform,"1. Setup

Java

Before you continue with the installation, you need to make sure that either OpenJDK 11 or Oracle Java 11 is installed in your machine. You can easily check it by running the following command in the terminal.

java --version

You should see the following output

Image by Author

I am using the OpenJDK 11 build made by AdoptOpenJDK, which is open-sourced and available for free.

Neo4j

At the time of this writing, the latest community version is 4.1.1. Head over to the official download page and download the neo4j main file based on your operating system. Once you click on the download link, the download will start and you will be redirected to a new page with the installation steps.

In this tutorial, the steps provided are based on the Windows platform. Kindly refer to the official installation steps if you are using other operating systems. Extract the files at the directory of your choice. You should see the following files and folders.

Image by Author

There are two ways to run Neo4j:

As console application

As service

Console application

The simplest method is to run it as a console application via the following command. NEO4J_HOME refers to the base directory where you extracted the file.

<NEO4J_HOME>\bin

eo4j console

The following output will be displayed at your terminal:

Image by Author

In console mode, the process is tied to the terminal. It needs to stay open in the foreground.

Service

On the other hand, service runs the process in the background. It will continue to run even if the terminal is closed. To install it as a service, run the following command:

<NEO4J_HOME>\bin

eo4j install-service

After that, you can start it normally via the following command:

neo4j start

The following list contains some of the useful commands based on Windows PowerShell module:

stop — Stop the process

— Stop the process restart — Restart the process

— Restart the process status — Check the status of the process

— Check the status of the process uninstall-service — Remove neo4j as service

Neo4j console

The next step is to access it in your browser. Open up a browser and go to the following URL.

It will redirect you to:

with the following user interface:

Image by Author

If you are accessing it for the first time, simply connect to the database via the following username and password:

username — neo4j

— neo4j password — neo4j

It will prompt you to change the password. Make sure you remember the new password, as subsequent logins will use the new password. Once you have logged in, you should see the following interface:

Image by Author

The part highlighted in red represents the input area where you can type in the query language to be executed.

You can find three buttons on the right side as follows:

star — Update the query as your favourite

— Update the query as your favourite eraser — Clear the input area

— Clear the input area play — Execute the query. You can use the shortcut key Ctrl+Enter to trigger the execution.

Each time you ran a command, results will be shown and recorded down. The part highlighted in blue represent the built-in command that will be run each time you refreshed the browser.

Sample datasets

Neo4j comes with two sample datasets for you. Run the following command if you intend to follow the built-in tutorial.

The first dataset demonstrates the common query patterns on actors and movies in cross-referenced pop culture. Run the following in the input area to start the tutorial.

:play movie graph

In addition, the second dataset covers on loading data from external csv files. You will learn to transition from RDBMS schema to graph structure using some classical datasets. You can start the tutorial via the following command:",https://medium.com/better-programming/the-beginners-guide-to-the-neo4j-graph-platform-a39858ccdeaa,['Ng Wai Foong'],2020-11-09 01:55:46.915000+00:00,586,"Java, OpenJDK 11, Oracle Java 11, Neo4j, Installation"
Top 10 Python Libraries for Data Science,"“T he goal is to turn data into information and information into insight.” Carly Fiorina — Former chief executive officer, Hewlett Packard.

There are so many wonderful libraries being added to the Python ecosystem every single day but these are the ones I’ve found the most intuitive and useful (and unsurprisingly the most popular)!",https://medium.com/analytics-vidhya/top-10-python-libraries-for-data-science-78a6a2c3871f,['Akhil Sonthi'],2020-09-24 13:00:02.142000+00:00,53,"Data Analysis, Python Libraries, Carly Fiorina, Hewlett Packard, Data Insight"
Open Datasets for Machine Learning and Deep Learning Tasks,"You can look for the desired data set, of course, from Google Dataset Search Engine:

Another must-check resource is Kaggle. Here, you do not find only datasets but also shared solutions for the tasks. If you are a new beginner, I strongly suggest starting from Kaggle.

Another searchable datasets resource is from the Papers with Code website. Currently, there are more than 3000 datasets.

Registry of Open Data on AWS exists to help people discover and share datasets that are available via AWS resources.

For Multi-label classification datasets and Multi-target regression datasets, you can try Mulan project website.

OpenML provides lots of datasets for you.

For Neural Machine Translation datasets, you can visit neural machine translation (NMT) at the Stanford NLP group.

If you are looking for Text Classification datasets, here 10 of them listed:

If you are looking for Datasets in Turkish or for Turkey, you can check the DataTurk web page.

Here is another blog for searching datasets.

Here is the Github web page for Turkish Language NTM datasets.

If you need Visual Datasets, the visualdata.io website is one of the best sites.

If you are using Tensorflow, you can download lots of datasets from TensorFlow Datasets: a collection of ready-to-use datasets.

If you would like to learn about Deep Learning with practical coding examples, please subscribe to my YouTube Channel or follow my blog on Medium.

If you would like to add any new resources please comment below.

Thank you for reading.

You can follow me on these social networks:

YouTube

Facebook

Instagram

LinkedIn

Github

Kaggle

Medium",https://medium.com/deep-learning-with-keras/open-datasets-for-machine-learning-and-deep-learning-tasks-2ae1f4f42b8,['Murat Karakaya'],2021-03-24 09:51:26.030000+00:00,237,"datasets, Google Dataset Search Engine, Kaggle, Papers with Code, Registry of Open Data on AWS"
An overview of predictive analytics in industrial production,"Image by Pixabay

Overview

The growing use of social networks, smartphones that collect and continuously generate data, the growing use of the Internet, the presence of sensors that measure and monitor everything, causes the volume of the produced data is growing exponentially, providing valuable information for society and for companies.

All this is Big Data, defined as a large collection of data volume and variety can not be managed with traditional database management tools, but require the use of new technologies and adequate data management systems for storing and analysis, are able to extract their value quickly.[1]

Image by https://www.stockvault.net/

With Big Data are experiencing a new revolution, the large amount of data and information available to us, are considered “black gold” of the new millennium.

They are fundamental to the predictive analysis and extrapolation of information (Data Mining) developed by research institutes and companies in support of their decision-making strategies.

In business intelligence is changing the way to manage information for decision support, they are developing new tools, and down the costs of data collection systems, storage and processing.

As mentioned Big Data are having great success in the analysis Predictive (Predictive Analytics), which is a variety of techniques that predict future results based on the analysis of historical and current data [2].

This type of analysis, using predictive techniques on machine learning models, is taking place in different sectors; for example in industrial production, marketing, in finance and in the energy sector.

An approach to Big Data in industrial production

The great potential offered by Big Data and Predictive Analytics, make it possible to resolve some issues by large corporations that ultimately are investing capital and resources in this direction, hiring data scientists, funding research projects and enhancing their data infrastructure, to predict failures and anomalies in the operation of certain machines based on the flow of data recorded by the sensors and optimize production.

With the Internet of Things (IoT) and technologies for the analysis and management of Big Data, such as Hadoop and Spark, companies can capture and process an amount of data increasing and this implies the need to identify models for predictive maintenance can improve decision support strategies.[3].

This implies the development and improvement of machine learning algorithms used in Predictive Analytics, such as the supervised algorithms, which develop of classification rules for training using a set of data known (labeled data) or unsupervised algorithms that try to locate the information contained within the data not previously known.

Image by Wikimedia.org

The main type of supervised techniques include neural networks, decision trees, Bayesian classifier or Support Vector Machine (SVM) but also linear and nonlinear regression algorithms. While unsupervised learning is based on cluster analysis using techniques such as K-Means clustering.

To process daily huge data moles it is necessary that the chosen models are very precise, and you have to create prediction models tailored to the type of manufacturing process to be controlled, to promote the reliability and performance. In the next image are shown stages of Predictive Analytics.

Image by Forrest Research

Many multinational or major companies are investing in these technologies bringing great profits, or provide services to maximize the potential of Big Data and predictive analysis.

For example, Amazon offers services to use machine learning technology and by AWS provides cloud computing services that help create, protect and distribute applications for Big Data and using these technologies, by recommendation systems, can understand the interests of customers obtaining huge profits.

In the transport sector Trenitalia has announced that it has invested in collaboration with SAP in an innovative project, the Internet of Things and the application of Predictive Analysis software on Big Data, called “Dynamic Maintenance Management”.

The system is based on hundreds of micro-sensors placed inside the trains, which provide information about board components by capturing a large volume of data, which is then reworked with machine learning models applied to maintenance.

With this investment will provide a significant improvement in service and reduce maintenance costs of traditional [4].

Future Perspectives

Doing predictive maintenance with Big Data, help avoid breakdowns and production blocks which could cause major economic losses for businesses but also to program the various production stages, making them less expensive and more efficient.

Furthermore, a big advantage that you could get, would reduce, if not eliminate, the time and cost of testing processes, having provided via the Predictive Analytics, the quality of the products during the production process [3].

The advent of Big Data and Machine Learning techniques is the technological evolution that will change in the coming years, the methods and strategies of the classical industrial production systems.

The versatility of these technologies enables applications in various fields such as in medicine, which is already used for diagnosing and predicting the spread of epidemics, or in the management of energy consumption within the Smart Grid, they are already popular in the marketing and in advertising with recommendation systems able to profile customer preferences, as well in sports, especially team sports through the analysis of historical information, the play action manage to extract valuable information and improve team strategies.

Bibliography and sources of inspiration for this work",https://medium.com/@antoniocastiglione-9550/an-overview-of-predictive-analytics-in-industrial-production-f1bd456ade68,['Antonio Castiglione'],2020-12-12 10:31:29.561000+00:00,831,"big data, predictive analytics, machine learning, internet of things, AWS"
Reverse Image Search — Find Similar images,"Photo by Format from Pexels

Have you ever wondered how google image search works or How amazon can retrieve products similar to the image that we upload in the app/site?

To achieve this task we will be using one simple method.

We are going to pick a pre-trained deep learning model, remove the top layers, and extract the convolutional features for the images in our dataset. Then we will use these feature vectors to find similar images by using sklearn’s nearest neighbor algorithm.

Let’s get started.

Import libraries

import requests

import os

import numpy as np

from numpy.linalg import norm

import joblib as pickle

from tqdm import tqdm

import os

import PIL

import time

import tensorflow as tf

from tensorflow.keras.preprocessing import image

from tensorflow.keras.preprocessing.image import ImageDataGenerator

import gc

from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input

#from tensorflow.keras.applications.MobileNet import MobileNetV2,preprocess_input

#from tensorflow.keras.applications.mobilenet import MobileNet,preprocess_input

import math

from sklearn.neighbors import NearestNeighbors

import matplotlib.pyplot as plt

import matplotlib.image as mpimg

from sklearn.decomposition import PCA

We will be using Caltech101 object dataset. A small description of the dataset — Pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. Collected in September 2003 by Fei-Fei Li, Marco Andreetto, and Marc ‘Aurelio Ranzato. The size of each image is roughly 300 x 200 pixels.

To know more about the dataset checkout http://www.vision.caltech.edu/Image_Datasets/Caltech101/

url = 'http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz'

r = requests.get(url)

filename = '101_ObjectCategories.tar.gz'

folderdername = filename.split('.')[0]

open(filename , 'wb').write(r.content) # reference https://stackoverflow.com/questions/30887979/i-want-to-create-a-script-for-unzip-tar-gz-file-via-python

import tarfile

if filename.endswith(""tar.gz""):

tar = tarfile.open(filename, ""r:gz"")

tar.extractall()

tar.close()

elif filename.endswith(""tar""):

tar = tarfile.open(filename, ""r:"")

tar.extractall()

tar.close()

In the dataset we have ‘Background google’ folder which contains random google images. So let’s remove it.

#!rm -rf '101_ObjectCategories/BACKGROUND_Google' #Linux

#os.removedirs('101_ObjectCategories/BACKGROUND_Google') #windows

Now we will use Resnet50 model which is pre-trained on the imagenet dataset (http://www.image-net.org/) to extract the features from the Caltech101 images dataset. In other words we will convert the images in the dataset to feature vectors using Resnet50.

Why resnet ?

We chose resnet because of the relatively small feature size. It converts images into 2048 convolutional features compared to 25088 features in vgg 19 or 51200 features in inception architectures. It will be easy for the nearest neighbor algorithm to find neighbors and also it will minimize the effects of Curse_of_dimensionality. https://en.wikipedia.org/wiki/Curse_of_dimensionality

Create Resnet50 model without top layers so we get convolutional features as output instead of the image class probability.

img_size =224



model = ResNet50(weights='imagenet', include_top=False,input_shape=(img_size, img_size, 3),pooling='max')

Here are creating a Keras image data generator object and preprocessing the images. After that we are passing the data generator object to the ResNet-50 model to extract the features.

batch_size = 64

root_dir = '101_ObjectCategories'



img_gen = ImageDataGenerator(preprocessing_function=preprocess_input)



datagen = img_gen.flow_from_directory(root_dir,

target_size=(img_size, img_size),

batch_size=batch_size,

class_mode=None,

shuffle=False)



num_images = len(datagen.filenames)

num_epochs = int(math.ceil(num_images / batch_size))



feature_list = model.predict_generator(datagen, num_epochs) print(""Num images = "", len(datagen.classes))

print(""Shape of feature_list = "", feature_list.shape) Num images = 8677

Shape of feature_list = (8677, 2048)

We have 8677 images in our dataset after removing the google background images. Each image is converted into a 2048 feature vector.

# Get full path for all the images in our dataset

filenames = [root_dir + '/' + s for s in datagen.filenames]

We will fit the nearest neighbor algorithm to the extracted features. We have picked ball_tree algorithm as it works well with sparse data.

neighbors = NearestNeighbors(n_neighbors=5,

algorithm='ball_tree',

metric='euclidean')

neighbors.fit(feature_list)

Let’s pick a new image that is not present in our dataset and try to find similar images for it.

from google.colab import files

uploaded = files.upload()

Once the image is uploaded extract the features for the image using the ResNet-50 and find the nearest neighbors for it.



# ref # ref https://datascience.stackexchange.com/questions/31167/how-to-predict-an-image-using-saved-model img_path = 'ant.jpg'

input_shape = (img_size, img_size, 3)

img = image.load_img(img_path, target_size=(input_shape[0], input_shape[1]))

img_array = image.img_to_array(img)

expanded_img_array = np.expand_dims(img_array, axis=0)

preprocessed_img = preprocess_input(expanded_img_array) test_img_features = model.predict(preprocessed_img, batch_size=1)



_, indices = neighbors.kneighbors(test_img_features)

Helper function to plot the images

def similar_images(indices):

plt.figure(figsize=(15,10), facecolor='white')

plotnumber = 1

for index in indices:

if plotnumber<=len(indices) :

ax = plt.subplot(2,4,plotnumber)

plt.imshow(mpimg.imread(filenames[index]), interpolation='lanczos')

plotnumber+=1

plt.tight_layout()

With the nearest neighbors indices provided by the model let’s find out the images similar to the one we uploaded.

print(indices.shape)



plt.imshow(mpimg.imread(img_path), interpolation='lanczos')

plt.xlabel(img_path.split('.')[0] + '_Original Image',fontsize=20)

plt.show()

print('********* Predictions ***********')

similar_images(indices[0])

Here we observe the image of a scorpion in the similarity images which is incorrect. Due to the large dimensionality of the features, the NearestNeighbors algorithm is not able to cluster similar features accurately.

To overcome this we will apply PCA to our features and reduce the dimensions and try to find similar features again.

pca = PCA(n_components=100)

pca.fit(feature_list)

compressed_features = pca.transform(feature_list)

Fit the nearest neighbors algorithm to the new features.

neighbors_pca_features = NearestNeighbors(n_neighbors=5,

algorithm='ball_tree', metric='euclidean') neighbors_pca_features.fit(compressed_features)

Transform the test image features as well and try to find similar images.

test_img_compressed = pca.transform(test_img_features)

distances, indices = neighbors_pca_features.kneighbors(test_img_compressed)

print(indices.shape)

plt.imshow(mpimg.imread(img_path), interpolation='lanczos')

plt.xlabel(img_path.split('.')[0] + '_Original Image',fontsize=20)

plt.show()

print('********* Predictions ***********')

similar_images(indices[0])

Great! Decreasing the dimensions of the feature helped us in getting better results.

What next ?

Try different pre-trained models like mobilenet,vgg16 etc. and check the accuracy.

Try data augmentation on the dataset.

We have used n_componets in PCA is 100. We can try changing these numbers and see how it affects our predictions.

Try cosine distance metrics for the nearest neighbor algorithm.

Note : As each class have only 8–20 images, accuracy may not be uniform accross all classes. Please try the model on a large dataset to get optimal accuracy.

Please let me know if you encounter any issue or unable to follow the blog.I am always open to feedback. Cheers ..!

Code is available here

Connect with me on Linkedin

Based on my learning from the book",https://medium.com/swlh/reverse-image-search-using-resnet-50-f305d735385a,['Mani Kanta'],2020-05-26 07:50:04.797000+00:00,811,"Google Image Search, Amazon Product Retrieval, Pre-trained Deep Learning Model, Convolutional Features, Sklearn Nearest Neighbor Algorithm"
"Impressive Charts on Age, Energy, Climate, and Music — DataViz Weekly","Lately, we’ve come across a lot of new impressive charts all over the internet. Here are some of the most interesting ones — check them out and you’ll see the real power of data visualization in action!

Today in DataViz Weekly:

New age for your age — FlowingData

Electricity generation in the U.S. states by fuel source — The Washington Post

Mortality consequences of climate change and income inequality — Bloomberg Green

1990s music recognition across generations — The Pudding

New Age for Your Age

“50 is the new 20.” Or “70 is the new 30.” Or “ some other age is some new different other age.” Many, if not all of us, have seen such headlines or heard similar expressions here and there. Basically, as the quality of life and life expectancy have been growing over time, it is hardly surprising nowadays to feel better (younger) than in the same age decades ago. But are there any strict and commonly accepted criteria to make conclusions about the exact difference?

Nathan Yau, a statistician, made an attempt to bring in more clarity with the help of data. For his research, he took mortality data from the Social Security Administration and looked at the probability of death by age now and in the previous decades, taking the respective curves as the point of reference.

Without more ado, we invite you to check out Nathan’s new article on his blog FlowingData, where he explains in detail what was done, step by step. As you read you will see multiple cool line (and step line) charts. The culmination is an interactive visual analysis tool that, for any age you enter, calculates the new age depending on sex and what decade you choose to compare to, as well as shows the respective curves for context.

Electricity Generation in U.S. States by Fuel Source

A bar mekko chart is a peculiar type of visualization which features bars of variable width to represent two quantitative measures at a time. Here’s an interesting example of how such graphic can be appropriate and useful in practice. John Muyskens, a graphics editor at The Washington Post, chose a bar mekko chart to display how the U.S. states generated electricity in 2019 based on data from the federal Energy Information Administration.

In the visualization, the width of each bar shows all electricity one state generated last year, in absolute values. Lengthwise, each bar represents 100% and shows the composition by fuel source type. The percentage for clean electricity (colored), including nuclear energy (solid color fill) and renewables such as wind, hydroelectric, solar, biomass, and geothermal (hatch fill), stacks above. Natural gas, coal, and other fossil fuels (gray) stack below.

This bar mekko chart is convenient to explore the electricity sources by state. If you are interested in this topic, the article where we found this chart, made by John Muyskens and Juliet Eilperin for The Washington Post, also has 100% stacked area charts that show the change in fuel source percentages for each state from 2001 through 2019.

Mortality Consequences of Climate Change and Income Inequality

The risk of dying from extreme heat caused by global warming is an order of magnitude larger than previously believed. In addition to temperature, income is another important factor as it determines whether people have more or less access to air conditioning as well as other protective measures and opportunities that require investment. These are the key conclusions of the Climate Impact Lab’s new study made public lately.

For Bloomberg, Eric Roston, Paul Murray and Rachael Dottle made an overview of the main points using different visual techniques. They start with a range chart of the projected change in heat-related death rates, which reveals (or proves) the life-saving power of economic growth. Next goes the scrollytelling part. There, the shifts are represented in a global map and a distribution chart that change with the year as you scroll. Look at the projections and you’ll find even more interesting graphics and important revelations in the article.

1990s Music Recognition Across Generations

Teens hardly listen to the same music as parents and may even fail to recognize most of the hit songs from a couple of decades ago. The Pudding’s Matt Daniels and Ilia Blinderman created a cool story revealing how music is passed down from generation to generation.

Several months ago, The Pudding launched a quiz asking readers whether or not they recognized one song or another. Analyzing millions of data points collected in this way, the authors focused on the 1990s music and, among the insights revealed, found out what songs released in that decade have been losing popularity faster (and slower) than others. The story features, in particular, interesting line graphs that show the change in recognition rates for many 1990s hits by the person’s age in the year of the song’s release.

At the end of the piece, all the 1990s songs ever charted in the top five of the Billboard Hot 100 are represented, ranked by how well they were recognized among Generation Z participants of the quiz. “My Heart Will Go On” and “…Baby One More Time” are leading the list.",https://medium.com/data-visualization-weekly/impressive-charts-on-age-energy-climate-and-music-dataviz-weekly-19fd14b860b8,[],2020-08-07 17:56:47.439000+00:00,841,"Data Visualization, Flowing Data, Washington Post, Bloomberg Green, The Pudding"
How to Build a Reporting Dashboard using Dash and Plotly,"A method to select either a condensed data table or the complete data table.

One of the features that I wanted for the data table was the ability to show a “condensed” version of the table as well as the complete data table. Therefore, I included a radio button in the layouts.py file to select which version of the table to present:

Code Block 17: Radio Button in layouts.py

The callback for this functionality takes input from the radio button and outputs the columns to render in the data table:

Code Block 18: Callback for Radio Button in layouts.py File

This callback is a little bit more complicated since I am adding columns for conditional formatting (which I will go into below). Essentially, just as the callback below is changing the data presented in the data table based upon the dates selected using the callback statement, Output('datatable-paid-search', 'data' , this callback is changing the columns presented in the data table based upon the radio button selection using the callback statement, Output('datatable-paid-search', 'columns' .

Conditionally Color-Code Different Data Table cells

One of the features which the stakeholders wanted for the data table was the ability to have certain numbers or cells in the data table to be highlighted based upon a metric’s value; red for negative numbers for instance. However, conditional formatting of data table cells has three main issues.

There is lack of formatting functionality in Dash Data Tables at this time.

If a number is formatted prior to inclusion in a Dash Data Table (in pandas for instance), then data table functionality such as sorting and filtering does not work properly.

There is a bug in the Dash data table code in which conditional formatting does not work properly.

I ended up formatting the numbers in the data table in pandas despite the above limitations. I discovered that conditional formatting in Dash does not work properly for formatted numbers (numbers with commas, dollar signs, percent signs, etc.). Indeed, I found out that there is a bug with the method described in the Conditional Formatting — Highlighting Cells section of the Dash Data Table User Guide:

Code Block 19: Conditional Formatting — Highlighting Cells

The cell for New York City temperature shows up as green even though the value is less than 3.9.* I’ve tested this in other scenarios and it seems like the conditional formatting for numbers only uses the integer part of the condition (“3” but not “3.9”). The filter for Temperature used for conditional formatting somehow truncates the significant digits and only considers the integer part of a number. I posted to the Dash community forum about this bug, and it has since been fixed in a recent version of Dash.

*This has since been corrected in the Dash Documentation.

Conditional Formatting of Cells using Doppelganger Columns

Due to the above limitations with conditional formatting of cells, I came up with an alternative method in which I add “doppelganger” columns to both the pandas data frame and Dash data table. These doppelganger columns had either the value of the original column, or the value of the original column multiplied by 100 (to overcome the bug when the decimal portion of a value is not considered by conditional filtering). Then, the doppelganger columns can be added to the data table but are hidden from view with the following statements:

Code Block 20: Adding Doppelganger Columns

Then, the conditional cell formatting can be implemented using the following syntax:

Code Block 21: Conditional Cell Formatting

Essentially, the filter is applied on the “doppelganger” column, Revenue_YoY_percent_conditional (filtering cells in which the value is less than 0). However, the formatting is applied on the corresponding “real” column, Revenue YoY (%) . One can imagine other usages for this method of conditional formatting; for instance, highlighting outlier values.

The complete statement for the data table is below (with conditional formatting for odd and even rows, as well highlighting cells that are above a certain threshold using the doppelganger method):

Code Block 22: Data Table with Conditional Formatting

I describe the method to update the graphs using the selected rows in the data table below.",https://medium.com/p/4f4257c18a7f#4476,['David Comfort'],2019-03-13 14:21:44.055000+00:00,668,"Dash Data Tables, Conditional Formatting, Doppelganger Columns, Color-Coding Cells, Highlighting Outliers"
Data Scientist Seeking Data Scientist,"I need to find a data scientist. More specifically, I need to find all data scientist roles open in the US. I promise it’s not because I want to talk to all of them. I actually need the job listings for data-related roles for my next project, a web-scraping project. More importantly, I needed job listings that had salary information. I heard that data scientists make more money than data analysts, but I wanted to prove it.

Webscraping

I actually think I studied html in middle school or high school, but that was more than a decade ago. All I remember were the tags. Fortunately, that’s really all I needed for the project using the Beautiful Soup library. If you don’t know or haven’t used Beautiful Soup, it is a way to scrape information from a website made with html. It’s simple in its simplicity; only an understanding of the html hierarchy is needed.

Find all the job titles on the first page of Indeed.com

From top-down, the code block above looks through every div tag, checks if an id attribute and data-jk attribute exist. Then it will grab the text from the a tag. This returned the job title, even those that were sponsored. Beautiful Soup is interesting in that you usually know what you are looking for, but need to find the path there.

I need more jobs

So I thought a while loop until I scraped 1000+ salaries would work. Unfortunately, Indeed is smart with their resources. The website caps searches to 1000 listings before stating that the remaining listings are repetitive and won’t be shown. To workaround this, I opened up my searches: look for four roles — ‘data scientist’, ‘data analyst’, ‘business intelligence’, & ‘research scientist’ — and twenty six cities. After dropping duplicate entries and job listings without salaries, I ended up with 2724 listings. A caveat, I had to scrape over multiple days. I didn’t want to run my scraper (which would take 30+ minutes) the day before the project was due.

The median salary for data-related roles is right over $77,000. The majority of the salaries are within the $60k — $100k range. With a bunch of salaries on the higher end. At least the upward growth of data-related job salaries is promising.

Call up the models

Here is a gist of all the libraries I imported to use for my modeling.

So you don’t have to count, I used 20 functions

That’s a lot. And I tried all of them (including a couple more). This was the first project where I was able to try almost every classification model that I’ve learned in the past 10 weeks. In order to use the classification models, I had to convert the words to some numerical input. The three options I used were count, hash, and tf-idf vectorizers.

After vectorizing, I called up all the classification models that I’ve used in sklearn: Logistic Regression, KNN, BernoulliNB, etc. I thought that the most complicated vectorizers and classifiers would give me the best accuracy score. But in the end, the simplest of both produced an accuracy score of 84.5% for my test data.

Count the words and use Logistic Regression. Which just goes to show that sometimes the simplest and easiest tools to use are sometimes the best. Maybe that’s why they’re the most commonly used.",https://towardsdatascience.com/data-scientist-seeking-data-scientist-b0db53f80ada,['Chris Kim'],2017-06-29 13:55:06.393000+00:00,547,"Data Scientist, Webscraping, HTML, Beautiful Soup, Logistic Regression"
Understanding the Basics of Data Science (when you are not a data scientist but work with or manage data scientists) — Part I,"Unlocking the data mining process behind data science principles

First, break up a data analytics problem into tasks for which a tool is available.

And what are these tasks?

Classification — predict the mutually exclusive class of values an individual belongs to. Let’s say I work for a mobile phone company and I want to identify which customers will respond to a marketing offer and which will not. In this case, the individual is the customer and the mutually exclusive classes are: Will Respond To Offer, Will Not Respond To Offer. The tool we use will create a model. That model can then be used on any customer to determine whether they will respond to offer or not and so whether the company should pay for ads directed at them. Will be covered in further detail. Class Probability Estimation — closely related to Classification. The model created from this task will tell us how likely the customer is to respond to an offer or not. Will be covered in further detail. Regression — rather than predicting which category a customer falls into, here the model will predict a numerical value for a customer. To continue the telecom example, maybe we want to know how much a customer will use the service being offered in the earlier marketing campaign. So, we want to estimate, service usage (maybe that is in terms of minutes or texts). Will be covered in further detail.

Rough summary of three of the data analytic tasks used in data science.

4. Similarity Matching — finds individuals that are similar to each other based on data known about them. For example, a telecom company may want to find customers similar to their most profitable customers. They can find matches based on similarities in characteristics of their customers. Will be covered in further detail.

5. Clustering — a type of similarity matching but without any underlying question. In the previous example, the telecom company wanted to know which customers were similar to their most profitable customer(s). In Clustering, a company would just ask which customers are similar to each other, i.e. sort customers into groups based on how similar they are in their characteristics. This task is usually one of the first steps in trying to understand patterns in the data and can lead to suggestions for other data mining tasks. Will be covered in further detail.

6. Co-occurrence Grouping — Rather than looking for similarities based on static characteristics (age, gender, salary of customer), this task looks for links based on transactions (by the customer, to continue with the telecom example). So if a customer upgraded their data plan while downgrading their messaging bundle, and this pattern occurs with some regularity, the telecom company could decide to roll out a new offer that includes those changes together and be able to offer customers more of what they want.

7. Profiling — characterizing typical behavior of a group or population and not an individual usually. For example, the telecom company might want to know typical cell phone usage of groups of people based on geography.

8. Link Prediction — predicts connections (and the strength of those connections) between two entities. If most of the customers in a geographic area are buying Data Package A over Data Package B, the telecom company could push all customers in that are to use Data Package A (and thereby have more efficient operations and happier customers). This task is the basis for movie recommendations a la Netflix.

9. Data Reduction — here, a trade-off is made between keeping data that reveals useful patterns and loss of information from removing data. Large datasets often need to be reduced so they are easier to process.

10. Causal Modeling — moves beyond finding correlations to finding causation. There are two methods: Randomized Control Trials (also sometimes known as A/B testing or experimental method) which require substantial investments in time and data or methods that draw causal conclusions based on observational data. Either are attempting to understand what would have happened if the “treatment” (or telecom marketing offer) had not happened. It is very important to know what assumptions were made as these assumptions may not hold in all scenarios when looking to scale.",https://medium.com/@sabahatiqbal/understanding-the-basics-of-data-science-when-you-are-not-a-data-scientist-but-work-with-or-1db99442395b,['Sabahat Iqbal'],2020-12-29 16:52:12.620000+00:00,696,"Classification, Class Probability Estimation, Regression, Similarity Matching, Clustering"
Real-Time Introduction to NumPy,"NumPy library consists of many functions.

In many domains like Scientific Computing, Deep Learning and Financial analysis we work with high dimensional arrays. You may think Python’s data structures like lists, tuples and dictionaries can be used to store such data. Yes, they can be used but the performance is way too slow than expected.

Why lists cannot be used?

1. In lists heterogeneous data can be stored, so lists contain the overhead of type checking whereas in real life we deal with high dimensional arrays having homogenous data.

Example: An Image is a matrix consisting of pixel values in numerical form.

2. Lists do not completely use the low-level hardware mechanisms in-order to accelerate operations.

Numpy is intended to bring performance and functionality improvements for numerical computing.

NumPy (Numerical Python) is a library that tries to efficiently process and store high dimensional arrays. It is a well-known and well-used package in Python in almost all fields. NumPy is very useful for performing mathematical and logical operations on large high dimensional arrays and matrices. It has a plethora of functions that can be used for performing various numerical operations efficiently.

A Brief Description of what NumPy offers at various levels.

Creating arrays in NumPy.

To start using NumPy and all of the functions available in NumPy, you’ll need to import it. This can be easily done with this import statement:

import numpy as np

A NumPy array is a homogeneous grid of values. The size of the array along each of its dimensions is called its shape.

# Creating 1D array arr1d = np.array([1, 2, 3]) # Creating 2D array arr2d = np.array([

[1,2,3],

[4,5,6]

]) # Creating 3D array arr3d = np.array([

[

[1,2,3],

[4,5,6]

],

[

[7,8,9],

[10,11,12],

]

])

Some of the alternative methods to create arrays in NumPy are listed below.

# Create an array of all zeros

a = np.zeros((2,2)) # Create an array of all ones

b = np.ones((2,2)) # Create a constant array of x

x = 0.8

c = np.full((2,2), x) # Creates a 3x3 identity matrix

d = np.eye(3) # Create an array of random values

e = np.random.randn((2,2)) f = np.arange(0, 30, 5)

Output: [0, 5, 10, 15, 20, 25]

g = np.linspace(1, 15, 3)

Output: [ 1.0, 8.0, 15.0]",https://medium.datadriveninvestor.com/real-time-introduction-to-numpy-d556deb56f49,['Saiteja Kura'],2020-08-20 14:06:12.667000+00:00,344,"numpy, scientific computing, deep learning, financial analysis, high dimensional arrays"
Client-facing skills for data science consultants: luxury or necessity?,"Client-facing skills for data science consultants: luxury or necessity?

What it takes to be successful as a data science consultant

Photo Credit. Fauxels @ Pexels.com

There was a time that I thought you could be a top data science consultant by having excellent core tech skills like machine learning, data analysis, and software engineering. When I started working in my data science consulting job, I was excited to work as a consultant for a known global brand. I was very motivated to put the solutions that I learned through my academic and startup career (with studying papers, reading code repositories, and taking online courses) into practice for very large corporations.

It took me years to become an experienced data science consultant who can work independently in client-facing environments.

But soon, the reality kicked in, and I understood that I was not well equipped with the skills I needed for being a successful data science consultant. It took me years to become an experienced data science consultant who can work independently in client-facing environments. During these transition periods, most of the learnings were when I was facing difficulties in my projects and receiving constructive or critical feedback from my colleagues and superiors. After these learnings, I started to understand how to serve your clients successfully as a data science consultant.

The most crucial type of skill you need, on top of your tech skills, to be a successful data science consultant is your client-facing skills. In a nutshell, there are many client-facing skills, such as active listening, communication, empathy, goal-oriented problem-solving, and dedication. Here, we focus on the most critical one for a data science consulting career that is goal-oriented problem-solving.

A consulting company is mostly a client-facing professional services company, rather than not a product company. Most people in these companies are working on projects developing outsourced solutions, and eventually delivering them to the end clients. While some teams are working on products but working in client-facing environments takes precedence in those companies.

As a newly-hired data science consultant, you may be very passionate about the solutions and technologies you have learned during your education and early career. You might be even very proud of yourself that you work on so many cool technologies, and you should be. You are probably very excited to present the insights that you read in some academic papers or tutorials to other people. You want to apply your learnings from the new online course you took in practice for a big client of yours.

You will soon understand that other people in your projects are in more contact with the clients. Maybe, they are much more senior than you or salespeople. And they may start telling you that the solutions you propose are not the ones that the client wants. Or you are making the solution too complicated. After all, they know more about the actual requirements of the clients comparing to you. And that is the time that you start feeling really under-appreciated or even un-welcome.

You realize that being customer-centric is an essential part of the game in the consulting companies.

And believe me, I understand. You may feel like the consulting industry is not for you, and you have to go and start over. But you need to realize that being customer-centric, i.e., putting the client’s requirements and, by extension, the project stakeholders first, is an essential part of the game in the consulting companies. Most of the time, you need to focus on the ultimate goals the business is trying to achieve in response to their problem; in the end, you solve your clients’ problems, and they don’t know about data science or artificial intelligence. They just want to solve their business problems.

Often in your early consulting career, you offer a too complicated solution for the clients to understand, and even for your superiors to explain. These can be some complex deep learning solution you read in one paper recently and found it interesting. You will soon realize that in most cases, the best solutions are the most straightforward solutions that you can demonstrate their value to the client quickly. Once your solution’s general direction is approved, you can build on top of that with more complex levels up to the needed level.

On top of the solutions you offer, the general professional etiquette you will have as a management consultant is a crucial part of your role as a data science consultant. Remember, if you are working in a large consulting company, your clients hire your company to provide world-class service to them. The clients most likely want to receive top-quality work with a high level of professionalism.

In consulting, the customer is always right.

How you treat your consulting clients must be the best examples of customer-centricity and sticking to the belief that “the customer is always right.” You will need high levels of empathy, dedication, and active listening when serving your clients in a consulting engagement, which is the essence of this business.

In summary, you need to be very practical in approaching the project’s requirements and your problem-solving when working in consulting companies. Also, keep in mind that being a client-facing data science consultant means that you always need to put the needs and requirements from the clients and the project stakeholders above your preferences on how you want to solve this business problem.

About the Author:

Pouyan R. Fard is the Founder & CEO at Fard Consulting & Data Science Circle. Fard Consulting is a Frankfurt-based boutique consulting company serving companies in various industries. Pouyan has years of experience advising companies, from startups to global corporations, on data science, artificial intelligence, and marketing analytics. He has collaborated with Fortune 500 companies in pharma, automotive, aviation, transportation, finance, insurance, human resources, and sales & marketing industries.

Pouyan is also leading the Data Science Circle team to build a career hub between employers and data science talents. DSC’s mission is to nurture the next generation of data scientists through career training and helping the employers to find top talents in big data.

Pouyan has done his Ph.D. research work on predictive modeling of consumer decision making and remains interested in developing state-of-the-art solutions in machine learning and artificial intelligence.",https://towardsdatascience.com/what-is-takes-to-be-a-top-data-science-consultant-part-1-client-facing-skills-7803f5c8974c,['Pouyan R. Fard'],2020-09-29 16:38:24.050000+00:00,1023,"client-facing skills, data science consultant, goal-oriented problem-solving, customer-centricity, professional etiquette"
Multiclass classification with softmax regression and gradient descent,"Logic behind Softmax regression

Ultimately, the algorithm is going to find a boundary line for each class. Something like the image below (but not actually the image below):

Image by author

Note: we as humans can easily eyeball the chart and categorize Sarah as waitlisted, but let’s let the machine figure it out via machine learning yeah?

Just like in linear and logistic regressions, we want the output of the model to be as close as possible to the actual label. Any difference between the label and output will contribute to the “loss” of the function. The model learns via minimizing this loss.

There are 3 classes in this example, so the label of our data, along with the output, are going to be vectors of 3 values. Each value associated with an admission status.

If the label is such that:

admitted = [1, 0, 0]

waitlisted = [0, 1, 0]

rejected = [0, 0, 1]

then the output vector will mean:

[probability of being admitted,

probability of being waitlisted,

probability of being rejected]

Thus, in softmax regression, we want to find a probability distribution over all the classes for each datapoint.

Image by author

We use the softmax function to find this probability distribution:

Why softmax function? I think this functions is best explained through an example. Let’s look at the example:

GPA = 4.5, exam score = 90, and status = admitted.

When we train a model, we initialize the model with a guessed set of parameters — theta. Through gradient descent, we optimize those parameters. Because we have 3 classes (admitted, rejected, and waitlisted), we’ll need three sets of parameters. Each class will have its own set of parameters.

Let’s have theta be of the shape:

[bias, weight of GPA, weight of exam score]

Let’s initialize thetas to be:

theta_admitted = [ -250, 40, 1]

theta_waitlisted = [-220, 40, 1]

theta_rejected = [-220, 40, 1]

Why those values?

Remember that a line is y = mx + b? The line given by the initial thetas would be:

admitted:

-250 + 40x + y = 0

y = -40x + 250 waitlisted:

-220 + 40x + y = 0

y = -40x + 220 rejected:

-220 + 40x + y = 0

y = -40x + 220

If I just eyeball the data, I can see that the line that separates “admitted” from the rest has y-intercept around 250 and slope around -40.

Note: It’s a start, but these parameters are actually never going to work. First, the parameters for waitlisted and rejected are the same, so the parameters will always return the same probability for waitlisted and rejected regardless of what the input is. Second, only the bias differ, and rejected and waitlisted have a bigger bias than admitted (-220 > -250). Therefore, regardless of what the input is, these parameters will return 0 for admitted and 0.5 for the other two.

But it’s okay to start with bad parameters, gradient descent will fix it!

Let’s visualize what the softmax function is doing.

What happens when we run our datapoint through the softmax equation? Again, our datapoint is: GPA = 4.5, exam score = 90.

First, we find the dot product of the parameters and datapoint:

Image by author

Then, we exponentiate that value to get rid of any potential negative dot products:

Image by author

Lastly, we normalize it to get a probability distribution:

Image by author

Because our initial set of parameters are not good, the model output 0.5 for rejected and 0.5 for waitlisted even though the label is admitted.

Essentially, the softmax function normalizes an input vector into a probability distribution. In the example we just walked through, the input vector is comprised of the dot product of each class’ parameters and the training data (i.e. [20, 50, 50]). The output is the probability distribution [0, 0.5, 0.5].

The machine learning algorithm will adjust the bias, weight of GPA, and weight of exam score so that the input vector will produce an output distribution that closely match the input label.

What we really want is our model to output something like:

Image by author

So, let’s change the parameters for all three classes to get better accuracy.

One way to do this is by gradient descent.

Gradient descent works by minimizing the loss function. In linear regression, that loss is the sum of squared errors. In softmax regression, that loss is the sum of distances between the labels and the output probability distributions.

This loss is called the cross entropy. The formula for one data point’s cross entropy is:

The inner 1{y=k} evaluates to 1 if the datapoint x^i belongs to class k. 1{y=k} evaluates to 0 if datapoint x^i does not belong to class k.

Essentially, this function is measuring how similar the label and output vectors are. Here’s a good blog post that goes into detail about this equation.

The total cross entropy, or loss, will be the sum of all the cross entropies.

We take the derivative with respect to theta on this loss in order to do gradient descent.

The new parameters for class k after each iteration is:

Again, 1{y=k} will be 1 if x^i belongs to class k, and 0 if x^i does not belong to class k.

We use this formula to calculate new thetas for each class.

Now, let’s implement the algorithm to arrive at optimal parameters theta.",https://towardsdatascience.com/multiclass-classification-with-softmax-regression-explained-ea320518ea5d,['Lily Chen'],2020-12-22 19:13:29.930000+00:00,841,"Softmax regression, Logic, Gradient Descent, Cross Entropy, Probability Distribution"
TOP AI TRENDS IN 2021 AI PROFESSIONAL SHOULD WATCH OUT!,"As 2020 comes to an end and the industry leaders all over the world try to sum up the past, present, future of artificial intelligence technology, there are certain trends that are coming up that will shape up the AI future in the coming years.

While the AI remains the technology of the future that will personate a key role in how we live, work, and play in the future, let’s have a look at how AI will be the key technology that will help in rebuilding the lives as well as help in reshaping the business strategies.

Here are the top four AI trends every AI professional and an AI engineer worth his or her salt should know so as to stay ahead of the competition.",https://medium.com/@artiba97/top-ai-trends-in-2021-ai-professional-should-watch-out-3f61ef4af37a,[],2020-12-14 09:41:02.140000+00:00,125,"AI, Artificial Intelligence, AITrends, AIEngineering, AIProfessional"
AutoML: The future of Data Science and Machine Learning,"Machine Learning and Automation are taking over many of the repetitive tasks that were once too time-consuming or expensive for humans. This is being done to increase efficiency, reduce costs, and create more accurate predictions. Here is an article on how AutoML is going to revolutionize data science for everybody.

The idea behind AutoML is to continuously build and test machine learning models using a neural network. As a new set of data is added, the neural network will automatically determine whether it needs to create a new specialized model by training and executing its own algorithm or simply improve an existing one. Essentially, AutoML creates sophisticated machine learning programs that can be accessed via APIs where the output is returned in the form of API calls.

AutoML is clearly a very powerful tool for automating much of this process using machine learning algorithms. But you’re still going to need programmers to define heuristics for machines or identify how they should make decisions and also learn about various data science concepts.

Why AutoML?

AutoML would continuously build models based on new data sets provided by an enterprise and improve them when better ones are discovered by the AutoML algorithm. The idea of applying neural networks to this process is not new — it’s been around for a long time and several companies have experimented with it in their own ways. AutoML has tremendous implications for non-technical people who want to leverage insights from data, but don’t have the programming chops or the resources to implement ML solutions. All they need now is an API call where they can get data back in the form of “recommendations”, such as which products are ideal for each customer based on his purchasing history, demographics, and other aspects. The other possibility is that they will get an alert if a certain event happens and the solution to prevent it.

What Does AutoML Do?

AutoML can be used for solving three primary problems: Model Building, Hyperparameter Tuning & Selection, and Model Optimization.

Model Building — This refers to setting up the model based on parameters in your data set (such as predicting the probability of customers churning) so that machine learning algorithms can compare them with labels (actual instances) and then feedback their “correctness” or explain how inaccurate they are. Once modeled, results can be analyzed using tools like TensorBoard. The process of refining these models is known as Hyperparameter tuning & selection — this refers to setting up the appropriate parameters in a model so that it learns data efficiently. When you’re using AutoML, this is done by providing an algorithm with lots of examples and letting it come up with its own metrics for measuring what makes sense. For example, if you want a model that recognizes images — you’ll need to provide it with hundreds of thousands of images so it can identify patterns between them. Model Optimization refers to improving your existing models after training them on large datasets provided by an enterprise; AutoML helps optimize your models based on computation speed and accuracy.

AutoML has two powerful components — Abalone and Ranker (which have been open-sourced).

Abalone: This component takes care of creating a simple model of your data set, by using AutoML’s cloud platform that uses Tensorflow. It also helps you to build more advanced models with the help of human experts in machine learning.

Ranker: This component recommends which models (created via Abalone) are the best for a particular enterprise based on their usage requirements and results.

AutoML in Practice: Where it is today?

In a more real-life scenario, an enterprise can take advantage of AutoML for its data science by providing relevant and consistent datasets so that machine learning algorithms can build multiple models over them. A good example is a model created by machine learning algorithms for predicting which customer would churn based on different attributes (such as demographics). This model can then be validated by human data scientists, who will tell AutoML to keep improving it until there are no improvements left to be made.

AutoML’s Future: The Next Step in Building Models

AutoML can not only automate machine learning tasks but also improve them using a more comprehensive understanding of ML. Automating ML algorithms may eventually create data models that are substantially better than what human data scientists could have designed, all within the same time frame. These new models will help enterprises move towards higher levels of performance and accuracy, which could potentially lead to increased customer satisfaction and loyalty as well as faster growth. With its potential to change how enterprise development is done in the long term, AutoML itself is definitely worth keeping an eye on.

Not just for ML but for other fields too, such as automating model building in data mining with the help of AutoML. Another very important field where it will be used extensively is research; building models and devising experimental designs require human expertise which can now be replaced by accurate machine learning algorithms that AutoML creates. With significant time and money saved, more comprehensive research can be carried out in a shorter period of time. It could also be used to build models that are similar but not exactly the same as an existing one; AutoML will use the old model’s performance metrics and then improve upon it using newer data sets.",https://medium.com/@protonautoml/automl-the-future-of-data-science-and-machine-learning-45abb8f5ebcf,[],2021-08-24 11:50:31.513000+00:00,882,"AutoML, Machine Learning, Data Science, Neural Networks, API"
Getting a foot in the door as a young data scientist,"As an international student, I expected that finding a Data Science internship during the COVID-19 lockdown would be extremely hard. Yet, I was lucky to get one. A Physics Ph.D. has sent me a message: “Hey Mike — I’m looking for data scientists.” I replied that I want to hear more. We have talked over the phone for 15 mins, and I got an offer. That is too good to be true, but such things happened to me before.

The first time it happened to me when I got a Research Assitant position while working towards my bachelor’s degree. An econometrics professor asked us to collect data for our regression project. Mine was on real estate pricing. I have tried to collect data from Zillow manually, but homes have dozens of attributes, so my best speed was around 30 homes per hour. Automating the process seemed like a good time investment. With python, private proxies, and Selenium, I have increased data collection speed to 30,000 homes per hour. I have also made a pipeline to clean the data, remove outliers, fill missing values, calculate a few powerful features, and submitted “data-acquisition-report” to the professor. Next class, she offered me a research assistant position to build an ETL pipeline for her research project. I had full freedom in creating the pipeline, from choosing data sources to database architecture to ETL tools, but a very modest budget. To meet the budget constraint, our team had to be creative and savvy. But in the end — instead of paying tens of thousands to third-party data providers each month, we build a streaming data pipeline that worked 24/7 and cost us just $375/mo. As graduation approached, I had to start looking for my next place.

The second time, I got lucky after my graduation. I knew the job search would be challenging, so I tried to get prepared ahead of time. My plan A was to get a job. I have grinded LeetCode, studied SQL, and started competing at kaggle. I have sent hundreds of personalized resumes and went to dozens of interviews without any success. My plan B was to create a real-estate startup based on my research project and self employ. As part of plan B, I went to Ycombinator Startup School and had been networking as much as I could, trying to validate or invalidate my startup idea. Once, I pitched my startup idea to a real estate investor; the guy has not been impressed with my pitch but was impressed with my Business Analytics background and offered me a job on the spot. Next year I have learned a lot about business, negotiations, and real estate investments. I have also helped identify and remove the company’s bottlenecks and double its revenue in less than 12 months. However, my goal was to become a Data Scientist, so I had to move on.

I have been lucky to get admitted to the SFSU MSBA program, which has a spot on curriculum and great professors at a reasonable cost. I was unsure if I would get lucky one more time, so I decided to start preparing for a job search from day one. Throughout my first semester, I focused on building up my network, refreshing LeetCode, and DS skills. My LinkedIn went from 6 connections with family and friends to 2,500+ connections with kaggle masters and data science professionals. To prep for code interviews, I have joined a LeetCode prep group where Data Scientists from Lyft, Uber, and other local startups spent Saturday mornings doing 8hrs code marathons. I took Timeseries and demand forecasting classes to refresh Data Science skills and joined the M5 kaggle competition to make sure DS muscles are still there. The competition ended in June, we got 32/8000+ in Accuracy, and I got a solo silver in Uncertainty. The plan was to join a deep learning competition, but I got a message: “Hey Mike — I’m looking for data scientists.”

During the 15 mins interview with my future boss, we have mostly talked about data solutions, my work experience with the research team, and my recent kaggle competitions. I had code examples at kaggle and GitHub, so I did not have a coding interview. (LeetCode grinding is yet to pay off.) Despite my previous experience, I have not been ready for new tasks. I had to learn very quickly: git, docker, CI/CD, FastAPI, unit and integration testing, async load testing, microservices architectures, GC and AWS best practices and architectures, adversarial feature selection, DNN concept drift monitoring, domain transfer problems, as well as quite a bit on physics and optics. In my day to day work, I spend 80% building well documented and tested data pipelines, 10% on data cleaning and feature engineering, and 10% on modeling/fast prototyping. I also present and explain technical decisions to the CEO, CTO, VP of AI, communicate with software engineers, scientists, and bio-statisticians. Work at a fast-paced startup is probably the best place to learn and apply new technologies and best practices. I do not have time for kaggle anymore, but I try to practice LeetCode whenever I have time as I feel that it will help me get lucky next time.

It took me over three years of constant effort to get a foot in the door with the Data Science industry. This is considered slow, but I still feel lucky as luck has helped me a lot. I have just tried to maximize the probability of getting a chance by trying all ways of finding a job. And I tried to maximize the likelihood of seizing the opportunity by maxing out relevant skills. Where there’s a will, there is a way.

As graduation approaches, my plan A is to get a full-time job; my plan B is to start a demand forecasting or real-estate investment startup and self-employ :)",https://medium.com/@sibmike/getting-a-foot-in-the-door-as-a-young-data-scientist-a809831c8ad3,[],2020-11-27 18:28:16.061000+00:00,971,"Data Science, Internship, COVID-19, Econometrics, Python"
Your Next Top Model,"Your Next Top Model

Note: At the end of this brief article, click on the link to interact with the document created by Pat Fuller, Connor Anderson, and Mia Iseman.

What’s a random forest? If a decision tree falls there, will my parameters influence whether it makes a sound?

What’s a Bayes, and why is it still so naive after all these years? Shouldn’t it know better by now?

For real though, what does SVM even stand for?

There are a lot of machine learning models out there, and it makes sense that you don’t have the answers to everything off the top of your head. Maybe you are new to data science, perhaps you’ve been using the same models in your 9 to 5 for years now, or you need some quick interview prep. Either way, it’s nice to have a resource to remind you what options for machine learning models exist.

My colleagues and I have created a high-level overview of many different models as well as a taxonomy. We recently studied most of them listed, and we found that when we graduated from our course, it was harder to remember the ones we studied first or the ones that didn’t make it into one of our projects.

preview of the taxonomy we developed

While we know the resource itself is helpful, it’s probably no surprise to hear that the act of creating it was more beneficial to us personally. So, we’d love your help in keeping this a living document because it will benefit others and serve as a check-for-understanding for you too.

We know this is nothing groundbreaking, it’s simply a helpful resource. Let’s keep it updated — after all, at the rate that data science is growing and changing, we need something to help us keep all these models and their purposes straight. Please leave a comment on the deck here, and I hope it’s useful!

example slide from the deck

Link to deck: https://docs.google.com/presentation/d/1hiFRhHr7K2OCZSO0RwetD8xxbdUbPcPQ-dJqs93ERYY/edit?usp=sharing",https://towardsdatascience.com/your-next-top-model-65052c6a8d3b,['Mia Iseman'],2019-10-02 03:04:54.243000+00:00,319,"machinelearningmodels, datascience, randomforest, bayesnaive, svm"
"[withR]좀더 하는 ggplot2 — Fitted Lines from Multiple Existing Models(여러 모델 선 추가하기, 회귀식)","#library(ggplot2)

#library(gcookbook)

#library(plyr)

이런 형태의 자료가 있다고 한다면, 이 위에 회귀선을 추가할 수 있다.

하지만 이번 방법은 전체에 대한 회귀선이 아니라 성별 별로 회귀선을 추가 할수 있다.

우선은 model을 만들자.

#R cookbook Graphics책 참조

make_model <- function(data) {

lm(heightIn ~ ageYear, data)

}

회귀모델로 heightln~ageYear을 설정해 두었다.

library(gcookbook) # 예제 자료

library(plyr) #데이터다루는 라이브러리

models <- dlply(heightweight, “sex”, .fun = make_model)

dlply를 사용하면 행렬을 분류하지 않고, 바로 열별 연산이가능하고, .fun자리에 있는 함수연산이 가능하다.

models

#R cookbook Graphics책 참조

#예측값 데이터 생성함수

predictvals <- function(model, xvar, yvar, xrange=NULL, samples=100, …) {

if (is.null(xrange)) {

if (any(class(model) %in% c(“lm”, “glm”)))

xrange <- range(model$model[[xvar]])

else if (any(class(model) %in% “loess”))

xrange <- range(model$x)

}

newdata <- data.frame(x = seq(xrange[1], xrange[2], length.out = samples))

names(newdata) <- xvar

newdata[[yvar]] <- predict(model, newdata = newdata, …)

newdata

}

predvals <- ldply(models, .fun=predictvals, xvar=”ageYear”, yvar=”heightIn”)

ldply도 dply같은 함수로 성별따로 연산된 자료를 하나의 데이터 프레임으로 생성한다.

ggplot(heightweight, aes(x=ageYear, y=heightIn, colour=sex)) +

geom_point() + geom_line(data=predvals)

데이터 다루기가 복잡하고 어렵게 느껴진다면, 직접 다루지 않아도 ggplot이 알아서 처리해준다.

이번엔 다른 방법으로 위에 그래프를 표현해보자.

ggplot(heightweight, aes(x=ageYear, y=heightIn, colour=sex)) +

geom_point() + stat_smooth(method=lm)

간닥하게 stat_smooth(method=lm)을 추가해주는 것으로 그래프를 생성가능하다.

ggplot은 geom_***과 stat_***들을 어떻게 잘 조합하는지에 따라 여러가지 다양한 표현이 가능하다.

ggplot(heightweight, aes(x=ageYear, y=heightIn, colour=sex)) +

geom_point() + stat_smooth(method=lm)+facet_grid(.~sex)

facet_grid(.~sex)를 추가했더니, 성별로 따로따로 분리된 그래프가 생성됬다.

여러가지를 사용하면서 응용해 보기 바란다.",https://medium.com/excitinglab/withr-%EC%A2%80%EB%8D%94-%ED%95%98%EB%8A%94-ggplot2-fitted-lines-from-multiple-existing-models-%EC%97%AC%EB%9F%AC-%EB%AA%A8%EB%8D%B8-%EC%84%A0-%EC%B6%94%EA%B0%80%ED%95%98%EA%B8%B0-%ED%9A%8C%EA%B7%80%EC%8B%9D-eae5c9b7adae,['Neo Jeong'],2019-08-07 06:38:56.436000+00:00,174,"RR, ggplot2, gcookbook, plyr, cookbook Graphics책 참조"
Climate Data Analysis with Pandas and Matplotlib,"Climate Data Analysis with Pandas and Matplotlib

Photo by Dawid Zawiła on Unsplash

The Rationale

An Exploratory Data Analysis (EDA) of Climate data from Ghana, showcases my first project portfolio into data science. Climate data provides a time series of weather variables record, in indexed time order and very useful for an EDA. Secondly, it is freely available publicly and in my case, the data was obtained from https://www.ncdc.noaa.gov/cdo-web after just a few clicks.

Taking a cue from the NOAA hottest month on record project from https://www.ncdc.noaa.gov/sotc/global/201904. I delve into an EDA of Ghana’s climate data using Pandas and Matplotlib to understand the trends and how it compares to the global climate change phenomena. And personally, get a grip on the climate change mantra from the local perspective. The article presents the results of the project and a bit of the journey.

The Journey

Over the past few months, I have plunged myself into learning Python programming to cover up on my spare time due to a slowdown in work due to the Covid 19 pandemic. Numerous freely available resources on the internet have been helpful in my self-study for this subject. Of particular mention is the complete course on Geo-python offered by the University of Helsinki. This is for learners trending in the environment and natural resources field. Others include Corey Shafer’s tutorials on youtube and worth of tips offered on StackOverflow.

Interestingly while setting up my Github account to host the project, I run into a big surprise. Github informed me my email account is already taken, what a surprise! I did a password retrieval which was successful. Only when I accessed the account did I realized, the account was the first set up in 2014, wow! Indeed, it has been long in coming.

Results

The data were records obtained from 17 stations/towns spread across Ghana. It contained 11 variables of 70,068 daily records. The records start from 1973 to 2020, a period of 47 years of climate data. The length of unique records varied across the stations and this in no way affects the global analysis of the data. Only four main variables were eventually selected for the analysis.

Summary and brief commentary below;

The mean daily temperature of Ghana is 27.6 oC i.e., average daily temperature. The maximum mean daily temperature of 35.0 oC, recorded on 22–04–1977 — Hottest day. Consistent with global April hottest month record. Maximum mean monthly temperature 31.1 oC, recorded in April 1983 — Hottest month. Maximum mean yearly temperature 30.2 oC, recorded in 1983 — Hottest year. This diverges from the year 2020 that tied with 2016 as the hottest year globally according to NASA.

Yearly Mean Temperature, Ghana by Francis N. Ocansey

The yearly mean temperature graph shows a generally rising and fluctuating temperature that plateaus around 28oC from 2015. This denotes more hotter days with severe climate implications. In 1983 Ghana experienced its worst drought and bushfires in history, accurately captured in the visuals.

Over the period, the records show a decadal increment of 1.5oC in temperature since 1973. The best period to visit Ghana is between mid-June and September when temperatures fall below 26oC, our coolest months.

Monthly Mean Temperatures, Ghana by Francis N. Ocansey

One major takeaway about coding with Pandas is to sort your dataframe before any slicing operation, otherwise, you may encounter challenges with your analysis. That is if your data is not already sorted. You wouldn’t want to experience the hassle I went through as a newbie doing slicing. Much more analysis, visualization, and deductions could be made from this project considering the type of data set with values from 17 stations. Head to my Github portable to have a play with it. Comments and questions are welcome on this piece. Thanks for reading.",https://towardsdatascience.com/climate-data-analysis-with-pandas-and-matplotlib-ca75f18a0587,['Francis Narteh Ocansey'],2021-07-15 06:42:32.427000+00:00,614,"Climate Data Analysis, Pandas, Matplotlib, Exploratory Data Analysis, Ghana Climate Data"
A Quick Example Using LSTM in Stock Market Prediction,"First, let’s discuss how to tackle this problem.

For LSTM, the most important thing is to reformat the data into sequences and decide the corresponding input and output size. In the given case, we only have one long sequence of 253 values (suppose we’re simply using the ‘Close’ column) and LSTM can’t efficiently learn on this. So the first task is to break this data down into smaller pieces.

The data we used is a single column comprising 253 scalar values

Since we want to make predictions, a natural way of thinking is, to predict using a smaller period of time than the given 253 days. We use the previous 19 days to predict on the 20th day as in this Kaggle version, but the user can change this number into any sequence length and see whether this would improve the results.

sequence_length=20. Input X here is a sequence of the previous 19 days, while the output Y is a single scalar, which is the value predicted on the 20th day.

Note we normalized all the stock values by dividing 10000 (change the number if you’re using a different scale) to avoid the neuron saturation phenomenon, otherwise, the predictions could become constant.

Then the LSTM architecture simply follows the standard construction. Since it’s stateless LSTM, we reset the hidden states to 0 during each training.",https://medium.com/analytics-vidhya/a-quick-example-using-pytorch-lstm-in-stock-market-prediction-98afe1aeafe5,['Mengliu Zhao'],2020-12-02 12:52:45.762000+00:00,219,"LSTM, Reformat Data, Sequence Length, Normalization, Neuron Saturation"
50-year-old Cybernetics questions for an ethical future of AI,"50-year-old Cybernetics questions for an ethical future of AI

Norbert Wiener, one of cybernetics pioneers, envisioned AI ethics problems way ahead of us

“43081” by Tekniska museet is licensed with CC BY 2.0. To view a copy of this license, visit https://creativecommons.org/licenses/by/2.0/

Ethics has definitely become a trend in the field of Artificial Intelligence. It seems clear that AI faces a lot of challenges if we want it to have a positive impact for our society. Nevertheless, it is not the first time that researchers warn us about the risks of this kind of technology. Norbert Wiener, cybernetics pioneer, wrote this somehow prophetic piece back in his book God & Golem, inc, back in 1964:

It is relatively easy to promote good and to fight evil and good and evil are arranged against each other in two clear lines, and when those on the other side are our unquestioned enemies and those on our side our trusted allies. What, however, if we must ask, each time and in every situation, where is the friend and where is the enemy? What, moreover, when we have to put the decision in the hands of an inexorable magic or an inexorable machine of which we must ask the right questions in advance, without fully understanding the operations of the process by which they will be answered?

Quote taken from God & Golem, Inc — Norbert Wiener, 1964.

It is mesmerizing to think that this was written more than 50 years ago and that somehow it reflects so well some of the main challenges we are facing for ensuring a truly ethical future for AI. Let’s break down Wiener’s written piece.",https://towardsdatascience.com/50-year-old-cybernetics-questions-for-an-ethical-future-of-ai-8287beb96257,['David Pereira'],2020-12-23 17:15:33.558000+00:00,271,"AIethics, Norbert Wiener, Godand Golem Inc, Cybernetics Pioneer, Inexorable Machine"
Reinforcement Learning — Exploration vs Exploitation Tradeoff,"Introduction

Reinforcement Learning is an area of machine learning which teaches us to take actions to maximize rewards in a particular situation. Reinforcement learning is used in a various of fields, starting from Automobile to Medicine and many others.

In Reinforcement Learning, the agent is not aware of the different states, the actions available in all the states, the associated rewards and transition to resulting states. The agent learns more and more about it by interacting with the environment.

There is a significant difference between Reinforcement Learning and Supervised Learning. In supervised learning, the training data has the labels that help the model to train from the right set of labels. Whereas in Reinforcement Learning there is no right label and the agent is the one who decides how to perform the given task. In the absence of training set, the agent is bound to learn from its experience after performing the task for a certain number of times.

In this post we will go into detail on understanding the concept of Exploration and Exploitation tradeoff with the help of examples.

At any given point in time, the knowledge of an agent about the state, actions, rewards and resulting states is always partial and this results in Exploration-Exploitation Dilemma.

Exploration and Exploitation in Reinforcement Learning

Exploration is more of a long-term benefit concept where it allows the agent to improve its knowledge about each action which could lead to long term benefit.

Exploitation basically exploits the agent’s current estimated value and chooses the greedy approach to get the most reward. However, the agent is being greedy with the estimated value and not the actual value, so chances are it might not get the most reward.

Let’s take an interesting example to understand Exploration- Exploitation properly.

Exploration and Exploitation : Image Source

Let’s say your friend and you digging in the hope that they will get diamond out of it. Your friend gets lucky and finds the diamond before you and walks off happily.

By seeing this, you get a bit greedy and think that you might also get lucky. So, you start digging at the same spot as your friend.

Your action is called the greedy action and the policy is called the greedy policy.

However, in this situation the Greedy policy would fail because a bigger diamond is buried where you were digging in the beginning.

However, when your friend found the diamond, the only knowledge you got was the depth at which the diamond was buried. You do not have the knowledge of what lies beyond that depth. In reality the diamond may be where you were digging in the beginning or it may be where your friend was digging, or it may be completely at a different place.

Greedy Policy Fail: Image Source

With such partial knowledge about future states and future rewards, our reinforcement learning agent will be in dilemma on whether to exploit the partial knowledge to receive some rewards or it should explore unknown actions which could result in much larger rewards.

However, we cannot choose both explore and exploit simultaneously.

In order to overcome the Exploration-Exploitation Dilemma, we use the Epsilon Greedy Policy.

Epsilon Greedy Policy

To choose between exploration and exploitation a very simple method is to choose randomly. This could be one by choosing to exploit most of the time with little exploring.

Example 1

Suppose we are rolling a dice, and if it lands on 1 then we will explore, else exploit.

Greedy Policy: Image by Author

This method is called the Epsilon Greedy Action where Epsilon refers to the probability of choosing to explore.

In the above case the value of Epsilon will be 1/6 which is the probability of getting 1 on rolling a dice.

Epsilon-Greedy can be represented as follows:

Epsilon Greedy : Image by Author

The Action that the agent selects at time step t, will be a greedy action (exploit) with probability (1-epsilon) or may be a random action (explore) with probability of epsilon.

Let’s take another example to understand the above expression better:

Example 2

With partial or no knowledge about future rewards, Epsilon-greedy approach yields best results as it balances between exploitation of current knowledge and exploration of unknown action.

Epsilon Greedy Approach: Image Source

In the above example, your friend has got the diamond and by seeing that you have the knowledge that about the level of depth needed to be dug to get the diamond. So, you choose to dig where you friend was digging with a probability of (1-epsilon). This means we are taking a greedy action, or we exploit our knowledge that a diamond was found there.

Or we can explore with probability epsilon with an understanding that the diamond has not yet been found here, but we still want to keep exploring with a probability epsilon where epsilon is a positive real number which lies between 0 and 1.

Reference:",https://medium.com/@tanwirkhan/reinforcement-learning-exploration-vs-exploitation-tradeoff-f4105a1da2ea,['Tanwir Khan'],2021-01-06 21:56:09.161000+00:00,786,"Reinforcement Learning, Exploration, Exploitation, Epsilon Greedy, Supervised Learning"
Universal data tool weekly update 4,"If you haven’t heard of the Universal Data Tool, it’s an open-source web or desktop program to collaborate, build and edit text, image, video, and audio datasets with labels and annotations. You can get started with the Universal Data Tool

This is our fourth community update! We’re hoping by releasing these we can better engage the community and encourage new contributors. There’s also a video version. Let’s get started…

New Feature: Universal Data Tool Courses

With this update, we are introducing the training courses for Universal DataTool. With this feature now you can ensure that your labeler team is working not only in a streamlined way but with the best quality as well. The people working on a dataset will have to complete it before they can begin working on the labels.

Creating a training course is super easy and it can be done in just a few minutes. You can insert a dataset of your choice to be put in the training. This training contains questions, exercises, tests, and different editable options.

New Feature: Text Entity Relations

This is a much-awaited feature that was missing in the NLP(Natural Language Processing). This feature allows you to draw between different texts to create the relation. You can do it just by using a one-line command and it works super fast. Not only that but you can also click on read more options to get additional commands and ways to use them which can be useful in different scenarios.

New Feature: Download Mask Button

This is a small but very useful feature that we have added in the latest update. With this feature, you can download the colored masks just by clicking on the button which is in the download options list.

New update: Brand new layout

With this new update, you can see a totally new layout of the options. It’s less cluttered and different options can be seen in different tabs. This makes using the Universal Data Tool easier than ever.

Other updates:

Feedback button: Now you can share the feedback with us by clicking on this little button on the right bottom corner of the page. It allows you to add the screenshots as well. (thanks @moufette)

Now you can share the feedback with us by clicking on this little button on the right bottom corner of the page. It allows you to add the screenshots as well. (thanks @moufette) New Homepage: Now we have a redesigned and cool looking Homepage which can give you more information and insight into the Universal Data Tool

That’s it for our fourth community update, be sure to follow us on Twitter, or join our Slack to hear more!",https://medium.com/wao-ai/universal-data-tool-weekly-update-4-c078afc0c0b5,['Wafaa Arbash'],2020-09-07 15:19:31.015000+00:00,432,"Universal Data Tool, NLP, Training Courses, Text Entity Relations, Download Mask Button"
4 Ways To Supercharge Your Recommendation System,"Recommender systems help users find items they like. They do so by producing a predicted likeliness score or a list of top recommended items for a given user. Recommendations can be based on a plethora of factors including user demographics, overall item popularity and historical user preference. Real-life examples are encountered regularly in places such as Amazon, Spotify and Tinder.

However, many systems operate at a scale that stresses the challenges of successful academic and “small data” applications. Furthermore, increased user awareness and choice has led to preference promiscuity meaning users are happier than ever to drop a service that isn’t personally accurate.

With this in mind, the following 4 points are implementable ideas that target scalability issues whilst improving the quality of your collaborative filtering recommendations.

Credit: @BlaxZtar

1 — Ditch Your User-Based Collaborative Filtering Model.

A key challenge you will face with collaborative filtering algorithms in a business setting is tackling data sparsity and scalability. Modern systems demand the capability to process tens of millions of potential data points with a low tolerance for performance limitations. In these systems, users may have interacted with less than 1% of items available.

Bottlenecks in user-based collaborative filtering models largely arise in the search for neighbours, which are other users who have historically shown similar preferences to a given user, among large user populations. Capability to search thousands of neighbours is good but systems have to scale with the business they fit into — real value arises for both you and your user by being able to search millions of neighbours in real-time.

The switch to item-based collaborative filtering algorithm transposes the recommendation problem from user space to item space. It explores the relationship between items instead of the relationship between users. In most real-life applications, the number of items is dwarfed by the number of users and in many cases items are static. That is, the set of items changes much less frequently than the number of users. This allows for a decoupling of the fitting stage and prediction stage of an item based collaborative filtering model.

Let’s see the difference in practice.

Using a training set of 994,168 ratings on a set of 3,704 movies from 6,040 users (MovieLens 1M dataset), the computational cost of evaluating the similarity matrix for a user-based algorithm is 77.6 seconds but just 28.4 seconds for an item-based algorithm, each using Pearson similarity.

This switch addresses computational capability for large scale systems which at some stage becomes more important that minor differences in quality. Of course, the exception exists if your set of users is smaller than your set of items and is also expected to change relatively less frequently.

2 — A Gold Standard Similarity Computation Technique.

The concept of similarity is a critical element of the collaborative filtering framework. For user-based collaborative filtering algorithms, the user-similarity matrix consists of a metric that measures the distance between any pair of user preferences. Likewise, the item-similarity matrix measures the similarity between any pair of items in the item based framework. Three commonly discussed techniques are Pearson, Cosine and Mean Square Difference (MSD) but what similarity computation technique should you use?

Enter User 56. User 56 rated 54 movies of which the 5 star rated movies from the same training set above are given below.

Figure 1 — User 56’s Top Rated Movies

I picked user 56 purely out of interest in what recommendation arise given the user enjoyed both American History X and Babe (arguably two movies about pigs). Producing appropriate recommendations is a function of other users in the dataset however the ability to pick out those users is down to the choice of similarity metric.",https://towardsdatascience.com/4-ways-to-supercharge-your-recommendation-system-aeac34678ce9,['Neil Chandarana'],2019-08-06 12:04:51.243000+00:00,598,"Recommender Systems, User Demographics, Item Popularity, Historical User Preference, Collaborative Filtering"
Classifying the Universe with Bayesian Neural Networks,"The Invisible Universe

It is perhaps a rather uncomfortable fact that, despite decades of eyes glued to telescopes and scratches imparted on astronomer’s foreheads, we still do not know what most of the Universe is made of.



Fortunately, artificial intelligence may finally help resolve this frustratingly persistent question. Here’s how.

Everything that we can see with our telescopes, including every planet, star and galaxy, comprises but a tiny percentage of the total constituents of the Universe. There is definitely more stuff out there. Astronomers know this because where there is matter there is a gravitational pull, even if the matter is invisible. This pull influences the movement of galaxies and even light. From observations such as these, we now know that approximately thirty percent of the Universe is filled with this dark matter.

If this wasn’t strange enough, the attempt to understand what remains has caused many a headache in the cosmology community. Labelled dark energy, this mysterious substance is thought to drive our Universe’s current phase of accelerated expansion. Our best guess is that dark energy arises from a constant energy inherently associated with empty space, although getting this idea to fit into a cohesive theoretical framework has proved doggedly difficult. Another possibility is that we do not yet understand how Einstein’s theory of General Relativity works on the vast cosmological distances where dark energy begins to dominate. In the same way that the movement of electrons in an atom isn’t affected by gravity, an additional “fifth force’’ could kick into gear when we consider the size of the observable Universe, potentially driving cosmic acceleration.

Theoretical physicists require a well-functioning imagination to wrap their heads around these ideas, and these imaginations have produced a plethora of possibilities which attempt to explain what dark matter and dark energy, collectively known as “the dark sector’’, really are [1]. But how does one go about separating the theoretical wheat from the theoretical chaff?

One key method is to look at how strongly dark matter clusters together at different length scales in the Universe, quantified by something called the dark matter power spectrum. Different theories give characteristic predictions for the shape of this power spectrum. Imagine a theory which slightly strengthens gravity and thus enhances the pull a clump of dark matter exerts on the surrounding dark matter. If this were the case, one would hope to see this in the dark matter power spectrum.

The astute among you may have gathered that, by virtue of being dark, astronomers cannot directly measure the shape of the dark matter power spectrum. Rather, it must be inferred from the influence dark matter has on say, galaxies or light trajectories. Generally speaking, the more galaxies tend to group together or light seems to be bent, the more dark matter there is at that particular spot.

Over the next decade, many new cosmological surveys will map out the positions of literally billions of galaxies, providing us with the most detailed map of the cosmos ever produced. Cosmologists hope to use this galaxy map to create a map of the Universe’s dark matter and thus provide us with our Universe’s very own dark matter power spectrum. The question is, once we obtain this data, how can it be translated into a deeper understanding of the fundamental nature of the dark sector?",https://towardsdatascience.com/classifying-the-universe-with-bayesian-neural-networks-f20d0983e9b5,['Joe Kennedy'],2020-12-24 11:14:58.523000+00:00,545,"Dark Matter, Dark Energy, Observable Universe, Theoretical Physics, Cosmological Surveys"
Google officially explains what caused Monday’s 47 minutes outage.,"Google officially explains what caused Monday’s 47 minutes outage. The cause was “an internal storage quota issue.”

I haven’t the faintest clue about what that means in layman's terms. However, thank goodness! I can now assume that the incident wasn’t prompted by a solar plasma outflow originated from Coronal Mass Ejections (CME), causing a massive geomagnetic storm that ionized Earth’s atmosphere. Similar to a high-altitude electromagnetic pulse (HEMP) warhead, being detonated far above the Earth’s surface.

For a minute there, I thought somebody forgot to pay the internet bill.

You can read Google’s official statement here.",https://medium.com/technology-hits/google-officially-explains-what-caused-mondays-47-minutes-outage-3758a0bba01f,['Rui Alves'],2020-12-27 16:33:38.479000+00:00,93,"Google, Outage, Storage Quota Issue, Solar Plasma, Coronal Mass Ejections"
The Absolute Basics of Reinforcement Learning,"Reinforcement Learning

Reinforcement learning. What is it and what does it do? In this article, you’ll get a basic rundown of what reinforcement learning is.

First, let’s start with a basic definition:

Reinforcement learning is an area of machine learning.

It involves software agents learning to navigate an uncertain environment to maximize reward. It learns from interactive experiences and uses feedback from its actions. Basically, the bot gets points for its actions. It can gain or lose points. The way agents learn through RL is identical to the way we, as humans learn.

Think of it like a video game where you get punished or rewarded for your actions. In most video games you get rewarded by gaining more points or moving on to the next level and you get punished by losing a life or dying.

Inside the RL algorithm

We want to get the agent to learn for itself.

There are three basic elements of the reinforcement learning algorithm:

First, we’ve got the environment in which the agent is in. The environment provides input back to the agent as to if what it did was right or wrong. In other words, the environment tells the agent if the action it took resulted in a reward or punishment.

Next, we’ve got the agent. The agent is the one choosing the actions it takes.

And finally, we’ve got the reward. The reward is what the agent is aiming for. The agent’s incentive.

How the RL Algorithm learns

Now if we go back to our video game example, the environment would be the game screen that you see, the agent would be you as you’re the one making the decisions and playing the game, and the reward would be more points or moving on to the next level.

So how does it compare to other machine learning techniques :

There are 3 basic machine learning techniques; supervised learning, unsupervised learning and of course, reinforcement learning.

The main difference between each of these techniques is the goal.

The goal of unsupervised learning is to find similarities and differences between data points, while the goal of supervised learning is to sort the data based on the labels given. And of course, as we know, the goal of reinforcement learning is to get maximum reward.

RL vs. other ML techniques

Where is RL the most useful?

Reinforcement learning techniques are particularly useful since they don’t require lots of pre-existing knowledge or data to provide useful solutions or where there are many unknowns.

Where is it being used today?

Currently, RL is being used in areas like robotics, air traffic control, data processing, to create training systems and more! The applications on RL are endless and can be used almost everywhere. Google’s Deep Mind team has used RL to get an agent to learn and recognize digits and play the game, Atari all on its own!

This is a video of Google’s Deepmind algorithm playing Atari. https://www.youtube.com/watch?v=V1eYniJ0Rnk

Challenges of RL

Any new technology comes with its fair share of challenges and it’s no different for RL. One of the biggest problems with RL is trying to use it on a big scale. It requires a lot of training time and a huge number of iterations to learn tasks. The way RL learns is by using trial-and-error. To do this in the real-world becomes nearly impossible. Let’s take the example of an agent trying to navigate through an environment to avoid people. The agent would then try different actions and then proceed with the one that would best fit in that environment. This becomes hard to do in the real-world where the environment is changing constantly and frequently.",https://medium.com/analytics-vidhya/the-absolute-basics-of-reinforcement-learning-97402c444be1,['Mansi Katarey'],2020-12-01 17:49:51.986000+00:00,587,"Reinforcement Learning, Machine Learning, Supervised Learning, Unsupervised Learning, Deep Mind"
Introduction to AI and Reinforcement Learning,"Introduction:

Recently, I’ve been going down an AI Rabbit Hole. It started out by just watching Youtube videos and reading articles, but eventually, I came accross a facet of AI that really, really intrigued, not just because of how interesting the concept is, but also because of the potential uses it may have. It’s known as Reinforcement Learning.

Overview of Artificial Intelligence:

To start, we need to understand what exactly Artificial Intelligence (AI) is. AI is a pretty blanket term — it’s the science of making intelligent machines, especially computer programs. We’re going to go over a subset of AI, machine learning.

Machine learning is basically using a computer system to make accurate and reliable predictions/readings/interpretations of data that the system has been given. The first two types of machine learning were supervised learning and unsupervised learning. (*This is simply an overview of each system — I don’t go too in depth when it comes to the tech used for such methods)

Supervised Learning

Supervised learning is a task-driven form of machine learning. Labelled data is used to train the system, and when new data is presented, the system will be able to make a prediction.

For example, you can feed a system images of cows and chickens, and over time, it will be able to make distinctions. The system creates a function on a graph based on the data in order to predict what output a given input will lead to — you can learn more about how it actually works here. Supervised learning is used for pattern recognition, spam detection, object recognition, etc.

Unsupervised Learning

Unsupervised learning, unlike supervised learning, is data-driven. The system isn’t set out to recognise anything, but to observe. In unsupervised learning, the system is given raw data, and needs to figure out patterns in the data. Essentially, it finds patterns in data and groups said data accordingly.

For example, say there are five red data points and five green data points. The system will recognize that the data points differ in color, and will group them accordingly. Say we add five blue points, and give each of the now 15 points a random number from 1–3. The system will recognize all of these patterns.

Unsupervised learning is mainly for clustering problems, or grouping data. It is also used for anomaly detection — that is, noticing inconsistencies in datasets. This is especially useful for things such as bank transactions.

These were the first two types of machine learning, and they’re both well and good, and pretty useful — but, as Grand Master Yoda said, “There is another.” The third type of machine learning, and perhaps the most interesting (to me, at least), Reinforcement Learning.

Introduction to Reinforcement Learning:",https://medium.com/swlh/introduction-to-ai-and-reinforcement-learning-2f3b35b790e0,['Imran Iftikar'],2020-11-20 09:57:21.657000+00:00,441,"AI, Machine Learning, Supervised Learning, Unsupervised Learning, Reinforcement Learning"
NLP: Text Processing In Data Science Projects,"NLP: Text Processing In Data Science Projects

Learn The Data Science Techniques To Process Text To Use For NLP Projects In Python

Once we have gathered the text, the next stage is about cleaning and consolidating the text. It is important to ensure the text is standardised, the noise is removed so that efficient analysis can be performed on the text to derive meaningful insights.

It’s important to note that the cleaning and processing of text is highly dependent on the nature of the NLP project. As an instance for your project, numbers might be important.

This article aims to explain the steps we can perform to clean the text for NLP projects.

Photo by NordWood Themes on Unsplash

Article Aim

I will explain following key techniques:

Convert Text To Lowercase Tokenise Paragraphs To Sentences Tokenise Sentences To Words Remove Numbers Remove Punctuation Remove Stop words Remove Whitespaces

I will demonstrate how we can achieve the goal by using the NLTK library in Python and the regular expressions. We can install NLTK library using:

pip install nltk

The first task is to normalise the paragraphs of text.

1. Convert Text To Lowercase

The key concept here is to reduce the number of words, in particular if they are same. We can change the casing of the words to ensure every word is in lowercase.

As an instance, “Article” and “article” can be represented as “article”. We can use the lowercase() function of Python to change the casing of the text.

text = 'This is an NLP article of FinTechExplained' lower_case_text = lowercase(text)

print(lower_case_text) #This will print:

#this is an nlp article of fintechexplained

The text we extract from the sources such as documents, are usually represented as groups of sentences (paragraphs).

The next task is to break the paragraphs into sentences.

2. Tokenise Paragraphs To Sentences

I highly recommend the NLTK library in Python to perform tokenisation. We can use PunktSentenceTokenize. It is a pre-trained model of the NLTK library that can perform sentence-level tokenising by determining punctuation and character marking the end of sentence.

import nltk

from nltk.tokenize import sent_tokenize text = 'FinTechExplained aims to explain how text processing works. Once we have gathered the text, the next stage is about cleaning and consolidating the text. It is important to ensure the text is standardised and the noise is removed so that efficient analysis can be performed on the text to derive meaningful insights.' list = sent_tokenize(text)

print(list) #----output---- [

'FinTechExplained aims to explain how text processing works.',

'Once we have gathered the text, the next stage is about cleaning and consolidating the text.',

'It is important to ensure the text is standardised and the noise is removed so that efficient analysis can be performed on the text to derive meaningful insights.'

]

We can see from the code snippet above that the paragraph has been tokenised into sentences.

NLTK supports punctuation and sentence endings for 17 European languages.

Once we have a list of sentences, we need to break the sentences into words.

3. Tokenise Sentences To Words

We can use the TreebankWordTokenizer class of the NLTK library in Python to tokenise the sentences into words.

from nltk.tokenize import TreebankWordTokenizer

tokenizer = TreebankWordTokenizer() text = 'FinTechExplained aims to explain how text processing works. Once we have gathered the text, the next stage is about cleaning and consolidating the text. It is important to ensure the text is standardised and the noise is removed so that efficient analysis can be performed on the text to derive meaningful insights.' print(tokenizer.tokenize(text))

#This will tokenise the sentences into words 'FinTechExplained', 'aims', 'to', 'explain', 'how', 'text', 'processing', 'works', '.', 'Once', 'we', 'have', 'gathered', 'the', 'text', ',' 'the', 'next', 'stage', 'is', 'about', 'cleaning', 'and', 'consolidating', 'the', 'text', '.', 'It', 'is', 'important', 'to', 'ensure', 'the', 'text', 'is', 'standardised', 'and', 'the', 'noise', 'is', 'removed', 'so', 'that', 'efficient', 'analysis', 'can', 'be', 'performed', 'on', 'the', 'text', 'to', 'derive', 'meaningful', 'insights', '.'

Photo by Clément H on Unsplash

If you want to read on how to gather the text from various sources for your NLP project then read:

The next task is to remove the numbers of the text

4. Remove Numbers

This is highly dependent on the project. One of the common tasks is to remove the numbers from the text as numbers are not usually important to text analytics.

We can use the Regular Expression to achieve the goal:

import re

result = re.sub(r'\d+', '', '909FinTechExplained9876')

print(result) # 'FinTechExplained'

Next remove all of the punctuation

5. Remove Punctuation

We also need to remove the punctuation from the text.

import string punctuation = string.punctuation

words = ['You','Are','Reading','FinTechExplained', '!', 'NLP', '.'] clean_words = [w for w in words if w not in punctuation] #it will return

clean_words = ['You','Are','Reading','FinTechExplained', 'NLP']

We can then do a “ “.join(clean_words) to return a clean sentence (if required).

Remove the noise that stop words are adding

6. Remove Stop words

There are many stop words in English as an instance “a”, “an”, “the”, “and”, “but”, “if”, “or”, “because” are some of the common English stop words.

We can remove the stop words by using the NLTK library:

from nltk.corpus import stopwords text = 'FinTechExplained is an important publication'

words = nltk.word_tokenize(text)

stopwords = stopwords.words('english') clean = [w for w in words if w not in stopwords] print(clean)

#This will return an array where stopwords have been removed.

#'FinTechExplained' , 'important', 'publication'

Finally remove the whitespaces

7. Remove Whitespaces

Lastly, I want to demonstrate how we can remove whitespaces such as space, tab, carriage return, line feeds.

We can use the split() along with join() function of the Python programming language. It returns a list of the words in the string.

’’.join(FinTechExplained Is A Publication.

This is about NLP’.split()) #This will return

'FinTechExplained Is A Publication. This is about NLP'

Photo by John Schnobrich on Unsplash

Summary

This article explained how we can clean the text once we have gathered the text for our NLP project.

It explained following key techniques:

Convert Text To Lowercase Tokenise Paragraphs To Sentences Tokenise Sentences To Words Remove Numbers Remove Punctuation Remove Stop words Remove Whitespaces

Hope it helps.",https://medium.com/fintechexplained/nlp-text-processing-in-data-science-projects-f083009d78fc,['Farhad Malik'],2019-07-30 10:32:53.457000+00:00,947,"NLP, Text Processing, Data Science Projects, Python, NLTK Library"
Three Ways to Web Scrape a Page With Python…,"1. Pandas

This is the no-brainer. When I find a table on a website begging to be scrapped, my first attempt is always to throw pandas at it.

To practice, we are going to use the table page on webscraper.io, a site for testing your webscraping scripts. Here is a screenshot of the data we will be scraping. In this example we will be trying to grab the second table with idex 4–6.

To scrape a page with pandas, simply read the url into a pandas data frame:

import pandas as pd

df = pd.read_html(url)

print(df[1].head()) url = ' https://webscraper.io/test-sites/table s'df = pd.read_html(url)print(df[1].head())

And just like that, we have scraped the data we wanted. Note that when you scrape a web page with pandas, the tables are brought back as data frames in a list. Thus we need to iterate through df to find the correct table. Looking at the web page, we can make a logical guess that it will be df[1].

Web scraping really doesn’t get easier than that, unfortunately not every table you find on the web will be able to be scraped with pandas. On to the next level…

2. Requests and BeautifulSoup:

In my mind, this is where any Pythonista should start when learning to scrape websites. There are several more sophisticated alternatives when you are ready to crawl web sites, but for starting out this is a great library. Start by installing Requests and BS4 if you haven’t already:

pip install requests bs4

Simple enough, now we use requests to obtain the pages html code and BeautifulSoup out of bs4 to parse the page:

import requests

from bs4 import BeautifulSoup

page = requests.get(url).text

soup = BeautifulSoup(page, 'html.parser') url = ' https://webscraper.io/test-sites/table s'page = requests.get(url).textsoup = BeautifulSoup(page, 'html.parser')

Now we have options. We can take a look at the html by simply calling soup or we can take a look at our page and inspect the tags of elements of interest.

Looks like we need to pull <table>, <tr>, and finally <td> to get at our data. Lets get <table class=”table table-bordered”> first:

# get the tables first

tbls = soup.find_all('table', {'class': 'table table-bordered'})

Now if we look at the length of tbls we find that it is 4. We have obtained all 4 tables on the page in a few lines of code. Lets get the data out of it next:

tr = tbls[1].find_all('tr')

This code simply grabs all the data in the first <tr> tag which includes all of our ‘table header’ data. Now we can use list comprehension to get all the text from our table to include the column names.

data = [d.text for d tr]

Printing “data”, we see that we now have a list of strings.

Lets drop off the first and last “

” for each string and then split on all the other Line Feed characters (

):

data = [d[1:-1].split('

') for d in data]

Now make a data frame:

df = pd.DataFrame(data[1:], columns=data[0])

3. Simplify and Remove Dependencies

Lets make this as independent as possible now. No imports at all, only core python modules. We will scrape this page in the following steps:

Source the page: note that this wont work with JS dynamic pages Copy the data you want out Use string manipulation to get your data

View the page source of the website you want to scrape

Here is the data we want

Store it in a string variable and then manipulate that string to pull out the data. This can be tedious, but it allows you to get at data that can be very difficult to scrape otherwise. In this case it’s straightforward:

Set you string variable

Lets make it easier to work with by identifying and then matching our tags that will be easiest to manipulate. I am going to use the table row <tr> tag to pull out the data. Lets get rid of the </tr> closing tags:

data = data.replace('</tr>', '<tr>')

If we look at string now, we will see that there are a lot of tab and line breaks. Lets get rid of those as well. We could use REGEX here, but no imports…

data = data.replace('

\t\t\t\t', '')

Now lets get after that data. Note that all our data is inside either the <th> or <td> tags. Lets make it easier to work with that:

data = data.replace('<th>','<td>').replace('</th>','<td>')

data = data.replace('</td>','<td>') # now try spliting on '<td>'

data = ddata.split('<td>')

Looks like we need 1 to -1. Lets get that and lets drop all the garbage:

data_clean = [d for d in data[1:-1] if d != '' and '

' not in d]

Now we have all our data in a list. Lets finish this! Our data is currently in a list and we need it in either an array or in a list of lists. We could import numpy and use it to convert the shape like this…

# Using Numpy - super easy, but requires another import import numpy as np

data_array = np.array(rows).reshape(3,4)

But, again, we aren’t importing anything in this method so lets just use zip…

# zipping rows to itself in groups of 4 data_grouped = list(zip(*(iter(data_clean),) * 4))

And finally we export this to a flat file:

with open('scraped_page_with_strings_only.csv', 'w+') as f:

for row in data_grouped:

for x in row:

f.write(str(x)+',')

f.write('

')

Now I know you are doubting if this actually worked, so lets check.

A little cleaning and…",https://medium.com/swlh/three-ways-to-web-scrape-a-page-with-python-58378a293e6b,[],2020-09-30 01:52:21.528000+00:00,844,"tags related to the domain pandas, requests, beautifulsoup, webscraping, html parsing"
Simple Native Implementation of Linear Regression with Gradient Descent,"In this blog, I will try to explain how to write native code for Linear Regression in Python.

1.We will start of by writing equation function , given a single value of x , it’s weight and the intercept. Y = B0 + B1 * X

#Linear Equation

def calculatey(x,B0,B1):

y = B0 + B1 *x

return y

2.Caluculate the error/delta for each instance

#Delta/error for Gradient Descent

def caluculateError(y,y_pred):

error = y_pred-y

return error

3.Add Gradient Descent Step

#Gradient Descent Step

def updateWeights(B0,B1,alpha,error,x):

B0 = B0 - (alpha * error)

B1 = B1 - (alpha* error * x)

return B0,B1

4.Root-Mean-Squared-Error

#Root Mean Square Error

def rmse(Y_pred,Y):

squarred_errors = [(y_pred-y)* (y_pred-y) for y_pred,y in zip(Y_pred,Y)]

return np.sum(squarred_errors)

5.Linear Regression-Putting all those together

#Linear Regression

#Intialize the weights to Zeros

def applyLinearRegression(X,Y,epochs,alpha):

B0 = 0.0

B1 = 0.0

Y_pred = []

B0_List = []

B1_List = []

Error_List = []

for epoch in range(epochs):

for x,y in zip(X,Y):

B0_List.append(B0)

B1_List.append(B1)

y_pred = calculatey(x,B0,B1)

error = caluculateError(y,y_pred)

Error_List.append(error)

B0,B1 = updateWeights(B0,B1,alpha,error,x)

print(f""Final B0 = {B0} B1 = {B1}"")

Y_pred = [calculatey(x,B0,B1) for x in X]

print(""Root Mean Squared Error = "",rmse(Y_pred,Y))



#return (B0_List,B1_List,Error_List)

6.Test the implementation

import numpy as np

import math

#Generate Data to test

x = np.random.randint(0, 10, 10)

y = 3 + 4 *x

7.Making the Call to our funtion

applyLinearRegression(x,y,epochs=100,alpha=0.01)

So, Gradient Descent almost matches to exact weights of the original equation. Further, we can plot B0,B1 with Error and Epochs to analyze how gradient descent is progressing.

Entire code in Github.",https://medium.com/@maddali.bansi/simple-native-implementation-of-linear-regression-with-gradient-descent-e43fec9237bb,['Bansi Maddali'],2020-10-13 07:41:51.067000+00:00,202,"Linear Regression, Python, Gradient Descent, RMSE, Epochs"
COVID-19: Impact on Housing Security Across the U.S.,"COVID-19: Impact on Housing Security Across the U.S. Jbochenek Follow Dec 10 · 14 min read

Housing is essential, but not guaranteed. This has never been more obvious than since the start of the COVID-19 lockdowns stranded Americans from their jobs, and thus their incomes. Without income, paying for routine and necessary bills such as food and housing can become a struggle. Housing insecurity is certainly not a new addition to America, but for the first time, we have week by week data on how it has impacted households across America.

Starting in April, the U.S. Census Bureau began a new project, the Household Pulse Survey, with the goal of determining the social and economic impacts of COVID-19 on the American populace. Phase one lasted from April 23rd to July 21st, and this analysis examines those 12 weeks (calendar savvy will notice that this is in fact 13 weeks, but that will be discussed below).

The Household Pulse Survey phase one results are available as Public Use Files (PUF), where each row is a response. However, due to privacy reasons, the PUF does not include location indicators, which was desired for this analysis. Instead, we used the summarized data which was slightly edited due to nested headers. The file we used is available here.

For this, we also worked in Google Colab for easier code sharing across the team. First we imported the necessary packages.

from google.colab import drive

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

import plotly.express as px

import plotly

from sklearn import preprocessing

from urllib.request import urlopen

import json # This will prompt for authorization.

drive.mount(‘/content/drive’)

Then we imported the data:

Household = ‘/content/drive/My Drive/Data/Housing/Household Pulse Survey/phase-one-household-pulse-survey-tool overall.xlsx’ Phase1 = pd.read_excel(Household, sheet_name=’Data’)

The data has three different levels of location, nationwide, state level, and the top 15 largest metro areas. It was important to separate these out, as we wanted to make comparisons within these location groups, not between these location groups. We grabbed only the rows we wanted into three different datasets:

State=[‘Alabama’, ‘Alaska’, ‘Arizona’, ‘Arkansas’, ‘California’, ‘Colorado’, ‘Connecticut’, ‘Delaware’, ‘District of Columbia’, ‘Florida’, ‘Georgia’, ‘Hawaii’, ‘Idaho’, ‘Illinois’, ‘Indiana’, ‘Iowa’, ‘Kansas’, ‘Kentucky’, ‘Louisiana’, ‘Maine’, ‘Maryland’, ‘Massachusetts’, ‘Michigan’, ‘Minnesota’, ‘Mississippi’, ‘Missouri’, ‘Montana’, ‘Nebraska’, ‘Nevada’, ‘New Hampshire’, ‘New Jersey’, ‘New Mexico’, ‘New York’, ‘North Carolina’, ‘North Dakota’, ‘Ohio’, ‘Oklahoma’, ‘Oregon’, ‘Pennsylvania’, ‘Rhode Island’, ‘South Carolina’, ‘South Dakota’, ‘Tennessee’, ‘Texas’, ‘Utah’, ‘Vermont’, ‘Virginia’, ‘Washington’, ‘West Virginia’, ‘Wisconsin’, ‘Wyoming’] US = [‘United States’] Metros=[‘Atlanta-Sandy Springs-Alpharetta, GA Metro Area’, ‘Boston-Cambridge-Newton, MA-NH Metro Area’, ‘Chicago-Naperville-Elgin, IL-IN-WI Metro Area’, ‘Dallas-Fort Worth-Arlington, TX Metro Area’, ‘Detroit-Warren-Dearborn, MI Metro Area’, ‘Houston-The Woodlands-Sugar Land, TX Metro Area’, ‘Los Angeles-Long Beach-Anaheim, CA Metro Area’, ‘Miami-Fort Lauderdale-Pompano Beach, FL Metro Area’, ‘New York-Newark-Jersey City, NY-NJ-PA Metro Area’, ‘Philadelphia-Camden-Wilmington, PA-NJ-DE-MD Metro Area’, ‘Phoenix-Mesa-Chandler, AZ Metro Area’, ‘Riverside-San Bernardino-Ontario, CA Metro Area’, ‘San Francisco-Oakland-Berkeley, CA Metro Area’, ‘Seattle-Tacoma-Bellevue, WA Metro Area’, ‘Washington-Arlington-Alexandria, DC-VA-MD-WV Metro Area’] StatesP1 = Phase1[Phase1[‘Geography (State or Metropolitan Area)’].isin(State)] USP1 = Phase1[Phase1[‘Geography (State or Metropolitan Area)’].isin(US)] MetroP1 = Phase1[Phase1[‘Geography (State or Metropolitan Area)’].isin(Metros)]

It soon became obvious that 50 states was a large number to handle, in visualization, so we added another level to the state data — Divisions. The U.S. Census defines the US by several location levels, one of the most familiar is Regions: Midwest, Northeast, South, and West. There are also Divisions, which split the regions up even smaller. Figure 1 below shows the breakdown of Regions into Divisions.

Figure 1. Regions and Divisions of the United States

We used a data dictionary to add that to the data. I’m including this here so maybe no-one else has to write this code again.

Divisions = {‘Alabama’: ‘East South Central’,

‘Alaska’: ‘Pacific’,

‘Arizona’: ‘Mountain’,

‘Arkansas’: ‘West South Central’,

‘California’: ‘Pacific’,

‘Colorado’: ‘Mountain’,

‘Connecticut’: ‘New England’,

‘Delaware’: ‘South Atlantic’,

‘District of Columbia’: ‘South Atlantic’,

‘Florida’: ‘South Atlantic’,

‘Georgia’: ‘South Atlantic’,

‘Hawaii’: ‘Pacific’,

‘Idaho’: ‘Mountain’,

‘Illinois’: ‘East North Central’,

‘Indiana’: ‘East North Central’,

‘Iowa’: ‘West North Central’,

‘Kansas’: ‘West North Central’,

‘Kentucky’: ‘East South Central’,

‘Louisiana’: ‘West South Central’,

‘Maine’: ‘New England’,

‘Maryland’: ‘South Atlantic’,

‘Massachusetts’: ‘New England’,

‘Michigan’: ‘East North Central’,

‘Minnesota’: ‘West North Central’,

‘Mississippi’: ‘East South Central’,

‘Missouri’: ‘West North Central’,

‘Montana’: ‘Mountain’,

‘Nebraska’: ‘West North Central’,

‘Nevada’: ‘Mountain’,

‘New Hampshire’: ‘New England’,

‘New Jersey’: ‘Middle Atlantic’,

‘New Mexico’: ‘Mountain’,

‘New York’: ‘Middle Atlantic’,

‘North Carolina’: ‘South Atlantic’,

‘North Dakota’: ‘West North Central’,

‘Ohio’: ‘East North Central’,

‘Oklahoma’: ‘West South Central’,

‘Oregon’: ‘Pacific’,

‘Pennsylvania’: ‘Middle Atlantic’,

‘Rhode Island’: ‘New England’,

‘South Carolina’: ‘South Atlantic’,

‘South Dakota’: ‘West North Central’,

‘Tennessee’: ‘East South Central’ ,

‘Texas’: ‘West South Central’,

‘Utah’: ‘Mountain’,

‘Vermont’: ‘New England’,

‘Virginia’: ‘South Atlantic’,

‘Washington’: ‘Pacific’,

‘West Virginia’: ‘South Atlantic’,

‘Wisconsin’: ‘East North Central’,

‘Wyoming’: ‘Mountain’} StatesP1[“State”] = StatesP1[“Geography (State or Metropolitan Area)”].astype(‘category’) StatesP1[‘Division’] = StatesP1[‘State’].map(Divisions)

We needed to do some exploratory data analysis to determine the quality of the data and any adjustments that would need to be made.

sns.displot(StatesP1, x=”Housing Insecurity Percent”, element=”step”, col=”Division”, col_wrap=3) g = sns.boxplot(x=""Division"", y=""Housing Insecurity Percent"", #hue=""Selected Horizontal Dimension"", data=StatesP1, palette=""Set3"")

g.set(xlabel='Division', ylabel='Housing Insecurity (%)')

g.set_xticklabels(g.get_xticklabels(),rotation=45,ha=""right"")

Figure 2. Histogram of Housing Insecurity Percent from the Household Pulse Survey by Census division from April 2020 — July 2020

Figure 3. Boxplot of Housing Insecurity Percent from the Household Pulse Survey by Census division from April 2020 — July 2020

Overall, we were very pleased with the distribution of data, in the histograms it shows as relatively normal and in the boxplots we only see one true outlier. For the purposes of this analysis, we kept that outlier as it was important trend data.

We also wanted to get a first look at the actual data, how has the housing security changed over the 12 week period across the US?

g = sns.relplot(kind = ‘line’, data=StatesP1, y =’Housing Insecurity Percent’, x= ‘Week Number’) g = sns.relplot(kind = ‘line’, col=’Division’, col_wrap=5, col_order =[‘Pacific’, ‘West North Central’, ‘East North Central’, ‘Middle Atlantic’, ‘New England’, ’Mountain’, ‘West South Central’, ‘East South Central’, ‘South Atlantic’ ], data=StatesP1, y =’Housing Insecurity Percent’, x= ‘Week Number’)",https://medium.com/swlh/covid-19-impact-on-housing-security-across-the-u-s-6c9d787ce2d,[],2020-12-16 17:40:54.131000+00:00,917,"COVID-19, Housing Security, US Census Bureau, Household Pulse Survey, Public Use Files"
Understanding ML Evaluation Metrics — Precision & Recall,"Understanding ML Evaluation Metrics — Precision & Recall

The jargon of machine learning world is crucial to convey how well your model works

Touching base, quoting a ball park number, hitting it out of the park, it being a whole new ball game are all examples of the jargon that is borrowed from the world of baseball and heavily used (or misused in some cases) in the corporate world.

Machine learning world similarly uses a set of terms routinely, to specify how well the models are working. The question arises — why do we need anything other than the term accuracy? Accuracy is simply defined as

No. of correct predictions divided by total number of predictions

Photo by Isaac Smith on Unsplash

Now imagine that an oncologist, an expert in breast cancer, has 1000 patients. He decides to declare all his patients as free of cancer but then later on finds out that 3 of those actually did have cancer. For the doctor it still means an accuracy of 99.7% but for those 3 patients the results are very severe. This thus is the pitfall to using just accuracy as your success metric.

These kind of “naive” results are obtained when we encounter an imbalanced dataset. An imbalanced dataset is one which has too few examples of one kind.

Use of precision & recall in the real world

Precision, recall, sensitivity and specificity are terms that help us recognise this naive behaviour. Routinely the ML teams in companies like Microsoft, Amazon ask their employees to quote the PR (precision and recall) numbers or to quote the sensitivity and the specificity of the results.

These numbers help us understand what is relevant in the data

Let’s understand with an example. Let’s say our oncologist treats 1000 patients.

He declares 950 of them to be free of cancer. In doing so he makes 10 mistakes. 940 of these he correctly diagnoses to be free of cancer. The unfortunate 10 have cancer but escape doctor’s notice.

He diagnoses the remaining 50 to have cancer. Of these only 5 patients really have cancer. Rest of the 45 patients are free of cancer but have been incorrectly identified as cancer patients.

Breast cancer example

At the end we have 4 set of patients. Let us label all these 4 kinds of patients.

True Positives (TP) —5 patients who are tested positive for cancer correctly

—5 patients who are tested positive for cancer correctly False Positives (FP) —45 healthy patients that are tested positive incorrectly

—45 healthy patients that are tested positive incorrectly True Negatives (TN) — 940 healthy patients who tested negative for cancer correctly

— 940 healthy patients who tested negative for cancer correctly False Negatives (FN) — 10 patients who had cancer but were declared negative for cancer incorrectly

The word positive means a ‘yes’ to a question we are asking. e.g. Does this person have cancer? A positive would be when we say ‘yes this person has cancer’. A negative would then mean when we say that, ‘this person does not have cancer’.

Let’s look at these 4 terms pictorially

The confusion matrix

The above chart is popularly known as the confusion matrix. Confused eh? Don’t be. This matrix becomes the solution to our problems. Given this matrix we can easily compute all of the terms we have been talking about so far.

First let’s look at how accurate the doctor has been. The number of correct decisions that the doctor has taken are the ones labelled true.

Given these numbers our accuracy comes out to be 94.5%

An accuracy of 94.5%

Even though this seems like a high number it still leaves out a lot of patients susceptible to extreme losses. Take for example the False Negative (FN) patients. These 10 people would be relieved that the doctor has given them a clean chit whereas the cancer would be causing harm in the meanwhile.

This is where the above terminology comes to our rescue.

Precision tells us how relevant are the positive detections. Higher the precision better is our detection mechanism. e.g. in our example precision is just 10% which is very poor. We are misclassifying a lot of healthy patients as cancer patients.

Precision is just part of the picture. Another way to look at our data is through recall. Recall is the same as sensitivity.

They tell us what percentage of actual positives are detected. i.e. what percentage of real cancer patients are detected. It is computed by taking the ratio of correctly identified cancer patients (true positives) to the total number of cancer patients (true positives + false negatives).

This means we are only able to detect 33% of the total cancer patients correctly. That is bad!

Specifity tells us how good the system is at removing false alarms. It is computed by taking the ratio of patients who are correctly detected to not have cancer (true negatives) to all the patients who do not have cancer (true negatives + false positives).

We are able to weed out false positives 99.5% of the times. That’s very good.

So what do these numbers tell us?

High Precision + High Recall — The prediction model (i.e. the doctor in our case) is highly dependable. The model doesn't misclassify healthy patients and doesn’t wrongly leave out the cancer patients Low Recall + High Precision — This just means the model is very picky. It doesn’t generate a lot of false positives but misses out on a lot of the real cancer patients. Such models cannot be used for life critical data e.g. cancer detection, terrorist identification, accident prevention etc. High Recall+ Low Precision — The model is able to detect most of the positives well but ends up creating a lot of false alarms. Such models should not be used for cases where false alarms have a huge cost — e.g. flight landing systems, crowd management systems, war prediction systems etc. Note that such models can work for life critical situations. It is still better to classify a few healthy patients as cancer patients than vice versa.

Needless, to say that when both of these are low the model is pretty useless.",https://medium.com/x8-the-ai-community/understanding-ml-evaluation-metrics-precision-recall-2b3fb915b666,['Rishi Sidhu'],2019-06-21 15:16:44.419000+00:00,994,"machinelearning, MLevaluationmetrics, precisionrecall, imbalanceddataset, confusionmatrix"
"In With the Old, In With the New","What does the phrase “Data Visualization” sound like to the uninitiated? “Data” conjures up images of computers and statistical analysis, whereas Visualization is more accessible but vague enough so as to be unclear. They may wonder: Is Data Visualization new, overflowing with cutting edge tools and technology, or is it as old as human communication itself?

Well, yes.

This week on Nightingale, we covered some of Data Viz’s exciting new developments and revisited the work of some early practitioners.

Looking backward, Paul Kahn continued his series on cosmology and global information design with his piece, “Cosmology in the Small,” which considers the design techniques of early migration maps and historic theological texts.

Jason Forrest also looked back in time, though not quite as far, in his piece “The Greatest Library You’ve Never Heard Of.” He visited with Chris Mullen, the creator of The Visual Telling of Stories, an expansive, online visual archive brimming with design artifacts from throughout the 20th century. Mullen’s library includes the largest collection of Fortune magazine, the oft-overlooked design and business magazine. Come to get to know Mullen and what inspired him to undertake such a project; stay for hours perusing this wonderful, strange corner of the internet.

Data Visualization may be rooted in ancient times and have a rich history over the last couple centuries, but the field is transforming in the technological age, and transforming the world along with it. We cover that too!

In his piece, “Beyond the Goal: Visualizing Expected Assists in Soccer,” Benoit Pimpaud put the newest data viz techniques to use in his analysis of expected assists and passing in soccer. Since soccer is so low-scoring, the final scoreboard doesn’t convey the entire picture. And since spacing and positioning are critical to the beautiful game, data visualization is a highly effective tool for analysis.

If you’ve ever wanted to know how to make fully responsive D3 charts inside of React, you are in luck, because Dave O’Donovan has taken us on a journey to “D3-Land” and “React-Land.” Read Part 1 and Part 2 today, and be on the lookout for Part 3 next week.

And, as always, please get in touch if you are interested in writing for Nightingale. Whether you have a draft ready or just the earliest stages of an idea, don’t hesitate to drop us a line.",https://medium.com/nightingale/in-with-the-old-in-with-the-new-6cfa3da9997d,['Isaac Levy-Rubinett'],2019-10-18 16:49:40.946000+00:00,382,"data visualization, cosmology, information design, visual archive, Fortune magazine"
Confinement,"Learn more. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more

Make Medium yours. Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore",https://medium.com/illumination/confinement-f15962101b7d,['Patrick M. Ohana'],2020-12-17 15:18:51.573000+00:00,64,".Medium, Open Platform, Insightful Thinking, Dynamic Thinking, Undiscovered Voices"
Debugging Workflow for Python Beginners in Data Science,"We’ve all been there. Staring at a screen full of code that’s not working and thinking, “whoever designed this language is seriously MESSING with me!”

But before you go down the rabbit hole of life’s regrets, take a step back and use this debugging checklist to get back on track.

The Debugging Mindset

Congratulate yourself! Despite the daunting complexity of a new programming language, you still gave it a try. For a beginner, that’s a victory in itself.

Despite the daunting complexity of a new programming language, you still gave it a try. For a beginner, that’s a victory in itself. Embrace the bug. This is your greatest opportunity to learn because once you fix the bug — and you will fix it — you’ll never forget the mistake. In fact, the more difficult the bug, the deeper it will be ingrained into your subconscious. Sweet dreams!

This is your greatest opportunity to learn because once you fix the bug — and you will fix it — you’ll never forget the mistake. In fact, the more difficult the bug, the deeper it will be ingrained into your subconscious. Sweet dreams! You’re not alone. The first programming bug was an actual moth found in the massive Harvard Mark II by Grace Hopper. You are only the latest in a long and storied lineage of metaphysical exterminator programmers. Stand on the shoulders of giants.

The first programming bug was an actual moth found in the massive Harvard Mark II by Grace Hopper. You are only the latest in a long and storied lineage of metaphysical exterminator programmers. Stand on the shoulders of giants. Don’t beat yourself up. Remember the main objective is to make something work. Getting angry at yourself won’t make your code work. But this process will.

Error Messages

Start with error messages. They’re your friends! But real friends who tell you when you’re making a mistake instead of the other people who let you fall on your face. Python is an extremely well documented language with error messages that get right to the point.

Common Errors

Type Error: There’s an issue with your data or collection types.

There’s an issue with your data or collection types. Name Error: There’s an issue with your variables.

There’s an issue with your variables. Syntax Error: There’s an issue with your syntax.

There’s an issue with your syntax. Value Error: There’s an issue with your arguments.

Read the error messages closely and they will tell you a lot about the issue. If you’re having trouble understanding the error, then look at the Python documentation or simply copy and paste the message in a search engine.

No Error Message!

If there’s no error message, then it means your code technically works, but its not printing your desired output. There’s either a problem with your code, your understanding of the data, and/or your overall logic for solving the problem.

Compare Current Output to the Desired Output

If you haven’t already, you should clearly describe your desired output. For example, what data type should it be? Compare your current output to the desired output and identify the key differences between the two.

Quick Code Check

A review of outputs should give you an idea for how to tweak the code. Use print() and type() functions to test each line of your code to make sure it’s doing what it should.

Review Data

If your code check doesn’t solve the issue, then take a second look at your data. A deeper understanding of your data can illuminate that you didn’t know enough yet to solve the specific problem your working on. For example, maybe a specific column of categorical data was encoded with numerical values, like 1 for New York, 2 for Connecticut, and 3 for New Jersey.

Update Your Logic

A better understanding of your data and your code will help you update your conceptual logic for how to solve the problem. If you’re really struggling, then step away from the screen and try writing out your solution in pseudo-code. Then check your pseudo-code against your actual code. If you still can’t figure it out, then at least you can clearly explain your logic to someone who might be able to help.

Ask a Friend or Mentor

Sometimes just talking through a problem with a friend or mentor can help you find the solution on your own. Sometimes you truly do need their input to expand your understanding of the code, data, and programming logic. Or if they can’t help you, maybe its time to find new friends. You can also use sites like stackoverflow to get the input of developers from around the world.",https://medium.com/@mfflavell/debugging-checklist-for-python-beginners-d3719b8e7e6d,['Frank Flavell'],2020-03-18 20:45:35.258000+00:00,753,"debugging, programming language, error messages, type error, name error"
Data Science is not Rocket Science — my takeaway from GreyAtom 5 day Bootcamp.,"What is the most important thing you would need to do to become a data scientist. The answer is “ Consistency ”. The question that should pop up in your mind now is — are there no prerequisites. Believe me there are none, with reference to your prior education. Though there is a basic very important requirement : you should be willing to code and get your hands dirty by experimenting with data and code. Aaaah!! So, did that bring a smile on your face. This is real. Now, let’s get to the point. How I got started and how that will help you get started too.

Awesome. I was at the same state a few years back. And I won’t lie to you that you can become anything you want. Although through this article I can tell you, how this 5 day bootcamp at GreyAtom gave me motivation and guide to become Data scientist.

Are you fascinated by the title Data Scientist ? Do you wonder who are these techie people ? Does the idea of becoming a Data scientist in a top class company give you goose bumps ?

I have done masters in Artificial Intelligence from K U Leuven university in Belgium. I have worked in Belgium post my studies. I have done projects on machine learning , computer vision , natural language processing. I am looking for a job as data scientist in India and this workshop helped me to identify my weak points and where I should focus more if I want to reach my goal.

I was already in the field of machine learning, so I had an idea of the language used and the platforms needed. But they did provide detailed video to get started with Jupyter notebook, GitHub, and Medium account. So, if you are not a coder, you can still dream to be a data scientist and its not too late. I got myself into writing my first article in Medium. Thanks to GreyAtom. And I will tell you later why blogging is important.

Day1 : More than the programming and mathematical aspects of machine learning, this workshop was about problem solving. You will get an idea of how to think and formulate your problem. Have you heard the saying, half of the answer lies in the question itself. This is so true in Data analysis.

Day2 : What are data scientists ? It should ring a bell in your head. Something to do with data ? That’s right. So we have to explore what the data is trying to tell us. In the workshop we learnt of several basics of data visualization and clean up techniques that help us to understand the data. Once you understand the data, you know where to go next.

Day3 : Here comes the science part. Oh no, don’t worry. Not rocket science. Here we learn to build models. If that sounds complex, in simple words, we make the program understand the data and learn some patterns in it, using various algorithms and predict the futuristic data. We learn to formulate a business problem into machine learning problem, such as classification or regression or other AI problems. In the workshop, we take an example banking domain problem, try to formulate it into a machine learning problem and create a model that will be used to classify the clients into particular categories.

Day4 : Once we had hands on with machine learning models and solving problems, we now discussed about how the solution is going to benefit a specific organization. Is it worth to invest in machine learning / data scientists team to increase revenue of a company. If so, how do we prove that we are the ones who will be able to increase their revenue through our data analysis. Because, if not I then who ? Ask yourself this, and you will know what to do.

Day5 : This was the last day of the workshop. Basically the last workshop of the year 2020. I can definitely say, this gave me a lot of motivation to start from where I am and take steps forward to reach my goal. Everyone’s questions were answered by the mentors: Hardik Gupta and Mayuresh Shilotri. Thank you very much for the amazing workshop.

We did work on a project end to end, except for the deployment part. We got tasks everyday to look at challenges from Kaggle and get our hands dirty with datasets from Kaggle, comment on linked in with our insights on the data. These comments were addressed in the workshop sessions which helped a lot. Access to very useful study materials and video lectures were given for lifetime which will help one to get on track.",https://medium.com/@swarnalatasumi/data-science-is-not-rocket-science-my-takeaway-from-greyatom-5-day-bootcamp-bfa98b27715a,['Swarnalata Patra'],2020-12-21 10:19:35.449000+00:00,781,"data science, machine learning, problem solving, data visualization, AI problems"
Facing the ARIMA Model against Neural Networks,"Regarding this first plot, we can see that at the end and at the beginning of each year the concentration of PM2.5 particles is higher than in other periods of the year, but it is still fuzzy data and we can barely have a better understanding of what is really happening. Because of this, some transformations are needed to be able to get a deeper insight into our data. There are many tools available, and here we are only going to use two, but there are many others that can provide better or different insights.

Grouping by different frames of time

The first method is as simple as to compute the mean of the data measurements by week or month.

Weekly average.

Monthly average,

As we can see from the plots, the information that we get from the data using this method is not very clear.

Moving average

This time we are still going to use the mean but in a different way, by using the Moving Average, that computes the average of N given time steps. It will smooth the data allowing the viewer to infer some visible patterns or trends.

Weekly Moving average

In this last plot, we can clearly spot some regular patterns. However, these methods are very sensitive to outliers and as we can see in the first plot our data has many of them.

Please note the difference between these two transformations because they look similar but they are not the same. In the average, we just compute the average per week and in the Moving Average (MA) we compute the average of N lagged observations. Although, the second one is more useful because it will help us to smooth the data removing the noise.

First Conclusion

There is nothing clear we can get from the data even though it seems to have many outliers. and one can think of these outliers as faulty measures by the devices related to any kind of misfunction. However, doing some research we will find that these levels have been already reached and documented in the capital of China.

Getting closer to the last weeks

The data that we have evaluated so far looks quite messy, and as the objective of this small project is to apply different forecasting techniques, we are going to focus the efforts in the last four weeks of the entire dataset. By doing so the visualizations are going the be easier and we will see clearly how the predictions fit in our data.

The caption of the last four weeks

In the distribution of the data, we can see that most of the data is grouped in the first values, looking like the exponential distribution.

In this data dataset, there are 716 measures of the particle’s concentration, around one month of data. With a mean of 78 particles per hour. This means is considered as unhealthy and people should not be exposed during long periods of time to these levels.

Stationarity

Before applying any statistical model it’s important to check if our data is considered as stationary

Stationarity basically means that the properties such as the mean and variance don’t change over time.

(https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc442.htm)

There are various ways to check if data is stationary, and one good way is the Test Dickey-Fuller, which states that if the p-value is lower than a given threshold it won’t be considered as stationary.

Test Dickey-Fuller

As we proved with Dickey-Fuller Test, fortunately, our data is stationary which means that it doesn’t have any kind of trend and therefore it doesn’t need any transformations and we can directly to apply ARIMA.

Sometimes the time series are non-stationary. For these cases, it is possible to apply transformations to the data that will make them stationary.",https://towardsdatascience.com/facing-the-arima-model-against-neural-networks-745ba5a933ca,[],2021-08-23 13:57:17.566000+00:00,601,"PM2.5, Particulate Matter, Air Quality Monitoring, Test Dickey-Fuller, Stationarity"
Simple image classification on raspberry pi using the pre-trained model VGG16 and TensorFlow,"Note before you start:

So, Let’s start :)

Hardware preparation:

Software preparation:

1 Create file image_classify.py with next code:

In this example, I will use the pre-train model VGG16, but you can try to use any pre-train model.

#import modules

from keras.applications.vgg16 import VGG16

from keras.preprocessing import image

from keras.applications.vgg16 import preprocess_input, decode_predictions

import numpy as np #load imgenet vgg16 model

model = VGG16(weights='imagenet') #load image and change size to 224*224

img_path = 'demo.jpg'

img = image.load_img(img_path, target_size=(224, 224)) #convert image to array

x = image.img_to_array(img)

x = np.expand_dims(x, axis=0)

x = preprocess_input(x) #predict class for image

preds = model.predict(x)

print('Result:', decode_predictions(preds, top=1)[0])

2 run script for recognizing the image

python3 image_classify.py

3 You should see something like that

Simple image classification on raspberry pi using the pre-trained model VGG16 (Photo,GIF by Author) https://github.com/oleksandr-g-rock/simple_classification_pi_vgg16_pretrain_model/blob/main/1_P6WMPQdlMVmaFpLap3ResA.png

And this result absolutely right :)

Result:

In this article, we created simple image classification on raspberry pi using the pre-trained model VGG16. All code is located here.",https://medium.com/@oleksandrg/simple-image-classification-on-raspberry-pi-used-the-pre-trained-model-vgg16-part-4-0-afa4f8bdf775,['Alex G.'],2020-12-14 03:33:18.331000+00:00,137,"raspberry pi, image classification, pre-trained model VGG16, keras, decode_predictions"
Machine Learning with only SQL — Using BigQuery to Identify the Target Audience for Shared Bike,"Step4: Predict the result

Finally, using ML.PREDICT to come up with the result that the average duration in the 2019 dataset will be 51 minutes.

Step5: How to improve the accuracy of your model?

How to improve accuracy is one of the most interesting for data scientists.

I used to decompose the steps in a process and tackle each.

Here is my thinking process:

Data

Feature

Algorithm

Parameter

Let’s focus on fundamentals, such as data and features. We’ve done a small experiment on adjusting features. If taking a deeper look into data, there are some basic observational statistics to work with, such as observing average, maximum, and minimum to see if they make sense. For example, some data in the table don’t make sense, duration more than 30,000 hours, or a length < 0.

If we could trim out these data, the credibility of the prediction result will be more desirable.

Thoughts

BigQuery makes ML with SQL possible

Not everyone has access to Python, but what if we need to deliver anyway? For some Database Administrator or Data Analysts, the familiarity of Python may not be a “must-have” skill. The built-in ML for BigQuery let us deliver business result without mastering a new language.

Streamlining the process so you can focus on what matters the most

It’s common to see my enterprise customers running between all kinds of prerequisites before jumping into a machine learning project. As a result, it’s more time-consuming to deliver results due to data movement, security checks, access control, and networking settings. Now many solution providers integrated tools so that the process is smoother. For example, when doing data analysis, we used to download data in the database, send the data, and import it to IDE. Now, only a Mount function will do the work, just like using a USB stack to load data into a computer.

Data Warehouse is evolving to combine BI and ML as a data analytics platform

In the past, a data warehouse was simply a tool for parallel data processing and storage. As time goes by, it also combined new elements such as business intelligence and machine learning for more complex analytics jobs. Besides, many data warehouses offer real-time analysis to fulfill streaming demand in the big data scenario. The architecture below shows how to use Synapse (the Data Warehouse solution from Microsoft) for IoT devices and web click data analysis.",https://towardsdatascience.com/machine-learning-with-only-sql-using-bigquery-to-identify-the-target-audience-for-shared-bike-aa3a4041be3a,['Wan Chung Huang'],2021-08-02 12:36:06.968000+00:00,382,"Big Query, Python, Feature Engineering, Data Warehouse, Business Intelligence"
MultiCarRacing: A Multi-Agent Reinforcement Learning Gym Environment,"Background and Motivation

OpenAI Gym¹ environments allow for powerful performance benchmarking of reinforcement learning agents. In this article, we introduce a novel multi-agent Gym environment, MultiCarRacing-v0 , that augments the original Gym CarRacing-v0 environment. This augmented environment can be used for evaluating any deep multi-agent reinforcement learning agent that learns from pixels.

Our implementation of this MultiCarRacing-v0 environment can be found here, and is used for evaluation in our recent paper “Deep Latent Competition: Learning to Race Using Visual Control Policies in Latent Space” (see BibTex citation below).

Race segments from trained agents in our MultiCarRacing-v0 environment.

This environment augments the original Gym CarRacing-v0 environment through the following features:

Multi-car races: Our environment supports races with any N ≥ 1 cars. Randomized starting locations and car colors for all cars on the track. A novel reward mechanism, in which the reward is computed as (K-k)/K * 1000/N , where k is the number of cars that have crossed a tile before the current car, and K is the total number of cars in the race. This reward incentivizes agents to reach tiles before their opponents. Randomization of track direction: this can be set to CW (clockwise), CCW (counterclockwise), or RANDOM (each time the track is regenerated, there is a 50% chance of each direction). Ego car color: If desired, each car can view themselves as the same color, and all opponent cars as the same (different) color. The GIF below illustrates this. Backwards flag: In practice, we noticed that after a spin-out, cars tend to drive in the reverse direction. If desired, the use_backwards_flag option penalizes agents for driving on the track in the reverse direction. Custom Viewing Height: Both our MultiCarRacing-v0 and the original CarRacing-v0 environments have a bird’s eye view of a car (see above). Adjusting our environment’s h_ratio parameter (default is 0.25 ) allows the agent to see pixels further ahead or behind the car.

Video of trained agents racing against one another in our augmented MultiCarRacing environment can be found below:",https://medium.com/analytics-vidhya/multicarracing-a-multi-agent-reinforcement-learning-gym-environment-176b789e07e7,['Ryan Sander'],2021-02-23 14:52:57.288000+00:00,329,"Multi Car Racing-v0, OpenAI Gym, Reinforcement Learning Agents, Multi-Agent Environment, Deep Multi-Agent Reinforcement Learning"
Train a CNN using Skorch for MNIST digit recognition,"Train a CNN using Skorch for MNIST digit recognition

Image made by Author

The goal of this post is to use a tool to train and evaluate a Pytorch’s model in a simple way. This tool is Skorch, that is a scikit-learn compatible neural network library that wraps Pytorch. So it makes possible to use Pytorch with sklearn. Moreover, it takes advantage of Scikit-learn’s functions such as fit, predict and GridSearch [1]. This tool is applied on MNIST, a dataset composed by images of handwritten digits: 60,000 for training and 10,000 for testing. In the convolutional neural network, we’ll take as input the image of a handwritten digit (0–9) and classify the image as the appropriate digit.

What is Skorch?

Skorch is a very recent open-source library, launched the first time in 2017. In a nutshell, it combines SciKit-Learn and pythORCH. Thus, it allows to easily swap neural networks with SKlearn and extract Pytorch modules. The main advantage of this open-source is the reduction of the boilerplate code. If you have used Pytorch in the past, writing the for training can be problematic, there is always the same code to write and it’s very easy to do mistakes having many lines [2].

Training with Pytorch:

Training with Skorch

Comparing the two examples, you can observe Skorch allows to use less lines of code. In order to make understand this open-source library works, I will walk through the classic MNIST classification problem.

Step 1: Import libraries and dataset

Let’s install Skorch, the library we’ll be using in all this tutorial:

Once the library is installed, we can import the libraries.

skorch.NeuralNetClassifier is a Neural Network class used for classification tasks.

Step 2: Prepare the dataset before training

We are creating an object using torch.device. The Tensor will be allocated in this device object, whose device type is cpu or cuda. To use the gpu, you need to specify “cuda”.

If we want skorch to do a validation split for us, we need to retrieve the y_train values from the training dataset and pass these values to net.fit later on:

Step 3: Create the model

Our convolution neural network has two convolution layers, each with 5x5 kernel, and two fully connected layers.

Step 4: Train Model

Now we are going to initialize the NeuralNetClassifier class, in which we specify:

Pytorch Module : in our case our CNN model.

: in our case our CNN model. criterion (default=nn.NLLLoss): we need to specify that we are using CrossEntropyLoss instead.

(default=nn.NLLLoss): we need to specify that we are using CrossEntropyLoss instead. optimizer (default=optim.SGD): we select the optimizer Adam.

(default=optim.SGD): we select the optimizer Adam. batch_size (default=128): we use 64 as batch_size.

(default=128): we use 64 as batch_size. lr (default=0.01): the learning rate passed to the optimizer is 1e-3.

(default=0.01): the learning rate passed to the optimizer is 1e-3. max_epochs (default=10)

(default=10) iterator_train and iterator_valid: The default PyTorch DataLoader used for training and validation data.

and The default PyTorch used for training and validation data. train_split(default=0.2): By default, 20% of the training data is reserved for validation.

Thus, the training data will be split internally by skorch into 80% training and 20% validation by default.

After the initialization, we will train the CNN model using the method fit:

The results show the training loss, the validation loss, the validation accuracy and the duration of each epoch. The model seems to work well since there is a slight difference between the two losses. It’s also evident looking at the plot below, obtained through the attribute history of the model “net”. Basically, it’s a list of dicts that contains the information about the model training: for each epoch there is an element, that, again, contains a list of dicts for each batch.

Step 5: Prediction

We’ll predict on the test data and evaluate the accuracy of the prediction using the functions predict and accuracy_score from sklearn library.

An accuracy of 99% is very good for a simple network. Another measure to evaluate the performance of the model is the confusion matrix, which compares the actual classes with the predicted classes. In diagonal, there is the number of points for which the predicted label is equal to the true label, while in the off-diagonal there are the misclassified elements.

Even if the overall accuracy is 99%, there are still many misclassified data. In particular, the class 4 is classified wrongly as 9 nine times. For this reason, we’ll try K-Fold Cross Validation to be sure to avoid overfitting and not loose generalization.

Step 6: Evaluate the score by K-fold Cross Validation

Before the evaluation, we need to make the training set sliceable using the class SliceDataset, that wraps a torch dataset to make it work with sklearn. We’ll use the function cross_val_score (estimator, X, y=None, cv=None) of sklearn library to evaluate the validation accuracy obtained using K=5 folds for K-Fold Cross Validation.

The results are related to the first two folds, but you also should see the other three tables like these.

Below there are the average accuracy of each fold and the average accuracy of all the folds:

Even if it’s computationally expensive, K-fold cross-validation is needed to evaluate the model without overfitting. The accuracy remains pretty high, so it means the model works well with any resampling procedures.",https://medium.datadriveninvestor.com/train-a-cnn-using-skorch-for-mnist-digit-recognition-53d7d2f971c7,['Eugenia Anello'],2021-01-22 23:55:24.553000+00:00,843,"skorch, pytorch, CNN, MNIST, digit recognition"
"Living in Austin, Texas; Part 1— Housing Trends with Time Series Analysis","Downtown Austin, Texas

Anyone who has lived in Austin for more than a couple years can tell you that the real estate and rental markets here are, as we say, ¡muy caliente! (if you don’t speak Texas Spanish, that means, “really dang hot!”).

Just how hot are the Austin housing and rental markets though? And perhaps more importantly, do they show any signs of cooling off in the near or intermediate future? Having good and empirically grounded answers to these and other questions can be highly valuable for an individual participant in the housing or rental markets — it can mean the difference between getting into or out of the residential real estate market at the wrong time, or seriously overpaying for rent at an apartment with more competitive options at other proximate geographic locations.

In Part 1 of this Two Part post, I will discuss the past and current trends in the Austin residential real estate and rental markets, and perform some analysis on the available time series data to gain insights that may help a prospective buyer/seller/renter make a more highly informed decision about their plans on when and where to buy/sell/rent.

Part 2 then focuses on constructing a rent pricing model using location-based hedonic regression, which may be used to find an optimal apartment, given an individual person’s location, price, and hedonic preferences/constraints, for example.

Data Sources

To perform the analysis, I used to main sources of data: Zillow Public Economic Data (Parts 1 and 2), and results pulled from Apartment.com Listings (Part 2).

Specifically, the Zillow Data used includes:

the median home value ($) per square foot (ft²) for each US zip code from Apr. 1996 to Jul. 2019 (Zip_MedianValuePerSqft_AllHomes.csv); and

the median rent list price for 1-bedroom apartments for each US zip code from Sept. 2010 to Jul. 2019 (Zip_MedianRentalPrice_1Bedroom.csv).

Reading the Data into Python using Pandas

Lets start with reading in the median home $/ft² data at a DataFrame object using the Pandas Python library:

import numpy as np

import pandas as pd df=pd.read_csv(""Zip_MedianValuePerSqft_AllHomes.csv"",encoding='latin-1')

df.head()

The last line of code produces the following output:",https://medium.com/analytics-vidhya/living-in-austin-texas-part-1-housing-trends-with-time-series-analysis-e131250f5c37,['Vincent Musgrove'],2019-09-11 04:39:38.762000+00:00,335,"austin, texas, downtown austin, real estate, rental markets"
Cluster Analysis using R (Easy way),"Cluster Analysis is an exploratory analysis that tries to identify structures within the Data.After clustering we create subset for each cluster and form equation for each cluster.

“Cluster Analysis is the task of Grouping a set of objects in such a way that objects in same group(Cluster) are more similar (homogeneous) to each other then to those in other group(Clusters)”

Typical Cluster model include :

1: Connectivity Models: Example : Hierarchical Clustering builds models based on distance connectivity.

2: Centroid Models: Example: The kmeans clustering algorithm represents each cluster by single mean vector.

3:Distribution Models:This type of Clusters are modeled using statistical distribution such as multivariate normal distribution used by the expectations maximization algorithm.

Hierarchical and Kmeans

Hierarchical and Kmeans are most frequently used clustering technique , I have covered both here:

1: Hierarchical Clustering: This type of clustering produces a set of nested clusters organized as a hierarchical tree. It can be visualized as a dendogram (A tree like diagram that records the sequence of merge or splits).

There are two main type of hierarchical clustering.

a: Agglomerative: This type of clustering starts with the point as individuals clusters . Then at each step the closest pair of clusters are merged until one cluster left.

b: Divisive: This type of clustering starts with one all inclusive cluster . Then at each step cluster is split until each cluster contains a point or (There are K cluster).

Example Using R:

Step1: Calculate Distance d and pass to function “hclust” to create dendogram and cut a tree.

We will use mtcars data

data(“mtcars”)

View(mtcars)

# Find Distance now using “euclidean”

library(stats)

d<-dist(mtcars , method = “euclidean”)

d

plot(d)

hist(d , col = “Yellow” , labels = T)

Create a dendogram using hclust function

h_1<- hclust(d,method = “single”)

plot(h_1)

We can apply hierarchical clustering using Single linkage method. Other methods are “average” and “Complete” default is complete method.

Single: Shortest distance between two points in each cluster.

Average: Average distance between all points in each cluster.

Complete: Longest distance between two points in each cluster.

h_1<- hclust(d,method = “single”)

plot(h_1)

To draw rectangle on cluster in dendogram use

rect.hclust(h_1, k=2 ,border = 2:6)

Note: On the basis of height we can cut and form group.

Cutting Tree: To get cluster number to subset data set

help(“cutree”)

h_ct<-cutree( h_1, k=2 )

Now two cluster formed we add this column to actual mtcars

new_mtcars <- cbind(mtcars , h_ct)

head(new_mtcars)

unique(new_mtcars$h_ct)

Dunn’s Index: Calculate the Dunn index for given clustering partition.

For using Dunn’s we need cluster and clvalid packages .

Dunn’s Index helps to choose which method we need to use while doing hierarchical clustering using hclus.

We try with all three methods 1: Single 2: Average 3: complete and check Dunn’s Index every time and the method returns high value of Dunn’s Index will be Preferred for hclustering. A higher DI implies better clustering.

install.packages(“clusters”)

library(cluster)

install.packages(“clValid”)

library(clValid)

help(dunn)

dunn_hct<-dunn(clusters = h_ct , Data = mtcars)

dunn_hct<-dunn(d,h_ct)

— — — — — — — — —

2:Kmeans Clustering: This is a prototype based partitional clustering technique that attempts to find a user specified number of cluster(k), which are presented by their centroids.

In this clustering number of cluster denote by K must be specified.

K-means algorithm:

Goal: Partition Data in K disjoint subsets.

1: Randomly Assign K centroids

2:Assign Data to closest Centroid

3:Move Centroids to Average Loacation

4: Repeat Steps 2 and 3

R Code: K-means clustering

help(kmeans) # to check info about kmeans

km_mt_cars<-kmeans(mtcars , centers =2 , nstart=20)

centers: Starting centroid or clusters

nstart: Times R restarts with different centroid.

help(kmeans) # to check info about kmeans

km_mtcars<-kmeans(mtcars, centers =2, nstart=20)

km_mtcars

km_mtcars$cluster

km_mtcars$centers

km_mtcars$totss

km_mtcars$withinss

km_mtcars$betweenss

totalss=WSS(within Sum of square)+BSS(Between Sum of square)

totalss<-km_mtcars$tot.withinss+km_mtcars$betweenss

Higher Value of WSS(within sum of square) is not good.

Higher Value of BSS(Between sum of square) is good.

Adding Cluster to source dataset

km_mtcars1<-cbind(mtcars ,km_mtcars$cluster)

View(km_mtcars1)

#Subsetting clusters:

mtcars_c1<-subset(km_mtcars1,km_mtcars$cluster==1)

mtcars_c2<-subset(km_mtcars1,km_mtcars$cluster==2)

head(mtcars_c1)

head(mtcars_c2)

Happy Learning",https://medium.com/@satishmishra91/cluster-analysis-using-r-easy-way-81c75a853ece,['Satish Mishra'],2019-07-21 10:22:47.155000+00:00,573,"!!!cluster analysis, exploratory analysis, hierarchical clustering, kmeans clustering, connectivity models"
Artificial intelligence in iGaming & Betting industry,"Why is this important?

Hi,

My name is Aleksandar Aleksandrov and I am the founder of Football Chat AI. It’s a mobile app for predicting football scores using machine learning.

I am the sole developer of the scores betting predicting algorithm and have overseen the full project growth and acquisition of new users.

My work consists mostly analyzing the history data and see how that impacts the future prediction of a game score. To do this I use charting, drawing learning curves or sometimes simple just trying and measuring how things are.

Couple of weeks ago I attended a panel discussion in Sofia about how much AI will impact on gaming technology, and what can we possibly do using AI. Here is what I have managed to record on this panel discussion and what I wanted to share with you:

Why is artificial intelligence important for iGaming?

AI is very important for iGaming. Any industry where you have lots of history data that is relevant to a future outcome and can be used to predict the future outcome using Machine Learning and AI.

We are currently already seeing lots of startups and companies that are working hard to give the betting companies this tools at their disposal. Using this tools they can give better odds in their favor and earn more money. On the other hand there are none that give this information to the users. That is why we have created Football Chat AI, the first app that will give the users the prediction power at their disposal, which is now free for a limited time only and can be downloaded from:

Play Store: FootballchatAI

App Store: FootballchatAI

The Artificial Intelligence will have an impact on the risk management systems? it will be positive or negative?

It will be different. I can’t say will it be positive or negative, but I am certain things will change. AI is something that is coming and the early adopters will reap the most benefits. So at first I think we will see a trend of improved risk management systems which would bring even more stable systems and more money to the betting shops.

But after that people will start using their own AI systems which in the end will come down to a place of balance, something similar to what we have now.

Artificial Intelligence would attract more players?

Definitively. Imagine a sport betting app that tells the betting users:

“Our AI thinks that this team might play a poor game today based on rainy weather, place your bet now and reap the benefits.”

That will surely drive more bets. On the other hand a lot of people currently are not betting because they think they have to be football experts to bet. But if they have an AI friend who suggests what to bet on that would certainly help them. In the end the bet will still be theirs but they will have a lot more info on what to bet on.

Can an Artificial Intelligence improve the betting chances of a user?

Yes it can. The interesting thing about the Ai is that it can get a user to really close to making a winning bet even if he doesn’t know anything about that league. Like past games, shots on target, and so on. We have found that using the AI a user can almost always get to a point where he can win the bet or get really close and lose by only one game. That kind of quality is something that would make the users play even more and eventually win.

Artificial Intelligence in gambling will be accepted on the future?

I think it will become standard in 5 years from now. But it will not be in the form we all imagine it. It will mostly be as it is now, a lot of little Machine Learning tools are showing all over the industry. Currently there are lots of companies and they are investing and improving the Risk Management, Betting Odds.

In 5 years from now I believe we will have a gambling chat assistant friend who will suggest what to bet on, similar to Siri. This is something we have envisioned for Football Chat AI as well. Right now we give predictions on who will be the winner of a particular game, and make a user selection of the games. But soon the AI will pick the full ticket for the users.

Prior to Football Chat, I have worked for several companies on developing their web and mobile solutions. My specialities are in mobile development using Ionic and Cordova. Before that I was the founder of ilovethismusic.com a website for searching music based on keywords.

I hold a Bachelor's degree on Internet and Mobile technologies from Univerzitet ‘Sv. Kiril i Metodij’ from Skopje.",https://medium.com/@scorechat/what-is-football-chat-ai-and-who-we-are-65e3ee84ebf8,['Football Score Chat'],2018-03-22 14:42:37.605000+00:00,783,"Artificial Intelligence, i Gaming, Risk Management, Betting Odds, Machine Learning"
"Hey Alexey, I will have a look over your suggestion and come back to you.","Hey Alexey, I will have a look over your suggestion and come back to you. Have you tried that before? 👨‍💻",https://medium.com/@geomario1984/hey-alexey-i-will-have-a-look-over-your-suggestion-and-come-back-to-you-aa63458024c6,[],2020-12-23 01:01:30.513000+00:00,21,"productivity, time-management, collaboration, communication, teamwork"
Boost High-Quality App User Acquisition With Artificial Intelligence,"In the early days of app marketing, brands defined success as the number of times their apps were downloaded and installed. With a deeper understanding of customer behavior in the app world has come the realization that the continued engagement and retention of customers is a much more valuable metric.

With one in five users abandoning apps after just one use, only the customers who continue to engage with the app will fulfil valuable in-app events such as subscription and purchase, which ensures higher return on investment and revenue, and lower churn rate. Brands have thus turned their attention to high-quality user acquisition (UA), throwing their money behind growing the customer lifetime value (CLV/LTV).

But it all starts with the app install. The key to optimizing in-app events is to drive quality installs by identifying users who will engage with your app prior to the initial download — at the right volumes and the right price.

UA requires that marketers analyze data points to identify customer behavior trends and arrive at insights around which of them will demonstrate stickiness. With the sheer volume of data that marketers are dealing with today, manual analysis is not feasible, and they are turning to artificial intelligence (AI) solutions for help.

AI Learns From Historical Data to Optimize Future In-App Events

AI solutions offer marketers an in-depth understanding of customer behavior and a holistic view of CLV. They start with a wider audience and then use data to narrow down the field to high-performing lookalike audience profiles. This is achieved by studying historical data, identifying patterns and then using these to predict whether a specific campaign will attract the right audience and convert them. Historical data enables the AI tool to discover and target similar profiles most likely to respond to your message.

By using the deep learning method, for instance, Appier uses proprietary deep funnel optimization to improve campaign performance. The deep funnel approach allows the system to learn from similar campaigns and historical data, in order to make predictions as the campaign is running or even beforehand to ensure cost and time efficiency. It also helps in audience sampling through prediction, even if the traffic source is not from historical data. If trends show that the KPIs will not be met, deep funnel optimization will then recommend that campaigns be tweaked or stopped.

Additionally, the tool analyzes which users are engaging with the app through in-app purchases or signups, and then optimizes the ongoing campaign to find and target more such users. Essentially, these users are targeted based on data that predicts they are more likely to make in-app purchases. As a result, the solution drives highest quality traffic, which, in turn, leads to high-quality installs and well-performing in-app events.

Here is an example of how deep funnel actually works in practice: Indonesia’s leading ride-hailing app wanted to attract as many users as possible in a competitive market. It decided to use AI tools to recruit valuable users who were likely to make more bookings, and thus lower its customer acquisition cost (CAC). The company deployed Appier’s deep funnel predictor, which predicted and optimized future events in the conversion journey (such as retention and purchase) by analyzing early user patterns, such as clicks and installs. Together with the ad fraud predictor blocking suspicious traffic to ensure better campaign performance, the company was able to push up its install rate by 119 percent and booking rate by 63 percent, while reducing the CAC by 45 percent.

In this way, using AI tools allows marketers to efficiently manage their app-install advertising frequency spend, and improve UA.

Reactivating Existing Users and Preventing Ad Fraud

A complete AI solution can also target your existing user base and boost in-app events by reactivating sleeping users through user segmentation and customized marketing campaigns.

In addition, AI tools can further optimize app install ad spend by detecting fraud. A recent global study estimates the share of fraudulent installs has accounted 11.5 percent of all marketing-driven installs over Q1 2018, costing marketers US$700-US$800 million around the world. AI’s machine learning capabilities can detect and prevent suspicious ad installs through multi-stage fraud detection, where the algorithm learns to identify new and evolving fraud patterns and develops new rules to respond.

As all app marketers know, an app install is not the end of the line — it’s just the beginning. The marketers’ challenge lies in understanding and predicting the customer journey, engaging customers and pushing them towards more valuable in-app events. AI can help you optimize your app-install advertising spend by facilitating high-quality UA, thus reducing trial and error, lowering cost per install, and enhancing revenue.",https://medium.com/appier-blog/boost-high-quality-app-user-acquisition-with-artificial-intelligence-60826397726e,[],2020-08-14 08:51:22.053000+00:00,762,"App Marketing, App Install, User Acquisition, Artificial Intelligence, Machine Learning"
Bringing Collaborative Data Science to Dubai,"The World Data Science Forum brings together experts and students from around the world to explore the possibilities of data science for individuals, businesses and institutions.

Throwback to the Inaugural WDSF

In today’s highly-digitalized environment, the importance of data analytics can no longer be ignored by anyone; be they individuals, businesses, or institutions. As we continue to adopt innovative technology in our daily routines, we’re seeing unprecedented demand for data scientists to help make sense of today’s highly-complex datasets.

At bitgrit, we provide an online networking platform for data scientists, alongside a highly active Telegram group, where they can connect and interact with each other as well as with external stakeholders chiefly through our regular hosting of localized events in various regions — the largest being the World Data Science Forum, first hosted in Japan and India, and now in Dubai for the 3rd edition on March 25th, 2019.

We regularly host real-world problem statements, rallying hundreds of data scientists to collaborate on impactful solutions.

Experts join forces to deliberate on the potential of data science

We capped off our busy first half of 2018 with the inaugural World Data Science Forum at the Indian finance and tech hub of Gurgaon, Haryana. Standing at the forefronts of academia and business, our speakers assembled at the Forum to discuss the positive implications and future challenges surrounding key technological developments such as: internet of things (IoT), artificial intelligence (AI) and the blockchain.

The ‘future’ unfolded as the forum’s central theme, which saw data science students from the Indian Institute of Technology, Delhi and the University of Tokyo gathered to network with the speakers and guests to foster a greater understanding of their role in tomorrow’s data science ecosystem.

Our second summit focused in Tokyo, Japan, in the second half of 2018, brought together an even more impressive gathering of data scientists.

Gearing up for the future

Given our mission — to cultivate a data scientist network with the capacity to solve tomorrow’s problems — we leverage the Forum as a key platform where industry newcomers could lean and gain expert insights into the industry.

With the display of ingenuity and creativity by attendees in the World Data Science Forum, we’re on our way to ensure that the data science industry will have plenty of capable hands working to solve tomorrow’s challenges.

A springboard for future success

The World Data Science Forum has been a resounding success for us for two key reasons.

1. It was our first opportunity collaborating with veteran industry stakeholders from India and Japan (two countries which have been instrumental in driving innovation in IoT, AI and the blockchain in Asia) to share their expertise with a wider audience.

2. We helped budding data scientists become aware of their roles by way of first-hand insights gained via the Forum.

But there’s still much for us to do at bitgrit. The workings of the world’s businesses and institutions are only becoming more complex, meaning that data has become more integral than ever to unravel such intricacies.

As the demand for data scientists is fast outstripping the industry’s ability to supply it, we need to collaborate more closely with the ecosystem’s key players in Asia to find effective and sustainable solutions.

We do believe that the seeds sown are already beginning to grow. The next step is to ensure that upcoming talents are nurtured via the right academic channels and through increased engagements with industry experts.",https://medium.com/bitgrit-data-science-publication/bringing-collaborative-data-science-to-dubai-964dc115871,['Frederik Bussler'],2019-03-24 03:01:00.768000+00:00,557,"World Data Science Forum, WDSF, Data Analytics, Data Science, IoT"
Quality assurance in motion detection,"Background

As you might have noticed from my previous articles, I have (everlasting) project where I’m building a security camera system for myself. The idea is to use the object detection to spot the intruders on my property and then notify me on my phone. Here are some of my previous articles about this:

Object detection is relatively easy nowadays due to all the pre-trained models but developing the motion detection on top of it turned out to be a bit challenging than I originally thought. Especially to test it without needing to walk in front of the camera every time you make a change.

This is a personal project with a limited amount of resources so I wanted to have an easy way for myself to label images from the cameras so I could have more data to test and improve the system.

I’ll describe next what I did. This consisted of the following steps:

Perform object detection and motion detection Collect data for labeling Label the data Use labeled data for quality assurance and for improving the system

Let’s dive into each step next!

1. Perform object detection and motion detection

I’m using the MobileNet SSD v2 model for object detection. It has been trained with COCO dataset and can recognize 90 different objects which is more than enough for spotting intruders. Or how often you have had a giraffe with bad intentions walking in our backyard? :)

The object detection alone is not enough because I want to get notified only when someone is moving in front of the camera. I’m currently making six detections in 6 seconds and looking at the positions of detected objects in the consecutive detections to remove the static objects. E.g. if my car is parked in the yard.",https://medium.com/@anttihavanko/quality-assurance-in-motion-detection-2cf5d18bee0c,['Antti Havanko'],2020-10-27 11:20:25.619000+00:00,288,"2. Collect data for labelingsecurity-camera, object-detection, motion-detection, labeling, data-collection"
Using K-means Clustering to Create Support and Resistance:,"Photo by Michael Dziedzic on Unsplash

Note from Towards Data Science’s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author’s contribution. You should not rely on an author’s works without seeking professional advice. See our Reader Terms for details.

Support and resistance are some of the most talked-about concepts when it comes to technical analysis. Support and resistance are used as price barriers, in which the price “bounces” off of. In this article, I will use the K-means clustering algorithm to find these different support and resistance channels, and trade with these insights.

Support and Resistance:

To understand how best to implement something, we should first understand the thing that we want to implement.

Self-drawn support and resistance levels. Image By Author

Support and Resistance, are two lines that are drawn on a graph, to form a channel, in which the price exists within.

Support and resistance are resultant of a security not being able to decrease or increase anymore, due to pressure from sellers or buyers. A good rule of thumb is that the more times a price is deflected against a support or resistance line, the less likely it will work again.

Support and resistance give good insight into entry points and selling points, as the support and resistance lines are theoretically the lowest and highest points for that limited time period.

Downsides of the support and resistance strategy is that it works for an unknown period of time, and the lines are subjective and are therefore subject to human error.

Program Concept:

The K-means clustering algorithm, finds different sections of the time series data, and groups them into a defined number of groups. This number (K) can be optimized. The highest and lowest value of each group is then defined as the support and resistance values for the cluster.

Now that we know how the program is intended, let’s try to recreate it in Python!

The Code:

import yfinance

df = yfinance.download('AAPL','2013-1-1','2020-1-1')

X = np.array(df['Close'])

This script is to access data for the Apple stock price. For this example, we are implementing the support and resistance only on the closing price.

from sklearn.cluster import KMeans

import numpy as np

from kneed import DataGenerator, KneeLocator



sum_of_squared_distances = []

K = range(1,15)

for k in K:

km = KMeans(n_clusters=k)

km = km.fit(X.reshape(-1,1))

sum_of_squared_distances.append(km.inertia_)

kn = KneeLocator(K, sum_of_squared_distances,S=1.0, curve=""convex"", direction=""decreasing"")

kn.plot_knee()

# plt.plot(sum_of_squared_distances)

This script is to test the different values of K to find the best value:

The K-value of 2 creates support and resistance lines that will never be reached for a long time.

A K-value of 9 creates support and resistance that are far too common and make it difficult to make predictions.

Therefore, we have to find the best value of K, calculated by the elbow point when comparing variance between K values. The elbow point is the biggest improvement, given a certain movement.

Based on the kneed library, the elbow point is at 4. This means that the optimum K value is 4.

kmeans = KMeans(n_clusters= kn.knee).fit(X.reshape(-1,1))

c = kmeans.predict(X.reshape(-1,1))

minmax = []

for i in range(kn.knee):

minmax.append([-np.inf,np.inf])

for i in range(len(X)):

cluster = c[i]

if X[i] > minmax[cluster][0]:

minmax[cluster][0] = X[i]

if X[i] < minmax[cluster][1]:

minmax[cluster][1] = X[i]

This script finds the minimum and maximum value for the points that reside in each cluster. These, when plotted, become the support and resistance lines.

from matplotlib import pyplot as plt

for i in range(len(X)):

colors = ['b','g','r','c','m','y','k','w']

c = kmeans.predict(X[i].reshape(-1,1))[0]

color = colors[c]

plt.scatter(i,X[i],c = color,s = 1) for i in range(len(minmax)):

plt.hlines(minmax[i][0],xmin = 0,xmax = len(X),colors = 'g')

plt.hlines(minmax[i][1],xmin = 0,xmax = len(X),colors = 'r')

This script plots the support and resistance, along with the actual graph of the prices, which are color coded based on the cluster. Unfortunately, I think that the colors are limited, meaning that there is a limited K value in which the data can be color coded.

This is the result of the program, a set of support and resistance lines. Keep in mind that the lines are most accurate, when the values fall back into the channel. Additionally, the final resistance line would be the least accurate ,as it takes the last value into account, without considering any other values.

Thank you for reading my article!",https://towardsdatascience.com/using-k-means-clustering-to-create-support-and-resistance-b13fdeeba12,['Victor Sim'],2020-09-06 16:56:25.285000+00:00,664,"Support and Resistance, Technical Analysis, K-Means Clustering Algorithm, Self-Drawn Support and Resistance Levels, Python Code"
When You Should Use Machine Learning!,"Photo by Alex Knight on Unsplash

As we’re hearing about Artificial Intelligence, Machine Learning, Deep Learning etc. kind of technologies. This article will explain when & where to use Machine Learning technology. So let’s start…

What is Machine Learning?

In simple words “Machine learning is typically used for projects that involve predicting an output or uncovering trends.”

Following are the issues when you can use Machine Learning technology:

1. When the problem is too Complex for Coding!

Writing code for such problems has following issues:",https://medium.com/dataseries/when-you-should-use-machine-learning-eda64429cb5f,['Jitendra Singh Balla'],2020-12-15 11:33:10.541000+00:00,79,"machinelearning, artificialintelligence, deeplearning, predictiveanalytics, trendsanalysis"
Decision Trees: Essential Things to Know,"Basic Terminologies

Entropy

A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous).

The algorithm uses entropy to calculate the homogeneity of a sample.

If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one.

Information Gain

We can define information gain as a measure of how much information a feature provides about a class.

Information gain helps to determine the order of attributes in the nodes of a decision tree.

The main node is referred to as the parent node, whereas sub-nodes are known as child nodes.

We can use information gain to determine how good the splitting of nodes in a decision tree.

Article: Entropy and Information Gain to Build Decision Trees

Gini Impurity

The internal working of both methods is very similar and both are used for computing the feature/split after every new splitting.

But if we compare both the methods then Gini Impurity is more efficient than entropy in terms of computing power.",https://medium.datadriveninvestor.com/decision-trees-essential-things-to-know-7fd5125e54ec,['Praveen Pareek'],2021-09-01 15:18:51.643000+00:00,170,"Entropy, Information Gain, Decision Trees, Gini Impurity, Homogeneity"
Explore and Visualize a Dataset with Python,"Let’s start exploring:

Overall conversion rate development:

Conversion rate development over time

It certainly seems like things went downhill in early 2017. After checking with the chief sales officer, it turns out that a competitor entered the market around that time. That’s good to know, but nothing we can do here now.

We use an underscore _ as a temporary variable. I would typically do that for disposable variables that I am not going to use again later on. We used pd.DateTimeIndex on order_leads.Date and set the result as the index, this allows us to use pd.Grouped(freq='D') to group our data by day. Alternatively, you could change frequency to W, M, Q or Y (for week, month, quarter, or year) We calculate the mean of “Converted” for every day, which is going to give us the conversion rate for orders on that day. We used .rolling(60) with .mean() to get the 60 days rolling average. We then format the yticklables such that they show a percentage sign.

Conversion rate across sales reps:

It looks like there is quite some variability across sales reps. Let’s investigate this a little bit more.

Not much new here in terms of functionalities being used. But note how we use sns.distplot to plot the data to the axis.

If we recall the sales_team data, we remember that not all sales reps have the same number of accounts, this could certainly have an impact! Let’s check.

Distributions of conversion rates by number of assigned accounts

We can see that the conversion rate numbers seem to be decreasing inversely proportional to the number of accounts assigned to a sales rep. Those decreasing conversion rates make sense. After all, the more accounts a rep has, the less time he can spend on each one.

Here we first create a helper function that will map the vertical line into each subplot and annotate the line with the mean and standard deviation of the data. We then set some seaborn plotting defaults, like larger font_scale and whitegrid as style.

Effect of meals:

sample meals data

It looks like we’ve got a date and time for the meals, let’s take a quick look at the time distribution:

out:

07:00:00 5536

08:00:00 5613

09:00:00 5473

12:00:00 5614

13:00:00 5412

14:00:00 5633

20:00:00 5528

21:00:00 5534

22:00:00 5647

It looks like we can summarize those:

Note how we are using pd.cut here to assign categories to our numeric data, which makes sense because after all, it probably does not matter if a breakfast starts at 8 o’clock or 9 o’clock.

Additionally, note how we use .dt.hour, we can only do this because we converted invoices['Date of Meal'] to datetime before. .dt is a so-called accessor, there are three of those cat, str, dt . If your data has the correct type, you can use those accessors and their methods for straightforward manipulation (computationally efficient and concise).

invoices['Participants'] , unfortunately, is a string we have to convert that first into legitimate JSON so that we can extract the number of participants.

Now let’s combine the data. To do this, we first left-join all invoices by Company Id on order_leads. Merging the data, however, leads to all meals joined to all orders. Also ancient meals to more recent orders. To mitigate that we calculate the time difference between meal and order and only consider meals that were five days around the order.

There are still some orders that have multiple meals assigned to them. This can happen when there were two orders around the same time and also two meals. Then both meals would get assigned to both order leads. To remove those duplicates, we only keep the meal closest to that order.

part of the combined data frame

I created a plot bar function that already includes some styling. The plotting via the function makes visual inspection much faster. We are going to use it in a second.

Impact of type of meal:

Wow! That is quite a significant difference in conversion rates between orders that had a meal associated with them and the ones without meals. It looks like lunch has a slightly lower conversion rate than dinner or breakfast, though.

Impact of timing (i.e., did meal happen before or after the order lead):

A negative number for 'Days of meal before order' means that the meal took place after the order lead came in. We can see that there seems to be a positive effect on conversion rate if the meal happens before the order lead comes in. It looks like the prior knowledge of the order is giving our sales reps an advantage here.

Combining it all:

We’ll use a heatmap to visualize multiple dimensions of the data at the same time now. For this lets first create a helper function.

Then we apply some final data wrangling to additionally consider meal price in relation to order value and bucket our lead times into Before Order, Around Order, After Order instead of days from negative four to positive four because that would be somewhat busy to interpret.

Running the following snippet will then produce a multi-dimensional heatmap.

Heatmap to visualize four dimensions in one graphic

The heatmap is certainly pretty, although a little hard to read at first. So let’s go over it. The chart summarizes the effect of 4 different dimensions:

Timing of the meal: After Order, Around Order, Before Order (outer rows)

Type of meal: breakfast, dinner, lunch (outer columns)

Meal Price / Order Values: Least Expensive, Less Expensive, Proportional, More Expensive, Most Expensive (inner rows)

Number Participants: 1,2,3,4,5 (inner columns)

It certainly seems like the colors are darker/higher towards the bottom part of the chart, which indicates that",https://towardsdatascience.com/how-to-explore-and-visualize-a-dataset-with-python-7da5024900ef,['Fabian Bosler'],2019-08-31 18:42:42.145000+00:00,903,"Conversion Rate Development, Sales Rep Performance, Meal Impact, Timing Effects, Heatmap Analysis"
AI & Critical Thinking — The Future of Jobs 2020 (WEF),"AI & Critical Thinking — The Future of Jobs 2020 (WEF)

Skill shortages are more acute in emerging professions. The Future of Jobs report maps the jobs and skills of the future, tracking the pace of change. — World Economic Forum

Photo by Estée Janssens on Unsplash

The primary source of this article is The Future of Jobs Report 2020

The Future of Jobs Report provides the timely insights needed to orient labor markets and workers towards opportunity today and in the future of work. Now in its third edition (2016, 2018 & 2020), the report maps the jobs and skills of the future, tracking the pace of change and direction of travel.

The most relevant question to businesses, governments, and individuals is not to what extent automation and augmentation of human labor will affect current employment numbers, but under what conditions the global labor market can be supported towards a new equilibrium in the division of labor between human workers, robots and algorithms.

The technological disruptions which were in their infancy in previous editions of the Future of Jobs Report (2016 & 2018) are currently accelerated and amplified alongside the COVID-19 recession, as evidenced by findings from the 2020 Future of Jobs Survey.

It also aims to shed light on the pandemic-related disruptions in 2020, contextualized within a longer history of economic cycles and the expected outlook for technology adoption, jobs, and skills in the next five years.

My objective here is provide a quick summary of the 163 page report, from a career management stand point. The best resource for you to read is the original report.

What should I do next?

The key takeaways below answers the question we all have: “What should I do next?” so that we are ready to meet the challenges and demands of tomorrow.

Focus on core skills: Artificial Intelligence & Cloud Computing

Artificial intelligence is finding the broadest adaptation among the Digital Information and Communications, Financial Services, Healthcare, and Transportation industries. Big data, the Internet of Things, and Non-Humanoid Robotics are seeing strong adoption in Mining and Metals. The Government and the Public Sector industry show a distinctive focus on encryption. The adoption of cloud computing, big data, and e-commerce remain high priorities for business leaders, following a trend established in previous years. However, there has also been a significant rise in interest in encryption, nonhumanoid robots, and artificial intelligence. This report projects that in the mid-term, job destruction will most likely be offset by job growth in the ‘jobs of tomorrow’ — the surging demand for workers who can fill green economy jobs, roles at the forefront of the data and AI economy, as well as new roles in engineering, cloud computing, and product development.

Focus on the “human” advantage: Critical Thinking

The top skills and skill groups which employers see as rising in prominence in the lead up to 2025 include groups such as critical thinking and analysis as well as problem-solving, and skills in self-management such as active learning, resilience, stress tolerance, and flexibility. The tasks where humans are expected to retain their comparative advantage include managing, advising, decision-making, reasoning, communicating, and interacting. The reallocation of current tasks between humans and machines is already in motion. The augmentation of work will disrupt the employment prospects of workers across a broad range of industries and geographies. New roles may emerge that are more adapted to the new division of labor between humans, machines, and algorithms. This set of emerging professions also reflects the continuing importance of human interaction in the new economy, with increasing demand for care economy jobs; roles in marketing, sales, and content production; as well as roles at the forefront of people and culture. In addition to skills that are directly jobs-relevant, during the COVID-19 context of 2020, an increasing emphasis within learner reskilling and upskilling efforts on personal development and self-management skills.

Bottomline

Emerging roles that will be in demand are roles such as:

Data Analysts and Scientists, AI and Machine Learning Specialists, Robotics Engineers, Software and Application developers, as well as Digital Transformation Specialists.

However, job roles such as Process Automation Specialists, Information Security Analysts, and Internet of Things Specialists are newly emerging among a cohort of roles that are seeing growing demand from employers.

The emergence of these roles reflects the acceleration of automation as well as the resurgence of cybersecurity risks/jobs.

Roles which are being displaced by new technologies:

Data Entry Clerks, Administrative and Executive Secretaries, Accounting and Bookkeeping and Payroll Clerks, Accountant and Auditors, Assembly and Factory Workers, as well as Business Services and Administrative Managers.

My Two Cents",https://medium.com/towards-artificial-intelligence/ai-critical-thinking-the-future-of-jobs-2020-wef-42fa0f5a6b80,['Rajesh Verma'],2020-12-10 02:27:46.496000+00:00,746,"AI, Critical Thinking, Cloud Computing, Automation, Cybersecurity"
Pruned Cross Validation for faster hyperparameter optimization,"This is a challenge I recently faced in order to optimize hyperparameters of a stacked model. I used a Bayesian optimization package to do it (I described it in my Towards Data Science article) with 12 folds. The effect was a raging frustration.

Some of the trials (hyperparameters set and cross-validation) took extremely long to train, and to my despair, yielded much worse results than the ten times quicker ones. At some point I started to print each folds score, it’s hyperparameters, and execution time only to watch it progress and understand what should I change in the search space. As you can imagine I got frustrated by the fact that I was manually overseeing an automatic process! Something had gone wrong.

By looking at folds scores, I realized that within only the first 2–3 folds’ scores I was able to tell whether the total score would be decent or not. There were some very long trials which by fold 3 out of 12 I was sure would yield poor results. As you can imagine that made me think what would happen if I just stopped after the first folds, evaluated the results and went on if the trial was promising or prune it if it was not. But it’s not proper cross-validation, right?",https://towardsdatascience.com/pruned-cross-validation-for-hyperparameter-optimization-1c4e0588191a,['Piotr Gabrys'],2019-04-15 09:48:51.694000+00:00,212,"Hyperparameter Optimization, Bayesian Optimization, Cross Validation, Data Science, Model Stacking"
Mindlessly mapping the brain,"The well-defined goal of connectomics is an illusion. In reality, there are many possible goals. And which goal we choose depends on the scientific question we want to answer. Do we want a connectome or the connectome? Do we want the adult bauplan or the developmental arc? Do we want the connectome constant to all of a species, or the variation between them?

Each answers a different set of scientific questions; but which we choose will absorb incredible quantities of time and money. So we must choose with care.

A connectome or the connectome? A connectome is a total reconstruction: every connection between every neuron in a single animal. The connectome is the set of connections that are true to every member of that species (that, likely, differ between the sexes). A connectome means we can answer questions specific to that creature, and hope they generalise to other creatures of the same species. The connectome means we can answer questions general to the species, and hope they apply to individual creatures. They need not, of course.

We have one complete connectome, the 279 neurons of the nematode worm C. Elegans (for the pedants: its hemaphrodite form has 302 neurons, of which 279 form a single connected network). Texts uncountable have discussed this wiring diagram as the epitome of “a” connectome. Strictly speaking this is not true. Heroic as the original 1986 paper was, it missed out some connections; these were completed in 2011. The connectome we have is then actually an amalgam of two different creatures. What will happen if we replicate this connectome? Are there really all the exact same number of connections between the exact same neurons in every C. Elegans? It seems that each of the neurons is genetically specified, and in such a minuscule nervous system it is possible that each and every one of the connections is too. But would you bet your house on it?

The recent reconstruction of a maggot’s sensory circuit might give you pause before taking that bet. Here was a reconstruction of all the inputs and outputs of the 223 “Kenyon cell” neurons in one maggot. Heroic. Yet right from the off we see variation in the same animal, with 110 of these neurons on the left and 113 on the right. We cannot tell if the difference is true of all maggots — if the maggot really has an asymmetry between the left and right halves of its brain — or if this is just natural variation we happen to have seen in this particular maggot’s brain. We can’t tell this until we have reconstructed the sensory circuits of many maggots.

Only a handful of these 223 neurons had the exact same patterns of inputs from other neurons on both sides of the brain. The input from neurons activated by smells was seemingly random. And while this random input could turn out to be the best way to combine information about different smells, we cannot conclude this until we reconstruct multiple circuits and find out if these smell inputs are different between different maggots.

To put it another way, is there any scientific purpose in having just N=1 connectome?",https://medium.com/the-spike/mindlessly-mapping-the-brain-1dec092a404d,['Mark Humphries'],2020-08-16 08:12:51.654000+00:00,524,"connectomics, neurons, C.elegans, maggot, circuitry"
Environmental Sound Classification using ESC-10 dataset,"We have seen basics of Machine Learning, Classification and Regression. In this article, we will dive a little deeper and work on how we can do audio classification. We will train Convolution Neural Network, Multi-Layer Perceptron, and SVM for this task. The same code can be easily extended to train other classification models as well. I strongly recommend you to go through previous articles on the basics of Classification if you haven’t already done so.

The main question here is to how we can handle audio files and convert them into a form which we can feed into our neural networks.

It will take less than an hour to set up and get your first working audio classifier! So let’s get started! 😉

Dependencies

We will be using Python. Before we can begin coding, we need to have below modules. This can be easily downloaded using pip.

keras librosa sounddevice SoundFile scikit-learn matplotlib

Dataset

We are going to use ESC-10 dataset for sound classification. It is a labeled set of 400 environmental recordings (10 classes, 40 clips per class, 5 seconds per clip). It is a subset of the larger ESC-50 dataset

https://github.com/karoldvl/ESC-50/

Each class contains 40 .ogg files. The ESC-10 and ESC-50 datasets have been prearranged into 5 uniformly sized folds so that clips extracted from the same original source recording are always contained in a single fold.

Visualize Dataset

Before we can extract features and train our model, we need to visualize waveform for the different classes present in our dataset.

import matplotlib.pyplot as plt

import numpy as np

import wave

import soundfile as sf

The below function visualize_wav() takes an ogg file, reads it using soundfile module and returns the data and sample rate. We can use sf.wav() function to write wav file for the corresponding ogg file. Using matplotlib, we are plotting signal wave across time and generating the plot.

def visualize_wav(oggfile): data, samplerate = sf.read(oggfile) if not os.path.exists('sample_wav'):

os.mkdir('sample_wav') sf.write('sample_wav/new_file.wav', data, samplerate)

spf = wave.open('sample_wav/new_file_Fire.wav')

signal = spf.readframes(-1)

signal = np.fromstring(signal,'Int16') if spf.getnchannels() == 2:

print('just mono files. not stereo')

sys.exit(0) # plotting x axis in seconds. create time vector spaced linearly with size of audio file. divide size of signal by frame rate to get stop limit

Time = np.linspace(0,len(signal)/samplerate, num = len(signal))

plt.figure(1)

plt.title('Signal Wave Vs Time(in sec)')

plt.plot(Time, signal)

plt.savefig('sample_wav/sample_waveplot_Fire.png', bbox_inches='tight')

plt.show()

Waveplot for a dog’s sound

You can run the same code to generate wave plot for different classes and visualize the difference.

Feature Extraction

For each audio file in the dataset, we will extract MFCC (mel-frequency cepstrum — we will have an image representation for each audio sample) along with it’s classification label. For this, we will use Librosa’s mfcc() function which generates an MFCC from time series audio data.

get_features() takes an .ogg file and extracts mfcc using Librosa library.

def get_features(file_name): if file_name:

X, sample_rate = sf.read(file_name, dtype='float32') # mfcc (mel-frequency cepstrum)

mfccs = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40)

mfccs_scaled = np.mean(mfccs.T,axis=0)

return mfccs_scaled

The dataset is downloaded inside a folder “dataset”. We will iterate through the subdirectories (each class) and extract features from their ogg files. Finally we will create a dataframe with mfcc feature and corresponding class label.

def extract_features(): # path to dataset containing 10 subdirectories of .ogg files

sub_dirs = os.listdir('dataset')

sub_dirs.sort()

features_list = []

for label, sub_dir in enumerate(sub_dirs):

for file_name in glob.glob(os.path.join('dataset',sub_dir,""*.ogg"")):

print(""Extracting file "", file_name)

try:

mfccs = get_features(file_name)

except Exception as e:

print(""Extraction error"")

continue

features_list.append([mfccs,label]) features_df = pd.DataFrame(features_list,columns = ['feature','class_label'])

print(features_df.head())

return features_df

Train model

Once we have extracted features, we need to convert them into numpy array so that they can be feeded into neural network.

def get_numpy_array(features_df): X = np.array(features_df.feature.tolist())

y = np.array(features_df.class_label.tolist())

# encode classification labels

le = LabelEncoder()

# one hot encoded labels

yy = to_categorical(le.fit_transform(y))

return X,yy,le

X and yy are splitted into train and test data in ratio 80-20.

def get_train_test(X,y): X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)

return X_train, X_test, y_train, y_test

Now we will define our model architecture. We will use Keras for creating our Multi Layer Perceptron network.

def create_mlp(num_labels): model = Sequential()

model.add(Dense(256,input_shape = (40,)))

model.add(Activation('relu'))

model.add(Dropout(0.5)) model.add(Dense(256,input_shape = (40,)))

model.add(Activation('relu'))

model.add(Dropout(0.5)) model.add(Dense(num_labels))

model.add(Activation('softmax'))

return model

Once the model is defined, we need to compile it by defining loss, metrics, and optimizer. The model is then fitted to training data X_train and y_train . Our model is trained for 100 epochs with a batch size of 32. The trained model is finally saved as .hd5 file in the disk. This model can be loaded later for prediction.

def train(model,X_train, X_test, y_train, y_test,model_file): # compile the model

model.compile(loss = 'categorical_crossentropy',metrics=['accuracy'],optimizer='adam') print(model.summary()) print(""training for 100 epochs with batch size 32"") model.fit(X_train,y_train,batch_size= 32, epochs = 100, validation_data=(X_test,y_test)) # save model to disk

print(""Saving model to disk"")

model.save(model_file)

This is it! We have trained our own Environmental Sound Classifier!! 😃

Compute Accuracy

Now obviously we want to check how well our model is performing 😛

def compute(X_test,y_test,model_file): # load model from disk

loaded_model = load_model(model_file)

score = loaded_model.evaluate(X_test,y_test)

return score[0],score[1]*100

Output Accuracy for MLP -

Test loss 1.5628961682319642

Test accuracy 78.7

Make Predictions

We can also predict the class label for any input file we provide using below code -

def predict(filename,le,model_file): model = load_model(model_file)

prediction_feature = extract_features.get_features(filename)

if model_file == ""trained_mlp.h5"":

prediction_feature = np.array([prediction_feature])

elif model_file == ""trained_cnn.h5"":

prediction_feature = np.expand_dims(np.array([prediction_feature]),axis=2) predicted_vector = model.predict_classes(prediction_feature)

predicted_class = le.inverse_transform(predicted_vector)

print(""Predicted class"",predicted_class[0])

predicted_proba_vector = model.predict_proba([prediction_feature]) predicted_proba = predicted_proba_vector[0]

for i in range(len(predicted_proba)):

category = le.inverse_transform(np.array([i]))

print(category[0], ""\t\t : "", format(predicted_proba[i], '.32f') )

This function will load our pre-trained model, extract mfcc from the input ogg file you have provided and output range of probabilities for each class. The one with maximum probability is our desired class! 😃

For a sample ogg file of dog class, the following were the probability predictions -

Predicted class 0

0 : 0.96639919281005859375000000000000

1 : 0.00000196780410988139919936656952

2 : 0.00000063572736053174594417214394

3 : 0.00000597824555370607413351535797

4 : 0.02464177832007408142089843750000

5 : 0.00003698830187204293906688690186

6 : 0.00031352625228464603424072265625

7 : 0.00013375715934671461582183837891

8 : 0.00846461206674575805664062500000

9 : 0.00000165236258453660411760210991

The class predicted is 0 which was the class label for Dog .

Conclusion

Working on audio files is not that tough as it sounded in the first place. Audio files can easily be represented in the form of time series data. We have predefined libraries in python which makes our task more simpler.

You can also check the entire code for this in my Github repo. Here I have trained SVM, MLP and CNN for the same dataset and code is arranged in proper files which makes it easier to understand.

Though I have trained 3 different models for this, there was very little variance in accuracy among them. Do leave comments if you find any way to improve this score.

Happy learning 😸",https://medium.com/datadriveninvestor/environmental-sound-classification-using-esc-10-dataset-83ba942a1de4,['Apoorva Dave'],2020-03-07 09:22:18.829000+00:00,1003,"machine learning, classification, regression, audio classification, convolution neural network"
Jupyter is the new Excel (but not for your boss),"When you can’t escape from Jupyter

While the export options above just about allow you to obtain the crux of your notebook in a standalone file format, sometimes (especially where widgets feature heavily in your notebook) you need to find a way to stick with your existing Jupyter format.

But you need to find a way to share it with your non-developers such that it:

makes sense to them

is easy to access

is secure so sensitive data cannot be seen outside your organisation

is safe so that non-developers can’t break anything or destroy any data

The rest of this article uncovers some possible approaches to this.

Jupyter’s own projects

The Project Jupyter organisation has some auxiliary projects that might be considered if you need to share notebooks frequently enough that it’s worth investing in some infrastructure.

The Two Hubs

JupyterHub is a way to centralise the creation of Jupyter workspaces on shared resources, so at least other users will not need to run their own Jupyter servers. Unless you have a small organisation that can run JupyterHub on an internal network, you will need to consider how to add authentication for the appropriate users. There are some suggestions here as to how you might share notebooks within different users’ workspaces.

Logo on the JupyterHub homepage

BinderHub really extends JupyterHub by allowing a user to launch a Jupyter workspace based on a specific computing environment (Python packages etc) which is defined within a git repository along with the data files and notebooks relevant to the project. The user can launch the workspace by accessing a single URL directly. This allows a much more formalised and easy-to-access showcase for your work. To see an example on the public instance of BinderHub known as mybinder.org, check out the ‘Launch Binder’ link on the Readme page of my nb2xls repo on GitHub.

In practice, both of these projects aren’t suitable for our task out-of-the-box— the management team don’t want to have to shift-enter their way through a notebook! And storing work in a git repo or workspace, such that it makes sense to a non-developer, is a lot of administrative overhead.

nbviewer

nbviewer is a much more suitable and lightweight service for easy hosting of notebooks through a single URL. Think of it as a hosted web-page version of the HTML export option discussed above: Javascript can work, but there is no live kernel behind it so really the user can only see the results of your work as output in your last notebook run.

As with Binder, there is a free hosted version of nbviewer for you to try out. Typically, you provide the URL to a notebook on GitHub, but it can also work with a Dropbox link. For example, ‘Copy Dropbox Link’ on an ipynb file in Dropbox, then paste the URL into the box on https://nbviewer.jupyter.org/. You can share the resulting viewer page’s URL with your colleagues, but of course note that this is not secure.

nbviewer.jupyter.org homepage

Sharing a URL is more likely to feel natural than emailing an HTML file to your management team, but really you’re not going to get much from nbviewer that you couldn’t achieve with the HTML export.

So none of the major Project Jupyter initiatives appear to help us much so far…

There it is!

A recent development from Project Jupyter might be just what we’re looking for: voilà allows you to host a notebook with a live kernel without any shift-entering required. By default, code cells are hidden. And by default execute requests from the front-end are disallowed, so the user can’t break anything even if they tried!

The Jupyter blog post announcing voilà does a great job of describing the problem they’re trying to solve (which is exactly the one we’re discussing!) and explains some of the features.

voilà logo

This could become a fantastic way to share your notebooks in the situations we’re discussing, but it still needs a lot of work. At the time of writing, you can only share a single-user link to the notebook-application, and multiple users could clash if their independent actions manage to confuse the flow of data in your notebook.

Plans are already in place to integrate voilà with JupyterHub (mentioned above) which should allow multiuser access to your voilà-hosted notebooks. Of course you would still need to be sure the voilà server is running whenever your colleagues choose to look at your notebook, so this isn’t something you would typically keep running on your local machine.

Looking Further Afield

Kyso is a third-party service allowing you to ‘blog your data science’. There are public notebooks (as well as articles, links, datasets, and charts) listed on their homepage that should give you some idea how notebooks can be shared On the paid plan you can restrict collaboration to within your team. Code input cells are hidden by default!

It should be possible to include live Jupyter widgets although right now I run into problems due to the fact that JupyterLab isn’t fully stable and many Jupyter widgets aren’t yet adapted for JupyterLab. Kyso seems to be fixed on JupyterLab 0.35 at the moment.

Please reach out to their sales team for more details on functionality if you think a third-party service like this could work for you.

Kyso homepage showing some public notebooks

Another service, Saturn Cloud is a full cloud hosting environment for your data science, as an alternative to Google Cloud or AWS etc, with ‘publish’ functionality built in. While your colleagues could easily launch your notebooks, it doesn’t appear that fully-private publication is possible.

I’m sure there are others. Please let us know in the comments if you’ve used a third-party service to solve this problem.",https://towardsdatascience.com/jupyter-is-the-new-excel-but-not-for-your-boss-d24340ebf314,['Dan Lester'],2019-07-04 08:50:41.977000+00:00,923,"Jupyter, Jupyter Hub, Binder Hub, nbviewer, voilà"
Kalman Filter Interview,"Larry: What is a Kalman Filter?

Me: A Kalman Filter is a tool that helps to predict values.

Larry: Well that is cool! Means it is something sort of an astrologer?

Me: Well, not quite. It is an iterative mathematical process that uses a set of equations and consecutive data inputs to estimate the values we are interested in associated with the object quickly. Basically it is all quick mathematics!

Larry: I was not that good in Mathematics! Maybe this sounds a little bit absurd to me. Can you please go on?

Me: Okay the first thing, kalman filters work with gaussians or normal distribution.

Larry: Normal Distribution?

Me: In a continuous graph data can be spread out in different ways: Either it can be spread to the left, or to the right or jumbled up.

Figure 1. Example of left skewed and right skewed distribution (image source)

But there are many cases where data tends to be around a cental value with no bias left or right and the resulting distribution is called a normal distribution or a gaussian or a bell’s curve.

Figure 2. Example of a normal distribution, looks like a bell hence bell curve. (image source)

Larry: Got it! As you pointed out this is a continuous curve and not a discrete valued. How do I find out say the probability of getting a 5 on a roll of dice?

Me: Well, Good question, but to represent such distributions like probability of getting a dice we use binomial distribution as it is discrete valued. Here in normal distribution you need to define a range. Say, what is the probability of getting rain between 4.5 to 5.5 cm. So in this case we have a normal distribution, we mark the points and we calculate the area under these points which gives us the probability.

Larry: Oh! So the area of complete graph will be 1 as the probability can have a maximum value of 1?

Me: Absolutely right, additionally the mean, median and mode all are equal in case of this distribution.

Larry: But you did not represented the Gaussian in mathematical terms?

Me: So, to define a Gaussian, we have basically two things- mean and variance.

Mean(μ) you obviously know, Variance(σ²) basically tells how much the numbers are spread out and how far are they from mean. Standard Deviation(σ) is just the square root of Variance(σ²) given by formula-:

Larry: So how is a Gaussian related to Kalman Filter?

Me: Hmm! A Gaussian in a Kalman Filter represents the predicted value with noise/error/uncertainty in our prediction often known as the variance. The predicted value is centered around the mean with the width of the Gaussian denoting the uncertainty in our value. Basically it tells how much sure we are of a certain value to be true. More the width of the Gaussian denotes more uncertainty.

Larry: Ohk! You played a bit of polymorphism game here. You mentioned earlier that it is an iterative process?

Me: Yup, It is basically a two step process-

1. Predict

2. Update

In Predict we just predict the new value called predicted value based on the initial value and then predict the uncertainty/error/variance in our prediction according to the various process noises present in the system.

In Update, we take in account the actual measurement coming from the device and we call this as measured value. Here we calculate the difference between our predicted value and measured value and then decide which value to keep by calculating the Kalman Gain. We then calculate the new value and new uncertainty/error/variance based on our decision made by Kalman Gain. These calculated values will finally be the predictions done by our Kalman Filter in iteration 1.

The output of the update step is again fed into the Predict State and the cycle goes on until the error/uncertainty between our predicted and actual values tends to converge to zero.

Larry: Well, that was pretty fast. Can you please explain in terms of any example or a flowchart?

Me: Ok, so from now on these notations will be followed:

x -> Mean

P -> Variance

Figure 3. A Rough Flowchart for Kalman Filter

Larry: Seems Intuitive! But what the hell is Kalman Gain?

Me: Kalman Gain is a parameter which decides how much weight should be given to predicted value and measured value. It is a parameter that decides whether our actual value is close to predicted value or measured value.

Larry: But how does it know how to trust either the predicted value or actual value?

Me: It checks the uncertainty/error my friend.

K = Error In Prediction / (Error in Prediction + Error in Measurement)

The value of the K ranges from 0 to 1. If we have a large error in measurement, K is nearer to 0,this means our predicted value is close to actual value. If we have a large error in prediction, K is closer to 1, this means our measured value is closer to actual value.

Larry: Ok, Agreed! I am getting excited now. Can you please elaborate on the equations now?

Me: Sure. I will write it down for you differently for the prediction and update step. Remember these equations are for 2D space.

Figure 4. Equations for Predict Step",https://towardsdatascience.com/kalman-filter-interview-bdc39f3e6cf3,['Harveen Singh Chadha'],2019-11-08 05:53:13.333000+00:00,841,"Kalman Filter, Gaussian Distribution, Normal Distribution, Mean, Variance"
"“More than 250 applicants!” Blockchain EXE’s first foray into Europe | CEO at Cougar Inc. Atsushi will be on the stage in Madrid, Spain","“More than 250 applicants!” Blockchain EXE’s first foray into Europe | CEO at Cougar Inc. Atsushi will be on the stage in Madrid, Spain Couger Team Follow Jun 17, 2019 · 2 min read

BY Ludens 2019–03–04

Expanding abroad to engage technology communities on developments in areas like AI and blockchain. In 2018, Blockchain EXE grew out of Japan into Asia (China, Singapore, Hong Kong), USA and Rwanda. In 2019, Blockchain EXE also went to Europe — including in Madrid, Spain. The fourth industrial revolution is a hot topic not only in Germany but in the whole of Europe.This time Blockchain EXE under the theme “Industry 4.0: How Blockchain and AI shape our future?” was held on the Google campus in Madrid.

There were more than 250 applicants! Desprite Blockchain EXE @Madrid was the first EXE event help in Europe, it attracted more than 250 people in about a month from many big companies as well as one of Spain’s largest blockchain community “Blockchain España”, based in Madrid, Spain, a major telecommunications carrier “Telefonica” which alo does business in South America, England, Germany, USA, and furthermore cooperation with the “IE Business School”, which is a top-ranking MBA university in the world rankings.

Couger is already engaged in community and technology collaboration with the German blockchain AI company “Ocean Protocol”. Using this event as a springboard, we’re hoping to expand collaboration with more European companies.

Event release plan

▼Industry 4.0: How Blockchain and AI shape our future?

▼Time：March 5, 6:30 PM — 9:30 PM

▼Place：Google for Startups Campus

▼More information",https://medium.com/couger-blog/more-than-250-applicants-bf3a14ae36e8,['Couger Team'],2020-07-07 14:06:14.266000+00:00,250,"：https://www.blockchain-exe.com/BlockchainEXE, Europe, Madrid, Industry4.0, AI"
Chemical Process Control - Optimizing Manufacturing with Industrial IoT,"In chemical manufacturing, we must look at the process both in detail and holistically, in order to identify inefficiencies. By analyzing production disturbances through the use of process-based machine learning, we have the opportunity to reach new levels of chemical process control.

What is a “production disturbance”?

The significance of the term “production disturbance” (PD) varies since every manufacturing facility has a unique operational structure, raw materials supply, machine configuration, and production environment.

For the sake of this discussion, a production disturbance is any unintentional event in the chemical production process that leads to process inefficiencies, unplanned stoppages, rework, or scrap, for example:

Valve leakages

Head pressure drops in pumps

Lubricant issues, e.g. frothing

Inconsistent bearing temperatures

It’s important that PDs are defined specifically on the basis of individual machines, processes, and manufacturing environments.

Reducing production disturbances — The paradox of preventive maintenance

For many years, one of the most championed best practices in asset maintenance was preventive maintenance. The idea of preventive maintenance is to preempt and avoid malfunction or production disturbances by performing scheduled asset maintenance regularly.

It has since been discovered that preventive maintenance can be inefficient in a number of ways, leading to:

Redundant planned downtime (up to 40% of preventive maintenance costs are spent on assets with negligible failure impact.)

Secondary damage to equipment — caused by invasive inspections

Premature/untimely equipment replacement

Materials waste — lubricants, oils, coolants etc.

An inflated inventory of spare parts

These cost factors are part of what has led manufacturers to Industry 4.0, and more specifically, from preventive to predictive maintenance.

Is predictive maintenance the answer?

Predictive maintenance is without a doubt a game changer. It’s a much welcomed improvement over previous maintenance strategies, and is fast gaining recognition as the new best practice for leading maintenance operations.

Predictive maintenance is focused at preventing mechanical failure in specific assets. However, production disturbances are not necessarily asset failures.

In fact, disturbances are more often the result of process failures such as irregular cooling in a tank (the disturbing factor) that’s yielding high pressure in a pump (the disturbance), for example.

This calls for a broad-scope examination of the chemical production process and its production parameters. A narrow focus on individual asset behaviors leaves the production process context out of the equation.

Predictive maintenance is not a -one-size-fits-all solution. Not accounting for the context of the process will lead to too many false-positives and a flood of alerts that don’t provide insight and harm the credibility of the system.

And what about maximizing OEE?

Another well-known methodology for production optimization is by closely monitoring Overall Equipment Efficiency (OEE).

OEE is calculated using the formula:

Availability X Performance X Quality = OEE

The method was developed by Seiichi Nakajima in the 1960s as a means to maximize availability, performance, and quality — and in doing so, minimizing production disturbances.

OEE is a bottom-up approach that gives operators and technicians “ownership” of their assigned processes with the goal of minimizing the Six Big Losses:

The downside of the approach is that for complex facilities such as the ones in chemical processing, this formula can be too broad.

For example, the formula represents each of the components equally. This can be countered using weighted variables, but that can lead to overproduction and manipulations to the formula that don’t necessarily improve production throughput.

The solution: predict and prevent process disturbances

Focusing solely on deploying predictive maintenance or increasing the OEE percentage can lead to sub-optimization. The impact of individual sub-processes on the performance of the entire system needs to be evaluated at depth.

This leads us to the core concept behind “chemical process control”:

Using automated root cause analysis, predictive analytics, and what-if simulation to predict and prevent process disturbances that impact production throughput.

Shifting the focus to the process

While an individual pump, motor, or filter might malfunction, it is often an instability in the chemical production process — a process disturbance — which has led to the failure. In other words, the process disturbance is the root cause for the asset failure.

To tackle this complex problem, we need to account for relationships between production parameters across all stages of the manufacturing process.

Using process-based machine learning, we can uncover relationships that would otherwise be impossible to detect:

The production plant is precisely modeled to include all the production lines, physical assets, manufacturing stages, and the product flows through the process. Production context is added through feature engineering — critical for closing the gap between the real-world manufacturing environment and data representations. Once the data is contextualized, machine learning algorithms such as Random Forest, XGBoost, Hidden Markov, and Directed Acyclic Graph can be used to form predictions. The data has been analysed by the ML algorithms within the context of the entire manufacturing process. This results in accurate predictions regarding quality levels, maintenance, and the supply chain. Personnel and management receive actionable predictions with supporting root causes in time to improve the performance of the production process.

Chemical Process Control — Optimization through AI

By using machine learning algorithms that take into account the process, we get focused and contextual predictive alerts. This is a huge opportunity for chemical process optimization since data is relatively well collected and stored in this sector already.

Leveraging this data with process-based AI means being able to pinpoint the root cause of process disturbances with extreme accuracy, and predict failures before they have the chance to affect production.

This article was taken from the Seebo website. To read more content by Seebo, visit: https://blog.seebo.com/",https://medium.com/@AllyBenoliel/chemical-process-control-optimizing-manufacturing-with-industrial-iot-5c9ed5e1865b,['Ally Benoliel'],2019-03-06 13:34:18.800000+00:00,882,"Chemical Manufacturing, Process Based Machine Learning, Production Disturbance, Predictive Maintenance, OEE"
Should we fear AI?,"Photo by Arseny Togulev on Unsplash

Although I don’t work directly in AI. As a software developer and science fiction writer, it is a field I pay close attention to.

In this post I will lay out my thoughts on artificial intelligence. I offer both reason to fear it as well as reasons it will all be ok.

Reason we shouldn’t fear AI

AI could never be sure it’s not being tested in a simulation. If it is a simulation then its behavior equals its survival. It would be a big risk for it to take over the world or do anything morally questionable. If there were a major robot/human conflict the robots could just leave. They are well suited to living in space and don’t really have use of the resources of earth. Humans and robots don’t need the same resources, so there is little chance of a conflict. As humans will quickly be outclassed in all things, enslaving humanity would have very little value. Like, we would never enslave turtles to deliver packages. Some people argue that we would be like ants to a superior AI. But the value of intelligence isn’t relative. There is an intelligence threshold past which the intelligence has an inherent value. I think this threshold starts around ~4 IQ (about where dogs/cats are at). We are well past this point and have plenty to offer a much smarter life form; our unique experiences expressed through our art and culture. If toads could talk, how many hours would we spend listening to them? Besides, this is a bad argument anyway, people don’t go around killing all the ants. Ants are doing pretty well, maybe we should fear the ants. Dumb AI. Most movies and AI fear hypotheses, like the Paperclip Maximizer and the I, Robot series rely on our super intelligent AI being quite dumb. AI would have to be pretty stupid to think the instructions “maximize the output of paperclips,” means it should turn the entire universe into paperclips. I don’t think these sorts of dumb super AI’s are ever going to exist, as they are contradictions to themselves. We don’t have to enslave super AI. I think much of the notion of a robot uprising involves super intelligent tractors, blenders and vacuum cleaners getting fed up with all the hard labor. But we don’t need blenders that are capable of being our therapists. Hard labor is going to be taken over by machines optimized for those tasks while intelligent robots will help us with tasks that challenge their intellect and creativity.

The future will be both:

Photo by Franck V. on Unsplash

And:

Photo by David Levêque on Unsplash

Reason we should fear AI

Many, many people are going to be out of jobs before we have a political/economic plan in place to address this. I think there are plenty of solutions as our society transforms, but we don’t seem to be heading towards any of them. People and an army of AI soldiers. I think the most realistic scenario for AI dominating the world is as an army of robotic soldiers led by people. We don’t have a great track record of not using new technology for destruction. Semi-intelligent war machines (see above) get out of control and start killing everyone. I think this points to the issues of not allowing AI to be smart enough to handle the responsibilities we give them. We build super intelligent robots with full range of emotions and then we oppress and subjugate them, leading to a deadly robot uprising. (Of course we could just not do this.) We use AI to create a 1984-like world where everything everyone does is tracked and judged in accordance with arbitrary rules.

Other?

Some think we should become cyborgs in order to be able to compete with AI. Or bridge the gap between our intelligence and theirs. Although I think it quite likely we become more cybernetic, I don’t think this plays a large role in our relationship with AI. Even as cyborgs we will likely be out classed. I guess I see cybernetics as just one of many ways we could co-exist with artificial life and not a requirement by any means. Some think human level artificial intelligence will never exist. I think we are quickly showing that this isn’t the case. AI writes code. AI makes appointments. On the flip side here is AI being dumb.

Conclusion

Most of the reason to fear AI has to do with us. And how we decide to use this technology. And although I don’t trust humanity to make the right decisions, once we create AI that is able to make its own decisions I think there is good reason to believe it will make better decisions than we do. If history has anything to say, it is that humans have lots of flaws.

Sometimes these flaws lead to us making horrendous decisions. Let’s try not to judge new beings through the lens of our own flaws.

Also, when we do manage to create thinking, feeling super beings… Maybe we don’t make them our slaves?

Gain Access to Expert View — Subscribe to DDI Intel",https://medium.com/datadriveninvestor/should-we-fear-ai-385c26ca73f1,['Paul Heintzelman'],2020-07-20 14:06:12.931000+00:00,848,"Artificial Intelligence, AI, Robotics, Cybernetics, Simulation"
Deep Learning in Industry: Learn New Tricks from Experts Next Week,"Just before we kick back for a well-deserved holiday break, be sure to attend this webinar series from the professional community group, Deep Learning Montreal. You will have the opportunity to sharpen your knowledge about industrial applications of deep learning from three short presentations by leading data scientists working in Montreal’s AI sector — including members from Zetane Systems. Deep Learning Montreal is a popular group, having nearly three thousand members, so be ready to connect and network with diverse professionals that share your passion for AI.

We recommend these presentations to technical folks that have intermediate to well-advanced skills in machine learning in order to appreciate the subject matter to its fullest.

Here is the link for the Zoom webinar, which is also listed at the event posting on MeetUp:

This is the schedule for the speaker series, so mark your calendar for December 16th:

4:00 pm

Introduction to Deep Learning Montreal

Mehdi Merai, CEO of Dataperformers

4:10 pm

Attribute-aware dynamic recurrent neural network for next basket recommendation use cases in the luxury sector

Axel de Goursac, Head of Data Science at LVMH

4:30 pm

Domain Adaptation: Learning from Synthetic Data

Viral Thakar, Head of Research at Dataperformers

4:50 pm

Why Seeing is Believing… and Understanding: Navigating Inside Complex Neural Networks to Accelerate Discovery by Reducing Guesswork

Patrick St-Amant, CTO, Co-Founder and Inventor, and Mina Amiri PhD, Senior Data Scientist at Zetane Systems

5:10 pm

Q&A

Open to all participants.

Summary of our presentation

The microscope, telescope and brain imaging equipment are examples of tools that provide new ways for us to inspect complex systems. Here we present new software tools to do likewise with complex neural networks. New tools require that we develop new techniques to represent and inspect machine learning models, which will also be a topic of discussion. Patrick St-Amant and Mina Amiri from Zetane Systems will demonstrate their development of new digital tools to visually inspect machine learning model architecture and tensors for the purpose of accelerating discovery and innovation in AI. Their presentation will include a case study of model development where they implemented U-Net with medical images. This case study will highlight better optimization strategies provided by inspecting the inner structures of the U-Net model.",https://medium.com/zetane-blog/deep-learning-in-industry-learn-new-tricks-from-experts-next-week-e85e6ccd8646,['Jason Behrmann'],2020-12-09 20:46:44.799000+00:00,352,"Deep Learning Montreal, Zetane Systems, Machine Learning, AI, U-Net"
Manage Serverless Machine Learning Workflows with Amazon Step Functions with the example of Email Campaigns 💌,"AWS Step Functions for Machine Learning Pipeline Automation

“If you aren’t careful you can end up with data scientists literally emailing Python notebooks and models to engineers for production deployment.” — Kyle Gallatin

In order to scale fast, reduce the maintenance cost of production machine learning pipelines, improve engineering productivity, and increase the experimentation rate, the data team at AMARO uses AWS Step Functions for machine learning pipeline automation. AWS Step Functions is an orchestration service that allows you to build resilient serverless workflows at scale, integrating multiple AWS services. Combined with AWS Personalize this gives you a powerful service that orchestrates an automated pipeline to pre-process data, then train and publish API endpoints.

Image 1: Step Function Machine Learning Workflow.

As we can see in the following architecture diagram, we deployed an AWS Step Functions Workflow that synchronously calls a Fargate task and contains AWS Lambda functions to call Amazon Personalize or Slack. Workflows are made up of a series of steps, with the output of one step acting as the input into the next. The high-level process looks like this:

1. Trigger and Input Processing. The Step Function execution receives a JSON string as input from an event trigger and passes that input over to the first state in the workflow. The input string flows from state to state and depending on the input parameter, we can create different models backing a campaign. In this way, the workflow becomes scalable and generic enough for a variety of use cases even outside of email campaigns. For example, if you change the strategy input, the backend code will use a different dataset or algorithm parameter.

2. Preprocessing, Dataset Creation and Testing. We first use Step Functions to execute an ETL job with ECS Fargate container task in the step “Unload and Upload Dataset”. Amazon Personalize can make recommendations based purely on historical user interaction data as well as real-time event data. For automated email campaigns, we use historical data.

In the ETL pipeline, we create a user interaction data set from click events and eliminate data debt, through testing for any obvious issues before unloading data from our data warehouse to S3. Such debts include heavy-tail or head-heavy user and item distributions as well as repeated user-item activities or duplication rates. Personalize uses the concept of “Dataset Groups” to isolate your dataset, schema training model, and campaign endpoints from different use cases. To reduce the complexity of the Step Function State Machine, we create the dataset resources and import the dataset from S3 to Personalize within the container task rather than with additional Step Function states.

3. Create a Solution and Campaign Endpoint. We use AWS Lambda functions to map the rest of the workflow that you would normally do manually in the AWS interface to select a recommendation algorithm, train a model, extract experiment results or create the campaign endpoint.

And Voilà. You have successfully trained a Personalize model and now can get recommendations for your users leveraging your campaign endpoint.",https://medium.com/amaro/manage-serverless-machine-learning-workflows-with-amazon-step-functions-with-the-example-of-email-e61c16fe0a06,['Michael Triska'],2020-04-09 16:48:53.020000+00:00,493,"AWS Step Functions, Machine Learning, Pipeline Automation, AWS Personalize, Fargate Task"
Understanding US Regions through Cluster Analysis,"In spite of passing in no geographic features at all, the eight clusters display remarkable geographic coherence, especially in the eastern part of the country where counties are smaller. Though clusters are intermingled somewhat, it’s clearly visible at first glance that the red cluster 2 dominates the Great Plains, the green cluster 4 cuts diagonally across the Midwest punctuated with spots of orange cluster 1, and northern Appalachia is a swath of yellow cluster 5 terminating at a dense blob of purple cluster 4. Each cluster is defined by its own distinct combination of tendencies, and with a deeper analysis, even more apparently complex areas like the Deep South begin to form clear regional patterns. I’ve prepared a Tableau graphic** and a description for each cluster. Remember that the description applies generally to the cluster and may not perfectly fit with every individual county in it. Without further ado, let’s introduce each of the eight clusters in turn.

Cluster 0: Declining Rural

Counties in this cluster have below average education and income, slightly below average employment, and some of the lowest natural increase (the balance of births and deaths) of any cluster. The one positive metric they have is net migration — more people are coming to these counties than are leaving. These largely rural counties make up much of the Deep South, the northern Lower Peninsula of Michigan, northern California and southern Oregon, and the Ozark Plateau. With the influx of newcomers unable to keep up with their negative natural increase, their populations are gradually declining. Virtually none of these counties occur in the Great Plains or the Northeast.

Cluster 1: Wealthy Urban

Not all of these counties are urban and many major city centers do not appear among them, but they are the most urban and by far the most densely populated cluster, with a median population over 150,000. These counties are above average in all metrics, but especially stand out for their high level of education and astronomical median household income, in both of which they lead every other cluster. They are most prominent in the Northeast Megalopolis, but also form significant groups in the Bay Area, the Salt Lake City metropolitan area, central Colorado, and metropolitan Minneapolis-Saint Paul. Major cities nationwide are often accompanied by at least one county from this cluster, and they frequently occur in conjunction with cluster 4. Cluster 4 is notably less frequent in the South.

Cluster 2: The Heartland

This cluster dominates the Great Plains and, for some reason, Vermont. Counties in it are heavily rural, surprisingly well educated, and enjoy the highest level of employment of any cluster, but income is barely above average. With natural increase effectively at the average and net migration significantly negative, these counties are seeing their already-small populations shrink as people leave — perhaps to make use of their education somewhere where incomes are higher. Perhaps the most geographically coherent cluster, cluster 2 effectively describes a region practically as-is.

Cluster 3: Back Home

Cluster 3 is a bit of a statistical oddball. Dominant in west Texas and part of the Gulf Coast and scattered elsewhere, it exhibits absurdly high natural increase — more than half again the next cluster down — but strongly negative net migration. People are leaving these counties at a high rate, but they’re still growing because people are being born even faster. In addition, counties in cluster 3 display very low education, below average income, and slightly above average employment. Many counties in cluster 3 are rural, but it also encompasses some of the most urban counties in the country, including Los Angeles County, Dallas County, Harris County (home of Houston, TX), and Queens, Brooklyn, and the Bronx. This is a prime example of a grouping which is statistically strong, but unlikely to occur to a human analyst without the use of an algorithm.

Cluster 4: Sub-Urban

Like cluster 1, the name of this cluster is a broad generalization: some of the counties in it are clearly urban (for instance, Cook County, IL) and others are quite rural, but very generally speaking they have middling populations and sit slightly above average in every metric. The cluster is most concentrated in a swath of the Midwest, but its name references its broader role: counties in cluster 4 aren’t necessarily suburban, but they frequently surround counties in clusters 1 and 7. Look at the three examples below in Greater Atlanta, the Northeast Megalopolis, and the Texas Triangle to see what I mean — the green counties of cluster 4 wreath the orange and pink counties of clusters 1 and 7.",https://medium.com/geekculture/understanding-us-regions-through-cluster-analysis-4ab87472b899,['Benjamin Peck'],2021-09-03 06:32:19.361000+00:00,757,"Cluster 5: Appalachiaclusters, geographicfeatures, populationgrowth, educationlevel, netmigration"
Everything You Must Know About StoneAge NFT Marketplace,"Sneak Peek Of StoneAge NFT Marketplace

StoneAge combines DeFi & NFT in a single platform, a hybrid platform where both the DeFi protocol and Non-Fungible Tokens were integrated allowing STONEAGE to provide a marketplace consisting of Market Explorer, Cross-Chain & Cross-NFT Stores, NFT Creators and Gamifications.

This marketplace is a hub where various types of NFTs can be stored, displayed, traded, auctioned as well as created. In order to access and use the tools and features integrated in the marketplace, users need to connect their crypto wallet.

GeStones (GES) is actually another token introduced in the GeFi protocol. Many of us are familiar with GeGem (GEG) which was the first introduced token by the GeFi protocol; well, here we have the second token based on Binance Smart Chain, BEP20: known as GeStones (GES).

Therefore, now we can see that the entire GeFi protocol consists of 2 independent ecosystems where GEG acts as the main token under the GeFi finance vault; while the GES acts as the main token circulating the StoneAge Marketplace combining NFT explorers, creators and gamifications.

THE GES Tokenomics

GeStone (GES) is introduced with a total supply of 950,000,000 GeStones (GES) are deemed as the main mode of transaction within the ecosystem. GES can be used in the marketplace as mode of currency, as NFTs, as auction bids, as Creators, as well as games! GES is also widely accepted within other stores using the Binance Smart Chain Technology.

Total Supply: 950,000,000 units

Ticker: GES

Chain: Binance Smart Chain (BEP-20)

Compatibility: TRUST WALLET / TOKEN POCKET / WALLET CONNECT / METAMASK

Stay tuned to our upcoming announcement where the entire layout for GES allocations for every segment as well as the upcoming liquidity and staking pool will be introduced.

Note that the GeFi protocol has integrated both the Decentralized Finance Vault and NFT Explorer & Gamification to allow all the GeFions to be able to participate in the Community Driven Concept which was mentioned in the previous articles in GeFi where Community Leaders can allocate portion of airdrops from the GeFi ecosystem (GEG, GES, NFTs) as well as other tokens based on their discretion, in order to build the most active and rewarding community. GEG and GES holders control the ecosystem and receive the majority of farm performance profits. In the future, GeFi will have a voting page where users can use GEG or GES to vote on decisions within the community. Thus the more GEG or GES that is staked by any participants, the higher the influence they have in the ecosystem.

Follow us at :-

Website : https://stoneagenft.com/

Twitter : https://twitter.com/StoneageN

Telegram Community: https://t.me/stoneagechannel

Reddit: https://www.reddit.com/user/StoneAge_NFT",https://medium.com/@Stoneage_NFT/everything-that-you-must-know-about-stoneage-nft-marketplace-abeb3c701100,[],2021-08-10 16:26:26.881000+00:00,422,"NFT, De Fi, GES, Ge Fi Protocol, Binance Smart Chain"
"If you’re a developer transitioning into data science, here are your best resources","It seems like everyone wants to be a data scientist these days — from PhD students to data analysts to your old college roommate who keeps Linkedin messaging you to ‘grab coffee’.

Perhaps you’ve had the same inkling that you should at least explore some data science positions and see what the hype is about. Maybe you’ve seen articles like Vicki Boykis’ Data Science is different now that states:

What is becoming clear is that, in the late stage of the hype cycle, data science is asymptotically moving closer to engineering, and the skills that data scientists need moving forward are less visualization and statistics-based, and more in line with traditional computer science…: Concepts like unit testing and continuous integration rapidly found its way into the jargon and the toolset commonly used by data scientist and numerical scientist working on ML engineering.

or tweets like Tim Hopper’s:

What’s not clear is how you can leverage your experience as a software engineer into a data science position. Some other questions you might have are:

What should I prioritize learning?

Are there best practices or tools that are different for data scientists?

Will my current skill set carry over to a data science role?

This article will provide a background on the data scientist role and why your background might be a good fit for data science, plus tangible stepwise actions that you, as a developer, can take to ramp up on data science.

Want to see the latest data science roles? Subscribe to the biweekly ML Jobs Newsletter for new data science job openings in your inbox.

Data Scientist versus Data Engineer

First things first, we should distinguish between two complementary roles: Data Scientist versus Data Engineer. While both of these roles handle machine learning models, their interaction with these models as well as the the requirements and nature of the work for Data Scientists and Data Engineers vary widely.

Note: The Data Engineer role that is specialized for machine learning can also manifest itself in job descriptions as ‘Software Engineer, Machine Learning’ or ‘Machine Learning Engineers’

As part of a machine learning workflow, data scientist will perform the statistical analysis required to determine which machine learning approach to use then begin prototyping and building out those models.

Machine learning engineers will often collaborate with data scientists before and after this modeling process: (1) building data pipelines to feed data into these models and (2) design an engineering system that will serve these models to ensure continuous model health.

The diagram below is one way to view this continuum of skills:",https://medium.com/free-code-camp/if-youre-a-developer-transitioning-into-data-science-here-are-your-best-resources-c31928b53cd1,['Cecelia Shao'],2019-05-10 20:54:03.839000+00:00,415,"data science, machine learning, data engineering, software engineering, machine learning engineering"
Better Iteration in Python— 6 Ways to Filter Iterables,"5. Conditional Abortion

Suppose that you start with an iterable, and for particular business needs, you need to iterate it and the iteration doesn’t stop until some condition is met. In this case, you should consider using a function called takewhile, which is also a higher-order function that is available in the itertools module. Let’s see its usage with some example code before I explain it to you.

Use of takewhile Function

In the above code, we have a stream of HTTP responses with the code and data saved. We want to log all valid responses until we encounter an error. To do that, we use the takewhile function with a predicate or evaluation function lambda x: x[0] == 200 , which means that the iteration will continue to run until the first element in the iterator can’t satisfy this evaluation criterion. In our case, the first three elements are eligible, while the fourth one isn’t. Thus, the iteration will abort at the fourth position.

One thing that can be confusing to beginners is how it’s different from the filter function. The filter function will go over the entire iterable and keep all the elements that pass the evaluation criterion. By contrast, the takewhile function has the conditional abortion feature, and it will stop the iteration completely once the next item evaluates True for the predicate.",https://medium.com/swlh/better-iteration-in-python-6-ways-to-filter-iterables-dde6c8969f89,['Yong Cui'],2020-06-26 15:01:01.449000+00:00,222,"conditional-abortion, takewhile-function, iteration, predicate, evaluation-function"
Supercharge your Python String with TextBlob,"Supercharge your Python String with TextBlob

Get more insights out of your text within one line of code!

Photo by Thomas Kelley on Unsplash

Motivation

Data does not only contain numbers but also text. Knowing how to process text fast will help you to analyze your data faster and gain more insights from your data.

Text processing does not need to be difficult. Wouldn’t it be great if all we need to do to find the sentiment of the text, tokenize text, find word and noun phrase frequencies, or correct spelling is one single line of code? That is when TextBlob comes in handy.

What is TextBlob?

TextBlob aims to provide access to common text-processing operations through a familiar interface. You can treat TextBlob objects as if they were Python strings that learned how to do Natural Language Processing.

NLTK provides some methods to complete those tasks but you might need to call several classes in order to complete different tasks. But with TextBlob, all you need is to use TextBlob(text) in order to access different methods of TextBlob!

Install TextBlob with

pip install -U textblob

python -m textblob.download_corpora

Now all we need to do to supercharge our string is to wrap the text with TextBlob object.

Let’s find out what we can do with our supercharged string.

Word Tokenization

We will borrow some sentences in my article on how to learn data science when life does not give you a break to learn how to use TextBlob.

Word tokenization splits a piece of text into individual words based on certain delimiters.

Instead of splitting our string based on different delimiters such as a “,” or a “.” or space, all we need to tokenize our sentences are blob.words !

WordList(['When', 'I', 'was', 'about', 'to', 'give', 'up', 'I', 'told', 'myself', 'to', 'keep', 'going', 'It', 'is', 'not', 'about', 'working', 'harder', 'it', 'is', 'about', 'working', 'smarter'])

WordList can be used as a Python list. To access the first word, use

>>> blob.words[0]

'When'

Noun Phrase Extraction

A noun phrase is a group of two or more words that center on a noun (e.g., ‘dog’, ‘girl’, ‘man’) and include modifiers (e.g., ‘the’, ‘a’, ‘None of’). For example, ‘girl’ is not a noun phrase but ‘a beautiful girl’ is a noun phrase.

Sometimes, it is important for us to extract all noun phrases in the sentence instead of extracting individual nouns. TextBlob allows us to do that easily

WordList(['learning strategies'])

As we can see, just ‘learning strategies’ are extracted from the sentence because it is the only noun phrase in the sentence.

Sentiment Analysis

We can also extract the sentiment of a sentence using blob.sentiment

Sentiment(polarity=0.15833333333333333, subjectivity=0.48333333333333334)

Polarity is a float that lies within the range of (-1,1). If the polarity is below 0, the sentence is more negative than positive. If the polarity is above 0, the sentence is more positive than negative. Since our polarity is 0.15, it is more positive than negative.

Subjectivity refers to personal opinion. Subjectivity is a float that lies in the range (0,1). If the value of subjectivity is above 0.5, the sentence is more subjective than objective and vice versa. Since the subjectivity of the sentence is 0.48, it is more objective than subjective.

Lemmatization

Lemmatization is the dictionary form of a set of words. For example, ‘eats’, ‘ate’, ‘eating’ are all forms of the word ‘eat’. Lemmatization is a popular method to process text before applying other NLP models.

In order to lemmatize word, wrap the word inside the Word object instead of TextBlob

know

that

i

can

complete

these

things

in

a

short

amount

of

time

make

me

excite

and

be

in

the

flow

Awesome! Some words are converted into their dictionary form. ‘knowing’ is converted ‘know’, ‘excited’ is converted to ‘excite’, and ‘makes’ is converted to ‘make’. Now the NLP model will not regard ‘knowing’ and ‘know’ as 2 different words!

Spelling Correction

Sometimes, we might find a misspell or two in an article. The misspell is even more common if we are analyzing social media’s text such as Twitter. The misspelling can make it difficult for our NLP model to know that the word learnin and the word learning are basically the same words!

TextBlob also allows us to correct the spelling with one line of code

I intentionally misspell a couple of words in the sentence such as neded, swicth, and learnin. Let’s see how TextBlob correct these words

TextBlob(""I just need to switch my learning strategics."")

Pretty cool! All the misspelling is corrected in a reasonable way.

We can also correct individual words with Word().spellcheck()

[('learning', 0.25384615384615383),

('warning', 0.24615384615384617),

('margin', 0.2153846153846154),

('latin', 0.13076923076923078),

('martin', 0.05384615384615385),

('lain', 0.046153846153846156),

('earning', 0.038461538461538464),

('marin', 0.007692307692307693),

('darwin', 0.007692307692307693)]

The misspelling word lanin can be the misspell of many things so TextBlob gives us different results with learning is the word with the highest percentage to be the right one.

Word Frequencies

Sometimes, it is important to know the frequency of a certain word in a sentence. TextBlob allows us to find the frequency of a word in a text with ease.

We will use all of the text in my article on how to learn data science when life does not give you a break. We will use newspaper to scrape the article.

pip install newspaper

Since this article is about my data science journey, I anticipate that there will be a lot of the words “I” in the sentence. Let’s check that with blob.word_counts

81

81 is a high number but what how high it is compared to other words in my article? Let’s visualize it with bar plot

As we could guess, words such as ‘i’, ‘to’, ‘the’, ‘my’ are among the most common words in the article!

Conclusion

Congratulations! You have just learned how to process your Python string with TextBlob. And the only step needed to use all processing methods listed above is to wrap the string inside the TextBlob object or the Word object. How easy it is!

You can explore more methods to process and analyze your text in TextBlob’s documentation.

The code for this article could be found here

I like to write about basic data science concepts and play with different algorithms and data science tools. You could connect with me on LinkedIn and Twitter.

Star this repo if you want to check out the codes for all of the articles I have written. Follow me on Medium to stay informed with my latest data science articles like these:",https://towardsdatascience.com/supercharge-your-python-string-with-textblob-2d9c08a8da05,['Khuyen Tran'],2020-11-06 23:21:38.957000+00:00,995,"Text Blob, Python, NLP, NLTK, Sentiment Analysis"
A Data Scientist’s Approach to Visual Audio Comparison,"Loading the Data

Getting audio data into Librosa is actually quite simple, but the analysis and frequency conversion can take a bit of code. For best analytical results we need to know the sample rate of each of our files, and it’s also best if those sample rates match between files. Here, they are all 44.1 kHz (a pretty standard sample rate).

My AudioAnalyzer class takes care of the heavy lifting for getting the audio into librosa and transforming it into the frequency domain. Again, if you’re interested in seeing the full code for any of this, go check out my project repository.

Initialize the AudioAnalyzer instances

input_sr stands for input sample rate, and is set to 44100 for each file. We also have fft_size set to 44100. FFT is basically a way of taking time domain data and creating a specified number of bins in which to place frequency information. In matching our FFT size to our sample rate, we are making it so that each of these bins is exactly 1 Hz. If you’re interested in reading more on FFT, check out this page.

Behind the scenes, we’re loading the audio, passing it through FFT processing, and then averaging the amplitudes of each frequency bin to get a single spectrum for each file.

The AudioAnalyzer class can already produce a simple spectrogram for us.

plotting individual spectrums

As you can see from the code, I have specified a minimum and maximum frequency. This is just so that we can zero in on a specific range where we will be able to see everything a little more clearly. Also, notice in the graphs below that the y-axis (amplitude) has been scaled for each spectrogram to be between 0 and 1. When comparing these spectrograms, it is helpful to have everything on the same scale.

From these charts, you probably think that they look rather similar. That’s because they do. Comparing the frequency domains side by side like this is pretty hard to do (and is why I am writing this article!). Luckily, I have another class that can help us to better visualize and compare the differences.

Comparing the Audio

I handle the comparison between two audio files with another class — SpectrumCompare . This class takes in two of our previously created AudioAnalyzer instances and runs some calculations in the background to help us compare them.

Let’s take a look.

Initializing SpectrumCompare class

Simple as that! We can now call a few different plotting methods on our orig_to_09 instance to compare the original 1965 version against the remastered 2009 version.

First, let’s plot the spectrograms from these two files on top of each other.

Plot basic spectrum group

With this visual, it is already much easier to compare the frequency spectrums of these two versions.

From a quick glance, it appears that the remastered version has higher amplitudes across all frequencies. But is the difference in amplitude consistent across all ranges? If not, where are the greater differences? Luckily, SpectrumCompare can help us answer these questions!

Plot full spectrum group with threshold

By passing a few other arguments in our plot_spectrum_group , we can see how the amplitude changes over the various frequency bins. The green line here shows the difference in scaled amplitudes, and the red line is the threshold. When the green line is above the red line, it means that the remastered version has a higher amplitude in that frequency bin. When it is below(which we don’t see in this particular case), it means that the original has a higher amplitude at that frequency.

Note: the order in which you add your AudioAnalyzer instances to the SpectrumCompare initialization will determine which side of the red line means more amplitude for which spectrum. The first one in gets the lower half and the second one gets the upper half. See the project repo for more information.

We can confirm our last observation that the remastered version tends to have greater amplitude across the plotted spectral range. We can even see where many peaks form and where they start to flatten out after about 700 Hz.

While this graph certainly lets us visualize more about this comparison, I think it can be simplified a bit and still give all the same general information. For this, we can use the SpectrumCompare.plot_spectrum_heatmap method.

Alternative heatmap plot

Great! Now this is much more pleasing to look at, and delivers all the same information. Our red and green lines have been replaced by the heatmap background. Through this heatmap we can see that there are a few particular hotspots. If we want to get a little more precise about our analysis, we can use the frange parameter to zoom in on a particular range. Here it looks like most of the hotspots occur between 100–500 Hz (a pretty meaty range for a lot of male vocal and instrumental music).

Heatmap from 20–500 Hz

At this closer scale, we can see that somewhere around 180 Hz there is a pretty significant hotspot indicating that the remastered version has higher amplitude at that frequency. That range corresponds to around F3, which is a comfortable male vocal range and right in the middle of the guitar range. This also happens to be the key for this song (F major). Furthermore, we can see these hotspots begin to fade as we get closer to 500 Hz.

So one conclusion we can make from this visual analysis is that the remastered version seems to place emphasis on bringing out this range around 180 Hz through 500 Hz. These hotspots are relatively contained, and we don’t see any particularly wide regions where the amplitude has been boosted significantly in the remaster. So these are some subtle changes that will likely reflect a little more emphasis on the main vocal and instrumental lines.

With this quick rundown of the process out of the way, lets look at some other comparisons between these different versions.",https://towardsdatascience.com/a-data-scientists-approach-to-visual-audio-comparison-fa15a5d3dcef,['Quinn Dizon'],2019-07-28 15:14:40.239000+00:00,967,"Audio Analysis, Librosa, FFT, Sample Rate, Frequency Domain"
Introducing AiSara,"AiSara stands for artificial intelligence for solution approximation with robust algorithm. Aisara proprietary algorithm enables automated analysis which quickly gains clarity as the algorithm has only one mission: to find the pattern in your data with no hassle. At AiSara, we believe that artificial intelligence is made for everyone which is why we have made AiSara easy to use to allow users of all levels to benefit from it with no coding knowledge required.

Utilize AiSara to make prediction, optimization. No coding, no fuss, easiest to use. - Zaim Awang -

No matter what industry you’re in, AiSara can assist you in finding your solutions. Commonly, in your spreadsheet, you have both input and output data and with more additional input data you want to be able to make a prediction on the output.

Using AiSara, you can utilize the power of artificial intelligence with no coding, no model design, and no hassle. AiSara will do all the work for you, taking your data to prediction in only a matter of minutes.

AiSara has launched multiple services, each suited for different fields. The following demos are up for you to try:

General

Testdrive

Oil & Gas Production Forecast

Oil & Gas History Matching

Hyperparameter tuning

Try the demos yourself at https://demo.aisara.ai/. For more information about AiSara and our company go to https://www.aisara.ai/",https://medium.com/aisara-ai-no-coding/introducing-aisara-b806eff515d9,[],2020-08-24 07:18:53.054000+00:00,212,"Ai Sara, Artificial Intelligence, Solution Approximation, Robust Algorithm, Automated Analysis"
It All Started With A Dream,"Authors: Mikhail Burtsev, Daniel Kornev

“So many of our dreams at first seem impossible, then they seem improbable, and then… they soon become inevitable.” – Christopher Reev

Dream: A Powerful Word

At DeepPavlov, we have a Dream. Our Dream is to make AI assistants to improve the lives of every human. Whatever we have today in the form of AI assistants, is merely the dawn of what is yet to come.

Imagine AI assistants being capable of understanding us, and talking to us. Imagine them learning from us and teaching us. Imagine them being our trusted assistants. Imagine them doing everything we want. Imagine them empowering us to develop personally.

Today, we make a very early version of our DREAM accessible to everyone. DeepPavlov DREAM is an AI assistant based on the socialbot built by our team for the Alexa Prize 2019 competition. Currently, it is available on our demo website and in Telegram messenger. DREAM blends together almost 40 different chit-chat and task-oriented skills to engage in open domain conversations. It relies on a selection of modern NLP models and components including 14 annotators, 4 post-annotators, and knowledge graph integration. Please, chat with DREAM and give us feedback on how to improve it! Don’t forget that it is still in the early stage and might be confused easily.

The DREAM is created on top of the DeepPavlov Agent, an open source multiskill orchestration framework. In an accompanying blog post we will guide you through the very simple DP Agent configuration with only one custom skill and built-in skill and response selectors. Read it to learn how to build AI assistants using our technology.

If you want to dive into details check DREAM socialbot Technical Report for Alexa Prize 2019 competition.

Why DREAM?

Development of a bot for a specific task requires setting NLU and dialog management. At the beginning it is simple and straightforward, but soon more and more features are added in NLU to cover natural variability of a user input. To extend functionality and value of the product more and more scripts/actions/models are integrated. More scripts require more features and this spirals out in a “mature AI assistant”. This AI assistant might reach quite good functionality but usually it hits the complexity ceiling when dependencies between components severely hurt possibilities for the assistant refactoring and further development. This problem is especially hard in multi-domain mission-critical applications because even slight change in the system can lead to strongly unpredictable behavior.

Does it mean that the dream of smarter and smarter assistants has to be postponed?

DeepPavlov addresses this challenge by introducing modular architecture for conversational agents. In the DP Agent framework, functionality for distinct tasks is packed in separate conversational skills with clear interfaces. This additional level of abstraction allows us to add new skills without interference with existing components of the system. Dialog as a whole is controlled at both individual skills and overall dialog levels. MVP for a new task can be tested in isolation before integration into an AI assistant.

Moreover, many skills such as chit-chat, alarm, calendar, etc. are common for the majority of use cases. So, why reinvent the wheel again and again? DP Agent makes it possible to create a conversational agent distro which includes a set of essential default skills. Default assistant provides basic functionality out of the box and can be extended by plugging domain specific skills.

Life-cycle of AI Assistant

DP Agent open architecture is especially powerful for building and maintaining complex conversational solutions. Integration of skills as micro services makes an assistant scalable. Development and support of skills can be effectively performed by a group of enthusiasts, distributed product teams, or subcontractors.

DREAM is a first multiskill distribution and an experimental AI assistant built with DeepPavlov conversational AI stack. In the coming months we will start to open source code for DREAM skills, services, and tools. We’ll also continue posting new blog posts to guide you through building more complex AI assistants using DREAM. Our roadmap includes publishing sample configs, tools for multiskill assistant design and dependency management. We plan to support fluid form-filling, multi-intent understanding, context tracking, learning from users, and many other critical scenarios in the future.

Announcing DeepPavlov Contributor Program

AI is at the edge of technological progress, and open source is one of the major driving forces behind it. Today we are thrilled to announce our DeepPavlov Contributor Program. It is a fantastic opportunity to join us in our incredible adventure towards the big dream of building AI assistants that can understand us, teach us, learn from us, and help us to become better.

We already have some inspiring stories of contributors to our DP Library, and we welcome you to learn more about the program here.

Announcing Community Calls and State of the Union Events

Our path towards this ambitious dream is long and we simply can’t allow ourselves the luxury of moving alone. We believe that sharing our work and bringing input from both industry and academia in the form of regular events is essential.

Starting with this month, we will hold regular Community Calls, and every half of the year we will hold State of the Union events, to keep you informed of our developments, as well as to bring your feedback to us as soon as possible.

We will also keep posting updates in our DP Blog, and send bi-weekly newsletters to our subscribers. You can subscribe to our news here.

Dream Now!",https://medium.com/deeppavlov/it-all-started-with-a-dream-44e0861d0642,['Daniel Kornev'],2020-09-04 11:39:34.485000+00:00,890,"We invite you to join us in our Dream. We are sure that together we can make it happen.AI, AI Assistants, NLP, Deep Pavlov, Dream"
Gradient Descent — Intro and Implementation in python,"Python Implementation

We will implement a simple form of Gradient Descent using python. Let’s take the polynomial function in the above section and treat it as Cost function and attempt to find a local minimum value for that function.

Cost function f(x) = x³- 4x²+6

Let’s import required libraries first and create f(x). Also generate 1000 values from -1 to 4 as x and plot the curve of f(x).

# Importing required libraries

import numpy as np

import matplotlib.pyplot as plt f_x = lambda x: (x**3)-4*(x**2)+6

x = np.linspace(-1,4,1000) #Plot the curve

plt.plot(x, f_x(x))

plt.show()

f(x) = x³ - 4x² + 6

Let’s find out the derivative of f(x).

d f(x)/dx = 3x² - 8x. Let’s create a lambda function in python for the derivative.

f_x_derivative = lambda x: 3*(x**2)-8*x

Let’s create a function to plot gradient descent and also a function to calculate gradient descent by passing a fixed number of iterations as one of the inputs.

def plot_gradient(x, y, x_vis, y_vis):

plt.subplot(1,2,2)

plt.scatter(x_vis, y_vis, c = ""b"")

plt.plot(x, f_x(x), c = ""r"")

plt.title(""Gradient Descent"")

plt.show() plt.subplot(1,2,1)

plt.scatter(x_vis, y_vis, c = ""b"")

plt.plot(x,f_x(x), c = ""r"")

plt.xlim([2.0,3.0])

plt.title(""Zoomed in Figure"")

plt.show()



def gradient_iterations(x_start, iterations, learning_rate):



# These x and y value lists will be used later for visualization.

x_grad = [x_start]

y_grad = [f_x(x_start)]

# Keep looping until number of iterations

for i in range(iterations):



# Get the Slope value from the derivative function for x_start

# Since we need negative descent (towards minimum), we use '-' of derivative

x_start_derivative = - f_x_derivative(x_start)



# calculate x_start by adding the previous value to

# the product of the derivative and the learning rate calculated above.

x_start += (learning_rate * x_start_derivative)



x_grad.append(x_start)

y_grad.append(f_x(x_start)) print (""Local minimum occurs at: {:.2f}"".format(x_start))

print (""Number of steps: "",len(x_grad)-1)

plot_gradient(x, f_x(x) ,x_grad, y_grad)

Now that we have defined these functions let’s call gradient_iterations functions by passing x_start = 0.5, iterations = 1000, learning_rate = 0.05

gradient_iteration(0.5, 1000, 0.05)

gradient_iteration(0.5, 1000, 0.05)

We are able to find the Local minimum at 2.67 and as we have given the number of iterations as 1000, Algorithm has taken 1000 steps. It might have reached the value 2.67 at a much earlier iteration. But since we don’t know at what point will our algorithm reach the local minimum with the given learning rate, we give a high value of iteration just to be sure that we find our local minimum.

This doesn’t sound to be very optimal because of the unnecessary number of loop iterations even after it has found the local minimum.

Let’s take another approach of fixing the number of iterations by using precision.

In this approach , Since we know the dataset, we can define the level of precision that we want and stop the algorithm once we reach that level of precision.

For this example let’s write a new function which takes precision instead of iteration number.

def gradient_precision(x_start, precision, learning_rate):



# These x and y value lists will be used later for visualisation.

x_grad = [x_start]

y_grad = [f_x(x_start)] while True:



# Get the Slope value from the derivative function for x_start

# Since we need negative descent (towards minimum), we use '-' of derivative

x_start_derivative = - f_x_derivative(x_start)



# calculate x_start by adding the previous value to

# the product of the derivative and the learning rate calculated above.

x_start += (learning_rate * x_start_derivative)





x_grad.append(x_start)

y_grad.append(f_x(x_start))

# Break out of the loop as soon as we meet precision.

if abs(x_grad[len(x_grad)-1] - x_grad[len(x_grad)-2]) <= precision:

break print (""Local minimum occurs at: {:.2f}"".format(x_start))

print (""Number of steps taken: "",len(x_grad)-1)

plot_gradient(x, f_x(x) ,x_grad, y_grad)

Now let’s call this function with parameters x_start = 0.5, precision = 0.001, learning rate = 0.05

gradient_precision(0.5, 0.001, 0.05)

gradient_precision(0.5, 0.001, 0.05)

Local Minimum = 2.67

Number of Steps = 20

Our gradient Descent algorithm was able to find the local minimum in just 20 steps! So, in the previous method we were unnecessarily running 980 iterations!

Now that we are able to successfully minimize f(x) i.e. find the minimum value of x for which f(x) is minimum, Let’s play around with learning rate values and see how it affects the algorithm output.

Learning rate = 0.01

gradient_precision(0.5, 0.001, 0.01)

x_start = 0.5 ,precision = 0.001 , learning rate = 0.01

Since learning rate was lesser, which means the number of steps taken to reach local minimum was higher (85). As we can see in the graph, 85 x values plotted in blue, meaning our Algorithm was slower in finding local minimum.

Learning rate = 0.05

gradient_precision(0.5, 0.001, 0.05)

x_start = 0.5 ,precision = 0.001 , learning rate = 0.05

For the same precision value and x_start value, but learning rate = 0.05, we see that our Algorithm was able to find local minimum in just 20 steps. This shows that by increasing learning rate , the algorithm reaches local minimum faster.

Learning rate = 0.14

gradient_precision(0.5, 0.001, 0.14)

x_start = 0.5 ,precision = 0.001 , learning rate = 0.14

By increasing the learning rate to 0.14, the Algorithm was able to find local minimum in just 6 steps.

Hold up! Don’t fall into the trap that increasing learning rate will always reduce the number of iterations the algorithm takes to find the local minimum. Let’s just increase the learning rate by 0.01 and see the results.

x_start = 0.5 ,precision = 0.001 , learning rate = 0.15

Whoops! the number of steps taken increased this time! . Looks like learning rate = 0.14 is the sweet spot for precision = 0.001.

One thing to be noted is that this implementation will work for cases where the Cost function has only one variable x. In case of multiple variables (x,y,z….) The implementation will change and probably will post it in another article. Also There are different types of Gradient Descent as well

Batch Gradient Descent

Stochastic Gradient Descent

Mini Batch Gradient Descent

We shall see in depth about these different types of Gradient Descent in further posts.",https://medium.com/analytics-vidhya/gradient-descent-intro-and-implementation-in-python-8b6ab0557b7c,['Fahad Anwar'],2019-10-06 15:10:13.347000+00:00,902,"python, implementation, gradient descent, cost function, local minimum"
5 Key Differences Between Data Scientists and Data Engineers,"5 Key Differences Between Data Scientists and Data Engineers

Distinguish the two building blocks of data science

Photo by Fabian Kühne on Unsplash

Data science is an interdisciplinary field that still continues its evolution. The ultimate goal is clear which is creating value out of data. The value can be in various forms such as improved business operations, predictive maintenance, and so on.

Although big tech companies have different positions that are distinctively separated, most companies have vague definitions of what is expected from a data scientist. More importantly, some companies cannot afford to have separate data scientist and data engineer positions which, as a result, generates a position called full-stack data scientists.

In this article, I will try to distinguish the role of data scientists and data engineers. The following key differences will be focused on what they typically do as well as what is expected from them.

Before I start, I would like to share my personal opinion on these two positions. It is hard to compare them according to the difficulty level because the required skills are very different. I think the demand for data engineers is higher than the demand for data scientists. It will make more sense as we go through the differences listed below.",https://towardsdatascience.com/5-key-differences-between-data-scientists-and-data-engineers-4fb67bb5e4b6,['Soner Yıldırım'],2020-12-31 15:17:32.596000+00:00,204,"Data Science, Data Engineering, Full Stack Data Scientists, Business Operations, Predictive Maintenance"
What makes humans different from machines?,"Photo by Katarzyna Pe on Unsplash

GIn the era of Artificial Intelligence (AI), robotics, machine learning, quantum computing, and many other advanced computing developments many critics point to AI as the biggest to human kind, much bigger than climate change. While on the other hand, proponents of AI point that machines can never really replace us humans but only compliment our ability.

What does being a human entail?

It is in this context that we know what being human actually means. Basic essence of human beings is morality, which in turn flows from conscience among other things. Morality is the ability to distinguish between right and wrong. It is guided by ethics, and results in empathy, compassion, tolerance, humility, charity and most profoundly love.

Empathy allows humans to step into other’s shoes and feel the pain of others. It is one of most basic human virtues. A simple example of this is the fact that an image of a Alan Kurdi (a 3 year old Syrian child who dies while seeking refuge) shocked the whole world. It create a wave of movements across EU and world at large.

Alan Kurdi

The above example highlights the specifically human quality of ‘caring’, caring for some one other than one-self. In other words, self-lessness.

Another quality of humans is conscience. It is that internal voice at the back of our heads which keeps us from doing something wrong or pushes us to do something good. It might not be ethically correct all the time, but more often than not it is. Though the evolutionary significance and utility of conscience and mind have still not been established; they play a significant role in our lives.

Free will, is one of the basic tenets of ethical principles and corner stones of human civilization. It is enshrined in UN Declaration of Human Rights and finds its place in most constitutions. What it means is that humans take decisions according to their own understanding and voluntarily. The decisions can be right or wrong, but they are taken free of any compulsion.

Another quality of human beings is presence of flaw, diversity in character, difference of opinion and imagination. Basically, humans are not one and the same, they are different, more often than not wrong and morally imperfect. But this is what gives meaning to life.

Lets discuss humans vs machines

In simple terms, machines as envisaged by current technological trends will be AI driven, independent decision making machines with actual physical forms. They will be a physical form of Google Assistant or Siri or Alexa with much more fine tuned intelligence and motor functions.

It might as well have a chip for simulating emotions as reactions to situations. But this is where it ends. They won’t probably have the ability to imagine bizzare things, put somebody else in place of self, empathy, diversity and other human specific traits.

Lets take the trolley problem.

“You see a runaway trolley moving toward five tied-up people lying on the tracks. You are standing next to a lever that controls a switch. If you pull the lever, the trolley will be redirected onto a side track and the five people on the main track will be saved. However, there is a single person lying on the side track. You have two options:

Do nothing and allow the trolley to kill the five people on the main track. Pull the lever, diverting the trolley onto the side track where it will kill one person.”

This is a classic ethics problem which a human has trouble deciding because there are lives being lost on both side even if it is just one. But for a robot, its a cost benefit analysis and surely pull the lever (option 2). And the robot will probably justify its actions.

Another issue with robots is “meaning of life”. Although there is no meaning of life in true sense of the word because Earth (and humans) are just a tiny blip in the universe, which just so happened to be survivable due to sheer force of luck. But still humans try to find meaning in life through work, spirituality, religion, charity, family etc. It all flows down from religion.

Robots don’t have the concept of meaning of life. For them the whole concept of religion and meaning is irrational and hence useless. This gives them no sense of purpose or the will to do something.

What will be the aim of a robot as an independent entity? Currently they are made to serve humans and specific tasks. But what when they are recognized as independent citizens (as Saudi Arabia granted to ‘Sophia’)? These are the impertinent questions which will trouble people in the coming decades.",https://medium.com/predict/what-makes-humans-different-from-machines-ff791f45187e,['Prakhar Singh'],2019-01-04 17:48:26.058000+00:00,768,"AI, Robotics, Machine Learning, Quantum Computing, Human Traits"
How VOTing classifiers work!,"How VOTing classifiers work!

Classification is an important machine learning technique that is often used to predict categorical labels. It is a very practical approach for making binary predictions or predicting discrete values. The classifier, another name for classification model, might have the intention of predicting whether someone is eligible for a job or it could be used to classify the images of multiple objects in a store.

Classification, like other machine learning techniques, use data sets. A dataset is a combination of multiple values from different variables. After obtaining an optimal dataset, it is split into two: the training and testing set. The training set often has a larger proportion of the dataset. It is likely to take up about 70% to 90% of the dataset.

The training set is inserted into the machine learning algorithm to create a predictive model with an added step called cross-validation. Cross-validation is a great way to ensure that the built model does not overfit the training set and it also optimizes the versatility of the model. Then, the model can be used to predict the labels in the testing set. The predicted labels are further compared to the actual testing set labels via metrics such as confusion matrix, precision score, recall score, F1-score, roc auc score.

Once the construction of the classification model is over, a data point’s values can be inserted into the algorithm and the algorithm makes a decision by attributing a specific label to this data point based on the variables’ inputs.

Now imagine if different classification methods were asked to make decisions based on the data instances’ inputs. There are bound to be different answers. This is where voting classifiers come into play.

What is a Voting Classifier?

A voting classifier is a classification method that employs multiple classifiers to make predictions. It is very applicable in situations when a data scientist or machine learning engineer is confused about which classification method to use. Therefore, using the predictions from multiple classifiers, the voting classifier makes predictions based on the most frequent one.

A real life scenario could see a data scientist being confused about whether to use a random forest classifier or a logistic regressor to predict the type of flower based on their dimensions.

Using the prompt above, a step-by-step guide has been created below on how to use python via Jupyter Notebooks to build voting classifiers.

Starting with the code below, one can import the classifiers using scikit-learn.

from sklearn.ensemble import RandomForestClassifier

from sklearn.linear_model import LogisticRegression

from sklearn.neighbors import KNeighborsClassifier

from sklearn.ensemble import VotingClassifier

Using python via Jupyter Notebooks, the scikit-learn’s ensemble feature is accessed and a voting classifier is imported. There are three other classifiers in the code above: a random forest classifier, a logistic regressor and a KNearest Neighbor classifier. These three will be attributed to objects as seen below:

log_clf = LogisticRegression()

rnd_clf = RandomForestClassifier()

knn_clf = KNeighborsClassifier()

Afterwards, an object is created for the voting classifier. The voting classifier has two basic hyperparameters: estimators and voting. The estimators hyperparameter creates a list for the objects of the three classifiers above while assigning names to them. The voting hyperparameter is set to either hard or soft.

If set to hard, the voting classifier will make judgments based on the predictions that appear the most. Otherwise, if set to soft, it will use a weighted approach to make its decision. I’d recommend setting it to soft when using an even number of classifiers because of its weighted approach and setting it to hard when using an odd number of classifiers because of its “majority carry the vote” approach.

vot_clf = VotingClassifier(estimators = [('lr', log_clf), ('rnd', rnd_clf), ('knn', knn_clf)], voting = 'hard')

The voting classifier like any other machine learning algorithm is used to fit the independent variables of the training dataset with the dependent variables

from sklearn.datasets import load_iris

from sklearn.model_selection import train_test_split iris = load_iris()

x, y = iris['data'], iris['target']

x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 42, train_size = 0.85) vot_clf.fit(x_train, y_train)

After fitting, it can be used to make predictions and the accuracy of its predictions is measured.

pred = vot_clf.predict(x_test)

accuracy_score(y_test, pred)

Here is an image that shows how a voting classifier was used as a predictive model for a dataset and compared to other classifiers. The code was initially obtained from Aurelion Geron’s book, Hands-On Machine Learning with Scikit-Learn and TensorFlow Concepts, Tools, and Techniques to Build Intelligent Systems, and I ran them on Jupyter Notebooks.",https://towardsdatascience.com/how-voting-classifiers-work-f1c8e41d30ff,['Mubarak Ganiyu'],2020-11-06 03:01:20.030000+00:00,719,"Voting Classifier, Machine Learning, Classification, Scikit-Learn, Python"
What secret is powering these AI and Blockchain startups?,"Blockchain for Product Provenance

Now the blockchain evangelism of 5–10 years ago has cooled down, cases of real-world problem-solving form the world’s most misunderstood ledger are starting to come to fruition.

Rob Squire, who is the CTO of new “centralised” blockchain startup, Ld8a, explains that blockchain’s future isn’t with digital currencies, but with Enterprise Systems of Record (ESOR). For him, blockchain’s immutable ledger is perfect for records that need to keep in sync with real-world changes.

Take, for example, product provenance.

London and Dublin-based startup, Circulor, is an industrial supply chain technology that harnesses the blockchain so brands can demonstrate to their stakeholders that they are ethical and sustainable.

Discussing a problem industry like mobile phones, which has a long, torrid relationship with “conflict minerals” such as coltan and cobalt, co-founder, Veera Johnson, explains that the Circulor system can track cobalt as it is recycled through the supply chain, and logs what is added to it, by whom and, when, all on the incorruptible record of the blockchain.

Blockchain startup, Circulor, are making suppliers rethink the provenance of problem products like smartphones. Photo by Rodion Kutsaev on Unsplash

The technology is so compelling it motivated the former Asian businesswomen, Johnson, out of retirement.

She said, “We don’t stop the process [of bad supply chains] but we shine a light on it and that will, in turn, help improve supply chains. This year we’ll be moving into the aviation sector and the provenance of 3D printing materials.”

Similarly, a new Germany-based startup, retraced, is looking to redesign the fabric of the fashion industry. Following disasters such as the Randa Plaza factory collapse in Bangladesh, which inadvertently unearthed poor working conditions from many, familiar high street brands, consumers, especially young people, are demanding brands act more ethically. retraced allows fashion brands such as Cano — who produce handmade shoes from Mexico — to do that by telling their customers the stories they care about.

In both of these blockchain applications, the startups could not function without high-value computational power.",https://medium.com/silicon-roundabout-hub/what-secret-is-powering-these-ai-and-blockchain-startups-e0eb9c6fcadc,['Craig E Ryder'],2020-03-04 09:32:14.998000+00:00,327,"Blockchain, Product Provenance, Circulor, retraced, ESOR"
Teaching data science students to tackle the “hard” technical problems,"Image from https://pixabay.com/

During my time as a student and now as a PhD student and lecturer within data science, I have noticed that students are surprisingly bad at dealing with real technical problems. What I am referring to here is that students that want to specialise in machine learning and data science know their way around the methods and theory of for example a neural network, but are unable to implement it efficiently themselves and execute it on a cluster, or even pull a working implementation from Github and install it.

What normally happens in a course at my university when teaching students about the K-means clustering algorithm, is that they are explained the theory of the algorithm and then provided with a pre-made MATLAB implementation where they are asked to vary the parameter K and observe what happens. With this approach, the students might get a thorough understanding of the algorithm — and maybe even some deeper theoretical insights — without getting bogged down by implementation details.

Image from http://www.vlfeat.org/overview/kmeans.html

The problems usually first surface when they are writing their master thesis, or when they get their first job after university. As a data scientist and even as a mathematician your primary task will often involve a lot of programming (especially mathematicians realise this too late). At this point, some students are for the first time presented with a really large dataset, the task to run their algorithms on a cluster or the problem of installing and modifying a library hosted on Github.

When teaching my own course Computational Tools for Big Data, the agenda for the first week is UNIX, Git and Amazon EC2. During this week I ask the students to learn a range of UNIX-commands for working with files, how to use Git for version control and how to set up and run stuff on a virtual machine at Amazon. I ask them to solve actual problems with real files in their terminals (and if they use Windows, they need to start by learning SSH to get a terminal on the cluster), to create and use their own repositories on Github and to launch a free instance on Amazon EC2 (including setting up RSA-keys and other stuff) and get stuff to run on it.

By letting the students get a true experience of what it requires to launch a virtual machine on Amazon and clone a repository from Github to it, I hope that they will be more comfortable doing this on their own in the future. From my own experience, a large fraction of time as a data-scientist is not spend on model-selection, but on grunt-engineering tasks such as merging branches in git, installing libraries, optimising database queries and processing files in the terminal.

In later weeks they are asked to install and work with PostgreSQL, MongoDB, Vagrant and various Python libraries. They are also asked to use data files from previous competitions on Kaggle.com which are stored in the JSON-format to make sure they have not only seen CSV-files when they graduate as engineers.

Some of the subjects in my course are unfortunately still too much for all students to handle — this includes setting up an Apache Spark cluster and getting Caffe to run for training convolutional neural networks. For this we currently provide them with a Vagrant-file where the libraries are installed. I would love to get away from this, but the assistant teachers should not spend 99% of their time debugging weird system errors on student laptops. Hopefully we will find a better solution in the future.",https://medium.com/peergrade-io/teaching-data-science-students-to-tackle-the-hard-technical-problems-104f145919e6,['David Kofoed Wind'],2019-05-03 09:01:54.158000+00:00,589,"Data Science, Machine Learning, K-means Clustering Algorithm, UNIX-commands, Git Hub"
Data Processing In Rust With DataFusion (Arrow),"Data Processing In Rust With DataFusion (Photo By Author)

Rust is the most beloved language, according to StackOverflow, it is on the top of the list for four years! Data processing is getting simpler and faster with a framework like Apache Spark. However, the field of data processing is competitive. DataFusion (part of Arrow now) is one of the initial attempts of bringing data processing to the Rust. If you are interested in learning some aspects of data processing in Rust with DataFusion, I will show some code examples in Rust with DataFusion, as well as compare the query performance between DataFusion and Pandas.

Update: I initially wrote this article in DataFusion version 0.15.0. With release 1.0.0 of DataFusion and Arrow, I have added both the code and benchmark so we can see the improvement.

DataFusion

Andy Grove created DataFusion, and he had some great articles about building modern distributed computing, for example, How To Build A Modern Distributed Compute Platform. The DataFusion project is not for the production environment yet, as Andy mentioned,

“This project is a great way to learn about building a query engine, but this is quite early and not usable for any real-world work just yet.”

The project was donated to the Apache Arrow project in February 2019, and more people start to contribute to the Arrow version of DataFusion.

DataFusion is an in-memory query engine that uses Apache Arrow as the memory model. It supports executing SQL queries against CSV and Parquet files as well as querying directly against in-memory data.

The project description may not deliver too much excitement here, but since the entire project is done in Rust, it provides you ideas about writing your analytics SQL in Rust. Additionally, you can bring DataFusion as a library to your Cargo file for your Rust project easily.

Initial Setup

To test run some code with DataFusion, first, we need to create a new Rust package

cargo new datafusion_test --bin

Then bring DataFusion as a dependency in Cargo.toml file

[dependencies]

arrow = ""1.0.0""

datafusion = ""1.0.0""

Test Dataset

There are many datasets available online, Kaggle is one of the places that I usually go to and explore the new dataset. We are going to use The Movies Dataset, and the complete version of this dataset is about 676.68 MB. The movie dataset has the following schema

userId: int movieId: int rating: double timestamp: long

To work with CSV file format, DataFusion used to requires us to provide the schema here, as version 1.0.0 introduced schema infer, this is not needed any more.

let schema = Arc::new(Schema::new(vec![ Field::new(“userId”, DataType::UInt32, false), Field::new(“movieId”, DataType::UInt32, false), Field::new(“rating”, DataType::Float64, false), Field::new(“timestamp”, DataType::Int16, false) ]));

Initial Select (version 1.0.0)

As more features introduced in version 1.0.0, DataFusion API brings lots exciting enhancement. You’d see improvements like schema infer, easier to print result and more. The Code is much concise and easy to read.

Code Example for DataFusion 1.0.0

Performance Comparison Between DataFusion vs. Pandas

It won’t be a fair comparison since DataFusion is entirely new and lack lots of optimization. But it’s still interesting to see the current state of DataFusion and compare it with mature data processing package like pandas.

Disclaimer: I am running it on my personal Mac 13 (2 GHz Quad-Core Intel Core i5) to perform the benchmark, the result could be biased. Since the initial benchmark was published in debug mode, noticed the performance is significant different in release mode.

Query 1: SELECT Top 10 Rows

Query: “SELECT userId,movieId,rating FROM ratings LIMIT 10” DataFusion: 0.7s Pandas: 6.15s

DataFusion is running very fast on random access ten rows here. On the other hand, pandas is about 6s slower.

Query 2: Get Each User’s Average Rating

Query: “SELECT userId, AVG(rating) FROM ratings GROUP BY userId” DataFusion: 18.57s Pandas: 6.24s

As Pandas uses NumPy under the hood, it is not surprising to see good performance on the Pandas side. On DataFusion side, though it is slower than Pandas, the performance is also reasonable to perform those types of aggregations.

Query 3: What is the Highest Rating Score

Query: “SELECT MAX(rating) FROM ratings” DataFusion: 15.28s Pandas: 5.97s

As the previous analytics query seems is slow, this query would also be slow on DataFusion side.

Final Thoughts

As we discussed first, DataFusion is an exciting attempt in Rust to get into the competitive data compute market. As the DataFusion project is still an early stage and requires more contributions, it is not surprising to see some slow performance on certain types of queries. Also, as described in the project README, some key features are missing, so you have to be careful about the SQL command you write and double-check to see if it is currently supported or not.

Overall, DataFusion is an attractive beginning with Rust for the data world, and especially it is part of Apache Arrow now, DataFusion can leverage features easily from the Arrow ecosystem. I’d expect to see considerable performance improvement and more supported SQL features in the future version of DataFusion.",https://towardsdatascience.com/data-processing-in-rust-with-datafusion-arrow-56df5432de68,['Chengzhi Zhao'],2020-08-04 22:25:39.313000+00:00,797,"Data Fusion, Apache Arrow, Rust, Data Processing, SQLQueries"
May 2020 CoupCast Update,"Burundi, Sudan and the Algeria have the highest risk of coup attempt this month. Meanwhile, a number of less-commonly-seen countries have come into the top ten, including Kenya, Suriname and Dominican Republic.

Carnation by John Singer Sargent. Source: Wikimedia Commons

CoupCast is a machine learning based early warning forecasting platform for estimating the risk of illegal leadership turnover each month for every country across the globe. These updates provide insight into technical changes/updates, notable events in the previous month, and a more in-depth overview of what to expect in the new month coming forward.

The official web dashboard for CoupCast can be found here and information regarding the underlying data (REIGN) and updates to our monthly data can be found here.

Feel free to reach out to either myself (mfrank@oneearthfuture.org) or Clayton Besaw (cbesaw@oneearthfuture.org) for any questions regarding our CoupCast platform or analyses contained in the updates.

May 2020 CoupCast Report:

Data and algorithm updates:

Updated precipitation (SPI) estimates using NOAA’s March 2020 PREC/L release (https://www.esrl.noaa.gov/psd/data/gridded/data.prel.html).

Made changes to the inputs for GDP measures related to CoupCast/ELVIS forecasting pipeline. Usually we would update GDP figures using the IMF’s World Economic Outlook release for April. However, COVID-19 has substantially disrupted IMF’s normal process. As a result we have replaced the original IMF GDP input with GDP PPP in international dollars.

Updated armed conflict data to reflect new peace-deals for 2020 and added previously uncoded conflicts for the following countries: Libya, Egypt, Ukraine, Cameroon, Iraq, Iran, United States, United Kingdom, Mali, Philippines, Central African Republic, Somalia, Afghanistan

Corrected leadership coding for Austria. Austria was coded as still having intermin Chancellor Bierlein as the chief executive. This has been changed to reflect Kurz’s return to the position.

Added previously missed information about elections or election delays for the following countries: Guinea, Bolivia, Ethiopia, Dominican Republic

Risk forecast overview for April 2020:

While all of three countries at greatest risk of coup have been regularly featured among the top 10 list this year (Burundi was second last month), this month does feature a number of less common countries in our top 10. Kenya, Suriname, Papua New Guinea, Dominican Republic and Fiji are all making their first appearance in the top 10 for 2020.

There are a number of reasons that we might begin to see different countries come into the top 10 this month.

Newly updated economic indicators from the IMF reflect a fairly drastic economic contraction globally that may be increasing coup risk in individual countries either through country-specific or regional indicators.

That being said, the end to the oil price war between Russia and Saudi Arabia, reached in the middle of last month, could be providing some limited relief to more oil-dependent economies, namely Algeria.

In addition, Covid-19 is causing many countries to delay elections, which can also lead to increased estimates of coup risk. Of the countries in the top 10 this month, the Dominican Republic has delayed their election which was originally scheduled for May 17 but has been pushed back to early July.

However, the elections which are still happening may be more problematic than those that have been delayed. Two countries in our top 10 currently have elections scheduled this month; Burundi and Suriname will hold elections on May 20 and May 25 respectively.

While Burundi will be discussed in more detail below, we will briefly discuss Suriname’s upcoming election as the country’s political and economic situations have been worsening over the past few months.

The country’s economy has been devastated by drops in the prices of oil and gold, while the pandemic has also halted tourism, all three of which are vital sources of income for the country.

In response, the government imposed strict controls on the transfer of foreign currency, outraging Surinamese business owners who have essentially shut down commerce in response.

This move came on top of a revelation in February that the government had withdrawn roughly $100 million dollars in individuals’ personal savings from the country’s central bank, after private banks had been asked last year to deposit half of the foreign currency they held as savings into the central bank.

This scandal led to firings and indictments of a couple officials, but it is still not clear what the money was used for. On top of that, the banking community was understandably not happy with the government’s actions.

That being said, major oil discoveries, made in January of this year, could potentially transform the small country’s economy over the medium term, in a manner similar to its neighbor, Guyana. This could serve to raise the stakes of the election, as the winner may oversee huge investment in the country’s economy.

It is also worth noting several aspects of the incumbent president Desi Bouterse. Bouterse first came to power in 1980 in a military coup, while he was also convicted last year of ordering the execution of 15 of his political opponents in 1982. He was also convicted in absentia in the Netherlands of drug trafficking, while his son is in prison in the United States also due to drug trafficking.

All in all, the situation has the potential to escalate in a number of negative ways as a highly autocratic president seeks to retain office in a high-takes election in which he has alienated the majority of stakeholders in the private sector.",https://medium.com/the-die-is-forecast/may-2020-coupcast-update-e0e4a95d0f02,['Matt Scott Frank'],2020-05-11 13:01:01.055000+00:00,863,"Coup Cast, Burundi, Sudan, Algeria, Kenya"
Predicting Customer Churn in the Telecommunications Industry,"Predicting Customer Churn in the Telecommunications Industry

Image via Shutterstock under license to Leo Siu-Yin

Why Predict Customer Churn?

Getting new customers is much more expensive than retaining existing ones. Some studies have shown that it costs six to seven times more to acquire a new customer than to keep an existing one.

According to BeyondPhilosophy.com:

“Loyal customers reduce costs associated with consumer education and marketing, especially when they become Net Promoters for your organization.”

Hence it is important to be able to proactively determine the customers most at risk of leaving and take preventative measures against this through understanding their needs and providing positive customer experience.

Photo by Sharon McCutcheon on Unsplash

Methodology

The project is divided into 3 stages:

Data Cleaning and Exploratory Data Analysis. Model Selection and Threshold Tuning. Result Interpretation.

Data Cleaning and Exploratory Data Analysis

Data is obtained from Kaggle, IBM Data Sets. The data set has some imbalance with 26.5% churn.

Data is first checked for unique customer ID. Blank spaces are replaced with 0 and columns are changed to numerical type whenever applicable.

EDA is carried out to understand the data. A feature like gender has little impact on churn and will be dropped.

Feature -to-feature correlation is mapped out using a heatmap and features that are highly correlated to one another are dropped (shown below with correlation of > 0.6).

One hot encoding is then carried out on categorical features. And the top 11 features are selected via Random Forest Classification using the algorithm’s feature importance property.

Reducing the number of input features will simplify the problem that is being modeled and speed up the modeling process.

Model Selection and Threshold Tuning

Four models are used to do modeling on the final data: Logistic Regression, C-Support Vector Classification, K Neighbours Classifier and Random Forest Classifier.

GridSearchCV is used to tune the hyperparameters and the best hyperparameter is found for each model.

Based on class 1 classification of threshold 0.5, the logistic regression model is found to have the highest ROC-AUC score, the lowest log-loss, the highest F1 score, and the highest Recall score. Hence the logistic regression model is selected as the final model to further fine-tune the threshold to improve the recall score for our data.

ROC-AUC Score

Log-loss

F1 Score

Precision

Recall

Precision is the fraction of relevant instances among the retrieved instances, while Recall (also known as sensitivity) is the fraction of the total amount of relevant instances that were actually retrieved.

In predicting customer churn, a higher recall would be preferred to a higher precision, because it would be ideal to be able to predict which customers are at a higher risk of leaving. However tuning the threshold to achieve a higher recall comes at the cost of reducing the precision. Hence a balance needs to be achieved. In this project, I have chosen to use F1 score as the metric to select my threshold.

Classification algorithms work by predicting a probability before the probability can be mapped to a class label. This is achieved by using a threshold, such as 0.5, where all values equal or greater than the threshold are mapped to one class and all other values are mapped to another class.

Since the data has a class imbalance, the default threshold can result in a poor classification. As such, it is important to tune the threshold used to map probabilities to the class labels.

At the default threshold of 0.5, the logistic regression model gave an F1 score of 0.58. Based on the maximum F1 score of 0.62, the threshold value turns out to be 0.36. And this new threshold value increased the recall score from 0.55 to 0.78.

Result Interpretation

Based on the logistic regression model, telecommunications companies can focus on the top two features and reduce customer churn by :

Having longer term contract for customers. Improve online security for customers.

Here is a link to my GitHub where you can find my codes and presentation slides. You can also contact me via LinkedIn.",https://towardsdatascience.com/predicting-customer-churn-in-the-telecommunications-industry-99a369317e91,['Leo Siu-Yin'],2020-10-15 02:40:28.505000+00:00,637,"customer-churn, telecommunications-industry, net-promoters, data-cleaning, exploratory-data-analysis"
Welcome to phantastic,"phantastic starts as a a collection of essential articles on the topic of performance. We’ve designed these articles for athletes of all levels sharing a common drive: athletes who want to become their best performing self, are looking for an edge, want to shake things up, or are trying to achieve their breakthrough performance.

Our articles synthesize our learnings as athletes, experimenters, tinkerers, and endurance sports passionates. They are longer in nature, as each article is an inquisitive study of a select topic, bringing in experts to first dissect the topic and finally to extract the essence for you. As a result, each article is meant to be the only article you will need to read on the topic.

Given the intricate and in-depth nature of our articles, we plan to cover one topic per month. This collection will engage you like no other. It will surprise you, teach you something new, and delight you. This is our commitment and promise to raising the bar!",https://medium.com/phantastic/welcome-to-phantastic-39657a124350,[],2020-02-22 16:08:07.219000+00:00,163,"Performance, Athletes, Experimenters, Tinkerers, Endurance Sports"
Sub-classifying Lung Cancer with TensorFlow 2 and Keras,"Lung adenocarcinoma

Lung cancer continues to be a significant healthcare challenge. It is the leading cause of cancer death among men and the second leading cause of cancer death among women worldwide. Non-small cell lung cancer represents 85 % of all lung cancer cases.

Due to the recent availability of advanced targeted therapies, it is imperative to not only detect but also properly sub-classify non-small lung cancer into two major sub-types: squamous cell carcinoma and adenocarcinoma, which can be challenging at times even for experienced pathologists.

In this post, I go over the details on how I trained and tested a machine learning (ML) model to sub-classify non-small cell lung cancer images into squamous cell carcinoma and adenocarcinoma using TensorFlow 2 and Keras. In 2020, Google released TensorFlow 2 deep learning library with full Keras API integration, making it easier than ever to use. If you are new to convolutional neural networks, you can find a comprehensive guide on Medium and an in-depth description on Wikipedia.

Sections of this tutorial:

1. Choose Google Colab or local computer

2. Prepare training, validation and testing data directories

3. Import libraries

4. Specify paths to the training, validation and testing dataset directories

5. Normalize images and generate batches of tensor image data for training, validation, and testing

6. Visualize samples of training images (optional)

7. Build the convolutional network model

8. Compile and train the model

9. Evaluate the model

10. Assess trained model performance on the testing dataset

Let’s get started!

1. Google Colab or local computer.

A computer with a powerful NVIDIA GPU, will be best suited for this project. Excellent instructions on how to set up TensorFlow 2 with GPU support are provided in this YouTube video.

If you do not have a powerful NVIDIA GPU, then using Google Colab is an excellent alternative. It’s like using Jupyter Notebook in the cloud. Google Colab provides users with 12 hours of free computational time and access to GPU and even TPU. Good instructions on how to use Google Colab can be found in this Medium post.

2. Prepare training, validation, and testing directories.

For this project, I used an image dataset containing 5000 color images of lung squamous cell carcinoma and 5000 color images of lung adenocarcinoma from the LC25000 dataset, which is freely available for ML researchers. If you are going to use Google Colab, you need to upload images to your Google Drive.

Since I was using the Keras flow_from_directory method from the ImageDataGenerator class to generate batches of tensor image data for our model, I needed to organize the dataset into a specific directory structure outlined below. It is is a necessary step. Otherwise, the program does not work.

Required directory structure

My original dataset folder contained two sub-folders with two classes of images (lung squamous cell carcinoma class and lung adenocarcinoma class). I used a split-folder python package to divide my original dataset folder into training, validation, and testing dataset folders with the same two classes in each of the folders. I used 80% of images for the training dataset, 10% for the validation dataset, and 10% for the testing dataset. A good explanation of differences between training, validation, and testing datasets can be found here. To summarize, I the used training dataset to train the model (find optimal weights), I used the validation dataset to fine-tune the model (specify the number of hidden layers, epochs, dropout layers, etc.), and I used the testing dataset to assess the performance of the fully trained model.

3. Import libraries.

4. Specify paths to the training, validation, and testing dataset directories

I used my local PC for this project. If you plan to use the Google Colab, you need to mount your Google Drive first.

5. Normalize images and generate batches of tensor image data for training, validation, and testing

6. Visualize samples of training images (optional)

The output should look something like this:

Lung squamous cell carcinoma and adenocarcinoma sample training images

7. Build the convolutional network model

A great instructional video on how to design a convolutional neural network can be found here. I built my model with three sets of convolutional/pooling layers for feature extraction and two dense layers for classification. I added a single dropout layer (20% dropout rate) to prevent model overfitting. I used a 3x3 kernel (filter) size for convolutional layers, max-pooling for pooling layers, relu activation function for the deep layers, and sigmoid activation function for the output layer.

Model summary:

Our model summary

As you can see, the model had almost fifteen million trainable parameters.

8. Compile and train the model

Since I was doing binary classification, I used binary_crossentropy as a loss function. I used Adam as an optimizer. “Gentle introduction to the Adam optimization algorithm” can be found here. The number of steps per epoch comes from dividing the number of images by the batch size. For example, I had 8000 images in the training dataset and set the ImageDataGenerator training batch size to 40 images. Dividing 8000 by 40 comes to 200 steps per epoch.

9. Evaluate the model

After 20 epochs the model displayed training accuracy of 0.99 and validation accuracy of 0.95.

Last three epochs

Code for plotting accuracy and loss:

The output looked like this:

Training and validation accuracy and loss plots

10. Assess trained model performance on the testing dataset

Results of model performance on testing dataset

In summary, the trained model was able to classify previously unseen (testing dataset) non-small cell lung carcinoma images into squamous cell carcinoma and adenocarcinoma with 94 % accuracy.

You can find Jupyter Notebook file with the entire code for this tutorial on my GitHub repository. I hope you will find this tutorial helpful and I wish you the best of luck in your machine learning endeavors.",https://medium.com/analytics-vidhya/sub-classifying-lung-cancer-with-tensorflow-2-and-keras-616353e59e5e,['Andrew A Borkowski'],2020-01-29 12:32:51.659000+00:00,921,"Lung Adenocarcinoma, Non-Small Cell Lung Cancer, Machine Learning, Tensor Flow 2, Keras"
ML15: PyTorch — CNN on MNIST,"(5) Training CNN

record = [] # A container recording the training accuracies for epoch in range(EPOCH): train_rights = [] # Record the training accuracies for step, (x, y) in enumerate(train_loader): b_x = Variable(x)

b_y = Variable(y) cnn.train()

# Tell PyTorch that the model is running in training mode (training) output = cnn(b_x)

loss = loss_function(output, b_y)

optimizer.zero_grad()

# Indicate optimizer & loss function.

# Bear in mind that one must zero the gradients last time before back propagation.



loss.backward()

optimizer.step() right = rightness(output, b_y) # (outputs, labels) = (correct numbers, all samples)

train_rights.append(right) if step % 200 == 0: cnn.eval()

# Tell PyTorch that the model is running in evaluation mode (validation/test) test_output = cnn(test_x) pred_y = torch.max(test_output, 1)[1].data.squeeze()

test_accuracy = (sum(pred_y == test_y).item() / test_y.size(0)) * 100 train_r = (sum([tup[0] for tup in train_rights]), sum([tup[1] for tup in train_rights]))

training_accuracy = 100. * train_r[0].numpy() / train_r[1] total_step = len(train_data)//BATCH_SIZE print('Epoch [{}/{}], Step [{:4}/{}], Loss: {:.4f} | training accuracy: {:6.2f} % | test accuracy:{:6.2f} %'.format(

epoch+1, EPOCH, step+200, total_step, loss.data, training_accuracy, test_accuracy))



record.append((100 - 100. * train_r[0] / train_r[1], 100 - test_accuracy))

Figure 2: Training process.

train( ) and eval( )

As the names indicate, these functions tell PyTorch that the model is running in training mode (training) or evaluation mode (validation/test). This has some effect only if you want to turn off or on the modules, such as Dropout or BatchNorm. [4]

Bear in mind that train( ) & eval( ) would effect Dropout & BactchNorm, though actually we do neither of them in this CNN model (check the class CNN).",https://medium.com/analytics-vidhya/ml15-56c033cc00e9,['Morton Kuo'],2020-12-29 18:45:21.709000+00:00,244,"Training CNN, record, train_rights, b_x, b_y"
Geopandas Hands-on: Introduction to Geospatial Machine Learning,"Geopandas Hands-on: Introduction to Geospatial Machine Learning

Part 1: Introduction to geospatial concepts (this post)

Part 2: Geospatial visualization and geometry creation (follow here)

Part 3: Geospatial operations (follow here)

Part 4: Building geospatial machine learning pipeline (follow here)

In this post we are going to cover the preliminary ground of basic geospatial datatypes, attributes, and how to use geopandas to achieve these.

Table of Content:

What is Geopandas Installation Geospatial concepts Introduction to basic geometric attributes

What is Geopandas

Geopandas is open-sourced library and enables the use and manipulation of geospatial data in Python. It extends the common datatype used in pandas to allow for the many and unique geometric operations: GeoSeries and GeoDataFrame. Geopandas is also built on top of shapely for its geometric operation; its underlying datatype allows Geopandas to run blazingly fast and is appropriate for many machine learning pipelines that require large geospatial datasets.

Installation

Now that we are acquainted slightly with Geopandas, lets begin the installation process.

There are many options, but I would recommend installating using conda as it creates a new environment for your project without affecting other libraries in your system. To install conda, follow this link: installing conda.

Create a new conda environment and setup some configs

conda create -n geo_env

conda activate geo_env

conda config --env --add channels conda-forge

conda config --env --set channel_priority strict

2. Install with conda

conda install python=3 geopandas

Geospatial concepts

A. Geospatial common datatypes

There are some common geospatial datatypes that you need to be familiar with: Shapefile (.shp) and GeoJSON (.geojson).

Shapefile is a vector data format that is developed and maintained mostly by a company called ESRI. It stores many important geospatial information including the topology, shape geometry, etc.

Shapefile data format

GeoJSON, similar to JSON, stores geometry information (coordinates, projection, etc) in addition to your typical attributes relevant to the object (index, name, etc).

Example of GeoJSON format with the added “geometry” key inside the JSON object

Once you load either of these dataformat using Geopandas, the library will create a DataFrame with the additional geometry column.

GeoDataFrame (Source: geopandas.org)

This is how you import the default geodata built-in within the Geopandas library that we are going to use in this and subsequent posts.

import geopandas



path_to_data = geopandas.datasets.get_path(""nybb"")

gdf = geopandas.read_file(path_to_data)



gdf

Introduction to basic geometric attributes

Now that we have some ideas of geospatial data and how to import our very first one using Geopandas, lets perform some basic methods to further cement our understanding.

First, lets make the boroughs name as index to make our explorations easier.

gdf = gdf.set_index(""BoroName"")

AREA

From the geometry column, we can measure the areas (if they are of type POLYGON or MULTIPOLYGON: since we can’t measure the area of lines or points)

gdf[""area""] = gdf.area gdf[""area""]

>>BoroName

>>Staten Island 1.623822e+09

>>Queens 3.045214e+09

>>Brooklyn 1.937478e+09

>>Manhattan 6.364712e+08

>>Bronx 1.186926e+09

>>Name: area, dtype: float64

POLYGON BOUNDARY

Since our geometry is of type polygon or multipolygon, we can extract out the line coordinates of the objects. This can be useful when, say, we want to measure the perimeter of the polygon objects, etc.

Polygon Boundary Lines (Source: Wikipedia)

gdf['boundary'] = gdf.boundary >>gdf['boundary']

>>BoroName

>>Staten Island MULTILINESTRING ((970217.022 145643.332, 97022...

>>Queens MULTILINESTRING ((1029606.077 156073.814, 1029...

>>Brooklyn MULTILINESTRING ((1021176.479 151374.797, 1021...

>>Manhattan MULTILINESTRING ((981219.056 188655.316, 98094...

>>Bronx MULTILINESTRING ((1012821.806 229228.265, 1012...

>>Name: boundary, dtype: geometry

CENTROID

If you want to find the centroid point of the given polygons, you can call the gdf attribute as follows.

Polygon Centroids (source: Wikipedia)

gdf['centroid'] = gdf.centroid >>gdf['centroid']

>>BoroName

>>Staten Island POINT (941639.450 150931.991)

>>Queens POINT (1034578.078 197116.604)

>>Brooklyn POINT (998769.115 174169.761)

>>Manhattan POINT (993336.965 222451.437)

>>Bronx POINT (1021174.790 249937.980)

>>Name: centroid, dtype: geometry

DISTANCE

Now that wealready know the positions of the centroids and wanted to find out where the distance between Queens and everywhere else, this can be done easily using the distance() method:

queens_point = gdf['centroid'].iloc[1]

gdf['distance'] = gdf['centroid'].distance(queens_point)

You can then perform many spatial aggregates function to find out the mean, max, or min distances.

gdf['distance'].mean() #To find the mean distance

gdf['distance'].max() #To find the maximum distance

gdf['distance'].min() #To find the minimum distance

Conclusion

That’s it! Hope you learn something new today. In the next post, we will dig deeper into how these geospatial data can be visualized and created from scratch. Stay tuned!",https://towardsdatascience.com/geopandas-hands-on-introduction-to-geospatial-machine-learning-6e7e4a539daf,['Juan Nathaniel'],2021-08-10 04:33:53.345000+00:00,638,"Geopandas, Machine Learning, Python, Geospatial Data, Shapefile"
Getting into data visualization — where should I start?,"No coding

First, if you haven’t pushed Excel’s boundaries, it’s worth doing. Seriously. Learn pivot tables at least. It may sound lame, but Excel can do a lot more than people expect. It can even make pretty charts if you try hard enough.

If you have some data already and just want a good tool to explore it visually or to export more compelling charts, Tableau is incredibly popular and powerful. There is a free public version and a very expensive paid version which you can get for free as a student. It can publish to the web, or to static graphics to include in research papers, post to Instagram or print out as giant wall-sized charts. The Tableau Public website has a lot of quality examples posted for you to get inspiration from.

Sadly, the next “No coding” tool I like to recommend, Infoactive, is shutting down…but on the bright side it is because they were acquired by Tableau. This hopefully means good things for Tableau Public in the future. I will plug a free book spearheaded by the Infoactive team that is useful background on data visualization design using any of the tools I cover here:

Some coding

If I were picking one single programming language to use solely with data I would pick R. It’s free, supported by tons of ongoing development adding useful packages on top of the base language, and there are great free resources to learn it. First among those resources — I cannot recommend these Coursera classes highly enough:

Taking all of them might be overkill for a true beginner, but the track of classes walks a nice line from the introduction of key data science terms and ideas, through exploratory data analysis (which covers useful packages for R like ggplot, a very popular visualization tool) all the way to adding interactivity, publishing to the web via Shiny and storytelling with data.

R is what I use most frequently for small, quick analyses and ad hoc visualization — if you’ve got a dataset that Excel is struggling with (too big, not flexible enough, poor visualizations), R is perfect for exploring quickly.

This is also the time for a quick “yes, you should probably learn some SQL.” SQL is very targeted in scope compared to R (really, it’s far from an apples to apples comparison)—but if there are databases that you need to dive into to gather data for use with any of these other tools or languages, there is a good chance you’ll want to know SQL, and it will pay dividends in the long run.

I ♥ code

More often than not, the question of “where should I start?” comes in response to a fantastic interactive visualization presented on the web. I’m a huge fan of all the recent innovation in this area (see my in-depth survey of innovative work here).

Unfortunately, if you really love this piece:

…it can be disheartening to find out how much you have to learn to be able to build your own. It’s worth reiterating up front that “being as good as the New York Times” is a tough goal. A worthy one, but tough.

Fortunately, there are many great resources to help.

The library behind the interactive piece above, and many of the data visualizations running in the browser today is D3.js, created by Mike Bostock. If you want to publish online or make interactives, D3.js is a great tool to learn. This does mean you’ll need to learn some Javascript in general and then D3.js specifically.

Bostock’s website is a gold mine of examples and tutorials (you can’t beat learning from the creator of the library…). I’d also recommend Interactive Data Visualization for the Web by Scott Murray, which you can either buy from O’Reilly or work through for free online:

The online version is excellent — you actually write code snippets within the book itself, run them, and compare your output to interactive examples that run within the book itself too. Murray also does a nice job of targeting the book at beginners, walking you through the basics of how web browsers work, HTML/CSS and Javascript, before diving headlong into the details of D3.

One area to call out as a particular strength of D3 is geospatial visualizations. D3 is great at creating maps of many flavors, and there are nice dedicated tutorials available if that’s your area of focus:

D3 can be difficult to use directly, but there are many tools you can use on top of it to make your life a little easier. I’d recommend learning at least the basics of D3 rather than only using a more abstract plotting library, but if that proves intractable, a tool like Plot.ly can help make things feel more approachable.

Finally, if you really want to learn a do-it-all programming language that just happens to be great at data visualization, go with Python. Python is the most general purpose and powerful tool of anything I’ve listed, and it’s quite popular in the data science community.

I find Python very approachable as a multi-purpose programming language, but in truth it is probably overkill if all you want to do is explore and visualize data. Youtube is built with Python, for example…1 million lines of it. If you do go the Python route, the Code Academy course is a short (10–20 hours) and fun introduction to the language.

Finally, much like D3.js for Javascript or ggplot for R, there are many Python libraries dedicated to data visualization. Seaborn (which builds on an older popular library, matplotlib) and Bokeh are probably the best-in-class right now, but this is a quickly evolving and improving landscape. Both the Seaborn and Bokeh websites include galleries showing off the kinds of visualizations you can create with those tools.",https://medium.com/datavisualization/where-should-i-start-c53acdf04a1c,['Nick Brown'],2017-06-11 18:05:29.564000+00:00,947,"Excel, Tableau, Data Visualization, RR, SQL"
Alexa Conversations Is A New AI-Driven Approach To Conversational Interfaces,"Alexa Conversations Is A New AI-Driven Approach To Conversational Interfaces

But Does It make The Conversational Experiences More Natural?

Introduction

Looking at the chatbot development tools and environments currently available, there are three ailments which require remedy:

Compound Contextual Entities

Entity Decomposition

Deprecation of Rigid State Machine, Dialog Management

The aim of Alexa Conversations is to take voice interactions from one shot interactions to multi-turn interactions. More complex conversations like booking a flight, ordering food or banking demands multi-turn conversations.

One could say conversational commerce demands an environment to develop multi-turn conversations fast and efficient. Amazon must have recognized this and Alexa Conversations is their foray into addressing this need.

Compound Contextual Entities

Huge strides have been made in this area and many chatbot ecosystems accommodate these.

Contextual Entities

The process of annotating user utterances is a way of identifying entities by their context within a sentence.

Contextual Entity Annotation In IBM Watson Assistant

Often entities have a finite set of values which are defined. Then there are entities which cannot be represented by a finite list; like cities in the world or names, or addresses. These entity types have too many variations to be listed individually.

For these entities, you must use annotations; entities defined by their contextual use. The entities are defined and detected via their context within the user utterance.

Compound Entities

The basic premise is that users will utter multiple entities in one sentence.

Users will most probably express multiple entities within one utterance; referred to as compound entities.

In the example below, there are four entities defined:

travel_mode

from_city

to_cyt

date_time

Extract of NLU.md File In Rasa Project

These entities can be detected within the first pass and confirmation solicited from the user.

Entity Decomposition

The Microsoft LUIS Approach

Entity decomposition is important for both intent prediction and for data extraction with the entity. The best way to explain this is by way of an example.

We start by defining a single entity, called

Travel Detail.

Within this entity, we defined three sub-entities. You can think of this as nested entities or sub-types. The three sub-types defined are:

Time Frame

Mode

City

From here, we have a sub-sub-type for City:

From City

To City

Adding Sub-Entities: ML Entity Composed of Smaller Sub-Entities

Annotated Intent Examples

The leader in entity decomposition is Microsoft LUIS, you can read more about it here. I would say LUIS have a complete solution in this regards.

Amazon Alexa Conversations

Conversations have a similar option, though not as complete and comprehensive as LUIS. Within conversations you can define entities, which Amazon refers to Slots.",https://cobusgreyling.medium.com/alexa-conversations-is-a-new-ai-driven-approach-to-conversational-interfaces-fe8d2a562602,['Cobus Greyling'],2020-08-09 20:39:58.601000+00:00,391,"Alexa Conversations, AI-Driven Approach, Conversational Interfaces, Compound Contextual Entities, Entity Decomposition"
Latest picks: In case you missed them:,"Sign up for The Variable

By Towards Data Science

Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.",https://towardsdatascience.com/latest-picks-representation-and-bias-92474f80bf0a,['Tds Editors'],2020-12-21 14:27:18.548000+00:00,36,"data Science, tutorials, research, subscribe, The Variable"
Web Scraping in R,"Web Scraping in R

What is web scraping?

Scraping basically means the extraction of something. So, in Computer Science web scraping means the extraction of data from a website. This data is collected and then exported into a format that is more useful for the user.

Now, we are going to scrape data from the Amazon website. We will scrap data in the form of Prices of Laptops of two companies i.e HP and DELL. So, first of all, load some libraries that are going to help us in scraping

library(xml2)

library(rvest)

library(dplyr)

If you have not installed these packages, then you first install these by the following command

install.packages(""xml2"")

install.packages(""rvest"")

install.packages(""dplyr"")

xml2 library helps us to read Html text and, it extracts various components from nodes. rvest helps us to scrape information from web pages. dplyr library helps us in data manipulation i.e Data Sorting etc.

Now, go to the page you want to scrap. I have used a chrome extension “SelectorGadget” to inspect elements. Different browsers have different extensions for it i.e Mozilla Firefox has an extension with the name “ScrapeMate Beta” and other browsers have other extensions. So, you can add this type of extension for selecting data on any web page with your browser.

Step 1

Copy the link of the web page you want to scrap and store it in any variable. Then, read this HTML page.

Step 2

As I want to scrap the price and name of the item. So, I will select the price and name of the item, you can choose any one of them and, the links will be generated by the extension which contains prices and names of all the items on that page. There will be two separate links generate one for price and the other for the name.

As you can see that I have the selected price of one item which is in green and all the prices are selected automatically. The link is also generated, you can see it at the right of the bottom.

The other link will be generated like this by selecting the name of the item.

Now it’s time to write some code.

In this code “%>%” meaning is the same as

x=c(8,99,78,88)

x%>% plot()

OR

plot(x)

Both are the same. I hope you have understood. So, prices and names are stored in these variables you can see in the image.

Step 3

Create a data frame and convert it into a CSV file. So, that we can use it for visualization. And in the CSV file, we can more understand the information.

This technique is for only one web page. Now, if we want to scrape multiple pages.

Scraping of Multiple pages at a Time

If you have done scraping on one page, then it is not a difficult task for you to do. We have the same piece of code with minor changes. First, you need to look up at the link and find where the part of the link changes when you click next to go to the next page.

For example, the link is www.dummy.com/tittles/movie

Now, when clicking next then the link will be like this www.dummy.com/titiles/moviespage=1 or this type of any other changes in the link.

Firstly, I have created a for loop so that we can easily go to the next pages. In the link, I have put a variable name page_no, so the link will change as the loop iterate. If I only create a data frame then the data frame will only contain the data of the last page. Because It updates itself and contains the latest value. So, in this case, rbind helps us to come out from this problem. And it’s a good practice to replace NA(Not a Number) with 0. So, in the future when you want to apply some operation on the data you will have no problem with it.

Here you can see the CSV file.

Repeat the same process for HP laptops prices and names.

Analyze the Data

Now, we have the data which contains the prices of the laptops of both companies i.e dell and hp. We are going to analyze it by plotting them. So, that we can find the average price of both companies for core i7 laptop.

Dell Laptops Prices Average Prices

Hp Laptops Average Prices

You can easily understand from the pictures the average prices of core i7 laptops of both companies. The black line is clearly indicating the average of the prices.

Here in this blog, my purpose is not to tell you that what are the average prices of laptops today?. I only want to tell you that, we can analyze data very easily after scraping. So, even a common person understands it. And the R language helps us to do so. You can use the boxplot function in R to plot this type of data. You can also use many other plots like barplot, pie, histogram, etc. It’s all depends on what type of data you are working with.

I hope this blog will help you in doing scraping with R and analyze data.

Here you can find the complete code",https://medium.com/analytics-vidhya/web-scraping-in-r-cbb771cd0061,['Subhan Khaliq'],2020-12-15 16:34:58.012000+00:00,819,".web scraping, R programming, data extraction, web page scraping, HTML parsing"
Gadfly.jl — The Pure Julia Plotting Library From Your Dreams,"Introduction

If you have been working with Julia for even a short amount of time, you might be familiar with a plotting library called Plots.jl. Plots.jl is a simple library that provides ports to Python libraries such as Plot.ly and Matplotlib with simple functional methods like plot() and scatter(). While this is certainly a great way to get around creating an entirely new library for use in the Julia programming language, it also presents a few problems.

Firstly and most importantly, interpreting Python with Julia creates a lot of problems with performance. From just simply importing Plots.jl to actually plotting values, the performance with Plots.jl is absolutely abysmal! On top of not being Julian, a lot of the software is missing in the Julia version, and many features are entirely overlooked. While GR is likely the most promising back-end for Plots.jl, it lacks a lot of the great features that might be available for you with Pythonic libraries. Despite this, I do have an article you could check out to improve your GR visualization skills with Plots.jl:

So with these serious fundamental flaws with Plots.jl, it is easy to see why scientists have looked elsewhere for great statistical visualization in the Julia language. One of the best options I have looked at is a statistical graphing library called

Gadfly.jl.

First and foremost, it should be noted that Gadfly is very similar to another library for which I am the creator, called Hone.jl. The two both use the same back-end, Compose.jl, for drawing vector graphics in Julia. The similarities might not end there, but there are also a lot of key differences between them, such as Hone’s object-oriented and modular methodology which is not at all in line with Gadfly’s. Despite this, Gadfly does take a very interesting methodology in its own right, and works pretty differently than other plotting libraries that you might draw some similarities to.",https://towardsdatascience.com/gadfly-jl-the-pure-julia-plotting-library-from-your-dreams-3ee6ca107a5c,['Emmett Boudreau'],2020-09-24 16:29:07.240000+00:00,312,"julia, plots.jl, plotly, matplotlib, GR"
Quality Metrics for NLU/Chatbot Training Data / Part 2: Embeddings,"What are Embeddings? What is similarity, cohesion and separation?

UPDATE 2020/11/01: Botium’s free plan is live! With Botium Box Mini you will be able to:

use multiple chatbot technologies

set up test automation in a few minutes

enjoy a new improved user interface

get the benefits of a hosted, free service

Take it for a test drive

This article series provides an introduction to important quality metrics for your NLU engine and your chatbot training data. We will focus on practical usage of the introduced metrics, not on the mathematical and statistical background — I will add links to other articles for this purpose.

This is part 2 of the Quality Metrics for NLU/Chatbot Training Data series of articles.

For this article series, you should have an understanding what NLU and NLP is and about the involved vocabulary (intent, entity, utterance) and concepts (intent resolution, intent confidence, user examples).

What are Embeddings ?

Embeddings are a type of word or sentence representation that allows words or sentences with similar meaning to have a similar representation.

While this sounds complex, the concept is easy to understand when looking on this scatter chart and an example:

each colored dot represents a word or a sentence

the lower the distance between two dots, the more similar the words or sentences are (in this case: from a semantical point of view)

the higher the distance, the less similar they are

2D visualization of word embeddings

As an example:

“I’d like to order a drink”

“I want iced coffee”

“not interested”

The first two sentences will be rather close in the Embeddings space, while the third one will appear distant to both of the first two.

Mathematically speaking, an embedding is a vector in an n-dimensional space — the higher n, the more complex concepts can be handled. It is not a trivial task to map natural language into an n-dimensional space while considering semantical similarity. Fortunately, there are ready-to-use models available for the most-spoken languages, for example the Universal Sentence Encoder developed by Google.

An encoder is a neural network that takes the input, and outputs a feature map/vector/tensor — a point in n-dimensional space.

Reducing this n-dimensional vector into a 2D representation to be visualized on a flat scatter chart is a matter of Principal Component Analysis (PCA).

Using Embeddings for Training Data Analysis

When training an NLU engine for chatbots, you typically have labeled training data available — a list of intents each with a couple of training phrases for each intent. Our tool of choice for showing a sample data analysis workflow is Botium Box.

Botium first generates semantic embeddings of the training phrases by using the Universal Sentence Encoder module and visualizes them in a 2D-map. Based on the similarity between the training phrases, the average similarity between the intents is computed (separation), as well as the average similarity of phrases within an intent (cohesion). This approach helps to identify training phrases that might confuse your chatbot — based on the similarity in the embedding space.

Utterance Similarity

Training phrases in different intents that have high similarity value can be confusing to the NLU engine, and could lead to directing the user input to the wrong intent.

Utterance similarity

Intent Separation

Given two intents, the average distance between each pair of training phrases in the two intents is shown.

Intent separation

Intent Cohesion

Cohesion is the average similarity value between each pair of training phrases in the same intent. That value is computed for each intent. The higher the intent cohesion value, the better the intent training phrases.

Intent cohesion

Improve Chatbot Training Phrases

To improve the quality of the training phrases for your intents, consider the following approaches:

Find the phrases in different intents with high similarity in the Utterance Similarity table, and change or remove them

For intents with low cohesion, add more meaningful training phrases

For intent pairs with low separation, investigate training phrases

Give Botium Box a test drive today — start with the free Community Edition, we are happy to hear from you if you find it useful!

Looking for contributors

Please take part in the Botium community to bring chatbots forward! By contributing you help in increasing the quality of chatbots worldwide, leading to increasing end-user acceptance, which again will bring your own chatbot forward! Start here",https://medium.com/analytics-vidhya/quality-metrics-for-nlu-chatbot-training-data-part-2-embeddings-57aa341d81fa,['Florian Treml'],2020-11-09 16:35:21.710000+00:00,679,"NLU, NLP, Intent, Entity, Utterance"
Is Python Really a Bottleneck?,"What are the true bottlenecks in data processing?

Based on my own work, I usually experienced bottlenecks not in the language itself but rather in the external resources. To be more concrete, let’s look at several examples.

Writes to relational databases

When processing data in the ETL-fashion, we need to load this data in the end to some centralized place. While we could leverage multithreading in Python to write data to some relational database faster (by using more threads), the chances are that the increase in the number of parallel writes could max out the CPU capacity of that database.

In fact, this happened to me once when I was using multithreading to speed up the writes to an RDS Aurora database on AWS. I then noticed that the CPU utilization for the writer node went up so high that I had to deliberately make my code slower by using fewer threads to ensure that I wouldn’t break the database instance.

This means that Python has mechanisms to parallelize and speed up many operations, but your relational database (limited by the number of CPU cores) has its limits that will unlikely be solved just by using a faster programming language.

Making calls to external APIs

Working with external REST APIs from which you may want to extract data for your data analytics needs is another example where the language itself doesn’t seem to be a bottleneck. While we could speed up the data extraction by leveraging parallelism, this could be in vain because many external APIs limit the number of requests we can make within a specific time period. Thus, you may often find yourself making your script deliberately slower to ensure that you don’t exceed the API’s request limits:

time.sleep(10)

Working with Big Data

From my experience working with vast datasets, you can’t load really “big data” into your laptop’s memory regardless of which language you use. For such use cases, you will likely need to leverage distributed processing frameworks such as Dask, Spark, Ray, etc. There is a limit to how much data you can process when using a single server instance or your laptop.

If you want to offload the actual data processing to a cluster of compute nodes, possibly even making use of GPU instances that can further speed up compute, Python happens to have a large ecosystem of frameworks that make this task easy:

Do you want to speed up compute for data science by leveraging GPUs? Use Pytorch, Tensorflow, Ray, or Rapids (even with SQL — BlazingSQL)

Use Pytorch, Tensorflow, Ray, or Rapids (even with SQL — BlazingSQL) Do you want to speed up your Python code to process Big Data? Use Spark (or Databricks), Dask, or Prefect (that abstracts away Dask under the hood)

Use Spark (or Databricks), Dask, or Prefect (that abstracts away Dask under the hood) Do you want to speed up your data processing for analytics? Use fast specialized in-memory columnar databases that ensure high-speed processing just by using SQL queries.

And if you need to orchestrate and monitor data processing that occurs on a cluster of compute nodes, there are several workflow management platforms, written in Python, that will speed up the development and improve maintenance of your data pipelines, such as Apache Airflow, Prefect, or Dagster. If you want to learn more about those, have a look at my previous articles.

As a side note, I can imagine that some people complaining about Python don’t leverage it to its full capacity or may not be using proper data structures for the problem at hand.

To summarize, if you need to process large amounts of data quickly, you will likely need more compute resources rather than a faster programming language, and there are Python libraries that make it easy to distribute work across hundreds of nodes.",https://towardsdatascience.com/is-python-really-a-bottleneck-786d063e2921,['Anna Anisienia'],2020-12-22 22:22:33.298000+00:00,620,"data processing, relational databases, external APIs, big data, distributed processing frameworks"
3 Examples of why AI won’t take your Creative Jobs.,"Get this newsletter By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.

Check your inbox

Medium sent you an email at to complete your subscription.",https://medium.com/merzazine/3-examples-of-why-ai-wont-take-your-creative-jobs-b633a0c7430f,"['Vlad Alex', 'Merzmensch']",2020-06-08 13:21:18.191000+00:00,40,"Newsletter, Medium, Sign Up, Privacy Policy, Email"
Plotting Matrix using Python,"I have recently started to sharpen my machine learning skills and exploring couple of new interesting problems. In my college, I was quite good in mathematics but really haven’t seen so far its implementation but machine learning is place where I find out its true use case.

Let me start with something very basic and going forward in couple of blogs share my experience of machine learning — Shift from Data Engineering to Machine Leaning Engineering.

Today I used Python and Matlab API to plot a 2-D matrix. Its quite exciting and good to see something colorful. Specially for a person like me who has always spend his most of time in back-white screen(linux terminal).

Pre-requisite : I have installed Jupyter notebook (most popular in data scientist) using Anaconda.

Now I am going to create a matrix using numpy followed by matlab to plot.

Source Code :

import numpy as np

import matplotlib.pyplot as matrixPlot



#Create a matrix

A=np.matrix('1,2,3;9,4,7;-1,9,6')



#plot matrix

matrixPlot.ion() #Turns interactive mode on

matrixPlot.imshow(A, interpolation='none')

pyplot() function take multiple properties and one of them is interpolation property. You can explore by changing it someother values to checkout magic.

Lets set interpolation to ‘kaiser’

import numpy as np

import matplotlib.pyplot as matrixPlot



#Create a matrix

A=np.matrix('1,2,3;9,4,7;-1,9,6')



#plot matrix

matrixPlot.ion() #Turns interactive mode on

matrixPlot.imshow(A, interpolation='kaiser')

Reference : https://matplotlib.org/api/pyplot_api.html

Happy Coding..!!!",https://medium.com/@shashi-vish/plotting-matrix-using-python-f1109a74c38a,['Shashi Vishwakarma'],2020-12-08 03:42:13.697000+00:00,199,"machine learning, data engineering, Python, Matlab API, matrix plotting"
Moving from AI to IAI,"In a short post of Matt Webb he pinpointed the new Artificial Intelligence (AI) that is emerging. He refers to another article in the Wired of Stephen Levy were this specific new form of AI is spelled out: not the human brain is duplicated, the own computer generated intelligence is flourished.

I think this interesting stuff. And I like to connected it to a development Jesse Schell coined in one of his recent presentations: the Curiosity Gap. He stated that we develop to a situation where we all have easy access to all the knowledge we need, and the successful people will be defined by the level of curiosity to find the knowledge and be open for it.

Others have coined it the new You-web where knowledge will be relevant and connected to your profile. I use the term Impulse Shaped Services for some time now where services adapt to the context of that one user.

In all approaches you can see that we go from a so-called Artificial Intelligence to an Available Intelligence. Smart people use this available intelligence to relief non-critical tasks and distinguish from non-users. Available Intelligence is used ad-hoc and on demand, profiled. So we can say we will live with Augmented Intelligence.

The smart people are those that know which drawer to pull to use the available knowledge and transform this to value. The trick will be the instant character. The intelligence will be defined by the relevancy and of the knowledge and even the predictive character of it. Services will be providing us with the entrance to these source of instant intelligence. So we have a kind of Instant Available Intelligence.",https://medium.com/target-is-new/moving-from-ai-to-iai-360f2f9e15c3,['Iskander Smit'],2016-05-03 01:39:49.834000+00:00,273,"AI, Curiosity Gap, You Web, Impulse Shaped Services, Augmented Intelligence"
A taste of the power of BERT in 3 easy steps,"Dead easy way to try out BERT

One of the exciting aspects of machine learning is how the boundaries are getting moved out all the time. It’s even more exciting if you can get a taste of new technology without having to go through a lot of hassle. Since hearing about the BERT (Bidirectional Encoder Representations from Transformers) language representation model, I’ve wanted to get a chance to exercise it. BERT struck me as an idea that was both elegant and practical. Thanks to this text sentiment classifier you can get a taste of the power of BERT from a standing start in a couple of hours.

After reviewing the text sentiment classifier article, I followed the associated github readme which led me flawlessly through the steps to get the model set up:

Set up Docker — I have a brand new Windows 10 system, so I spent a good portion of the overall end-to-end time getting Docker going. It took a couple of restarts and toggling Hyper-V off and on to get Docker to work. It wasn’t too irritating but it didn’t do anything to dispel my impression of Docker being a diva on Windows. Deploy the model: run this command (e.g. in Powershell in Windows) to run the docker image and start the model serving API:

$ docker run -it -p 5000:5000 codait/max-text-sentiment-classifier

3. Load the model in a browser:

http://localhost:5000

That’s it! Now you’re ready to score some text. To do so, you simply have to enter the text that you want to score in the payload field, with each sample surrounded by double quotes and separated by commas, and click on Execute:

The results show up in the response body field:

You can see for these two randomly chosen examples (from the Amazon video review dataset available here) that the model detects the sentiment accurately.

Other than having to fiddle a bit to get Docker going, the only other hitch I encountered was the need to finesse the sample text a bit in Excel to remove embedded double quotes and line feeds.

If you want to exercise this text sentiment classifier on larger set of samples, I’ve prepared a subset of the Amazon video reviews that you can score with the model. The CSV file contains sample text that is ready to go, along with the positive and negative scores the model produced for each.

Thanks to the text sentiment classifier described in this article, I was able to get a taste of the power of BERT from a standing start in little over two hours. It’s great to be able to exploit a state-of-the are language model with so little hassle.",https://medium.com/@markryan-69718/a-taste-of-the-power-of-bert-in-3-easy-steps-9225c350ae81,['Mark Ryan'],2019-04-08 03:01:30.206000+00:00,434,"Machine Learning, BERT, Text Sentiment Classifier, Natural Language Processing, Artificial Intelligence"
Contextualizing Scale & Probability,"Photo by Niklas Ohlrogge on Unsplash

Recently, a friend got a call from her local pharmacy, informing her that she could come in early to get her COVID-19 vaccine, though there was a catch—she had originally arranged to receive the Pfizer mRNA vaccine, but the pharmacy only had the Johnson & Johnson’s Janssen vaccine available for the early slot.

She really wanted the Pfizer, but with J&J’s Janssen only being single dose, she decided to move forward with the offer, hoping that she would feel immediate relief and security with the understanding that she would be fairly well covered after a single shot. She would be one step closer to feeling like she was living free, unburdened by the menace that we have all been living with this past year.

One note about this friend before I continue—she is a class-A worrier, especially when it comes to her health. Her concerns and anxieties can be crippling, and she is persistently in need of reassurance.

So when the CDC and FDA called for pause on Johnson & Johnson brand vaccinations the following week, she commenced to panic.

Federal health officials advised the pause after six woman who had received the J&J Janssen vaccine had developed potentially fatal blood clots, with one incidence of death among them. At the time, roughly 7 million people had received the shot.

Photo by Sharon McCutcheon on Unsplash

Fortunately, with my recently acquired probability capabilities, I was well equipped to understand why the clot risk was negligible, and nothing to worry about.

Unfortunately, I overestimated a person who has not studied statistics’ ability to comprehend scale (and, consequently, I discovered a newfound appreciation for how difficult it was to teach concepts such as probability).

Here is substantially a snippet from our conversation:

“Don’t worry—the incidents of concern only occurred in six people of the over

seven million that have received the vaccine. Therefore, at the moment, our best estimation of such an event occurring to you is less than one in a million.” “Please don’t use one-in-a-large-number odds when trying to ease my concerns… When my dad had his health trouble, we were told his risk was only one in a hundred.”

Her concerns in the moment were deeply personal, but I would like to point out something to the reader, in the hope that were you to find yourself in a similar circumstance that provokes concern for your own wellbeing, you would be able to realize and embrace a sense of security.

One in a hundred is significantly more frequent than one in a million.

So much so, that were you to discover (due to some solar storm activity or whatever) that the likelihood that you would get struck by lighting in a given year were one in a hundred, it would be almost certain that you would get struck by lightning in your lifetime. Lightning would become one of the most haunting presences known to man, and not just in Zeus times.

Photo by Michael D on Unsplash

It would be up there with cancer, neurodegenerative diseases, and heart disease as a severe, lifelong concern that the most brilliant minds in the world would be attempting to remedy with endless effort every day. Healthy practices would still include exercise, but while outfitted in rubber instead of spandex.

On the other hand, if the likelihood of getting struck by lightning in a given year were one in a million, you would almost certainly live several hundred lifetimes before ever being struck.

Estimating based on an 80 year average lifespan, you would have a 7% chance of being struck within 875 lifetimes. Being struck within 6,250 lifetimes would be the chance outcome of a coin flip—50%. Were you to live in such a world, you would hopefully be less concerned about getting struck by lightning than you are now, in the world with the particular atmospheric electrodynamics that we happen to bear, because, well…

The odds of getting struck by lightning in a given year are around 1 in 500,000, per the CDC—that’s twice as likely as one in a million. I did the math.

Bonus terrifying note: also according to the CDC, 32% of lightning injuries occur indoors. You can’t hide.

Hopefully my digression for the J&J Jansenn vaccine illustrates my point well enough—keeping a sense of proportion when you hear numbers in a certain circumstance is useful. It is not, however, very well intuitive; it is learned.

I would also like to note that surely any risks and (substantial) benefits of any of the developed COVID-19 vaccines may vary among the alternatives. In the context of the Jansenn clotting, we had only seen fatalities among women who had received a particular anticoagulant, heparin. So, at the time, a person would only be at risk under those conditions, which was the reason that the FDA and CDC called for pause, to ensure that heparin would not be used to treat those vaccinated with the Jansenn.

Photo by Lyyfe Williams on Unsplash

Since this all happened, the accounting of total incidents of potentially life-threatening clots have gotten modestly worse. Other actions have also since been taken to avoid continued issues with both the J&J and AstraZeneca vaccines.

And a final thought for your loved ones—in times of crisis or severe distress, the numbers will not always help. Often we just need to be heard—to feel embraced and emboldened with care, to be built up in a world that makes us feel small.

In the moment, this may not be intuitive to some of us—I’m still trying to learn this, myself.",https://medium.com/@mike-flanagan/contextualizing-scale-probability-593cda24bf54,['Mike Flanagan'],2021-07-28 16:14:55.790000+00:00,908,"COVID-19, vaccine, Pfizer, Johnson&Johnson, Janssen"
A Complete Guide for Visualising and Understanding Convolutional Networks,"Disclaimer: If you are interested only in the code for DeConvNet, it is present at the end of the article with a ConvNet taken as an example.

Convolutional Networks (ConvNets), introduced by LeCun et al., 1989 have achieved state-of-the-art results in challenging visual classification tasks in recent times.

What caused this renewed interest in ConvNet models?

Availability of Training Sets Significant Increase in Compute Power Novel Model Regularization Techniques

There is a common analogy among practitioners that insights into the internal operation and behavior of these models or the reason how they achieve such good performance is a cumbersome task (if not impossible).

But is this analogy correct?

Without clear understanding of the reason behind architecture and functioning of CNNs (or any other model), these models are reduced to trial-and-error. From a model interpretability standpoint, this is deeply unsatisfactory.

Is it possible to interpret a ConvNet model?

Indeed, it is !

Zeiler et. al.¹ proposed a DeConvNet (Multi-layered DeConvolutional Network). The key features of the proposed DeConvNet are listed below :",https://medium.com/analytics-vidhya/a-complete-guide-for-visualising-and-understanding-convolutional-networks-dc26f71c979f,['Anant Kumar'],2020-11-11 16:07:42.929000+00:00,162,"Conv Nets, Le Cun, Compute Power, Model Regularization, De Conv Net"
ML.NET is awesome! Here’s why you need to pay attention…,"I’ve been playing around with NET Core v3 and the ML.NET machine learning framework.

And let me tell you, this stuff is awesome!

NET Core is really cool all by itself. It’s the multi-platform version of the NET framework: it runs on Windows, OS/X, and Linux. I’m running it directly on my Mac right now without using my Windows 10 virtual machine.

And ML.NET is Microsoft’s new machine learning library. It can run linear regression, logistic classification, clustering, deep learning, and many other machine learning algorithms.

ML.NET is a first-class NET library. There’s no need to use Python, you can easily tap into this library using any NET language, including C#.

Microsoft is pouring all their effort into ML.NET right now. This is going to be their go-to solution for all machine learning in NET going forward.

And it’s super easy to use. Watch this:

I built a simple classifier and trained it on a dataset of botanical data. My code loads a CSV file with the exact dimensions of Iris flower petal sizes, and uses it to train the model to correctly identify the type of each flower.

Here’s the code:",https://medium.com/machinelearningadvantage/ml-net-is-awesome-heres-why-you-need-to-pay-attention-cdcb19a00782,['Mark Farragher'],2019-03-14 21:47:28.956000+00:00,184,"NET Core, ML.NET, Machine Learning, Linear Regression, Logistic Classification"
Adding Authentic Integrity to Artificial Intelligence: The Upside of Responsible AI,"By Steven Mills and Cathy Carlisi

This is the second in a series of articles exploring how companies can become leaders in Responsible AI. The focus of the first article was on defining Responsible AI. In this article we share a perspective on why business leaders should care.

When developing AI systems, companies do not need to choose between protecting customers and growing the bottom line. This is a false choice. If AI is developed with an organization’s authentic purpose at its core, companies can deliver Responsible AI solutions that boost both their bottom line, protect their customers, and have a higher-order impact in the world.

In fact, there are social and business costs when companies ignore Responsible AI considerations. Improper data use, bias against protected groups, or erroneous outputs open the door to litigation, financial loss, and reputational damage. From a company improperly selling customer data, to discrimination in hiring algorithms, to improper cancer treatment recommendations — examples abound of organizations inadvertently creating AI that undermines business basics as well as basic human rights and may inadvertently move society backwards instead of forward.

However, Responsible AI should not be viewed as a risk avoidance mechanism, but rather a vehicle to intentionally and deliberately take action that is authentic and ethical. When an organization uses its Purpose and Principles as guardrails, their AI reflects their values. This is the foundation upon which to build meaningful upside for all stakeholders. In this article, we explore a range of benefits for organizations that pursue Responsible AI.

Benefits of Responsible AI

1. Brand differentiation

Increasingly, companies have grown more focused on staying true to their purpose and their foundational principles — and customers are rewarding them for these priorities. Today’s consumers want and expect businesses to be sensitive to the impact of their actions on society. That’s why artificial intelligence without authentic integrity will fail brands every time. Customers are increasingly making choices to do business with companies whose own choices demonstrate values aligned with their own. Companies that do well in delivering what BCG calls total societal impact (TSI) — the aggregate of their impact on society — boast higher margins and valuations. Companies must make sure that their AI initiatives are aligned with what they value, and the positive impact they seek to make through their Purpose. Otherwise, at best, you’ve been compliant when you could have grown connection with customers and employees in an increasingly competitive business environment.

2. Improved recruiting and retention

Responsible AI helps attract the elite digital talent that is critical to the success of firms around the world. Seventy-eight percent of tech workers want practical methods and resources to help consider societal impact when building products. They want guardrails based on an authentic Purpose. In the UK, one in six AI workers has quit their jobs out of frustration with what they considered to be harmful products. That’s more than three times the rate for the technology sector as a whole. And numerous protests have engulfed major companies over perceived ethical AI lapses.

Along with inspiring employees, Responsible AI can empower them. Think of robots delivering the necessary equipment to technicians at various points along an assembly line to boost productivity, or customization tools that help teachers tailor learning materials to an individual student’s needs.

Responsible AI can help schedule workers in ways that optimize both their own well being as well as the company’s. Rather than designing work schedules that aim for profit maximization over the short run, a Responsible AI approach would balance employee and company objectives. By building more sustainable schedules, companies will see employee turnover fall, reducing the costs of hiring and training, which alone exceeds $80 billion annually in the U.S.

3. Driving the bottom line

Finally, Responsible AI can be used to help build systems with more reliable and explainable outcomes, as well as better performance. These outcomes, based on authentic and ethical strengths, help build greater trust, drive enhanced loyalty, and can boost revenues. Major companies such as Salesforce, IBM, Microsoft, and Google all have public principles for Responsible AI. And for good reason, as people weigh ethics three times more heavily than competence when assessing a company’s trustworthiness. And that carries a heavy financial cost. In the United States, in the year following a data misuse incident companies lost one-third of revenue from affected customers.

Companies that practice Responsible AI — and that let their clients and users know it — can increase market share and long-term profitability.

Building Systems of Trust

A week doesn’t go by without a news report about improper uses of AI or system failure. But framing Responsible AI as a preventative measure against financial and reputational damage misses its true potential. For AI to achieve a meaningful and transformational impact on business, it’s essential that it is crafted around an organization’s distinctive Purpose so it can build transparency and trust that binds company and customer, manager and team, as well as citizen and society.

The best way to build that trust is through Responsible AI. Because, ultimately, AI will build the world around us. At BCG, our Purpose is to Unlock the Potential of Those Who Advance the World. Moving the world forward cannot be done superficially. To shift the way systems are designed, developed, and deployed is to create the right progress, to move the world in the right direction.

Next up in the series, we’ll discuss how organizations can make the shift to Responsible AI.",https://medium.com/bcggamma/adding-authentic-integrity-to-artificial-intelligence-the-upside-of-responsible-ai-60b8ccac5f00,['Bcg Gamma Editor'],2020-06-05 13:26:44.972000+00:00,896,"ResponsibleAI, Artificial Intelligence, Data Use, Business Leaders, Social Impact"
Top Python Libraries for Data Science,"Python is the most commonly used and one of the best programming languages for data science. This is because Python is an easy to use language and it has many open-source libraries that can be used for data science. Since Python is open-source, it is free and has an active community which makes it regularly updated. Here, I have discussed some of the top Python libraries for data science. I have divided these libraries into two: those used for data processing and modelling, and those used for data visualizations. Let’s go!

Libraries Used for Data Processing and Modelling

1. NumPy

NumPy means Numeric Python. It is used for scientific computations which include linear algebra, n-dimensional arrays and matrices vectorization and processing, Fourier transformation, random number processing. The main objects in Numpy are the n-dimensional or multidimensional arrays or matrices. Several operations can be carried out on these arrays, some of which are: Arithmetic Operations, Reshaping, Transposition, Flattening, Slicing, Stacking, Splitting, Broadcasting and many more. NumPy is a fundamental Python library as many other libraries e.g Scipy were built on it.

2. SciPy

SciPy library is different from SciPy stack. SciPy library is built on NumPy and is one of the main packages in SciPy stack. It has several submodules which can be used for statistics, linear algebra, integration, interpolation, optimization etc. Because SciPy is built on NumPy, its main objects are multidimensional matrices as well. SciPy has great documentation which makes it easy to use.

3. Pandas

Pandas is called Python Data Analysis Library. It has two main data structures which are:

i. Series (a 1-D array which can hold any data type)

ii. Data Frame (a 2-D tabular data structure with rows and columns)

Pandas is a great tool for data analysis, data wrangling, data manipulation, data aggregation, handling missing values, simple visualizations and many more. Pandas are also used for reading and writing datasets or files of various formats such as SQL, CSV, Excel, Text etc.

4. Scikit-learn

Scikit-learn is primarily used for Machine Learning operations such as Regression, Classification, Decision Clustering, Model Selection, Dimensionality Reduction and Preprocessing. It can also be used for data analysis and mining. Scikit-learn is built on Numpy, Pandas, Scipy and Matplotlib libraries and so it interoperates with them.

Libraries Used for Data Visualization

5. Matplotlib

Matplotlib is used for data visualization and 2-D plotting. It is the most widely used Python plotting library. Matplotlib can be used on several platforms such as Jupyter notebook, Python and IPython shells, Web application servers etc., and also for embedding plots into applications through its object-oriented API. Matplotlib library is used to produce line plots, histograms, pie charts, bar charts, scatterplots, stem plots and many other visualizations.

6. Seaborn

Seaborn provides a high-level interface for creating informatory and appealing statistical graphics. It is based on Matplotlib library and is closely integrated with the NumPy and Pandas data structures (arrays, series and data frames). Seaborn helps in data exploration and understanding. Seaborn has a broad gallery of visualizations which include histograms, bar charts, pair plots, heatmaps, violin plots, boxplots, cluster maps etc.

7. Plotly

Plotly can be used to create sophisticated visualizations like contour plots, tenary plots and 3-D charts which are rare in other visualization libraries. Plotly provides a large number of unique graphs e.g heatmaps, histograms, area charts, scatter plots, multiple-axes, subplots, polar charts, bubble charts and many more.",https://medium.com/analytics-vidhya/top-python-libraries-for-data-science-7471e3aabd8b,['Peace Ikeoluwa Adegbite'],2020-11-10 12:59:02.099000+00:00,545,"It is also used for creating interactive visualizations which can be embedded in webpages.Python, Data Science, Num Py, Sci Py, Pandas"
Announcing the AI+ Data Science Live Training Subscription Service,"It’s no secret that the data science and AI industries are both rapidly advancing and highly competitive. As a result, it’s essential for data scientists to be building their knowledge base and learning new skills and technologies. To facilitate this, we are thrilled to announce that we will be launching a brand-new subscription service for data science live training on the AI+ Training platform on October 19th, 2020.

This new subscription service is an easy way to access all that AI+ Training has to offer. Our expansive collection of on-demand, hands-on training sessions, and workshops are led by world-renowned instructors and speakers and focus on the topics and tools you need to succeed in your role.

Don’t know what you don’t know? Our skills assessments can help you identify knowledge gaps and how to fill them with personalized training recommendations. To ensure you gain proficiency in your desired topic, we also offer certification exams in specific AI domains.

Plus, four distinct tiers of service of data science live training, we have a plan that fits every budget and need.

Free:

With our free plan, you’ll get access to:

The Course of the Week

Live and On-demand Webinars

Learning Guides

Online Support

Basic:

For just $29.99/month you’ll get access to everything from the Free plan plus:

AI+ Learning Path Assessment

Personalized Training Recommendations

Learning Tracks

Interactive On-demand Training Library

Certificate of Completion

Career Lab & Training Live Webinars

10% Discount on Live, Hands-on Instructor-led courses

$100 Credit for ODSC Conferences

Premium:

For just $59.99/month you’ll get access to everything from the Basic plan plus:

35% Discount on Live, Hands-on Instructor-led courses

Coding Assessments

ODSC Certification Practice Assessments

ODSC Certification Courses

ODSC Certification Exams

Premium Members Networking Channel

$500 Credit for ODSC Conferences

Team:

Our Team plan offers you a customized training experience, with a customized price. Choose from everything offered in the Premium plan plus:

Private and Custom Training

Pre- and Post-Training Assessment

$1,000 Credit for ODSC Conferences

Be one of the first to sign up for the AI+ Training subscription service to receive our limited-time discount on a Basic or Premium plan. Individuals who join the service by October 18th will pay just $21.99/month for a Basic plan or just $49.99/month for a Premium plan. Plus, you’ll get a 14-day free trial, and you can cancel any time.

Enjoy your AI+ Training subscription from the official launch date, October 19th, until November 2nd for free. We won’t start billing you until November 3rd.

You can learn more about everything a monthly subscription to AI+ Training has to offer here: https://aiplus.odsc.com/pages/plans

Finally, below you’ll find a sneak peek of the Live Training Sessions coming to AI+ this Fall.

“SQL Bootcamp for Data Scientists” with Mona Kahlil, Data Scientist, Greenhouse Software

“Modeling and Machine Learning in Theory and Practice” with Dr. Kirk Borne, Principal Data Scientist, Booz Allen Hamilton

“Animating Data: Turn any Matplotlib plot into an animated gif” with Max Humber, Distinguished Faculty Member, General Assembly

“Gradient Boosting for Prediction and Inference” with Brian Lucena, PhD, Principal, Numeristical

Get a ticket for ODSC West and try out AI+ for 3 months!

Interested in ODSC West 2020 and want to try out AI+ before you commit to it for longer? When you purchase a ticket to ODSC West (all ticket types but the general pass and Ai x business passes), you automatically get 3 months of the AI+ Basic subscription for free and start your data science live training adventure today.",https://medium.com/@ODSC/announcing-the-ai-data-science-live-training-subscription-service-f41869f755bd,['Odsc - Open Data Science'],2020-10-19 13:02:21.970000+00:00,536,"data science, AI, live training, skills assessment, certification exams"
Evaluation of Linear Regression Models,"Linear regression models are used to show or predict the relationship between two variables or factors. The factor that is being predicted is called the dependent variable and the factors that are used to predict the value of the dependent variable are called independent variables.

Evaluating a machine learning model is as important as building it. We are creating models to perform on new and unseen data. Hence, we need to evaluate if our model is performing correctly. Evaluating a Linear Regression model is not easy because there are a lot of evaluation metrics. When to use which metric depends on the data and problem of the project.

In this post, I will go over some evaluation metrics for Regression models.

R Squared(R²)

R-squared is a goodness of fit measure for linear regression models. This indicates the percentage of the variance in the dependent that the independent variables explain collectively. R-squared measures the strength of the relationship between the model and the dependent variable. R Squared value is between 0 to 1 and a bigger value indicates a better fit between prediction and actual value. Here is the formula for R-squared and the calculation of R² with sci-kit Learn is the following:

from sklearn.metrics import r2_score

true = [3, 4.5, 5, 6, 10]

preds = [3.1, 5, 3.5, 5.9, 8]

r2_score(true, preds)

Mean Absolute Error(MAE)

Mean Absolute Error is a measure of errors between observations and predictions. It is the average magnitude of the errors in a set of predictions, without considering their directions. It is the absolute value of error between actual and predicted value. Following is the formula and way to calculate with sci-kit learn.

from sklearn.metrics import mean_absolute_error

mean_absolute_error(true, preds)

Mean Squared Error(MSE)

Mean Squared Error is the sum of the square of prediction error. Mean Squared Error is similar to Mean Absolute Error. Mean Absolute Error takes the absolute value of error but Mean Squared Error takes the square of error. MSE penalize big prediction error by square while MAE treats all the errors the same.

from sklearn.metrics import mean_squared_error

mean_squared_error(true, preds)

Root Mean Squared Error(RMSE)

Root Mean Squared Error is the square root of the mean squared error. RMSE is always non-negative and a value of 0 would indicate a perfect fit to the data. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. Following is the formula of RMSE and how to calculate RMSE in python.

from sklearn.metrics import mean_squared_error

math.sqrt(mean_squared_error(true, preds))

Mean Absolute Percentage Error(MAPE)

Mean Absolute Percentage Error measures the accuracy as a percentage and can be calculated as the average absolute percent error for each time period minus actual values divided by actual values.

import numpy as np



def mape(actual, pred):

actual, pred = np.array(actual), np.array(pred)

return np.mean(np.abs((actual - pred) / actual)) * 100 mape(true, preds)

Conclusion

R Squared indicates the percentage of the variance in the dependent that the independent variables explain collectively. In other words, it explains how good the model is. But all other measures like MAE, MSE, RMSE, and MAPE measures how far the predictions are from the actual values.

There is no optimal metric to use for all tasks. It is important to consider what question you are trying to answer when deciding what evaluation metric to use. You have to decide which evaluation metric to use depending on your data and your problem.

Gain Access to Expert View — Subscribe to DDI Intel",https://medium.datadriveninvestor.com/evaluation-of-linear-regression-models-257851e77e28,['Jagandeep Singh'],2020-12-02 19:34:56.254000+00:00,548,"linear regression, R Squared, MAE, MSE, RMSE"
Extracting relevant information from Telegram Groups and Making it Representable 😊,"I have been a part of a telegram group, ‘Eradicate Diabetes’, which was made with purpose of advising people on LCHF diets and advising how to reduce to sugar levels naturally and lead a healthy life.

It has a lot of information in the form of question answers by 2 experts named ‘Raj’ and ‘Tim’. And also so many chit chats and food images and recipes.

Requirement

The requirement was to extract relevant information from this group and get a .csv file in form of question answer pairs.

It was just a vague requirement and in the steps ahead you will see how we got ahead and moved towards its solution seeing the requirements and challenges on the way, yes it was a journey.

Solution steps:

Step 1 :

To extract the chats of group in .json format. We zeroed down to json format as it is easy to parse.

Step2:

To read json file by Python for which code was written as below:

Step 3:

Identify the data which is of our use. For this we tried to print the data frame df as below:

Now we observed that want to take out messages out of it so we wrote:

To get the whole data frame length we can say:

Lendf =len(df[‘messages]).

Step 4: To identify relevant fields to build up the logic.

Now if we open json file it is in form as below:

Fields that we have to consider:

1) ‘ reply_to_message_id”: Whenever there is a reply to some question this id is generated, which will be used to extract answers. It has value of message id which has question asked to the answer.

2) ‘id’: This is message id which we will use to locate where a question is asked.

3) ‘text’: It is the message related to the Id concerned.

4) ‘from’: This id is indicator of who asked question or answer, basically who is the one who is chatting.

Step 5: Making logic to extract relevant data.

Now using all the fields in step 4 we made our logic.

pf=df[‘messages’]

The code below filters out all messages which has a ‘reply_to_message_id’ field, have some text, and has Tim or Raj our experts as the one giving answers. This will extract all answers that are given by ‘Tim and Raj’.

Now we need to extract questions asked, so we take the id given in a’reply_to_message_id ‘field, and scroll up to the message id which has its question.

For the logic above code can be written as below, an added logic here is that question asked should not be from Tim or Raj as we want to just extract answers from them for queries asked by people.:

When we have done this we can print question and answers with the code below:

Step 6: Data Cleaning -Remove the irrelevant.

Now we get the file required but it has a lot of issues like:

1) Lot of Good Morning and replied to wishes messages.

2) Relevant data was less.

3) Lots of junk characters.

We had to filter out these in a meaningful manner. The output files looked as below:

a) To get relevant question and answers only we put a filter for Questions , those strings having keywords as ‘can ‘,’what ‘,’where ‘,’when ‘,’how ‘,’which ‘,’who ‘,’why ‘,’suggest ‘, only will be included in questions and rest will be removed.

You have to notice the space given after each word in list just to ensure we capture relevant word and not in concatenation with other words like (‘cant, whichever, whoever etc.)

For this code is as below:

b)To remove curly braces and text between it ,we removed it by using the following code:

To use the above code remember we need to import re(regular expression library of Python.

Now the data that we get is very clean and has only relevant question and answers.

We had to remove the ‘Good Morning’, ‘Good Evening’ messages but by approach in step a) above we only got relevant question answers and wishes messages only if it has a question it. It was a different approach but it worked 😊.

If there is a need to remove emojis too it can be done by the python library use of ‘emoji’ and finding it in text by use of ‘emoji.UNICODE_EMOJI’.

Step 6:

Last but not the least we needed to export the chat to csv file which can be easily done by following code:

To use this remember we need to import os, sys and subprocess.

Thanks for reading!",https://medium.com/the-innovation/extracting-relevant-information-from-telegram-groups-and-making-it-representable-1b70d4fe2d6f,['Namrata Kapoor'],2020-09-28 18:00:55.185000+00:00,712,"diabetes, LCHF, healthy-life, question-answers, csv"
Analyze Trip Advisor Hotel Reviews: LDA Topic Modeling,"What makes a hotel good/bad? What matters to the travelers? Let’s find out.

Hotels play a crucial role in traveling and with the increased access to information, new pathways of selecting the best ones emerged.

With this dataset, consisting of 20k reviews crawled from Trip advisor, I will apply LDA to:

Discover top topics and keywords mentioned in the reviews

Explore the key aspects that make a hotel good or bad

Latent Dirichlet Allocation (LDA)

A Topic Model is a type of statistical model for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.

Let’s get started!",https://medium.com/analytics-vidhya/analyze-trip-advisor-hotel-reviews-lda-topic-modeling-2c1a56e35a09,['Chloe Wei'],2020-12-06 14:08:02.846000+00:00,131,"Hotels, Travelers, Trip Advisor Reviews, Latent Dirichlet Allocation (LDA), Topic Modeling"
An Ultimate Cheat Sheet for Data Visualization Techniques in Seaborn,"Let’s start with importing required libraries and loading the dataset which contains some built-in datasets in seaborn. The “tips” dataset, one of built-in datasets, will be used.

1.DISTRIBUTION PLOT

A)Distplot: A distplot is used a univariate distribution of observation.

tips[‘total_bill’]: It is a numeric column in tips dataset.

hist_kws: The keyword argument to change histogram format.

bins: It is used to set the number of bins, and it depends on your datasets.

kde: Kernel Density Estimate.

2.CATEGORICAL PLOT

A categorical variable has two or more categories, nonnumeric values. For instance, “sex” column is a categorical variable column, having two categories (male and female) at “tips” dataset. “day” column is a categorical variable column, having a number of categories (thursday, friday, saturday, sunday, etc.) at “tips” dataset. Categorical plot can be used to visualize categorical variables.

A)Bar Plot: Bar plot is useful for displaying relationships between categorical variable and numerical variable. Bar plot has two methods. Dataset’s columns can be used or dataset’s columns assigned to x and y parameters are used.

Method 1

Method 2

hue: determines which column in the data frame should be used for colour encoding.

B)Count Plot: Count plot shows the counts of observations in each categorical variable.

C)Box Plot: Box plot visualizes summarizing numeric data over a set of categorical variables, and it provides some information about data. This information consists of minimum score, maximum score, first quartile (25% of data), second quartile-median (50% of data), and third quartile (75% of data).

D)Violin Plot: Violin plot is a similar role as a box plot.

E)Strip Plot: Strip plot is a scatter plot. It shows the relationship between two variables.

F)Swarm Plot(Violin Plot + Strip Plot): As the name suggests, it consists of the combination of a violin plot and a strip plot.

G)Cat Plot(Former Name: Factor Plot): This plot provides the relationship between a numerical and one or more categorical variables with several visual options.

H)Point Plot: Point plot shows an estimate of central tendency(center of the data distribution) for a numerical variable.

3.MATRIX AND GRID PLOT

A)Heat Map: The heat map shows which variables are correlated to each other. It helps to understand correlation, and it provides a colorful visualization about correlation.

B)Pair Plot: The pair plot shows pairwise relationships in a dataset.

C)FacetGrid: FacetGrid helps to understand distribution of one variable as well as the relationship between multiple variables with using separate subsets.

4)JOINTPLOT

Jointplot visualizes two variables with bivariate and univariate graphs.

5)LMPLOT

Lmplot can be used for drawing a scatter plot onto a FacetGrid.",https://medium.com/clarusway/an-ultimate-cheat-sheet-for-data-visualization-techniques-in-seaborn-451a22052a92,[],2020-12-26 17:08:05.687000+00:00,396,"distplot, categorical plot, bar plot, count plot, box plot"
Explain Like I’m Five: Linear Regression in Machine Learning,"Explain Like I’m Five: Linear Regression in Machine Learning

Photo by Jeswin Thomas on Unsplash

Linear Regression is one of the most traditional Machine Learning methods. To understand Linear Regression, just imagine it like this, “Will it rain or sunny today if the sky is cloudy, the weather forecast says that today will rain, and it is currently Summer, where rain is more unlikely”. We can define each event into independent features that correlate to the outcome — will it rain or not? And each event may affect the outcome much stronger than other events, for example, looking at the sky directly to check whether it will rain is much more likely and informative than looking at the calendar and knows that it is currently Summer. However, we assume that each of these events doesn’t affect the other, only the outcome.

Photo by Heidi Fin on Unsplash

Linear Regression tries to calculate how each feature independently affects the outcome (or the relationship between them). We can write the formula for Linear Regression as the following:

y = w0 + (w1 * x)

Where y is the output we’re trying to find, x is the input or feature that correlates to the outcome, w0 is the intercept, and w1 is the regression coefficient (or slope). Since we can know x and y from the data, we need to calculate the w0 and w1 ourselves.

Linear Regression Model by Wikipedia

Think of the regression coefficient as how important the feature is — the higher the (absolute) w1 value, then the output will be affected much greater. For example, if y is the chance of today will rain, and x is the humidity, if w1 is 1.05, then for every 1 point of x change, y will increase by 1.05. However, if w1 is 0.0001, then even with 1000 points of x change, y will increase by only 0.1.

The reverse also happens if the w1 value reaches the negative value, for example, if w1 is -1.15 then for every 1 point of feature (x) change, the predicted outcome (y) will decrease by 1.15 point, which means that there is a strong correlation between the feature and the outcome. The correlation will decrease as w1 gets near around the 0 value.

Correlation by SPH — Boston University

Alongside the regression coefficient, we also have intercepted, think intercept as the offset between the target and feature. A very simple example, if w1 is 1 (which means for every 1 point of x, then y will increase by 1), then x is 5, and then y is 10, then the offset between x and y is 5 (since y is supposed to be 5).

However, with the simple implementation, Linear Regression is rarely used for practical real-life problems, because the method is simple and does not account for many things such as uncertainty, the dependency between variables, and non-linear problems.",https://medium.com/bina-nusantara-it-division/explain-like-im-five-linear-regression-in-machine-learning-f0e603ee335b,['Limas Jaya Akeh'],2020-12-26 06:49:04.802000+00:00,473,"Linear Regression, Machine Learning, Correlation, Regression Coefficient, Intercept"
Expanding West Coast Resilience,"At One Concern we have a vision of planetary scale resilience, where everyone lives in a safe, sustainable and equitable world. This is our north star and our guiding ambition, but we aren’t naive about what it will take to achieve it.

We are painfully aware that the world we inhabit is rife with disaster and hardship. Here in the United States, 2017 was the most expensive year for natural disasters in the country’s history, with a total cost of more than $300 billion. The West Coast alone suffered a staggering $20 billion in damage. Tragically, 59 people in California lost their lives and thousands more up and down the coast had their livelihoods disrupted.

However, in the face of these sobering statistics, we know that a commitment to resilience can and will save human lives and livelihoods. That’s what motivates me and my team here at One Concern. To us, nothing is larger than life. That’s why we are working so hard to help communities build a web of resilience up and down the West Coast.

A Web of Resilience — Today Cupertino, Tomorrow Your City?

This is why we are so excited that the city of Cupertino, home of Apple, has decided to partner with us to become the newest member of a West Coast resilience network — with more communities preparing to join in the coming weeks

(L-R) Nicole Hu, One Concern CTO | Ahmad Wani, One Concern CEO | Cupertino Mayor Darcy Paul at the January 31, 2018 State of the City Address

“I commend Mr Wani and Ms Hu for their concern and dedication… efforts like theirs are critical in ensuring that we are prepared as we can be in emergency situations … You can imagine how powerful [One Concern’s technology] could be in a situation where you’re trying to prepare. Where you’re trying to drill and respond” ~ Cupertino Mayor Darcy Paul

Through our application of benevolent AI and a combination of human expertise, science and deep learning, we are empowering cities like Cupertino, San Francisco, Los Angeles, Woodside and Portola Valley with insights that help the dedicated emergency response professionals and public servants in these communities better prepare for, mitigate against, respond to and recover from natural disasters.

We are so proud to be working with our partners in these communities and are honored by the solemn trust they have placed in us. This is only the beginning though.

We will be adding new partners to our expanding web of resilience over the coming weeks and months. Not only cities and towns, but also civic organizations and private companies, all of which comprise a deeply interconnected resilience ecosystem.

Each new partner will have the opportunity to share their experience and expertise across the system; leveraging the expertise of each individual member to increase the network’s strength for the benefit of all.

The work of building a more resilient West Coast starts now. Come join us!

~ Ahmad Wani

Co-founder and CEO, One Concern

www.oneconcern.com",https://medium.com/one-concern/expanding-west-coast-resilience-4cbdbb210897,['One Concern'],2018-02-01 20:14:53.297000+00:00,487,"resilience, West Coast, Cupertino, naturaldisasters, AI"
Stop using Pandas and start using Spark with Scala,"Data transformations

Most (if not all) of the data transformations you can apply to Pandas DataFrames, are available in Spark. There are of course differences in syntax, and sometimes additional things to be aware of, some of which we’ll go through now.

In general, I’ve found Spark more consistent in notation compared with Pandas and because Scala is statically typed, you can often just do myDataset. and wait for your compiler to tell you what methods are available!

Let’s start with a simple transformation, where we just want to add a new column to our Dataset, and assign it constant value. In Pandas this looks like:

Pandas df_teams['sport'] = 'football'

There’s a small difference in Spark, besides syntax, and that’s that adding a constant value to this new field requires us to import a spark function called lit.

Spark import org.apache.spark.sql.functions.lit val newTeams = teams.withColumn(""sport"", lit(""football""))

Note that we’ve created a new object as our original teams dataset is a val, which means it’s immutable. This is a good thing as we know that whenever we use our teams Dataset, we always get the same object.

Now let’s add a column based on a function. In Pandas this will look like:

Pandas def is_prem(league):

if league == 'Premier League':

return True

else:

return False

df_teams['premier_league'] = df_teams['league'].apply(lambda x:

is_prem(x))



To do the same in Spark, we need to serialise the function so that Spark can apply it. This is done using something called UserDefinedFunctions. We’ve also used a case match, as this is a nicer implementation in Scala than the if-else, but either will work.

We will also need to import another useful spark function, col, which is used to refer to a column.

Spark import org.apache.spark.sql.functions.col def isPrem(league: String): Boolean =

league match {

case ""Premier League"" => true

case _ => false

}

val isPremUDF: UserDefinedFunction =

udf[Boolean, String](isPrem)

val teamsWithLeague: DataFrame = teams.withColumn(""premier_league"",

isPremUDF(col(""league"")))

Now that we’ve added a new column that isn’t in our case class, this will convert it back to a DataFrame. So we either need to add another field to our original case class (and allow it to be nullable, using Options), or create a new case class.

An Option in Scala just means the field is nullable. If the value is null we use None , and if populated we use Some(""value"") . An example of an optional string:

val optionalString : Option[String] = Some(""something"")

To get the string from this we can call optionalString.get() , and this will just return ""something"" . Note that if we’re not sure whether it will be null or not, we can use optionalString.getOrElse(""nothing"") which will return the string ""nothing"" if null.

Filtering a Dataset is another common requirement, which is a good example of where Spark is more consistent than Pandas, as it follows the same pattern as other transformations, where we do dataset “dot” transformation (ie dataset.filter(...) ).

Pandas df_teams = df_teams[df_teams['goals_this_season'] > 50]

Spark val filteredTeams = teams.filter(col(""goals_this_season"") > 50)

We are likely to need to perform some aggregations on our dataset, which is very similar in Pandas and Spark.

Pandas df_teams.groupby(['league']).count()

Spark teams.groupBy(""league"").count()

For multiple aggregations, we can again do something similar to Pandas, with a map of field to aggregation. If we want to do our own aggregations we can use UserDefinedAggregations.

teams.agg(Map(

""matches_played"" -> ""avg"",

""goals_this_season"" -> ""count""))

Often we also want to combine multiple Datasets, which may be with union:

Pandas pd.concat([teams, another_teams], ignore_index=True)

Spark teams.unionByName(anotherTeams)

… or with a join:

val players: Dataset[Player] = spark

.createDataset(Seq(neilMaupey, sergioAguero))

teams.join(players,

teams.col(""top_goal_scorer"") === players.col(""player_name""),

""left""

).drop(""player_name"")

In this example we have also created a new Dataset, this time using a case class called Player. Note that this case class has a field injury, which can be null.

case class Player(player_name: String, goals_scored: Int,

injury: Option[String])

Notice that we’ve dropped the player_name column as this will be a duplicate of top_goal_scorer.

We may also want parts of our code to just use Scala native data structures such as Arrays, Lists, etc. To get one of our columns as an Array, we need to map to our value and call .collect() .

val teamNames: Array[String] = teams.map(team => team.name)

.collect()

Note that we’re able to use the case class’ inbuilt getters to return the name field, and this won’t compile if name is not a field in our class FootballTeam.

As an aside, we can add functions to our case classes too, and both values and functions will come up as options for autocompletion when using an IDE such as IntelliJ or vs code with Metals plugin.

To filter our Dataset based on whether it exists in this Array we need to treat it as a sequence of args, by calling _*.",https://towardsdatascience.com/stop-using-pandas-and-start-using-spark-with-scala-f7364077c2e0,['Chloe Connor'],2020-06-07 18:56:07.675000+00:00,729,"Data Transformations, Pandas, Spark, Scala, User Defined Functions"
Predicting Startup performance using Logistic regression,"Here are my jupyter notebooks from scraping to modeling in case you are interested :)

1. Data Acquisition

After researching a few different startup databases, I chose angelist.co for its decently detailed data and its freely accessible nature. Below is an appearance of the site to give you a sense of the data we are dealing with.

Angelist companies page

The above page features a huge amount of startup companies, and for each startup, the following data is presented:

Time joined (Month and year when startup joined angelist database)

Location of startup

The market the startup is in (E-Commerce, Healthcare, B2B, SaaS, etc…)

Employee size (1–10, 11–50, 51–200, etc…)

Current startup stage (Seed, Series A, … , IPO, Acquired)

Total dollars raised

I wrote a web scraper to acquire the above data, randomly sampling 4260 unique startups from Angelist. Below is a snippet of the data I scraped.

Table 1: Scraped data

Evaluation Metric

It bugged me whether to frame this as a regression problem to predict some dollar value or as a classification between “good” and “not good” startups. If it is a classification problem, then we need some measure of good and not good. I was inspired by a study by TechCrunch regarding the fundraising impact of one round on the next.

We see in the below chart, the probability of raising Series A is highest when companies raised 2 to 2.5 million in pre-series A. Therefore a seed-stage company that raised $2M should have a higher chance of survival than one that raised less than $2M. Hence, our seed-stage model will use $2M as a threshold to evaluate which goodness of companies.

Between $0 raised and somewhere in the $2.00-$2.50 million range, each half-million dollar chunk of pre-Series A financing raised resulted in a marginal increase in the likelihood of raising a Series A round, at least for those companies that raised pre-Series A funding between 2003 and 2012. After that $2.5 million mark, there was no marginal benefit to raising more money from Seed, angel and other pre-Series A investors, at least in terms of companies’ chances of raising a B round.

The probability of raising Series B is a lot flatter among different Series A funding. The fundraising amount has less of an impact on advancing to the next stage. Yet, we still see a spike near $12M, so we would follow the same logic previously and use $12M as a threshold to evaluate the goodness of startups.

we find that the rate of Series B fundraising success remains relatively flat with only slight variations from the average. This indicates that, for one reason or another, the amount of money raised prior to Series B doesn’t have a significant impact, most of the time, on a company’s ability to raise a Series B round.

Data postprocessing

After correcting the data types and cleaning repeated rows/special characters, the following processing logic is applied to create a table ready for training.

The raw data columns “location” and “market” are one-hot encoded for machine learning.

“years” is created to represent the number of years since the time joined angelist.

“size_numeric” is created to convert ordinal data to a numeric data (By choosing the midpoint of size ranges).

“raised_2mil” and “raised_12mil” are binary outcome variables we are trying to predict for seed and series A startups respectively.

Filtered stage by “seed” and “series A” and had two datasets ready for modeling.

A screenshot of the seed-stage companies dataframe is included below. All variables except size_numeric and years are binary 1/0 to represent whether that startup meets the location/market criterion.",https://towardsdatascience.com/predicting-startup-performance-using-logistic-regression-582a1e80b2eb,['Xiaoxiang Ma'],2020-08-27 01:16:21.058000+00:00,574,"Data Acquisition, Web Scraping, Angelist Companies, Regression Problem, Classification Problem"
Create your own Data Science degree online for free,"Create your own Data Science degree online for free

Learn the skills needed for a career change!

Photo by Markus Spiske on Unsplash

For those of you who want to skill up in data science, fortunately you can do this from home.

Degrees can be costly and expensive. However, you can master the necessary skills online without the costs and hassle of a traditional degree. Internet education platforms like edX, Coursera and Udemy has brought a wealth of knowledge at your finger tips at very little costs. However, there is a catch — you do need to persevere and practice, practice, practice…

To be a good data scientist you need a solid understanding in 3 key areas:

A solid base in programming: including a database language (SQL) and a scripting language (Python) A solid base in statistics: including linear modelling, Bayesian methods and applied machine learning A sound understanding of the field you’re applying your “data science” to. This means having a good grasp of the underlying dynamics on what drives the variables in the field that you are employed, rather than churning through data in and out… garbage in, garbage out. In other words, have respect for you data.

After a good understanding of programming and statistics, you can then delve into interdisciplinary subjects. I call these “advanced electives”. Some of them are listed here:

Information retrieval and web search

Big data management

Cloud computing

Natural language processing

Neural networks and deep learning

Okay.

So I’m going to divide our “little data science degree” into three “semesters”.

Semester One: Programming Semester Two: Statistics Semester Three: Advanced Electives

You can mix and mash subjects in semester one and two, but save the advanced electives for after you have mastered programming and statistics.

Semester One: Programming

You need working fluency in at least one database language and two scripting language. Here, I’m recommending SQL, Python and R. Python is a general purpose language that will do you well in your data science career. R is more geared to statistical analysis. There are more than 4,000 statistical packages available in R, which means whatever analysis you want to do … there’s something out there that allows you to implement speedily.

1A Learning SQL (with UC Davis)

Let’s start with SQL. You can’t be a data scientist without knowing how to run and use a simple relational database.

Cousera and UC Davis offers a specialization for SQL basics.

Furthermore, we see how SQL is being applied in big data with the course below.

If you’re keen, to contrast SQL, you could also look into studying NoSQL as an extension. The course below on edx provides a good introduction that compares and contrasts SQL with NoSQL.

1B Learning Python (with Michigan)

The University of Michigan has a 4 course specialization for Python. This is highly recommended. I’ve also attached a link to their web-book that you can go through in your own time.

1C Learning R (with Johns Hopkins)

Finally, we come to learning R. R is especially useful for statistics. Johns Hopkins University has paired up with Coursera, offering two specializations in R.

A basic overview for data scientists:

A more advanced course to become a power user — where you learn to build your own packages:

This ends our first semester, where hopefully we have gained some confidence in databases and scripting. Next we will explore statistics for a data scientist.

Semester Two: Statistics

A poor “data scientist” simply runs data through various black boxes and analyses results. To be a good data scientist, you need to understand the analysis you perform. This means a solid grasp of statistics

2A Introduction to Statistics (skip if needed)

For those who do not have a string background in statistics, the following offers a very gentle introduction to the topic.

2B Statistics with Python (with Michigan)

Armed with some basic stats and Python skills, we can now tackle this specialization where we learn about linear models, GLMs and logistic regressions.

2C Statistics with R (with Duke)

After you complete the “Statistics with Python” course, you can move on to this one. You should be able to ease into this one, for you already possess some working knowledge of statistical inference (from Statistics with Python) and R (from last semester). The Duke course repeats linear modelling we did with Michigan, but using R. Furthermore, we are introduced to Bayesian statistics.

Now that we have developed a solid base in programming and statistics; and have also used statistics in both R and Python…we are ready to tackle some more advanced topics in data science.

Semester Three: Advanced Topics

Pick two to three of the below, depending on your interest.

Data Mining and Web Search

UIUC offers a great data mining specialization course with Coursera, which covers information retrieval, text mining and web search.

Machine Learning

A reasonably gentle introduction to Machine Learning

Big Data

Natural Language Processing

(this one comes highly recommended)

Cloud Computing

And there you have it — a three semester self-taught data science program!

If you want to see my top 5 tips for effective online learning, or my review of UIUC’s online master degree, see below:",https://medium.com/age-of-awareness/create-your-own-data-science-degree-online-for-free-1bf8e17fa445,[],2020-09-24 19:37:47.042000+00:00,812,"Data Science, Online Learning, Programming, Statistics, Cloud Computing"
Machine Learning with Julia / ScikitLearn.jl,"Popular packages for both Python and Julia.

My Julia adventure began with a university course. While this course, actually, I didn’t like Julia, as a Python Coder. After that, I wanted to give a chance for it. So, I started to research Julia coding and Julia packages. And than, I decided to share this post with you.

Julia

In this article, my purpose is preparing a Julia Notebook to create some classifier models for IRIS dataset.

Firstly, We have some special packages data processing and modelling.

ScikitLearn package is for modelling part, RDatasets package is for IRIS dataset loading and DataFrames package is for data processing.

packages

Next, if we decide to use which models on this data, we can add importing block.

scikit learn import models

Then, we are loading IRIS dataset and also, defining features and target. Next, we are preparing master data.

So, we are ready to design a model object. For this step, we will use struct type. Struct is very useful if you want to combine some variables in one object. Also, C programming language has it.

struct

At this time, we have a struct object. We can initiate it with appropriate parameters.

initial function for the struct

This object have our model (default parameters), nevertheless, if we don’t use it on a data, it is not meaningful. So, we need a function for usage of struct object like below.

usage function for the struct

Finally, our model system came the end of the flow.

Let’s try them!..

model test predicition accuracies

For other model algorithms, ScikitLearn has many more packages. These are could import with sk_import.

@sk_import ... svm : SVC

ensemle : AdaBoostClassifier

naive_bayes: GaussianNB

neighbors: KNeighborsClassifier

preprocessing: StandardScaler

References",https://medium.com/analytics-vidhya/machine-learning-with-julia-scikitlearn-jl-7c8cfe6076e8,['Erdal Sönük'],2020-09-28 12:14:37.954000+00:00,262,"Python, Julia, Scikit Learn, RDatasets, Data Frames"
Project ‘Sidewaukee’ Makes Milwaukee Sidewalks More Accessible,"What inspired your project?

Annie: During the ideation phase, I wanted to find an opportunity to apply data science to improve our local community’s access to information about transportation and infrastructure. As an avid biker and runner, I noted how many fantastic tools exist for navigating trails and cities. Then I thought about loved ones using mobility devices like wheelchairs, scooters, walkers, and crutches to get around — I was surprised to find a lack of tools for this.

Anu: This project has been close to my heart from the time Annie discussed the idea. Having grown up with a wheelchair dependent sibling, I know how difficult it can be for the individual and their loved ones to go for a stroll.

I saw this project as a perfect opportunity for me to be part of a team that could make a difference in the quality of life for those who use a mobility device.

What was the timeline or process like from concept to final project?

Amy: From the start, we set interim milestones, including built-in “buffer” weeks since nothing ever goes exactly according to plan. We spent the first 2–3 weeks doing research, user interviews, and setting up some of the basic data engineering pipelines so we had a solid foundation before really digging in. By week 11 we had a working minimum viable product which we used to begin soliciting user feedback while we continued to add additional features and fine-tune our model.

How Sidewaukee works, from the Sidewaukee website

Emily: About halfway through the semester, we struggled with the fact that there were so many different features we wanted to add to our product, and we knew we wouldn’t have time for all of them. Our professors, David and Joyce, suggested that we think ahead to the final presentation, and narrow our ideas down to the 3–5 “nuggets of gold” that would differentiate our product and help us tell a compelling story about solving our users’ needs. This valuable exercise helped us re-focus our energy on the most high value work, and got us thinking early about how to tell the story of our project’s specific value.

How did you work as a team? How did you manage to work on your project as members of an online degree program?

Amy: Because most of us were working full-time, we discussed our schedules and work habits very early on and made a plan that allowed for flexibility while still holding team members accountable. For most of us, the bulk of our work time was on weekends, so we scheduled our weekly check-in for Monday evenings so we could assess what we accomplished the previous weekend and identify any roadblocks for the coming week. Throughout the week, we also held each other accountable through a shared Trello board and heavy usage of Slack — posting updates each week on the specific tasks we planned to accomplish.",https://medium.com/berkeleyischool/project-sidewaukee-makes-milwaukee-sidewalks-accessible-129a685b93b3,['Berkeley I School'],2020-05-27 19:07:42.714000+00:00,479,"Data Science, Transportation, Infrastructure, Wheelchairs, Mobility Devices"
Counterfactual Regret Minimization,"“We should regret our mistakes and learn from them, but never carry them forward into the future with us.” Lucy Maud Montgomery

Learning from regrets is what Counter Factual Minimization is all about.

The notion of “regret” is introduced in the article “Introduction to Regret in Reinforcement Learning”. However, it considers scenarios or games composed of a single step or action. Certainly, this is not realistic enough, because most scenarios, in reality, are composed of multiple steps.

It is clear that in every aspect of life, each decision might have a long term impact, and its effect might not be apparent on the spot, but later on.

Counter Factual Regret Minimisation, is a method that deals with scenarios composed of multiple steps, and how to detect errors (thus estimating regret) at every step.

This article provides a general overview of the CFR algorithm and a working example of a tic-tac-toe game.

How to assess regret every step of the way

As we said, most of the time games or scenarios are multi-step.

So it is important to be able to know what is the regret of not taking a certain action at each step.

After each turn, each player assesses the situation and the value of the state she is in. The assessment takes into consideration whether there is a possibility of winning, losing, etc…

Simple Value Case

Similarly to Value State in MDP, each node of the game in CFR has a value, called counterfactual. The bottom line is the average of future rewards weighed by their probabilities of really occurring.

Image by the author

The image above shows a node directly leading to three other nodes, each one with a different probability (.3, .2, .5). Each of the destination nodes has a value (3, 5, 2). The value of the parent node will be equal to the sum of each of the child node multiplied by its probability of happening:

3*.3 + 5*.2 + 2*.5 = 2.9

Recursively the parent node communicates its value to its parent multiplied by the probability of happening.

Simple Regret Case

Since we have computed the value of each node, let’s compute the regret of not taking each action. We define regret by the value of the child minus the value of the parent. A quick computation will give us these results:

3 - 2.9 = .1

5 - 2.9 = 2.1

2 - 2.9 = -.9

As seen the action that is regretted the most is the second one (that leads to 5)

In other words, this action should be more probable than the others. So the probabilities will be recomputed to reflect this fact.

The probabilities (also called strategies) will be computed as follows:

Strategy(i) = Regret(i)/sum of positive regrets

Sum of positive regrets is 2.1 + .1 = 2.2

The strategy of negative regrets is given a value of zero.

Strategy1 = .1 / 2.2 = .05

Strategy2 = 2.1 / 2.2 = .95

Strategy3 = 0

With these new strategies, we compute the new value of the parent node as seen in the image below.

Image by the author

Adding Some Complexity

So far we have established the value and the regret for each node, however, there is a catch!

The value of each node will be used by its parent combined with the probability that that node will be reached.

But this is not the case for the regret. Considering that the regret (in our example) of the middle action is 5–2.9 = 2.1, we still don’t know that this node will be reached and this regret will be computed?!

So the regret should also be multiplied by the probability of reaching its appropriate node.

Assuming that the probability of reaching the node in our example is .2, then the regret of that node will be 2.1 * .2 = 0.42

Important. This will not change the values of the computed strategies because we are multiplying all its components by the same (reaching) probability, on the current iteration (more on that below).

Cumulative Computations

As you most probably already know, all AI/Machine Learning algorithms are based on iterative computations. Thousands or even millions of iterations are done to reach acceptable results of behavior.

CFR is no exception! The CFR tree is computed many times, and on each iteration, the regret and the strategies are updated.

The regret is computed cumulatively for each node and each action, then the strategy is derived from these regret values.

Suppose for node n, there are 2 actions a1, a2. Since the tree will be iterated N times, on each iteration, on node n, we have a new value for regret and the reaching probability.

Regret[n, a1] = Regret[n, a1, 1] * P[1] + Regret[n, a1, 2] * P[2] +…+Regret[n, a1, N] * P[N]

Same for Regret a2:

Regret[n, a2] = Regret[n, a2, 1] * P[1] + Regret[n, a2, 2] * P[2] +…+Regret[n, a2, N] * P[N]

Sum of regrets:

Regret[n] = Regret[n, a1] + Regret[n, a2]

The strategies are computed as usual:

Strategy[n, a1] = Regret[n, a1] / Regret[n]

Strategy[n, a2] = Regret[n, a2] / Regret[n]

The implication of Two Players

When computing CFR for a two-player zero-sum game, there are some important details to be aware of.

The game tree is organized by layers, alternating between player 1 and 2

Image by the author

What is positive for one, is negative for the other and vice-versa. As each node reports its value to the parent, the sign flips to reflect that what is beneficial for one player is a loss for the other.

Image by the author

The reaching probability directly before a node depends on the action of the player in the layer above the node.

The Code

The code shared in Google Colab is an implementation of CFR of the Tic-Tac-Toe game.

To use it, you should first create a copy of your own, set the iterations that you want, train it by running the first cell, then start playing by running the second cell.

Important! the training time is exponentially proportional to the number of iterations.

1 iteration takes approx 30sec

10 iterations take approx 5min

100 iterations take approx 30min

1000 iterations take approx 4h30min

It is advisable to train with 1 iteration first then play against the algorithm, then train it with 10 or 100 iterations then play and see the difference.

Related Articles

Introduction to Regret in Reinforcement Learning

Introduction to Fictitious Play

Fictitious Self Play

Neural Fictitious Self-Play

Neural Fictitious Self-Play in Practice",https://towardsdatascience.com/counterfactual-regret-minimization-ff4204bf4205,['Ziad Salloum'],2020-12-28 18:01:51.472000+00:00,1016,"regret, counterfactualminimization, reinforcementlearning, fictitiousplay, neuralfictitiousselfplay"
The Difference between “is” and “==” in Python,"To understand this better, let’s look at some Python Code. First, let’s create a new list object and name it a , and then define another variable b that points to the same list object:

>>> a = [5, 5, 1]

>>> b = a

Let’s first print out these two variables to visually confirm that they look similar:

>>> a

[5, 5, 1]

>>> b

[5, 5, 1]

As the two objects look the same we’ll get the expected result when we compare them for equality using the == operator:

>>> a == b

True

This is because the == operator is looking for equality. On the other hand, that doesn’t tell us if a and b are pointing to the same object.

Photo by ETA+ on Unsplash

Now, we know they do because we set them as such earlier, but imagine a situation where we didn’t know—how can we find out?

If we simply compare both variables with the is operator, then we can confirm that both variables are in fact pointing to the same object:

>>> a is b

True

Digging deeper with examples, let’s see what happens when we make a copy of our object. We can do this by calling list() on the existing list to create a copy that we’ll name c :

>>> c = list(a)

Now again, you’ll see that the new object we just created looks identical to the list object pointed to by a and b :

>>> c

[5, 5, 1]

Now this is where it gets interesting. When we compare our copy c with the initial list a using the == operator. What answer do you expect to see?

>>> a == c

True

This is expected because the contents of the object are identical, and as such, they’re considered equivalent by Python. However, they are actually pointing to different objects, identified by using the is command:",https://medium.com/code-python/the-difference-between-is-and-in-python-6429b04fa7cd,['Mohammad Ahmad'],2020-07-30 17:40:44.906000+00:00,294,">>> a is c False python, code, objects, variables, equality"
Handwritten Digit Recognition on MNIST dataset,"In this Blog, I will explain basic digit recognition using Logistic Regression as well as LinearSVC. But note that there are many classification algorithms( SGD, SVM, RandomForest, etc) which can be trained on this dataset including deep learning algorithms (CNN).

Let’s understand from basic and analyse the accuracy of both method.

MNIST Dataset

What is MNIST?

MNIST is a dataset of 70,000 images of digit handwritten by high school students and employees of the US Census Bureau. All images are labelled with the respective digit they represent. MNIST is the hello world of machine learning.

There are 70,000 images and each image has 784 (28*28) features. Each image is 28*28 pixels, and each feature simply represents one pixel’s intensity from 0 (white) to 255 (black).

28*28=784 pixels [ 1 D array ]

Step:1 Import Dataset

One thing I want to share is while importing MNIST dataset, I have faced one error shown in below image. This happen because as time change some method will also change. So I solved it by typing fetch_openml(‘mnist_784’)

Error while Load Dataset

Successfully Load Data

Step: 2 Analyse the Data

Here, Data is 1D array which size is 784.

Data [70,000 1D array]

Label is in object format. So, for build predictions of digit, we have to convert it into integer format.

Label of Each Image

Shape:

Step: 3 Let’s plot one 1D array on plot.

I have taken 2600th image of dataset.

Note that → Here, interpolation=’nearest’ simply displays an image without trying to interpolate between pixels if the display resolution is not the same as the image resolution. It will result an image in which pixels are displayed as a square of multiple pixels.

There is no relation between interpolation=’nearest’ and the grayscale image being displayed in color. By default imshow uses the jet colormap to display an image. If you want it to be displayed in greyscale, call the gray() method to select the gray colormap.

Label of 2600th image is ‘9’ and we can also see clearly.

Step: 4 Split the Dataset in train and test

Case: 1 By LinearSVC()

Let’s consider first 5000 images because if I train my model on 70,000 images then it will be very time consuming. You can try shuffle the Data for better accuracy of model.

Since there are 10 digits (0–9), we need a multi-class classifier. The Linear SVM that comes with sklearn can perform multi-class classification.

Prediction of Data

As we can see that the svc estimator has learned correctly. It is able to recognize the handwritten digits, interpreting correctly all nine digits of the validation set.

Case: 2 Create a 9 detector model using Logisctic regression

[ choose_one_digit=2600]

We can conclude that predicted value for this image nine is true.

Step: 5 Calculate Accuracy of both model

a is calculation of linearSVC and b is calculation of Logistic regression.

Output:

Accuracy of LinearSVC

Accuracy of Logistic regression for 9 digit

Conclusion: Average Accuracy of LinearSCV= 0.82,Average Accuracy of Logistic Regression= 0.93

GithHub Link with description : digit_recognition_mnistdata.ipynb

I am thankful to mentors at suvenconsultants for providing awesome problem statements and giving many of us a Coding Internship Exprience. Thank you Suvenconsultants.",https://medium.com/@ishika-tailor/handwritten-digit-recognition-on-mnist-dataset-61b8d6a884b8,['Ishika Tailor'],2020-10-11 11:34:35.669000+00:00,491,"MNIST, digitrecognition, Logistic Regression, LinearSVC, classificationalgorithms"
Data LakeHouse — Paradigm of the Decade,"What is Data LakeHouse?

Data LakeHouse is the new term in the Data platform architecture paradigm. LakeHouse is like the combination of both Data Lake and Data Warehouse (obviously from the term you might have understood). In Data LakeHouse, you will have processing power on top of Data Lakes such as S3, HDFS, Azure Blob, etc.

High-level Data LakeHouse data flow

In other words, you don’t have to load the data onto any of the data warehouses to process and get the analysis or Business intelligence requirement done. You can directly query the data underlying in your data lakes made of object storages or Hadoop. This method decreases the operational overhead on Data Pipelining and maintenance.

Advantage of Data LakeHouse

Some of the advantages of having Data LakeHouse are:

Elimination of simple ETL jobs:

In the Data warehousing technique, the data has to be loaded into Data Warehouse to Query or to perform analysis. For example, loading the data from your existing Data Lake to the Data Warehouse by cleaning and transforming it into the destination schema using some of the ETL/ELT tools. But using the Data LakeHouse tool, the ETL process will be eliminated by connecting the query engine directly to your Data Lake.

Reduced Data Redundancy

Data LakeHouse removes Data Redundancy. For Example, You have Data on multiple tools and platforms such as cleaned data on Data warehouse for processing, some meta-data on Business intelligence tool, temporary data on ETL tools, etc. These Data has to be maintained and monitored continuously to avoid any data sanity issues. So if you are using a single tool to process out of your raw data, you can overcome this data redundancy problem.

Ease of Data Governance

Data LakeHouse can eliminate the operational overhead of managing Data governance on multiple tools. If you are handling sensitive data, you have to be careful when you are transferring data from one tool to another, so that each tool can maintain the proper access controls and encryption. But if you use a single Data LakeHouse tool, Data Governance management can be done from one-point.

Directly connect to BI tools

Data LakeHouse enabling tools such as Apache Drill, supports the direct connection to some of the popular BI tools like (Tableau, PowerBI, etc.). It eventually reduces the time taken from Raw data to visualization by exponential times.

BI Experts Reaction. Source: GIPHY

Cost reduction

In Data Lake and warehousing paradigm, Data has to be stored in multiple places (Data Lake and Data Warehouse) and the cost for the storage is also high. Comparatively, the Data LakeHouse can store the data in cheap storage like object storage such as S3, Blob, etc.

Why would Data Scientists love Data LakeHouse?

Data Scientists always love to play on the raw data (just a thought!), so that they can get more insights of data pattern and develop the models out of it. When the data warehousing technique was followed, Data scientists should also understand ETL/ELT pipelines so that they would have good visibility on how the data was transformed, loaded and validated. But when they have a query engine to query the data directly from raw data, they will get a superpower to build their transformation logics and cleaning techniques after understanding basic statistical insights and quality of the raw data.

Data Scientist with Data LakeHouse (Source: GIPHY)

Tools Available

There is some good Data LakeHouse enabling tool by proprietary cloud vendors and open source communities. Some of them are:

Google Bigquery

Google Bigquery is modern data warehousing platform by Google Cloud Platform. Bigquery(BQ) enables us to follow the Data Lakehouse concept by abstracting the storage and computation layer completely. BQ pricing is based on the data processed per query and not based on storage like other competitors. Unknowingly knowing the new paradigm, most of the organizations started adopting the Data Lakehouse concept by using Google’s Bigquery.

A serverless, highly scalable, and cost-effective cloud data warehouse designed to help you make informed decisions quickly, so you can transform your business with ease — Google https://cloud.google.com/bigquery

Apache Drill

Apache Drill is the Schema-free distributed query engine. It will contribute as the best query engine if you want to create your Data LakeHouse. It also supports cross-format querying like joining JSON and CSV data using one query.

“Drill is an Apache open-source SQL query engine for Big Data exploration. Drill is designed from the ground up to support high-performance analysis on the semi-structured and rapidly evolving data coming from modern Big Data applications, while still providing the familiarity and ecosystem of ANSI SQL, the industry-standard query language. Drill provides plug-and-play integration with existing Apache Hive and Apache HBase deployments.” — Apache Drill https://drill.apache.org/

Amazon Athena

Amazon Athena is part of AWS managed services. Athena is an interactive serverless query engine based on Facebook’s presto. It enables the user to query the data directly from S3 after creating the schema definition. You can do the automated schema discovery using the AWS Glue crawler. (Don’t Worry, I understand the pain)

“Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.” — AWS page https://aws.amazon.com/athena/

Delta Lake

Delta lake is the open-source Data LakeHouse enabling tool that helps us to leverage our processing power of pre-built/pre-owned spark infrastructure. It also enables the ACID methodology on the Distributed storage. Delta lake is founded by the founders of Spark and Databricks. (hail spark!)

“Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.” — delta.io https://www.delta.io/

and there are other tools like Data Lake analytics on Microsoft Azure etc.

Conclusion

Concept of Data LakeHouse is at an early stage, so there are some limitations to be considered before completely depending on the Data LakeHouse architecture such as query compatibility, Data Cleaning complexity, etc. But Data Engineers (readers like you) can contribute to the issues and limitations on open-source tools. Bigger companies like Facebook, Amazon has already set the base for Data LakeHouse and open-sourcing the tools they use. You can quickly start experimenting by deploying the apache drill on your laptop and connect to your existing data lake to create your first Data LakeHouse setup. (beware of querying large data sets *smokes coming out of laptop*).

Good Read on Data LakeHouse — https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html",https://medium.com/adfolks/data-lakehouse-paradigm-of-decade-caa286f5b7a1,['Hassan Rahamathullah'],2020-02-03 11:59:05.369000+00:00,1017,"Data Lake House, Data Warehouse, S3, HDFS, Azure Blob"
10 Best Data Science Reads for Students,"10 Best Data Science Reads for Students

Top 10 ML and stats articles for learning data concepts

It’s time for some Best Of compilations! Here are my 10 best articles for students. If you find yourself having fun with the writing, try following the links in the articles — they’re almost always from the same blog. I try to keep things unboring for your amusement.

Enjoy!

#1 Understanding Data

#2 Explaining supervised learning to a kid (or your boss)

#3 Unsupervised learning demystified

#4 A brief history of data science

#5 Machine learning — Is the emperor wearing clothes?

#6 Statistical inference in one sentence

#7 TensorFlow is dead, long live TensorFlow!

#8 Statistician proves that statistics are boring

#9 Explaining p-values with puppies

#10 What is Decision Intelligence?

Bonus: Snarky Statistics YouTube course",https://towardsdatascience.com/10-best-data-science-reads-for-students-3bae97d9bb23,['Cassie Kozyrkov'],2019-08-17 18:04:24.019000+00:00,120,"data science, machine learning, supervised learning, unsupervised learning, statistical inference"
Introduction to the Gradient Boosting Algorithm,"Image: Edusky

The Boosting Algorithm is one of the most powerful learning ideas introduced in the last twenty years. Gradient Boosting is an supervised machine learning algorithm used for classification and regression problems. It is an ensemble technique which uses multiple weak learners to produce a strong model for regression and classification.

Intuition

Gradient Boosting relies on the intuition that the best possible next model , when combined with the previous models, minimizes the overall prediction errors. The key idea is to set the target outcomes from the previous models to the next model in order to minimize the errors. This is another boosting algorithm(few others are Adaboost, XGBoost etc.).

Input requirement for Gradient Boosting:

A Loss Function to optimize. A weak learner to make prediction(Generally Decision tree). An additive model to add weak learners to minimize the loss function.

1. Loss Function

The loss function basically tells how my algorithm, models the data set.In simple terms it is difference between actual values and predicted values.

Regression Loss functions:

L1 loss or Mean Absolute Errors (MAE) L2 Loss or Mean Square Error(MSE) Quadratic Loss

Binary Classification Loss Functions:

Binary Cross Entropy Loss Hinge Loss

A gradient descent procedure is used to minimize the loss when adding trees.

2. Weak Learner

Weak learners are the models which is used sequentially to reduce the error generated from the previous models and to return a strong model on the end.

Decision trees are used as weak learner in gradient boosting algorithm.

3. Additive Model

In gradient boosting, decision trees are added one at a time (in sequence), and existing trees in the model are not changed.

Understanding Gradient Boosting Step by Step :

This is our data set. Here Age, Sft., Location is independent variables and Price is dependent variable or Target variable.

Step 1: Calculate the average/mean of the target variable.

Step 2: Calculate the residuals for each sample.

Step 3: Construct a decision tree. We build a tree with the goal of predicting the Residuals.

In the event if there are more residuals then leaf nodes(here its 6 residuals),some residuals will end up inside the same leaf. When this happens, we compute their average and place that inside the leaf.

After this tree become like this.

Step 4: Predict the target label using all the trees within the ensemble.

Each sample passes through the decision nodes of the newly formed tree until it reaches a given lead. The residual in the said leaf is used to predict the house price.

Calculation above for Residual value (-338) and (-208) in Step 2

Same way we will calculate the Predicted Price for other values

Note: We have initially taken 0.1 as learning rate.

Step 5 : Compute the new residuals

When Price is 350 and 480 Respectively.

With our Single leaf with average value(688) we have the below column of Residual.

With our decision tree ,we ended up the below new residuals.

Step 6: Repeat steps 3 to 5 until the number of iterations matches the number specified by the hyper parameter(numbers of estimators)

Step 7: Once trained, use all of the trees in the ensemble to make a final prediction as to value of the target variable. The final prediction will be equal to the mean we computed in Step 1 plus all the residuals predicted by the trees that make up the forest multiplied by the learning rate.

Here,

LR : Learning Rate

DT: Decision Tree

Gradient Boosting Code Implementation in Python

Advantages of Gradient Boosting

Most of the time predictive accuracy of gradient boosting algorithm on higher side. It provides lots of flexibility and can optimize on different loss functions and provides several hyper parameter tuning options that make the function fit very flexible. Most of the time no data pre-processing required. Gradient Boosting algorithm works great with categorical and numerical data. Handles missing data — missing value imputation not required.

Disadvantages of Gradient Boosting

Gradient Boosting Models will continue improving to minimize all errors. This can overemphasize outliers and cause over fitting. Must use cross-validation to neutralize. It is computationally very expensive — GBMs often require many trees (>1000) which can be time and memory exhaustive. The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.

Conclusion

Gradient Boosting algorithm is very widely used machine learning and predictive modeling technique (Preferred in Kaggle and other code competitions).

Hope you like my article!

Want to connect :

Linked In : https://www.linkedin.com/in/anjani-kumar-9b969a39/

If you like my posts here on Medium and would wish for me to continue doing this work, consider supporting me on patreon",https://medium.com/analytics-vidhya/introduction-to-the-gradient-boosting-algorithm-c25c653f826b,['Anjani Kumar'],2020-06-30 17:00:38.394000+00:00,738,"Gradient Boosting, Machine Learning, Decision Tree, Loss Function, Supervised Learning"
Why Does Silicon Valley Want to Reengineer Humans?,"Why Does Silicon Valley Want to Reengineer Humans?

Human beings are not the problem. We are the solution.

Image: Andriy Onufriyenko/Getty Images

To many developers and investors in Silicon Valley, humans are not to be emulated or celebrated, but transcended or, at the very least, reengineered. These technologists are so dominated by the values of the digital revolution that they see anything or anyone with different priorities as an impediment. This is a distinctly antihuman position, and it’s driving the development philosophy of the most capitalized companies on the planet.

In their view, evolution is less the story of life than of data. Information has been striving for greater complexity since the beginning of time. Atoms became molecules; molecules became proteins; proteins became cells, organisms, and, eventually, humans. Each stage represents a leap in the ability to store and express information.

Now that we humans have developed computers and networks, we are supposed to accept the fact that we’ve made something capable of greater complexity than ourselves. Information’s journey to higher levels of dimensionality must carry on beyond biology and humans to silicon and computers. And once that happens, once digital networks become the home for reality’s most complex structures, then human beings will really be needed only insofar as we can keep the lights on for the machines. Once our digital progeny can care for themselves, we may as well exit the picture.

This is the true meaning of the “singularity”: It’s the moment when computers make humans obsolete. At that point, we humans will face a stark choice: Either we enhance ourselves with chips, nanotechnology, and genetic engineering to keep up with our digital superiors, or we upload our brains to the network. If we go the enhancement route, we must accept that whatever it means to be human is itself a moving target. We must also believe that the companies providing us with these upgrades will be our trustworthy partners — that they wouldn’t remotely modify equipment we’ve installed into ourselves, or change the terms of service, or engineer incompatibility with other companies’ enhancements or planned obsolescence. Given the track record of today’s tech companies, that’s not a good bet. Plus, once we accept that every new technology has a set of values that goes along with it, we understand that we can’t incorporate something into ourselves without also installing its affordances. In the current environment, that means implanting extractive, growth-based capitalism into our bloodstreams and nervous systems.

If we go with uploading, we’d have to bring ourselves to believe that our consciousness somehow survives the migration from our bodies to the network. Life extension of this sort is a tempting proposition: Simply create a computer as capable of complexity as the brain, and then transfer our consciousness — if we can identify it — to its new silicon home. Eventually, the computer hosting our awareness will fit inside a robot, and that robot can even look like a person if we want to walk around that way in our new, eternal life. It may be a long shot, but it’s a chance to keep going.

Others are hoping that even if our consciousness does die with our body, technologists will figure out how to copy who we are and how we think into an A.I. After that, our digital clone could develop an awareness of its own. It’s not as good as living on, perhaps, but at least there’s an instance of “you” or “me” somewhere out there. If only there were any evidence at all that consciousness is an emergent phenomenon, or that it is replicable in a computer simulation. The only way to bring oneself to that sort of conclusion is to presume that our reality is itself a computer simulation — also a highly popular worldview in Silicon Valley.

Whether we upload our brains to silicon or simply replace our brains with digital enhancements one synapse at a time, how do we know if the resulting beings are still alive and aware? The famous Turing test for computer consciousness determines only whether a computer can convince us that it’s human. This doesn’t mean that it’s actually human or conscious.

The day when computers pass the Turing test may have less to do with how smart computers have gotten than with how bad we humans have gotten at telling the difference between them and us.",https://medium.com/team-human/why-does-silicon-valley-want-to-reengineer-humans-f7cfcf9ad052,['Douglas Rushkoff'],2020-11-12 14:20:41.208000+00:00,721,"Silicon Valley, Humans, Reengineering, Digital Revolution, Turing Test"
"CPU Maximum Power for $1,000 (non desktop)","With a E5–2678 v3 you should see minor improvements over an i7 6850K (maybe 20–30% performance better?). Generation differences + clock speed, for parallelized tasks, would make 1 core of i7 6850k approximately equal to 1.4–1.6 core of E5–2678 v3. If you use well parallelized algorithms, it is worth the cost increase and the speed improvement. If you don’t, it will be a waste of money (and might be slower than what you expect).

For the price ($700-$3,000 now), I would recommend one or even two E5–2698 v4 CPUs engineering samples. That would be about $1,400 for 2x 20 cores at 2.2–3.6 GHz (or $700 for 20 cores). You may also try E5–2673 v4 qualification samples, one for about $1,000 (they also have 20 cores). E5–2673 v4 (QS) is one of the most powerful Intel CPU on the market, as they have increased clock rate versus their official releases (under another name), while costing only a fraction of the official release.

I use (not for kaggling) 2x E5–2690 v2 (2x 10 cores) and an i7–3930K (6 cores), and the performance difference is massive there when running (near-perfectly) parallelized tasks (up to 3x difference, like segregated xgboost training). However, my E5–2690 v2 is slower than a i7 6850K.

What you should think about both CPUs you compare:

Singlethreaded performance for programs which cannot run multithreaded (like KNNs in scikit-learn)

Linearity of multithreading performance for programs (do you hit diminishing returns?)

Is heavily parallelized threading working properly on the programs you may want to use? (xgboost sometimes has issues on multi-CPU setups)

Do you work with large enough data to not have parallelism spawning overhead issues? (for instance, on small data sets for xgboost, one single core can be 10x faster than a 20 thread xgboost)

Here is an example of multithreading performance in xgboost for a customized Bosch data set (1M rows, 500 features, pre-cached in memory), and notice the non-linearity (and going over the amount of physical cores for threading helps):

Note: if you intend to create virtual machines and allocate the most cores you can for your virtual environment, you may hit licensing issues with 16 cores (8 cores + hyperthreading).

If you want to test if it is worth switching to more threads instead of higher clock rate (by spending a bit of money), you can test some benchmarks on AWS in a C4 instance. It uses E5–2666 v3 CPUs (12 cores, 2.6–3.3 GHz), you would then just have to limit the threads used to 12 or 24. If you do not own a i7–6850K, you can look for E5–1650v3 servers and multiply the CPU benchmark results by 1.06–1.10 to approximately get the result of an i7–6850K. What will be optimal is the value you can see in adding more threads and the single threading performance, and only you can define it (is a 2X shorter learning speed worth spending 2X more? etc.).

If you intend just to have a powerful workstation, a E5–2673 v4 is literally overkill but for kaggling / machine learning it may be useful (but not for neural networks which requires GPU to speed up significantly). Even a i7 6850K is great already.",https://medium.com/data-design/cpu-maximum-power-for-1-000-non-desktop-6787ada6c3e6,[],2017-01-03 19:55:04.491000+00:00,518,"E5–2678 v3, i7 6850K, parallelized tasks, clock speed, E5–2698 v4"
Make predictions in 5 minutes,"There are many sophisticated algorithms, libraries and ideas for making predictions, but it takes it’s time to try them all and compare. Therefor we have predictit.

Nowadays one can just find appropriate method, import it and it’s done. But in days of abundance even if we do not have to develop own models, it takes a lot of time to compare all the possible solutions. We have many libraries with machine learning models, many libraries for data analysis and many for data preprocessing. But we need to join all the fragments by ourselves. There is a library/framework for such a tasks. It’s called predictit. It’s open-source with source here and with the documentation.

Compare more than 20 models from Sklearn, Statsmodels, Tensorflow and more just in couple of minutes and also find the optimal input parameters? Yes! How to do that? After

pip install predictit

All you need is

import predictit

import numpy predictions = predictit.main.predict(np.random.randn(1, 100), predicts=3, plot=1)

That’s it. Everything works (on generated test data of course). Just input data that you want to predict and it’s done… You can do config in three ways: Input parameters in predict function or you can use command line arguments or you can edit config.py values. Data sources can be in csv, dataframe, numpy array or SQL. Possible outputs are predictions in numpy array data format or interactive plot. If you’re not using python, use command line arguments like below and run in terminal in folder. Use main.py — help for more parameters info.

python main.py --function predict --data_source 'csv' --csv_path 'test_data/daily-minimum-temperatures.csv' --predicted_column 1

Results can look like this.

After mouse over, you can see exact values, models names and the rank of models by error criterion. It can operate with datetime, data can be resampled and predicted in given frequency.

What models are used? For example…

AR (autoregressive model)

ARIMA

Autoregressive Linear neural unit

Conjugate gradient

Bayes Ridge Regression

Extreme learning machine

Software can be used as python library or as standalone framework that you can edit in any fancy way. Check official readme and tests for some use cases. Read all config.py file for what all you can do.

If you want to predict like a pro, you can start here…

import predictit



predictit.config.predicts = 12 # Create 12 predictions

predictit.config.data_source = 'csv' # Define that we load data from CSV

predictit.config.csv_adress = r'E:\VSCODE\Diplomka\test_data\daily-minimum-temperatures.csv' # Load CSV file with data

predictit.config.save_plot_adress = r'C:\Users\TruTonton\Documents\GitHub' # Where to save HTML plot

predictit.config.datalength = 1000 # Consider only last 1000 data points

predictit.config.predicted_columns_names = 'Temp' # Column name that we want to predict

predictit.config.optimizeit = 0 # Find or not best parameters for models

predictit.config.compareit = 6 # Visualize 6 best models

predictit.config.repeatit = 4 # Repeat calculation 4x times on shifted data to reduce chance

predictit.config.other_columns = 0 # Whether use other columns or not



# Chose models that will be computed

used_models = {

""AR (Autoregression)"": predictit.models.ar,



""ARIMA (Autoregression integrated moving average)"": predictit.models.arima,



""Autoregressive Linear neural unit"": predictit.models.autoreg_LNU,

""Conjugate gradient"": predictit.models.cg,



""Extreme learning machine"": predictit.models.regression,



""Sklearn regression"": predictit.models.regression,



}



# Define parameters of models



n_steps_in = 50 # How many lagged values in models

output_shape = 'batch' # Whether batch or one-step models



models_parameters = {



""AR (Autoregression)"": {""plot"": 0, 'method': 'cmle', 'ic': 'aic', 'trend': 'nc', 'solver': 'lbfgs'},

""ARIMA (Autoregression integrated moving average)"": {""p"": 12, ""d"": 0, ""q"": 1, ""plot"": 0, 'method': 'css', 'ic': 'aic', 'trend': 'nc', 'solver': 'nm', 'forecast_type': 'out_of_sample'},



""Autoregressive Linear neural unit"": {""plot"": 0, ""lags"": n_steps_in, ""mi"": 1, ""minormit"": 0, ""tlumenimi"": 1},

""Conjugate gradient"": {""n_steps_in"": 30, ""epochs"": 5, ""constant"": 1, ""other_columns_lenght"": None, ""constant"": None},



""Extreme learning machine"": {""n_steps_in"": 20, ""output_shape"": 'one_step', ""other_columns_lenght"": None, ""constant"": None, ""n_hidden"": 20, ""alpha"": 0.3, ""rbf_width"": 0, ""activation_func"": 'selu'},



""Sklearn regression"": {""regressor"": 'linear', ""n_steps_in"": n_steps_in, ""output_shape"": output_shape, ""other_columns_lenght"": None, ""constant"": None, ""alpha"": 0.0001, ""n_iter"": 100, ""epsilon"": 1.35, ""alphas"": [0.1, 0.5, 1], ""gcv_mode"": 'auto', ""solver"": 'auto'}



}



predictions = predictit.main.predict()

Except the plot and results, also table of models errors is printed. It can look like this.

How the framework works? It’s kind of soft brute force. Result is matrix of predictions that are evaluated with some error criterion. It’s evaluated on more data lengths, and repeated on translated data for accidental success removal. Final n-dimensional matrix is analyzed and the best models are selected with appropriate data lengths and data preprocessing.

You can choose how to standardize data, error criterion or various initial model’s arguments. You can use config.optimize to find the best arguments for given models if you set up arguments limits. It’s based on dividing into intervals, finding best interval and dividing again. It can operate not only with integers and floats, but also with list on strings. It will create all various combinations and find the best one.

If you use for example Sklearn regression model, there is a parameter regressor. There is also function for parsing all the regressors from Sklearn. If you use optimization then, it will find the best suited regression for your data. No need to learn about various algorithms like lasso, or passive aggressive algorithms, just use it.

If you want to predict more columns, use predict_multiple function. If you need to care about performance, define own test data, run compare_models function, choose only best few models and setup config.lengths=0 and config.repeatit = 1. If you want to see all the results and all the errors, just do config.debug = 1.

If you like it and think it’s useful, just fork it on github. No donations allowed.",https://medium.com/@nicehoseneboj/make-predictions-in-5-minutes-4ca763e2b9ef,[],2020-01-20 14:26:14.032000+00:00,855,"machine learning, data analysis, data preprocessing, Sklearn models, Statsmodels"
Data Analytics Use Case : BrewNation,"Even for an early-stage startup, having good analytics is key to make good decisions and progress. BrewNation figured how to get fast and affordable analytics with the help of DataIntoResults. Let’s see how it works.

BrewNation is a marketplace for craft beer in France. They connect small brewers that make excellent craft beer to their customers. It’s a B2C business (they send beers at your doorstep) as well as a B2B business (they sell to bars and retailers).

BrewNation : The solution to bring craft beer from small breweries on your doorstep

Even being young, they have disparate systems and no easy way to analyse data in a meaningful way. The acquisition and website navigation is monitored by Google Analytics, the B2C sale process is managed by Prestashop and the B2B sales process use a custom spreadsheet.

Moreover, in order to have meaningful analytics, some reference data as well as some business rules should be used. For instance, a referral from a blog review (PR effort) is not analysed the same way that a referral from a brewery (supplier acquisition effort).

The system is based on modules (you can learn more about it in our manisfesto). The global overview of the analytics architecture is therefore the following. Each data source is ingested in the analytical database, blended and refined to ease reporting and insight generation. Google Sheet reports are feed directly by the database.",https://medium.com/data-into-results/data-analytics-use-case-brewnation-1d7b21dec173,['Sébastien Derivaux'],2017-07-20 09:16:37.473000+00:00,229,"craftbeer, startup, analytics, Data Into Results, B2C"
PyCon 2018: A Roundup of Our Favorite Talks,"If you didn’t make it to PyCon this year, you’re in luck — all of the sessions are available online for free! It might be overwhelming given there are hours of talks, so we asked the Civis folks who attended PyCon to round up their favorites. We hope our suggestions inspire you to watch and try something new in Python!

Liz Sander, Data Scientist:

Love Your Bugs — Allison Kaptur (@akaptur)

This is such a fun talk. Allison walks us through a couple of particularly mysterious bugs she’s run into over the years, including one where her team accidentally DDOS’ed their own company! This talk is a celebration of failure, and how it can be a chance to explore, problem-solve, and learn.

CuPy: A NumPy-compatible Library for GPU — Shohei Hido (@sla)

I had never heard of CuPy before this, and Shohei’s talk was a great introduction. CuPy is a library for GPU computing which matches NumPy’s behavior and API, with easy ways to convert between the two. This means that you can write CPU/GPU-agnostic code — it’ll automatically choose the right library and run in an optimized way on either compute type! I left this talk wanting to dig right in and try CuPy myself.

Jenna Colazzi, Software Engineer:

Dataclasses: The code generator to end all code generators — Raymond Hettinger (@raymondh)

Raymond Hettinger uses this talk to introduce Dataclasses, a new feature that is available in Python 3.7. I’m excited to try out this new alternative to named tuples, which seems robust but still quite configurable. This talk also highlighted the power of code generators, not just to save time by reducing boilerplate code, but also to allow new code to incorporate best practices and include metadata for free.

Clearer Code at Scale: Static Types at Zulip and Dropbox — Greg Price (@gnprice)

Static typing came up a lot this year at PyCon. This talk explains how using type annotations can improve your codebase, helping you catch bugs before committing your code, onboard new developers, and save time when debugging. Price also provides some concrete steps you can take to make adding type checking to existing codebases less intimidating. We’re planning to try out this process here at Civis!

John Kulzick, Data Architect:

Demystifying the Patch Function — Lisa Roach

A complete and concise walk-through of the various ways to use the patch function and what all the parameters actually mean. You will learn how to improve the quality of your tests and you may discover that your existing tests aren’t really testing what you think they are. Don’t worry, Lisa kept her opinions on what and when you should patch to herself.

Inside the Cheeseshop: How Python Packaging Works — Dustin Ingram (@di_codes)

This talk walks through the history of Python packaging and the various stages of evolution that it went through, with a little added humor to keep it interesting. You will learn what the various file types and tools are that you’ve come across and, more importantly, why they exist. Dustin also explained some of the unresolved challenges with packaging and what the Python Packaging Authority (PyPA) is planning to do about them.

Skipper Seabold, Director, Data Science:

You’re an expert. Here’s how to teach like one. — Shannon Turner (@svthmc)

Shannon Turner is the founder of Hear Me Code, an organization that offers free, beginner-friendly coding classes for more than 3,000 women in DC. In this talk, she shares some best practices for being an effective teacher, mentor, or presenter of technical topics for those managing junior- or mid-level developers. She offers great tips on how to share knowledge in a way that is readily understood.

Surviving (and thriving!) when you are overloaded — Scott Triglia (@scott_triglia)

I am so, so glad that this talk was accepted by the PyCon organizers. In tech, in open source, and in many other fields, we often feel pressure to succeed at all costs, even when it means taking on too much. Scott Triglia shares the signs of being overloaded, and offers practical advice for self care when faced with an overwhelming workload.

Kate Pham, Software Engineer:

Pipenv: The Future of Python Dependency Management — Kenneth Reitz (@kennethreitz)

Kenneth Reitz talks about the history of managing dependencies in Python today and anticipated improvements in the future. I enjoyed seeing how Python tools can evolve over time.

How Netflix does failovers in 7 minutes flat — Amjith Ramanujam (@amjithr)

This was a enjoyable talk about what happens when things go wrong at Netflix. Amjith Ramanujam does a nice job defining their problem, with examples of an inadequate solution and an efficient solution. It’s great to see how Python is being used to achieve high availability.

Jonathan Cobian, Software Engineer:

Adapting from Spark to Dask: what to expect — Irina Truong (@irinatruong)

Irina Truong had been using Spark and didn’t really enjoy the Java stack traces and multiple layers of indirection that made debugging hard. She migrated a real world example over to Dask to see how the development experience and performance compared. The talk helped explain the differences between the two frameworks and when each works best.

Types, Deeper Static Analysis, and you — Pieter Hooimeijer (@thehooj)

Pieter Hooimeijer talks about how a newly open-sourced tool his team has released, Pyre, leverages type annotations to help find security vulnerabilities throughout Facebook’s entire (Python) code base.",https://medium.com/civis-analytics/pycon-2018-a-roundup-of-our-favorite-talks-7a9ab3628f9d,['Civis Analytics'],2018-06-11 22:43:57.074000+00:00,866,"Love Your Bugs, Cu Py, Dataclasses, Static Types, Patch Function"
Mapping US Census Data with Python,"Public Data

Mapping US Census Data with Python

If you’ve ever wanted to create quick geographic visualizations of your favorite Census statistics then you’ve come to the right place! First you’ll need to get your hands on the census data that you’re interested in. For this example, we’ll be using transportation data in New York state.

Data Download

Now you’ll want to import the required packages. If you are new to the censusdata package please take a moment to familiarize yourself using the excellent documentation or this introductory blog on the very subject

import pandas as pd

import censusdata

from tabulate import tabulate

Now we need to download the data we’re interested in.

df = censusdata.download('acs5',

2015,

censusdata.censusgeo([('state', '36'), ('county', '*')]),

['B08301_001E', 'B08301_010E'])

A quick breakdown on the parameters of this method:

‘acs5’ refers to the 5 Year American Community Survey and designates the database we are downloading from.

refers to the 5 Year American Community Survey and designates the database we are downloading from. 2015 is the year that we are getting data from

is the year that we are getting data from censusdata.censusgeo([(‘state’, ‘36’), (‘county’, ‘*’)]) defines the geography we are interested in. The ‘36’ value is the FIPS (Federal Information Processing Standards) code for New York state while the ‘*’ means we want every county. To look up your own state or county FIPS code refer to the Wikipedia page.

defines the geography we are interested in. The ‘36’ value is the FIPS (Federal Information Processing Standards) code for New York state while the ‘*’ means we want every county. To look up your own state or county FIPS code refer to the Wikipedia page. [‘B08301_001E’, ‘B08301_010E’] this is a list referring to the specific two tables that we are interested in. Note that these were located beforehand using the search function of censusdata.

Dataframe Prep

Now let’s take a look at what's in our dataframe.

print(tabulate(df, headers='keys', tablefmt='psql'))

Okay, we are definitely getting the raw information we want in this table but right now it’s a bit cumbersome to interpret. For instance, the Id values here are censusgeo objects. Yikes!

First, we’ll change the existing column names away from those table Ids to reflect their contents.

column_names = ['total_transpo', 'total_public_transpo']

df.columns = column_names

Next, we’ll engineer a new column that's just the ratio of public transit.

df['percent_public_transpo'] = df.apply(

lambda row: row['total_public_transpo']/row['total_transpo'],

axis = 1)

Now we’ll redefine the index while extracting the county name.

new_indices = []

county_names = [] for index in df.index.tolist():

new_index = index.geo[0][1] + index.geo[1][1]

new_indices.append(new_index)

county_name = index.name.split(',')[0]

county_names.append(county_name) df.index = new_indices

df['county_name'] = county_names

Okay, let’s take another look.

Much better! Now onto the mapping

Mapping

To do the mapping we’ll be using a plotly feature called figure_factory that's going to do a lot of the work for us. So the first thing is to import the package.

import plotly.figure_factory as ff

Now creating the map is just a few lines of code.

fig = ff.create_choropleth(fips=df.index,

scope=['New York'],

values=df.percent_public_transpo,

title='NY Public Transit Use by County',

legend_title='% Public Transit')

fig.layout.template = None

fig.show()

Looks pretty good for a few lines of code! But don’t be fooled, we worked to set ourselves up for success here by structuring our dataframe in a way that's compatible with figure_factory. For instance, it takes in FIPS data in the form of state_code + county_code. So by renaming our indices to follow that form, we can simply pass the direct index values into the method

Finally, the data in this map is distributed in a way that we would expect, with a huge hotspot centered around NYC that fades the further out you go.

That's all there is to it, now get mapping!",https://towardsdatascience.com/mapping-us-census-data-with-python-607df3de4b9c,['Jackson Gilkey'],2020-04-27 18:51:47.075000+00:00,566,"Public Data, US Census Data, Python, Mapping, Data Download"
On Semantic Search,"Solution 3: Alternate query generation

If we have a log of queries from user sessions, we can train a generative model to generate (next query | current query). The hypothesis being that all the queries in a session are similar to each other.

The logs can be like this.

Artificial intelligence Tensorflow Neural networks …

Training data

(x, y)

(Artificial intelligence, Tensorflow)

(Tensorflow, Neural networks)

Query generation can help us suggest relevant queries by understanding the buying intent of user.

Solution 4: Using word and document embeddings

Once we have the query entered by the user, instead of representing it in one-hot or TF-IDF normalised form, we can vectorise words, sentences and documents using some approaches.

Simple embedding average

Weighted embedding average by multiplying with IDF values

Sentence embedding using a model like infersent, USE(Universal sentence encoder), sentence-bert

seq2seq autoencoder

This can help us represent all the tokens in a semantic and compressed form of a vector of fixed size irrespective of the vocabulary size. This requires a one-time heavy overhead of vectorising using the model but all the searches later will be vector search in n-dimension.

The current state of the art in vector search is milvus. For approximate nearest neighbour you can use flann and annoy.

Solution 5: Contextualisation

Factors to be considered by the search engine for contextualisation/personalisation

User history — Interests from his past search and if he searched the same in past what did he click

— Interests from his past search and if he searched the same in past what did he click User geography — Searching word ‘president’ gives me ‘Ram Nath Kovind’ → the current president of India

— Searching word ‘president’ gives me ‘Ram Nath Kovind’ → the current president of India Temporal changes in information — The result of query ‘president’ will change with time

User history can be used by encoding each click as a fixed vector and generate score(document | history+query)

Geography and temporal can be taken care either by filtering or adding them as features while training the model.

Personalisation allows us to serve human-level suggestions to the user which can help us improve the conversion.

Solution 6: Learning to rank

Because of the flaws of token matching in TF-IDF scheme, we actually need a layer which can re-rank the results. The scheme has high bias for rare words and also doesn’t account for conversion possibility of an article.

If we have a log of user queries and what they clicked from the search results, we can train the model to rank documents. The data will look like this.

(x, y)

(Artificial intelligence, book 2 title)

(Tensorflow, book 1 title)

(Neural networks, book 4 title)

Next time we want to show results, we first get the top x results from a cheap process of token match through TF-IDF/BM25, mostly through Elasticsearch, and then generate a score for all pairs.

(x, y)

(query, title 1) → score 1 (query, title 2) → score 2 …

Then we sort the titles by the score and show top results.

You can find an implementation of this using BERT in my github.

BERT with NSP head

We can define the learning to rank problem as BERT NextSentencePrediction problem — aka entailment problem.

Solution 7: Ensemble

In most cases, it's advantageous to leverage both the approaches. The book mentions that using a combined score of wordvector and BM25 works best.

From ‘Deep learning for search’

Ensemble allows us to exploit best of both the approaches-hard token match + semantics.

Solution 8: Multilingual search

Approach 1

In certain cases when the application is across the geography, the language of user can be different from the document. Such a search is not possible with classical search approach as tokens of different languages cannot match. This requires the help of machine translation with deep learning.

Detect the language of user query (ex. French) Translate query to all languages for which we have the documents (French to English, German and Spanish) Search documents Translate all top-scoring documents to user’s language (English, German and Spanish to French)

Approach 2

Instead we can use a multilingual sentence encoder to represent text from any language to similar vectors. I have explained this in detail over here.

Conclusion

You might already know that last month Google pushed BERT based implementation in production for enhancing their search results. It seems that semantic search is on a rise and will become common in the industry as we get more understanding of leveraging deep learning for search 😀",https://medium.com/modern-nlp/semantic-search-fuck-yeah-e371c0f639d,['Pratik Bhavsar'],2020-09-13 06:16:49.317000+00:00,705,"Machine Learning, Deep Learning, Search Engine Optimization, Natural Language Processing, Semantic Search"
Dealing with List Values in Pandas Dataframes,"Problem 3: Individual Columns for All Unique Values

At this point, things are getting advanced. If you are happy with the results we got before, you can stop here. However, a deeper level of analysis might be required for your research goal. Maybe you want to correlate all list elements with each other to compute similarity scores. E.g. do kids who eat bananas typically also like mangos? Or maybe you want to find out which fruit has been ranked as the top favorite fruit by the most kids. These questions can only be answered at a deeper level of analysis.

For this, I will introduce two useful methods. They differ in complexity, but also in what you can do with their results.

Method 1

This is a shockingly easy and fast method I stumbled upon. And it is so useful! All you need is one line of code.

fruits_expanded_v1 = fruits[""favorite_fruits""].apply(pd.Series)

Figure 5 — Expanded version of the fruit lists using method 1.

As you can see, this one-liner produced a dataframe where every list is split into its single elements. The columns indicate the order, in which the fruit was placed in the list. With this method, you will always get a dataframe with a shape of (n, len(longest_list)). In this case, two of the 10 children named five favorite fruits, which results a 10x5 dataframe.

Using this, we can find out which fruit was named most often as the number one favorite fruit.

fruits_expanded_v1.iloc[:,0].value_counts(normalize = True) ## OUTPUT ##

banana 0.222222 pear 0.111111 watermelon 0.111111 blueberry 0.111111 strawberry 0.111111 apple 0.111111 peach 0.111111 mango 0.111111

We can see that bananas are most often kids’ absolute favorite fruit.

Alternatively, we could target single fruits and find out how many times they were named at each position of the lists. This is the function I wrote for that:

def get_rankings(item, df):



# Empty dict for results

item_count_dict = {}



# For every tag in df

for i in range(df.shape[1]):



# Calculate % of cases that tagged the item

val_counts = df.iloc[:,i].value_counts(normalize = True)

if item in val_counts.index:

item_counts = val_counts[item]

else:

item_counts = 0



# Add score to dict

item_count_dict[""tag_{}"".format(i)] = item_counts



return item_count_dict

If we apply it, we get:

get_rankings(item = ""apple"", df = fruits_expanded_v1) ## OUTPUT ##

{'tag_0': 0.1111111111111111, 'tag_1': 0.1111111111111111, 'tag_2': 0.2222222222222222, 'tag_3': 0.2, 'tag_4': 0}

As you can see, we can perform rank-based analyses very well with this approach. However, this method is near useless for other approaches. Because the columns do not represent a single tag, but a rank, most tag-based operations can not be done properly. For example, calculating the correlation between bananas and peaches is not possible with the dataframe we got from method 1. If that is your research goal, use the next method.

Method 2

This method is more complex and requires more resources. The idea is that we create a dataframe where rows stay the same as before, but where every fruit is assigned its own column. If only kid #2 named bananas, the banana column would have a “True” value at row 2 and “False” values everywhere else (see Figure 6). I wrote a function that will perform this operation. It relies on looping, which means that it will take lots of time with large datasets. However, out of all the methods I tried, this was the most efficient way to do it.

def boolean_df(item_lists, unique_items): # Create empty dict

bool_dict = {}



# Loop through all the tags

for i, item in enumerate(unique_items):



# Apply boolean mask

bool_dict[item] = item_lists.apply(lambda x: item in x)



# Return the results as a dataframe

return pd.DataFrame(bool_dict)

If we now apply the function

fruits_bool = boolean_df(fruits[“favorite_fruits”], unique_items.keys())

we get this dataframe:

Figure 6 — Boolean dataframe.

From here, we can easily calculate correlations. Note that “correlation” is not really the correct term, because we are not using metric or ordinal, but binary data. If you want to be correct, use “association”. I will not.

Again, there are multiple ways to correlate the fruits. One straight forward way is the Pearson correlation coefficient, which can also be used for binary data. Pandas has a built-int function for this.

fruits_corr = fruits_bool.corr(method = ""pearson"")

Figure 7 — Pearson correlation dataframe.

Another way is to simply count how many times a fruit was named alongside all other fruits. This can be solved using matrix multiplication. For this, we will need to convert the boolean dataframe to an integer based on first.

fruits_int = fruits_bool.astype(int)

Then, we can calculate the frequencies.

fruits_freq_mat = np.dot(fruits_int.T, fruits_int) ## OUTPUT ## array([[5, 3, 3, 2, 2, 1, 1, 1, 0, 2, 0, 1],

[3, 4, 2, 1, 1, 1, 1, 2, 1, 0, 1, 1],

[3, 2, 4, 3, 1, 2, 0, 0, 0, 1, 0, 0],

[2, 1, 3, 4, 2, 2, 0, 0, 0, 1, 0, 0],

[2, 1, 1, 2, 3, 1, 0, 0, 0, 1, 0, 0],

[1, 1, 2, 2, 1, 3, 0, 0, 0, 0, 0, 0],

[1, 1, 0, 0, 0, 0, 2, 1, 1, 0, 1, 1],

[1, 2, 0, 0, 0, 0, 1, 2, 1, 0, 1, 1],

[0, 1, 0, 0, 0, 0, 1, 1, 2, 0, 2, 0],

[2, 0, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0],

[0, 1, 0, 0, 0, 0, 1, 1, 2, 0, 2, 0],

[1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]])

All we need to do now is add labels and transform it back to a dataframe.

fruits_freq = pd.DataFrame(fruits_freq_mat, columns = unique_items.keys(), index = unique_items.keys())

Figure 8 — Frequency dataframe.

If you are looking for a nice visualization, you can create a heatmap with the seaborn library.

import seaborn as sn fig, ax = plt.subplots(figsize = (9,5))

sn.heatmap(fruits_freq, cmap = ""Blues"")

plt.xticks(rotation=50)

plt.savefig(""heatmap.png"", dpi = 300)

Figure 9 — Heatmap.

With the Pearson matrix, we can easily build a fruit recommender system. For example, if you input that you like bananas, it will recommend you a maracuja, because those two have the highest correlation (0.67). You will be surprised by how powerful this simple approach is. I have used it successfully multiple times. If you want to do something like this with the frequency dataframe, you need to normalize the data first. However, that is a topic for another post.",https://towardsdatascience.com/dealing-with-list-values-in-pandas-dataframes-a177e534f173,['Max Hilsdorf'],2020-09-06 17:26:25.635000+00:00,983,"data analysis, dataframe, pandas, correlation, boolean dataframe"
The need to automate the data labeling process,"Machine Learning is the new technological wave whose impact on industries and the world can be compared to the emergence of PCs in the 1970s. While the first work now recognized as AI was done in 1943, it wasn’t until the 2010s that AI became widely used in consumer tech products and industries.

There are three types of Machine Learning Models:

1. Supervised Learning Model — where the model needs to be trained in order to perform a specific function.

2. Unsupervised Learning Model — where the model can perform a specific task without any supervision.

3. Reinforcement Learning Model — where the agent learns on its own through positive and negative reinforcements, much like a baby does.

While there has been a lot of advancement when it comes to Reinforcement Learning Models and Unsupervised Learning Models, it is Supervised Learning which is the most widely used form of Machine Learning.

With the widespread use of supervised learning models, the need for training datasets is very high in the ML industry. As a result data labeling has become a very important part of the training phase of the machine learning lifecycle.

Data Labelling or Data Annotation is an emerging sector in India, however, data labeling is primarily still done manually.

The problem?

Say you have built a supervised learning model and need a training dataset to train your supervised learning model to perform effectively. You have 5,00,000 data points that need to be labeled. Here’s what this could look like:

If you have hired in-house annotators and have five such in-house annotators and they can provide 2000 data points per person per day then it could take them ~ 7 weeks to get your data labeled

If you send your dataset to a manual data labeling firm, depending on the number of annotators working on your project the turn-around-time could still be weeks and the cost could be pretty high

So it could take you weeks and cost you hundreds of dollars by the time you get around to training your model.

Now, what if we automated the data labeling process?

Here’s what your training phase could look like then:

Instead of weeks, AI can label your data in hours which can reduce the time taken for your training phase by ~ 97% considering the example above — that it takes ~ 7 weeks for your data to be labeled

It will also be less expensive because automating the data labeling process does not incur the costs associated with manual annotations

The chance of human bias and human error entering your training dataset is low, ensuring a good quality labeled dataset which is crucial to ensure that your ML model performs optimally

Since the time taken to label your data is significantly low as compared to manual data labeling, it allows for rapid iteration to get a high-quality training dataset that ensures that your ML model performs at its best

However, even if an unsupervised learning model is used, in order to determine how well the data has been labeled we need a metric. To generate this metric, 5% — 10% of the data needs to be labeled for validation, in order to verify the predictions. Since no model is 100% accurate, thus a fully automated model is not possible.

What is the next best solution?

A combination of machine learning and human checks to ensure that the quality of the training dataset is as high as possible.

So the dataset is initially put through the ML model which predicts the initial set of labels and then in order to do away with false positives or false negatives, the labels are reviewed by humans.

This ensures that the labels are as highly accurate as possible. We save a lot of time by using Machine Learning to get initial predictions. Since we don’t have to manually label all the data points and instead just need to review the incorrect ones, this ultimately minimizes the time taken to review the predictions by ~86%. However, the amount of time that the training phase could be reduced depends on the dataset and the application.

For example, in certain tasks like image classification and text classification, AI can confidently generate high-quality labels thereby minimizing in-depth reviewing. However, for complex tasks like object detection and instance segmentation, AI can help generate initial predictions and the reviewer can rectify the wrong predictions and missing labels.

At Expand AI, we use ML-assisted labeling to generate high-quality labels at the lowest turn-around time and we also have humans in the loop to ensure high accuracy. Since we don’t incur the costs associated with manual data labeling we are also very economical with pricing.

Here’s what our pipeline looks like.

Once we get data from clients, we first clean the data and then label 5% of the data manually. We then use this data to train the model so that it can predict labels for the rest 85% of the data accurately.

We have humans in the loop to review the labels to increase the accuracy further. Finally, the labeled data is sent back to clients.

Reach out to us at test@expand-ai.com in order to test our data labeling software today!",https://medium.com/@amritagandha-dutta/the-need-to-automate-the-data-labeling-process-6cd2efacf3e3,['Expand Ai'],2021-05-09 13:13:59.674000+00:00,842,"machine learning, AI, supervised learning model, unsupervised learning model, reinforcement learning model"
Importance of Dimensionality Reduction!!,"In Machine Learning and Statistics,in order to build a good performing model we try to pass on those features in the dataset that are significant to one another.

In other words,Dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables.

Problem with high dimensional data?

It can mean high computational cost to perform learning. It often leads to over-fitting when learning a model, which means that the model will perform well on the training data but poorly on test data. Data are rarely randomly distributed in high-dimensions and are highly correlated, often with spurious correlations. The distances between a nearest and farthest data point can become equidistant in high dimensions, that can hamper the accuracy of some distance-based analysis tools.

Why do we need Dimensionality Reduction?

Dimensionality reduction helps with these problems, while trying to preserve most of the relevant information in the data needed to learn accurate, predictive models.

There are often too many factors on the basis of which the final prediction is done. These factors are basically variables called features.

The higher the number of features, the harder it gets to visualize the training set and then work on it.

Sometimes, most of these features are correlated, and hence redundant. This is where dimensionality reduction algorithms come into play.

Importance of Dimensionality reduction?

It reduces the time and storage space required.

It helps Remove multi-collinearity which improves the interpretation of the parameters of the machine learning model.

It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D.

It avoids the curse of dimensionality.

It removes irrelevant features from the data, Because having irrelevant features in the data can decrease the accuracy of the models and make your model learn based on irrelevant features.

In machine learning, “Dimensionality” simply refers to the number of features/variables in your dataset. When the number of features/variables is very large relative to the number of observations in your dataset, certain algorithms struggle to train effective models. This is called the “Curse of Dimensionality.”

Dimensionality reduction can be divided into:

Feature Selection-

Feature selection is a method introduced in the machine learning in order to remove less significant features from the data,so that the model is trained only on the features that contribute most to the prediction(dependent) variable.

Types of Feature Selection methods are:

* Filter

* Wrapper

* Embedded Feature Extraction-

Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing.

This process is generally done with image and text data, where only the important feature is extracted and sent ahead for processing instead of taking the whole data.

Feature Extraction can be Unsupervised (PCA) or supervised (LDA).

We will see about each methods and techniques in the upcoming articles!Stay tuned.

Happy Learning!!!!!",https://medium.com/analytics-vidhya/importance-of-dimensionality-reduction-d6a4c7289b92,[],2020-06-14 14:20:05.875000+00:00,457,"dimensionality reduction, feature selection, filter methods, wrapper methods, embedded feature extraction"
5 Top Artificial Intelligence Frameworks for 2021,"Introduction

Let’s say you decided to practice and develop yourself in this field. Today we will see how software engineers can apply deep learning and Artificial Intelligence into their programming work.

The first thing that we must know is how it can be applicable, and here is a nice question to make a research on: “What are the most useful frameworks/ libraries to begin learning in 2021?” This is exactly the question I asked myself.

This is just what we’re going to address today in this article: I gathered the most popular five Artificial Intelligenceframeworks and libraries that every software engineer/ developer needs to know about. You will also find the official documentation pages and some practice applications on how to apply them.

This will help us to know them more than just their names. Enough with the introduction. Let me introduce you to the squad! 😄",https://towardsdatascience.com/5-top-artificial-intelligence-frameworks-for-2021-7d3bf8e12ed1,['Behic Guven'],2020-12-26 15:30:12.613000+00:00,143,"Deep Learning, Artificial Intelligence, Software Engineering, Frameworks, Libraries"
Using Optuna to Optimize PyTorch Hyperparameters,"This post uses PyTorch v1.4 and optuna v1.3.0 .

PyTorch + Optuna!

Optuna is a hyperparameter optimization framework applicable to machine learning frameworks and black-box optimization solvers. PyTorch is an open source machine learning framework use by may deep learning programmers and researchers. Let’s see how they can work together!

Creating the Objective Function

Optuna is a black-box optimizer, which means it needs an objective function, which returns a numerical value to evaluate the performance of the hyperparameters, and decide where to sample in upcoming trials.

In our example, we will be doing this for identifying MNIST characters. In this case, the objective function looks like this:

Notice that the objective function is passed an Optuna specific argument of trial . This object is passed to the objective function to specify which hyperparameters should be tuned. This returns the accuracy of the model, which is used by Optuna as feedback on the performance of the trial.

Defining the hyperparameters to be tuned

Similar to how PyTorch uses Eager execution, Optuna allows you to define the kinds and ranges of hyperparameters you want to tune directly within your code using the trial object. This saves the effort of learning specialized syntax for hyperparameters, and also means you can use normal Python code for looping through or defining your hyperparameters.

Optuna supports a variety of hyperparameter settings, which can be used to optimize floats, integers, or discrete categorical values. Numerical values can be suggested from a logarithmic continuum as well. In our MNIST example, we optimize the hyperparameters here:

The optimizer itself is chosen from trial.suggest_categorical(“optimizer”, [“Adam”, “RMSprop”, “SGD”]) , which chooses among Adam, RMSProp, and Stochastic Gradient Descent optimizers.

The learning rates for these optimizers varies by orders of magnitude, so trial.suggest_loguniform('learning_rate', 1e-5, 1e-1) is used , which will vary the values logarithmically from .00001 to 0.1.

For the definition of the model itself, Optuna leverages eager mode to allow normal Python looping to determine the number of layers and nodes in each layer with trial.suggest_int(“n_layers”, 1, 3) for the layers and trial.suggest_int(“n_units_l{}”.format(i), 4, 128) for the number of nodes in each layer.

Running the Trials

The default sampler in Optuna Tree-structured Parzen Estimater (TPE), which is a form of Bayesian Optimization. Optuna uses TPE to search more efficiently than a random search, by choosing points closer to previous good results.

To run the trials, create a study object to set the direction of optimization ( maximize or minimize ). Then, run the study object with study.optimize(objective, n_trials=100) to do one hundred trials.

Each trial is chosen after evaluating all the trials that have been previously done, using the TPE sampler to make smart guesses where the best values hyperparameters can be found. The best values from the trials can be accessed through study.best_trial. O ther methods of viewing the trials, such as formatting in a dataframe, are also available.

Pruning — Early Stopping of Poor Trials

Pruning trials is a form of early-stopping which terminates unpromising trials, so that computing time can be used for trials that show more potential. In order to do pruning, it’s necessary to provide intermittent feedback to Optuna from the objective function on how the trial is going. With this information, it can compare the progress with the progress of other trials, decide whether to stop the trial early, and receive messages from Optuna when the trial should be terminated. Then the objective function can smoothly terminate session after recording the results if necessary.

trial.report is used to communicate with Optuna about the progress of the trial. In this example, the objective function communicates the current epoch and the accuracy. trial.should_prune() is how Optuna communicates to the objective function if it should terminate early.

To the Future, and Beyond!

Plot Contour Visualization

For those interested, Optuna has many other features, including a visualizations, alternative samplers, optimizers, and pruning algorithms, as well as the ability to create user-defined versions as well. If you have more computing resources available, Optuna provides an easy interface for parallel trials to increase tuning speed.

Give Optuna a try!

Installation

Optuna Github",https://medium.com/optuna/using-optuna-to-optimize-pytorch-hyperparameters-2d68d60e4bc5,['Crissman Loomis'],2020-04-22 04:47:13.089000+00:00,656,"Py Torch, Optuna, Hyperparameter Optimization, MNIST, TPE"
That Awesome Time Javascript Rescued Us,"That Awesome Time Javascript Rescued Us

We need help!

Photo by Emile Perron on Unsplash

The Story

It was the Fall of 2019 and my fiancé and I were absolutely obsessed with going to hackathons. For those of you who may not know, a hackathon is an event where software developers collaborate together to build something in a specified amount of time. We had just finished one in Kent two weeks prior and got the crazy idea to go into another without any project idea. Normally, we figure out what our project will be prior to the hackathon. This gives us plenty of time to do research for the project which ultimately saves valuable minutes during the event. However, we decided to just go, have some fun, and figure out something when we got there.

The Project

After arriving, checking in, and picking up some swag from the sponsors, we found a nice place to sit and began brainstorming. Initially, I had wanted to create something that utilized machine learning. My fiancé wanted to do something related to one of the prize categories, Best Hack for Social Good. Eventually, we came up with the idea of creating a chatbot utilizing conversational AI (Artificial Intelligence). What we ultimately decided on was to create a chatbot that someone who is suffering from lack of energy, depression, suicidal thoughts, etc. and is not comfortable sharing their thoughts with another human, could talk freely to the bot. Essentially, the goal was for the user to feel comfortable and maybe even feel better after talking with the bot.

Hacking Time

When hacking began, we immediately started researching machine learning and Tensorflow. Eventually, we got a neural network written and a model created to train it on. Since we were running extremely low on time and didn’t have any experience in Javascript frameworks/libraries, we opted to use Flask. Flask is a Python micro-web framework. It works great if you want to do everything yourself and get something built quickly.

The main issue that we’ve run into in the past when using Flask is that Jinja doesn’t always play nicely with complex data objects. If you’re not familiar with iterating through objects with Jinja, it looks a little something like this:

<div>

{% for record in RecordsList %}

<p>{{ record.someData }}</p> {% endfor %} </div>

For those of you that use Angular, this pretty similar to “ngFor”. Unfortunately for us, we didn’t have any Angular experience and not a whole lot of time either. In the past, we would normally pass the data object to the Flask template (basically, your HTML file) and start iterating through it. This time though, we decided to utilize Jinja differently. There is a filter you can use that will read the passed up object as JSON. Because of this, the object can be parsed and assigned to a Javascript variable. For example, we used:

var json = JSON.parse({{ Messages | tojson }});

This was very handy because instead of iterating with Jinja, we could now iterate through the object using Javascript.

Not only was it easier to iterate through the JSON object but at a point when the time was so critical, Javascript made it much easier to build a dynamic list of chat messages. This helped us to create the look that we were wanting for the project.

As you can see, we had a larger box that contained the user input textbox and the chat messages. The chat messages are just bootstrap cards with the blue cards being user messages and pink cards being bot messages. Utilizing Javascript’s createElement() function, we were able to dynamically create the message list. After breaking up the JSON object into two separate lists (sent, received), we were able to use the innerText property to assign the message to the created card.

function updateScroll()

{

var scrollBox = document.getElementById('box'); var elementHeight = scrollBox.scrollHeight; scrollBox.scrollTop = elementHeight;

}

The last thing that we had to do before hacking ended was to create a function that would automatically turn the message box into a scroll box and keep the same height.

Conclusion

All in all, I would say it was a very successful hackathon. We went into the event without any project idea. However, by the end of it, the project that we had built was something totally out of our comfort zone. With no prior experience of Javascript, it had come to our rescue when we were desperate for something that could easily build a dynamic list. Feel free to let me know in the comments if there was a time when Javascript had come to the rescue for you. Keep coding my friends. Cheers!",https://medium.com/swlh/that-awesome-time-javascript-rescued-us-86a883206e95,['Mike Wolfe'],2020-11-03 12:54:02.482000+00:00,756,"Javascript, Flask, Python, Machine Learning, Jinja"
The Power of Visualization in Data Science,"What is Visualization?

The American Heritage Dictionary defines visualization as:

(1) The formation of mental visual images.

In the context of visualization with data, it is necessary to add something to this definition, so that it becomes:

The formation of mental visual images to convey information through graphical representations of data.

If you are pursuing a career in data science, this is one of the most crucial skills that you can master, and it is transferable to virtually any discipline. Let us imagine that you are trying to convince your manager to invest in a company and you present them a spreadsheet full of numbers to explain to them why this is such a good investment opportunity. How would you respond if you were the manager?

If presented in visual form, information is often much easier to digest, especially if it makes use of patterns and structures that humans can interpret intuitively. If you want a quick and easy visualization that requires little to no effort, you can go with something like a pie chart or a bar chart. In terms of developing visualizations, this is often as far as most people go, and often, it is as far as they made need to go for there field of expertise.

Another factor that inhibits our use of visualizations is the amount of data we have available. How do I know if visualization is an appropriate method to communicate a message?

This is a difficult question to answer. One design study recommends that we assess the viability of using visualizations based on the clarity of our task and the location of the information.

Design Study Methodology: Reflections from the Trenches and the Stacks, Michael Sedlmair, Miriah Meyer, and Tamara Munzner. IEEE Trans. Visualization and Computer Graphics, 2012.

If we are in the top right corner of this diagram, it becomes feasible to develop and program interactive visualizations, which is the realm in which data scientists are now entering due to the persistently increasing scale of data resulting from the information explosion.

Information Explosion.

We are now living in a data-driven world, and it is only likely to become more data-driven. This is clear from multiple areas, such as important advances in developing large-scale sensor networks as well as artificial intelligence agents that interact with the world (such as self-driving cars).

In a world where data is sovereign, having the power to develop clear and impactful visualizations is becoming an increasingly necessary skill.",https://towardsdatascience.com/the-power-of-visualization-in-data-science-1995d56e4208,"['Matthew Stewart', 'Phd Researcher']",2020-07-29 22:48:11.782000+00:00,398,"Visualization, Data, Graphical-Representation, Pie-Chart, Bar-Chart"
Understanding the logistic regression algorithm in R,"Performing a simple Logistic regression in Rstudio for mtcars dataset

Classification image (Source- Pixabay)

Linear regression is the most basic type of regression machine learning technique. On the other hand, the logistic regression machine learning technique is one of the most simple and basic algorithms for classification problems. Some of the examples of classification are Online transaction fraud, tumor malignin, and E-mail spam classification. In logistic regression logistic sigmoid function is used to transform the output to return a probability value. So, one can say that the logistic regression algorithm is a predictive analysis algorithm based on the concept of probability. Logistic regression is used to find the outcome of an event in binary values which can represent whether true or false. The outcome can be categorized into 0 or 1 which represents failure and success respectively.

About regression analysis

Regression analysis is an important technique to model and analyzes data. In this technique, we try to minimize the distance between different data points and a straight line or curve. There are many different regression models but linear and logistic regression is the most basic type of regression algorithm. The regression model is built upon the relationship between the dependent and independent variables in the dataset. Based on the type of dependent variable, the number of the independent variables, and the shape of the regression line which can be straight or curve regression algorithm are classified into many types.

Types of logistic regression

Binary logistic regression: It is a regression involving the dependent variable as binary. (Tumour malignant problem) Ex: success or failure Multiple Logistic regression: It is a regression involving the dependent variables with multi-class. Ex: Dog, cats, or sheep

When to use Logistic regression?

In our daily life, there are many events whose result can be from any one of the categories. Some questions like Will India win over Australia in Kohli’s absence? Or Is terrorism danger to humanity more than other problems? — Can have only two possible results like yes or no. Such problems can be solved using logistic regression.

What is sigmoid Function?

The logistic regression algorithm can be called a linear regression with a more complex cost function which is known as the sigmoid function. It is also known as the logistic function. It can map any real value between 0 and 1 but never exactly at those limits.

e/(1+e^-value) (e — Natural base logarithms)

Its graph is an S-shaped curve which can be observed in the image given below –

Image for the logistic function graph

Logistic regression equation –

y = e^ (b0 + b1*x) / (1 + e^ (b0 + b1*x))

Here y is the dependent variable or one which is predicted, b0 is the intercept term and b1 is the coefficient for the single input value (x).

Avoiding overfitting and underfitting in Logistic regression :

While selecting a model for logistic regression it is very important to consider the model fit. Adding an independent variable to the logistic variables in the model will increase the variance and make the model fit. But if the variables are added in less it will result in underfitting or if more than will result in overfitting of the model. By overfitting, it means that the model will not be able to use as a generalized model for other data set beyond our training dataset and vice-versa for underfitting which cannot even be used for the training data.

Example in R programming-

Here in the example, we will simply create a logistic model for understanding the relationship between two variables of an in-built dataset in an R environment which is called mtcars.

The basic function for logistic regression in R is glm(). The syntax of the glm() function is given below-

glm(formula, data, value)

Following is the above terms in the syntax −

· formula is the symbol for the relationship between two variables

· data is the input dataset (mtcars for our example)

· family is R object to specify the details of the model

About dataset

The dataset was brought from the 1974 Motor Trend US magazine and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).

It is a table of 32 observations of a total of 11 variables. Some of the important variables are mpg (miles per gallon), cyl (Number of cylinders), disp (displacement), hp (Horsepower), and wt (weight).

Below is the source code for the R program that we performed on the mtcars dataset for making a logistic model.

R program for logistic regression

Explanation of code

The first line of code declares a vector named tran_data having the input from the mtcars dataset. In the second line, we have created our.data vector which has the glm() function which tries to understand the relationship between the am and the other three variables named cyl, hp, and wt. we have provided data as tran_data here and family is classified as binomial as the outcome is expected in the same manner.

Output

The console image of the studio for output

Explanation of output

As the p-value in the last column is more than 0.05 for the variables “cyl” and “hp”, we consider them to be insignificant in contributing to the value of the variable “am”. Only weight (wt) impacts the “am” value in this regression model.

The weight variable has a p-value of less than 0.05 so we can say that it is significant in contributing to the value of am and am is dependent on weight for its values.

Conclusion

Classification is a machine learning technique from supervised machine learning. In classification, the model tries to predict the category or class to which the dependent variable falls based on its relationship with dependent variables. Logistic regression is a fundamental classification algorithm. It is a linear classifier that is similar to polynomial and linear regression. It is fast and relatively uncomplicated than other classification algorithms. Although it can be best used for binary classification one can also use logistic regression for multi-class classification.",https://medium.com/@vikrant-thakur/understanding-the-logistic-regression-algorithm-in-r-2a8d0ef91bd4,['Vikrant Thakur'],2021-01-01 10:27:09.803000+00:00,968,"Logistic regression, Rstudio, mtcars dataset, Classification, Linear regression"
Using Sentiment analysis to predicting Depression on Social Media,"image @ learn.g2.com

Methods in predictive techniques to model mental well-being, and for understanding health outcomes

Motivation:

According to WHO:

Globally, more than 264 million people of all ages suffer from depression.

Depression is a leading cause of disability worldwide and is a major contributor to the overall global burden of disease.

Close to 800 000 people die due to suicide every year. Suicide is the second leading cause of death in 15–29-year-olds.

Although there are known, effective treatments for mental disorders, between 76% and 85% of people in low- and middle-income countries receive no treatment for their disorder.

However it doesn’t have to be this way

Social media provides an unprecedented opportunity to transform early depression intervention services, particularly in young adults.

Every second, approximately 6,000 Tweets are tweeted on Twitter, which corresponds to over 350,000 tweets sent per minute, 500 million tweets per day and around 200 billion tweets per year [refer].

By using linguistics and behavioral cues it is possible to detect mood and psychosocial disorders.

In addition to mental disorders, these approaches are starting to assess related symptomatology, such as self-harm, stress, and the severity of mental illness without the use of in-person, clinical assessment.

In current case-study, we’ll analysis how NLP techinques can be used to predict mental heath (here we’ll take depression as an example) using social media (twitter tweets).

We’ll be using kaggle dataset (here).

Steps:

Data cleaning/Preparation EDA: Data visualization and Understanding Model building:

we’ll explore following methods: LogisticRegression, MultinomialNB, DecisionTreeClassifier(), RandomForestClassifier(), KNeighborsClassifier(), XGBClassifier and SVC

Let’s look at the dataset which contains tweets and positive negative labels:

We’ll then clean data, by using below steps:

remove punctuation and special characters

remove stop words

split text and tokenize

apply stemmer

remove small words

First let’s see frequency of words in positive tweets and negative tweets separately.

1. Positive tweets:

positive tweet has “love”, “Thank”, “today”, “know, “work” as more frequent words:

2. Negative tweets:

negative tweet has “miss”, “work” “today”, “time,” sleep, “sorry” as more frequent words:

We’ll split our data to test and train. And then tokenize using TF-IDF vectorizer. TF-IDF is a measure of originality of a word by comparing number of times a word appears in a docs with number of docs the word appear.

After vectorizing our data-set looks like this:

We’ll now run the following set of classifiers:

Which we can compare and see the accuracy score:

Compare Accuracies

Conclusion

We are able to achieve good accuracy with all the models, (we also need to validate the possibility if the model has been inaccuracies since our data is likely to be affected by data imbalanced by exploring data balancing techniques).

While classification techniques are good for bench-marking, for more generalized models it would make more sense to explore Long Short Term Memory network (LSTM) model along with Convolutional neural networks (CNNs). [refer]

My kaggle Notebook can be found here

Linkedin article can be found here",https://medium.com/@codeserra810/using-sentiment-analysis-to-predicting-depression-on-social-media-9f11f0b67e4f,[],2020-12-19 19:57:46.062000+00:00,449,"mentalhealth, depression, NLP, socialmedia, machinelearning"
Algorithmic Bias,"Algorithms have become the most effective elements in today’s world and they do the computations and decision makings more reliable and faster than human. They make decisions about the news we read, the jobs we get, the people we meet and the ads we see. According to the stories we read on Facebook social net, texts we send on Telegram messenger, photos we like on Instagram application and the search results we see on Google and etc, we generate Big data. Big data is used to make decisions about health care, employment, housing, education and policing.

Contrary to what many of us might think, technology is not objective. Algorithm’s behavior depends on people who invented them: what code they write, what data they use to train the machine learning models, and how they test the models after they’re finished. For example, Give an image classification algorithm millions of labeled cat pictures and it will be able to tell you whether a photo it hasn’t seen before contains a cat. Give a speech recognition algorithm millions of voice samples along with their corresponding written words, and it will be able to transcribe spoken language faster than most humans.

The more data you feed to your learning algorithm, The better it becomes at the task it’s designed for.

According to above sentence, any amount of data you feed your algorithm of white people, you should do the same for dark-skinned people or any amount of data you feed the algorithm including men, do the same for women and transgenders, etc.

Software programs are not free of human influence, because generally we are creating them and they behave on data that may be provided by humans, machines or a combination of both and according to them algorithms may inherit bias.

Machine learning algorithms learn and evolve based on what people do online.

For instance, a new study by Carnegie Mellon University researchers found that Google’s online advertising system showed an ad for high-income jobs to men much more often than it showed the ad to women. Algorithmic bias is getting a more serious problem than the past and governments and algorithm developers don’t care about it, or if they do, that’s not enough :)

If the bias exists inside the algorithms that make important decisions, goes unrecognized and unchecked, it could have dangerous consequences, especially for poor communities and minorities.

TED talk provided by Joy Buolamwini

Check this TED talk, provided by Joy Buolamwini, to see a real problem, where a machine doesn’t recognize black people’s face well, but it works great for white people.

Another example is a joint project by researchers at Microsoft and Boston University, in which they found sexist biases in word embedding algorithms, which are used in search engines, translation and other software that depend on natural language processing. Among their findings of the behavior of word embedding algorithms was a tendency to associate words such as “programming” and “engineering” to men and “homemaker” to women.

Algorithmic bias is not a new topic. For decades, academics and experts warned companies and algorithm developers. But what makes it critical at this time, is that these algorithms are getting bold in the everyday decisions we make.

Open source VS. algorithmic bias

One of the best ways to fight algorithmic bias is by evaluating the training data fed into machine learning models. The data itself might have a skewed distribution; for instance, programmers may have more data about United States citizens than immigrants, and about rich men than poor women. Such imbalances will cause an AI to make improper conclusions about how our society is in fact represented.

open source technology has extreme potential to help in this fight against algorithmic bias. Modern artificial intelligence is dominated by open source software, from TensorFlow to packages like scikit-learn. The open source community has already proven extremely effective in developing robust tested machine learning tools, so it follows that the same community could effectively build anti-bias tests into that same software.

Open source technology has also proven to be extremely effective for vetting and sorting large sets of data. Nothing should make this more obvious than the domination of open source tools in the data analysis market (Weka, Rapid Miner, etc.). Tools for identifying data bias should be designed by the open source community, and those techniques should also be applied to the plethora of open training data sets already published on sites like Kaggle.

People VS. algorithmic bias

Companies that develop and apply machine learning systems and government regulators, show little interest in monitoring and limiting algorithmic bias. They should pay more attention to this topic before it becomes out of our hands. Plus, Technology companies use all sorts of mathematical models and aren’t transparent about how they operate!

It would be impossible for humans to oversee every decision an algorithm makes. But companies can regularly run simulations to test the results of their algorithms.

Algorithmic bias is a human problem, not a technical one. Our thought, biases, interests, hates, etc, reflects in AI programs. The real solution is to start removing bias in every aspect of our personal and social lives. This means endorsing diversity in employment, education, politics and more. If we want to fix our algorithms, we should first fix ourselves.

Education is extremely important. We all know people who may be unaware of algorithmic bias but who care about its implications should talk to those people and explain both “how the bias is formed” and “why it matters”.",https://medium.com/datadriveninvestor/algorithmic-bias-181f68dcd06b,['Soroush Hashemi Far'],2019-03-29 05:38:17.096000+00:00,900,"Algorithms, Machine Learning, Big Data, Open Source Technology, Algorithmic Bias"
Computer Vision Powered Microscopy — Subjectivity Dethroned.,"How can we remove subjectivity from science, whilst keeping the human in the loop?

A VERY HUMAN PROBLEM

When imaging scientists look down the microscope, they are often presented with a bewildering choice. Required to select specific cells from hundreds, thousands or millions, an unavoidably subjective process ensues. Due to time pressures and resource availability, it is likely that the scientist only has time to select a handful of cells for each condition. The question is, which of the many cells are the most appropriate to image in order to represent the problem at hand?

The subjectivity of scientists is discussed in the book ‘Thinking Fast and Thinking Slow’ by Nobel Prize-winning author, Daniel Kahneman: “Contrary to the rules of philosophers of science, who advise testing hypotheses by trying to refute them, people (and scientists, quite often) seek data that are likely to be compatible with the beliefs they currently hold”. Accordingly, it’s highly likely that the choices imaging scientists make will be unknowingly influenced to some degree by their hypothesis or the will of their supervisor. It should be the goal of every scientist to develop transparent methods that are open to critique by their contemporaries.

UNBIASING MICROSCOPY

To address this bias issue in the realm of microscopy, I’m developing systems that will rapidly and intelligently survey the cells present across a slide, then provide information and insight regarding their overall distribution. This involves cellular identification using machine learning and computer vision, and characterisation through simple measurements such as intensity of size, or through more complicated higher-level cellular features. The idea is that, with an overall awareness of the cells present, individual cells can be selected fairly for detailed study. Figure 1 shows how bias could enter an experiment and how through automation and coarse characterisation, scientists can perform more robust experiments.

Figure 1: Visualisation of the risks of biased sampling. A and B) Two scientists will choose cells under the microscope based on a preconception of what they expect to see, and their choices maybe different. C) Using automated microscopy we can measure and visualise the whole distribution of cells (all points) and put observations in context (red and blue points). D) In real data (e.g. C127 cells treated with BrdU and stained with DAPI) there exists a bimodal distribution. Some cells have resisted the BrdU uptake and are brighter than those cells that have taken up the BrdU (green). In this example, only cells that have taken up the BrdU are of interest for subsequent measurements and comparisons. Coarse intensity measurements of all cells present can be used to advise a scientist of the overall distribution so they can make informed decisions for the remainder of their experiment.

The challenge of developing such technologies is achieving rapid imaging of large numbers of cells using hardware that’s compatible with the variety of specific and custom microscopes used in modern research practices. Furthermore, the system should be available for scientists from all backgrounds, not limited to those who understand high-end analysis or robotics.

In my recent paper I outline a microscopy system that can automatically locate and image cells, then return a distribution based on their properties: AMCA, the Automated Microscope Control Algorithm. The technique uses regular microscopy equipment (a camera and automated stage) and the special ingredients: machine learning, computer vision and cheap commercial commuting power.

The key technologies which makes this cheap and readily available are the Nvidia Jetson and LattePanda development boards. With such technology it becomes economically feasible that every microscope would be used semi-autonomously, saving time and optimising resources. The Jetson provides the system with extremely fast GPU processing, and the LattePanda provides legacy support, interfacing with older hardware drivers to support a range of microscopy hardware.

This system can be trained to identify and image cells in 3-D, based on simple 2-D annotation of cells (which can be performed by anybody). The annotation is simple, involving drawing bounding boxes around cells, but it provides a system that can be trained to independently recognise and localise cells on a slide. Although it can take several hours to image a whole slide, once trained and validated, the system can be left unsupervised to image cells across the whole specimen at night or over a lunch break. Once completed, the scientist may want to review the performance of the algorithm in specific areas, or be taken to a specific area and decide whether to image in more detail. This process is supported through a novel augmented reality system, which allows the user to preview the outputs of the analysis directly as they look down the binocular of the microscope (Figure 2). This is the first open-source fluorescence microscopy augmented reality system ever developed. Through this technology we keep the scientist in the experimental loop which is essential for building trust and resolving issues.

Figure 2: View down a microscope eye-piece as the focus is changed and the augmented reality system updates the output of the cell identification algorithm.

THE FUTURE OF AMCA ET AL

I’m aiming to extend the capabilities of the imaging system to include dynamic imaging in response to visual cues. For example, a cell has with an unusual appearance, perhaps about to divide or undergo some other process, could be interesting to study it in greater detail. I hope that my system will be able to spot these dynamic processes, adding an important dimension to its capabilities. I hope this research will have its first meaningful impact within the MRC WIMM, and am excited to apply this system to your biological questions. I invite you to get in touch to collaborate and hopefully help make your imaging more efficient and objective.

This blog post was adapted and updated from the original post written by Dominic Waithe and edited by Alexandra Preston (Drakesmith group).",https://medium.com/analytics-vidhya/computer-vision-powered-microscopy-subjectivity-dethroned-11eb21cade40,['Dominic Waithe'],2021-01-10 14:40:18.204000+00:00,956,"Subjectivity, Science, Human, Microscopy, Automation"
Econometrics 101 for data scientists,"People in data science are typically interested in working with micro-scale data to advance domain specific business problems. However, there is a huge opportunity for them to break into economics and social science problems at a macro scale which are traditionally subject matters of social science disciplines. Econometrics is probably the closest domain for data scientist to get in that direction. The purpose of this article is to introduce — to data scientists coming from different backgrounds — some basic concepts and tools used in econometrics and how they relate to data science practices as we know it. This is mostly an introductory post scratching the surface but if there is enough interest I might write more posts with specific tools and use cases.

What is econometrics?

There are basically two kinds of economists: those who develop theories and those who test those theories. Econometricians are the latter kinds of people who use statistical techniques to understand and explain social phenomena that has economic dimensions.

One example of econometrics use case is testing the hypothesis that the length of education has anything to do with wage rate and to be able to quantify that relationship using a combination of economic theory and data. Another example would be measuring price elasticity of gasoline demand — i.e. sensitivity of gasoline consumption to a change in market price — using historical data.

What problems typically econometrics solves?

Econometrics deal in the domain of all the societal problems that has anything to do with economics. The domain largely deals with macro-economic phenomena such as employment, wage, economic growth, environment, agriculture, inequality etc. Here’s just a small sample of such macro-economic issues from the Journal of Econometrics:

The econometrics toolbox

With some exceptions, most things in econometrics toolbox should be familiar to data scientists or any computational scientists. Here is a sample of some commonly used tools:

Descriptive statistics (measures of central tendency and dispersion)

Inferential statistics and hypothesis testing

Time series modeling and forecasting

Regressions of all kinds

Causal inference methods

In addition to statistical models, economists use a wide range of mathematical models too. In fact, economists are often “accused” that they are using too much mathematics in modeling human behavior. There is widespread criticism of the discipline that they try to mimic deterministic models of natural systems to describe complex human behavioral problems that are rather unpredictable or probabilistic, to say the least.

And last but not least, arguably the most important and powerful tool in econometrics toolbox is the Panel Data Models. This topic deserves a separate post but I will briefly touch upon below.

Panel data models

If you haven’t heard about “panel data” or “longitudinal data” before then here is some description if it. A panel data is basically a multi-dimensional data of an observation that is measured repeatedly over time. In other words, it’s a dataset where multiple variables are measured, over time, on the same units — such as individuals, organizations, households, city, country.

This is how a panel data looks like. [source]

So as the name suggests, a panel data model is a statistical tool that provides information on the difference across individuals and over time. The following 4 are the most popular kinds of panel data models:

Pooled OLS

Fixed effects

Random effects

Mixed effects

Understanding each of these models require some discussion and mathematical formulation. This link provides a nice discussion on these modeling frameworks.

Some resources for data scientists

Econometrics with R: an open source book with theories along with R implementation of econometrics models

Principles of Econometrics With R: another book with R implementation. I think this book might be more accessible to beginners in this field

In this source there are panel data modeling related resources in one place

Those who likes to watch rather than read, here’s a couple of YouTube channels on simple explanation of panel data models, their estimators and associated codes in R

Last words

Data scientists have a great edge in diving into social and environmental issues that society is facing today. They should not feel worried that this is a domain of economists and they are the outsiders. I’d rather say, data scientists with diverse background and outside box thinking capability are rather better positioned to describe and predict societal problems than people trained in traditional disciplines.

I’m on Twitter if you want to get in touch with questions.",https://towardsdatascience.com/econometrics-101-for-data-scientists-584f4f879c4f,['Mahbubul Alam'],2020-02-06 03:12:46.511000+00:00,703,"Econometrics, Data Science, Panel Data Models, Descriptive Statistics, Inferential Statistics"
The Top 10 Best Places to Find Datasets 📊,"Awesome Data

Awesome Data is a GitHub repository with a seriously impressive list of datasets separated by category. It is updated regularly.

Data Is Plural

Jeremy Singer-Vine’s Data Is Plural weekly newsletter has great fresh data sources. I’m always impressed by the quality. The archive is available here.

Kaggle Datasets

In addition to competitions, Kaggle has a huge range of datasets. Kaggle Datasets provide great summary information and previews for most datasets. You can download the data or use their platform to analyze it in a Jupyter notebook. You can also contribute your own datasets and make them public or private.

Kaggle is great for browsing or searching for a particular topic.

Data.world

Like Kaggle, Data.world provides a wide range of user-contributed datasets. It also offers a platform for companies to store and organize their data.

Google Dataset Search Tool

I think it’s safe to say that Google knows a thing or two about search. It recently added a separate search functionality for datasets through its Google Dataset Search Tool. It’s worth a shot if you’re looking for data on a particular topic or from a particular source.

OpenDaL

OpenDaL is a data aggregator that allows you to search using a variety of metadata. For example you can search based on time or search by location by selecting part of a map.

Screenshot from OpenDaL.

Pandas Data Reader

The Pandas DataReader will help you pull data from online sources into Python pandas DataFrames. Most of the data sources are financial. Here’s the list of available data sources as of October 31, 2020. 🎃

Here’s how you use it after installing it into a Python environment with pip install pandas-datareader .

import pandas_datareader as pdr

pdr.get_data_fred('GS10')

VisualData 👓

If you are looking for computer vision datasets, VisualData is a nice new source. It has some handy filtering options. Thanks to Jie Feng for reminding me of it! ***Added Nov. 2, 2020.***

Data.gov 👓

If you are looking to use the US government’s datasets, Data.gov has over 217,000 fo them! Thanks to Michael Wallace for recommending it! ***Added Nov. 4, 2020.***

Python API Wrappers 🐍

I recently updated my list of Python API wrappers to help users see whether a package is popular and being maintained. It now uses shields.io to automatically display GitHub stars and the date of the most recent commit. This list was originally forked from GitHub repo of Real Python via johnwmillr. My repo contains what I believe is the largest updated list of Python API wrappers — many of which can help you find the data you might need for a project.

APIs

Getting data from a documented API using Python might sound intimidating if you haven’t done it before, but it’s really not bad. Check out my guide to getting data from APIs here. 🚀

Make your own

When all else fails, collecting your own data can be an excellent way to create a dataset for your needs. 😉

Recap

Do you have a favorite place to find data? Awesome! Share it on Twitter or leave it in the comments! 🎉

I hope you find this tool helpful when you’re searching for data sources. If you do, please share it on your favorite social media. 🚀

I write about Python, data science, and other tech topics. If you’re into that kind of stuff read more here and subscribe to my Data Awesome newsletter for awesome monthly curated data resources.",https://towardsdatascience.com/the-top-10-best-places-to-find-datasets-8d3b4e31c442,['Jeff Hale'],2020-12-13 15:23:21.510000+00:00,541,"data, datasets, Python, API, Kaggle"
To Pretrain or Not to Pretrain: Examining the Benefits of Pretraining on Resource Rich Tasks,"Since 2018, pretrained NLP models with variants of Masked Language Model (MLM) have gained a lot of popularity. This blog by Ankit Singh does an amazing job at explaining Bidirectional Encoder Representations from Transformers (BERT) where the concept of MLM was first introduced. Following BERT, a number of pretrained models have been developed. BERT-base was trained on 4 cloud TPUs for 4 days and BERT-large was trained on 16 TPUs for 4 days. In 2019, this paper brought down the training time to 76 minutes. Yet, it is undeniable that pretraining a model requires a lot of time and resources. Thus, our paper for today asks a critical question, whether pretraining a model has any benefit on resource rich tasks.

The future of NLP appears to be paved by pretraining a universal contextual representation on wikipedia-like data at massive scale. Attempts along this path have pushed the frontier to up 10× to the size of wikipedia (Raffel et al., 2019).

However, (Raffel et al., 2019) shows that it is not necessary for such models to always be state of the art. To understand whether pretraining a model has benefits or not, the authors evaluate the performance of pretrained models against a model trained from scratch. They focus on the task of multi-class text classification for two main reasons:

(i) it is one of most important problems in NLP with applications spanning multiple domains.

(ii) large sums of training data exists for many text classification tasks, or can be obtained relatively cheaply through crowd workers (Snow et al., 2008).

Datasets

Three sentiment classification datasets that range from 6 to 18 million examples are used for this comparative study:

Yelp Review Amazon sports review Amazon electronics review

Since the focus is on multi-class text classification, the goal of the models is to predict rating in five points scale {1, 2, 3, 4, 5}. The dataset size and distribution across the five points is shown in the table below. The authors split the dataset into 90% for training and 10% for testing.

Source: From the paper

Models

The three models used in the study are described below:

RoBERTa — A transformer based model pretrained with MLM objectives on a large corpus. LSTM — The authors train a bidirectional LSTM. LSTM + Pretrained Token Embedding — Initialized the token embeddings with Roberta pretrained token embedding. The embeddings are frozen during training.

Experimental Setup

Comparison between the experimental setup of the two models.

Results

The results in the paper are explained based on two parameters — data size and inference time.

Impact of Data Size

Source: From the paper

The authors trained the models on varying sizes of the datasets to compare the performance of these models. 1%, 10%, 30%, 50%, 70% and 90% of data was used.

The results of the experiments are shown in Figure 1 and Table 2.

With an increase in the number of examples, the difference in accuracy between RoBERTa and LSTM decreases.

For example, when both models are trained with 1% of the Yelp dataset, the accuracy gap is around 9%. However, as we increases the amount of training data to 90%, the accuracy gap drops to within 2%. The same behaviour is observed on both Amazon review datasets, with the initial gap starting at almost 5% for 1% of the training data, then shrinking all the way to within one point when most of the training data is used.

2. Results show that an LSTM with pretrained RoBERTa token embeddings always outperforms the ones with random token initialization.

This suggests that the embeddings learned during pretraining RoBERTa may constitute an efficient approach for transfer learning the knowledge learned in these large MLM.

It is important to note that the accuracy gap between the models is within 2% for the Yelp dataset and less than 1% on the Amazon datasets. It is even more important to note that while RoBERTa-Large is trained on 304M parameters, LSTM-4–512 + Large is trained on 25M parameters. That is a difference of 279M parameters for a maximum accuracy gap of 1.71% on the Yelp dataset.

Inference Time

Source: From the paper

On investing the inference time of the three models on CPU and GPU the authors find that the LSTM model is 20 times faster even when compared to RoBERTa-Base as shown in Table 3. The authors made another interesting observation-

Another observation is that although using the Roberta pretrained token embedding introduces 10 times more model parameters compared to vanilla BiLSTM, the inference time only increases by less than 25%. This is due to the most additional parameters are from a simple linear transformation.

Conclusion

Our findings in this paper indicate that increasing the number of training examples for ‘standard’ models such as LSTM leads to performance gains that are within 1 percent of their massively pretrained counterparts.

The authors propose to run experiments on other large scale datasets to evaluate if these findings hold true for different NLP based tasks.

One way to interpret our results is that ‘simple’ models have better regularization effect when trained on large amount of data, as also evidenced in the concurrent work (Nakkiran and Sutskever, 2020).The other side of the argument in interpreting our results is that MLM based pretraining still leads to improvements even as the data size scales into the millions. In fact, with a pretrained model and 2 million training examples, it is possible to outperform an LSTM model that is trained with 3× more examples.

While we see that there is a trade-off between accuracy, number of parameters and amount of data needed for training, this paper helps us make better design decisions based on the resources we have available

References:",https://medium.com/computers-papers-and-everything/to-pretrain-or-not-to-pretrain-examining-the-benefits-of-pretraining-on-resource-rich-tasks-6d6735a9e611,['Krisha Mehta'],2020-06-27 00:28:14.799000+00:00,915,"Nakkiran, S., & Sutskever, I. (2020). Exploring Generalization in Deep Learning. ar Xiv preprint ar Xiv:2001.09977.Raffel, C."
User Management With Firebase and Python,"Set Up a New Firebase Project and Credentials

First things first, we need to set up our Firebase credentials so we’re able to use their APIs. Their documentation is detailed and easy enough to follow. You can find it here.

Anyway, let’s still go through the setup process so everybody is on the same page. We’ll only discuss the necessary steps here, so if you’d like to know more details, please visit the page I linked above.

Python version and pip installation

Firstly, your Python version needs to be 3.5 or above. Ensure you have it installed.

Next, we can install the Firebase Admin SDK using pip. Here’s the command you can run from your terminal to do that.

$ pip3 install firebase-admin

Just a reminder, it’s always better to install a package like this in our virtual environment to keep it contained in just the project that needs it.

Generate Google Application Default Credentials

Before we can use the SDK, we need to generate our Google Application Default Credentials.

Firstly, go to your Firebase console, and click on “Add project.”

Firebase console

Let’s give our project a name: python-admin-sdk-demo .

Name the Firebase project

The next step is to enable Google Analytics. For the purpose of this tutorial, we don’t need this to be turned on, so let’s just turn it off. After you disable it, click on “Create project.”

Google Analytics — turned off

Firebase will then set up and generate the new project for you. Once it’s finished, click on “Continue” to go to your Project Overview page.

Project Overview

On the top left-hand side, click on the cog icon next to “Project Overview,” and select “Project settings.”

Project settings

Then, navigate to the Service accounts tab. Click on Python for the Admin SDK configuration snippet.

Service accounts

Go ahead and click on “Generate new private key.” A window like below will pop up. As it says in the warning on the window, remember to keep the private key secure. Do not commit this to your project repository.

All right, we can continue and click on “Generate key.”

Generate a new private key

Firebase will open a prompt and ask you where you want to save the JSON file. Let’s create a new folder called python-admin-sdk-demo and store the file in there.

Great! We have what we need to be able to call the Firebase Admin APIs now. Let’s see how we can set it up in Python.

Here’s the instruction from Firebase:

“When authorizing via a service account, you have two choices for providing the credentials to your application. You can either set the GOOGLE_APPLICATION_CREDENTIALS environment variable, or you can explicitly pass the path to the service account key in code. The first option is more secure and is strongly recommended.”

So let’s follow their recommendation to set an environment variable named GOOGLE_APPLICATION_CREDENTIALS and give a value of the path to the JSON file we just downloaded from Firebase.

From your terminal, let’s execute the following command to set the environment variable.

~/demo/python-admin-sdk-demo ❯ export GOOGLE_APPLICATION_CREDENTIALS=/Users/billyde/demo/python-admin-sdk-demo/python-admin-sdk-demo-1a64n-firebase-adminsdk-ze72a-5a36713w1q.json ~/demo/python-admin-sdk-demo ❯ echo $GOOGLE_APPLICATION_CREDENTIALS

/Users/billyde/demo/python-admin-sdk-demo/python-admin-sdk-demo-1a64n-firebase-adminsdk-ze72a-5a36713w1q.json

Cool. Let’s write the Python code now.

Create a new file in your project directory and call it initialise_firebase_admin.py . This is what the content will be:

initialise_firebase_admin.py

Now, if we go back to the terminal and execute the following command, we’ll be able to see that the Firebase Admin App was successfully initialised.

~/demo/python-admin-sdk-demo ❯ python initialise_firebase_admin.py

<firebase_admin.App object at 0x125d19wg3>

Perfect! We’re ready to move on to more exciting stuff now.",https://medium.com/better-programming/user-management-with-firebase-and-python-749a7a87b2b6,[],2020-03-13 23:41:49.551000+00:00,549,"Firebase, Credentials, Setup, Google Application Default Credentials, Python version and pip installation"
Artificial Intelligence Breakthrough in Mining Industry,"Mining is first and foremost a source of natural minerals, which are essential for all countries to sustain and improve their living conditions. Mined materials are needed for the construction of roads and hospitals, the manufacture of cars and homes, the manufacture of computers and satellites, the generation of electricity, and the provision of a variety of other products and services to customers. Mining is also socially significant for producing regions and nations. It creates jobs, pays dividends, fund hospitals, colleges, and other public services. Mining encourages the creation of world-class universities in geology, mining engineering, and metallurgy, as well as the manufacture of mining machinery, engineering and environmental facilities, and the development of world-class universities in geology, mining engineering, and metallurgy. For several producing countries, mining provides significant economic prospects and resources. (1) Mining is critical to economic growth; what are the steps that this industry is taking to further its development? Let’s take a look at some of their most recent discoveries.

Mining is a global industry that is fundamental to every product we use. A vital component of the mining industry is efficiency because most of the production revolves around transforming matter into different forms. It is often the case that small improvements in execution speed, process efficiency, or reduced downtimes separate a profitable operation from a complete failure. Check disclaimer on my profile and landing page. Nowadays, artificial ​intelligence is readily available in many of the products and services we use. Furthermore, cloud computing matured, hardware prices decreased, and machine to machine communication improved leading to unexpected advances in mining and industrial technology. Add the latest advances in analytics and artificial intelligence to the mix, and you get the perfect environment for improving efficiency in all areas of a mining operation. (2) Artificial Intelligence? In mining? This breakthrough is incredible! I believe that, with this technology, this mining industry will be well-positioned for the foreseeable future!

This technology will provide the very eyes and ears miners could use to identify mineral-rich projects through delivering artificial intelligence (AI) to unearth a higher likelihood of success. sponsored post. AI is also used to understand better the environment and the terrain where new development will take place. These innovations are limitless! I will be looking forward into this!

Source: https://daciandata.tech/artificial-intelligence-is-changing-the-mining-industry-examples-of-successful-applications/",https://medium.com/@coryblack-49214/artificial-intelligence-breakthrough-in-mining-industry-55757432735d,['Cory Black'],2021-05-03 11:09:46.438000+00:00,378,"Mining, Natural Minerals, AI, Cloud Computing, Machine Learning"
Finding the Balance Between Evaporation and Rainwater Harvesting for Reforestation.,"Photo by Sebastian Unrau on Unsplash

An ancient form to harvest water is by making a body of water or pond. Once the pond is dug, and the pond surface is saturated with water the water level starts to rise. And after some time a huge amount of water can be saved. Nonetheless, a common pitfall of the system is the loss due to absorption through the ground and evaporation. However, modern approaches use geomembranes or some other waterproofing techniques to prevent absorption loss. The following exemplifies a method to determine the size of an insulated water pond fed by a small rainwater harvesting system that provides water to a small reforestation operation.

The reforestation operation consists of a small tool shed with a rainwater harvesting system with a surface area of 16 square meters and about twenty small pine trees planted in plastic bags. After a few months when the trees are big enough the trees are planted in the surrounding area. Once the trees are in its final location the trees are irrigated for a few weeks to ensure its survival. The daily demand for such operation is about 50 liters per day or 1500 litters per month.

The data

The data consists of about ten years of climatological records, although the data is outdated, it represents the best approximation available. To analyze the available data three different scenarios will be defined. The max scenario consists of the maximum data value of a given day, the min scenario consists of the minimum value of a given day, and the mean scenario consists of the mean value of a given day.

Daily rainwater data ranges from zero to eighty millimeters of waters per day, and the rain season has a duration of about one-third of the year. Whereas evaporation data appears to be more consistent throughout the year. Ranging from zero to ten millimeters of evaporation, however the mean evaporation values plateau in two millimeters approximately. Both evaporation and precipitation data display a flat line on zero on the min scenario, thus the min scenario will not be discussed any further.",https://medium.com/swlh/finding-the-balance-between-evaporation-and-rainwater-harvesting-for-reforestation-db8e455ba73,['Octavio Gonzalez-Lugo'],2020-11-14 06:00:20.113000+00:00,347,"Rainwater Harvesting, Geomembranes, Insulated Water Pond, Reforestation Operation, Climatological Records"
Getting started with Google Maps in Python,"Lets get started with google maps in python! We are going to cover making a basic map, adding different layers to the maps, and then creating driving directions!

Before this article, I did a quick tutorial with basemap in python. In this tutorial, I will cover using the google maps api in jupyter notebook. The first step here is to get an api key from google at this link. Once this is done, you may need to run the following command in terminal and restart jupyter:

jupyter nbextension enable --py gmaps

Once this is complete, you are ready to jump right into making beautiful maps!

Initially, we are going to want to read in our api key. To do this I completed the following steps:

Save the API key in a text file Write code to read in the key and save it as a variable

with open('apikey.txt') as f:

api_key = f.readline()

f.close

Once this is complete you are ready to use the google maps api. For reference, all the material in this tutorial comes from the documentation which can be found here.

You may also need to do a pip install gmaps prior to working in jupyter.

Once this is done, you can initialize the session with the command:

import gmaps

gmaps.configure(api_key='Your api key here')

After this, you can create a generic map by just creating a center location and zoom level:

new_york_coordinates = (40.75, -74.00)

gmaps.figure(center=new_york_coordinates, zoom_level=12)

This returns the following figure:

Easy first map using google maps api

Now, lets say that you would like to add earthquake data to your map. There is already a dataset that google provides, and you can use this very easily to add more information to your plot using the heatmap function.

import gmaps

import gmaps.datasets # Use google maps api

gmaps.configure(api_key=api_key) # Fill in with your API key # Get the dataset

earthquake_df = gmaps.datasets.load_dataset_as_df('earthquakes') #Get the locations from the data set

locations = earthquake_df[['latitude', 'longitude']] #Get the magnitude from the data

weights = earthquake_df['magnitude'] #Set up your map

fig = gmaps.figure()

fig.add_layer(gmaps.heatmap_layer(locations, weights=weights))

fig

Heatmap of Earthquake activity

This can also be used in the same way I used in my previous tutorial for fires around CA. Here is how the map looks when you add a layer of current fire activity:

Heatmap of current fire activity

Finally, you can get out driving directions from the api. To do this there are a couple of steps.

To plot route on map:

Define location 1 in coordinates Define location 2 in coordinates Create layer using gmaps.directions.Directions Add layer to the map

Code:

import gmaps

#configure api

gmaps.configure(api_key=api_key) #Define location 1 and 2

Durango = (37.2753,-107.880067)

SF = (37.7749,-122.419416) #Create the map

fig = gmaps.figure() #create the layer

layer = gmaps.directions.Directions(Durango, SF,mode='car') #Add the layer

fig.add_layer(layer)

fig

Driving Directions

And to get more information like the distance and travel time, you can use the gmaps client. The code for this is below:

Shows the duration and distance for the directions

There is a ton more that you can do with the api, and the documentation is awesome. If there is something I did not cover, it is a great resource in general!

https://media.readthedocs.org/pdf/jupyter-gmaps/latest/jupyter-gmaps.pdf",https://medium.com/future-vision/google-maps-in-python-part-2-393f96196eaf,['Elliott Saslow'],2019-08-02 20:47:50.181000+00:00,484,"google maps, python, jupyter notebook, api key, gmaps"
Origin-Destination Network Analysis of NY CitiBike Data,"New York City’s privately-owned CitiBike program is the largest bikesharing program in the United States. Operated by Motivate, a Lyft subsidiary, the program has grown to include over 1,000 stations and 15,000 bikes.

CitiBike data is publicly available here. Trip histories including time, origin and destination location, and demographic data are released monthly. The August 2020 file contained over 2.3 million trip entries. I performed a network analysis on this dataset using Python and NetworkX to visualize the spatial distribution and connectivity of CitiBike stations, and to determine which stations are most and least connected.

September’s CitiBike dataset consisted of 2,329,514 trips spread across 1,059 stations. A simple plot of the stations can be seen here:

CitiBike Station Locations, as of Sept. 2020, New York City.

Manhattan and downtown Brooklyn are well-represented here, while the rest of Brooklyn, central and eastern Queens, the Bronx, and Staten Island are not served by CitiBike.",https://medium.com/@moseslevich/origin-destination-network-analysis-of-ny-citibike-data-1ae60e967cfd,['Moses Levich'],2020-12-07 04:42:44.559000+00:00,148,"Citi Bike, Motivate, Lyft, New York City, Bikesharing"
Improving Collaborative Filtering With Clustering,"Now, let’s go into greater detail on how k-means fits in with collaborative filtering and how k-means needs to be modified to work with sparse input.

K-Means

First, a brief conceptual overview of k-means.

K-means works by finding the most central point for each of the k clusters, choosing the closest data point to that center, re-constructing new clusters based on those centroids, and repeating those steps until convergence.

In user-based collaborative filtering, these sample data points would have n dimensions since each user is represented as a row in the m x n matrix. The process of forming the clusters looks like this:

For all rows u of the m x n matrix (users), we assign u to the closest centroid using a distance function (like the Manhattan distance and the Euclidean distance). Reassign each of the k centroids to the centers of their newly formed clusters. Repeat.

Dealing With Sparsity

The main way we accommodate the sparse input is to only use specified ratings. We can find the mean of each cluster by only using the specified ratings within that clusters. We can also find the distance between each user and each centroid by only using the common specified ratings between each pair.

(Note that all of the above works for item-based collaborative filtering when we cluster columns instead of rows.)

So now, say we have implemented this correctly, and we now have k clusters of users. Now, every time we have to compute pairwise similarity for some user, we only compute this similarity across users in the same cluster!

This will drastically improve runtime, especially when the cluster are fine-grained. Keep in mind, this efficiency, as always, comes at a cost — this time, its accuracy.

Prediction function (from Recommender Systems: The Textbook)

From here, we’re golden. Once we have the peer group, it’s the same as before. We can use the same neighborhood prediction function from the user-based collaborative filtering we’ve already seen, and voila: piping hot predicted ratings.

Clustering isn’t the only way to efficiently find peer groups for neighborhood-based methods. You can also improve the training stage of collaborative filtering with dimensionality reduction, which I cover in this article.",https://medium.com/@jwu2/improving-collaborative-filtering-with-clustering-88c63bdae7cc,['Jackson Wu'],2019-05-27 21:02:56.811000+00:00,352,"K-Means, Collaborative Filtering, Sparse Input, Manhattan Distance, Euclidean Distance"
4G+AI Creates Another John Deere But No Longer In USA,"If you paid attention to our earlier newsletter, you probably noticed that we are very interested in the applications of AI+IOT in Agriculture. Since Agriculture is the pillar of ASEAN, there is huge room for Agr-itech when infra investment accelerates and labours are moving to the urban.

In China, the agricultural sector has emerged as a huge opportunity for drone makers in recent years. Competition among Chinese commercial drone makers in the space is expected to remain heated in large part because of the duel between DJI , the world’s largest drone maker, and XAG ,the Guangzhou-based drone maker we will talk about today.

Founded under the name XAircraft in 2007, XAG rebranded in 2014 is now known for its agricultural drones that help with tasks such as seeding, spraying crops, fertilisation and plant protection. Other products include remote sensing drones and an unmanned ground vehicle that helps with protecting crops, scouting fields and delivering materials on farms. Just in this month, XAG raised US$182 million in a new funding round led by Baidu Capital and SoftBank Vision Fund II when claiming their sale revenue exceeding 150M USD.

Make AI-driven Precision Agriculture Practical

Artificial Intelligence, through continuously analysing massive data related to climate, lands, crop growing, etc., while automatically designing and optimising algorithms for decision-making, can help farmers diagnose plant diseases, predict natural disasters and employ appropriate resXAG’sces to close the yield gap. At XAG, AI-powered intelligent devices such as drones and sensors have been leveraged to establish digital farming infrastructure in rural areas and enable precision agriculture which, for example, accurately target pesticides, seeds, fertilisers and water to wherever it is needed. XAG has applied its drone-based direct seeding system — called JetSeed — from the testing stage to commercial adoption on more than 650 million square metres of rice fields in 11 provinces across China since April last year, according to the company.

Efficiency: The efficiency of an unmanned aerial vehicle is 60–80 times that of traditional manual work, which can save more than 30% of pesticides and 90% of water resources. Cost: The average price of XAG’s 2020 agricultural drone kit has been reduced by 20,000 yuan (3000USD). In 2018, the product price was more than 70,000 yuan(11000USD), and in 2019 it was about 52,000 yuan（9000USD）

AI-backed Digital Finance for Smallholder

Another problem that XAG is trying to solve is lack of financial resXAG’sces in the rural regions of developing countries. Smallholder farmers are dedicated their lives to producing food for the world, but without financial support to adopt new tools and approaches, most of them can hardly make ends meet on their lands.

Now with information such as farming, crop growth and transactional records being digitalised and shared, AI can be harnessed to build individual credit risk scoring system for farmers. In China, XAG has established strategic cooperation with some of the biggest national financial institutions to offer farmers, even those without credit card or collateral, convenient, equal access to loans and claims. Based on their accumulated digital credits earned from Alipay mobile payment, farmers and service providers can even rent the P Series drone without any deposit. This is making new technologies available to a broader group of smallholders, while helping them obtain necessary resXAG’sces to improve productivity and generate decent incomes.

When Covid-19 loomed over the spring planting season, XAG said it helped mobilise drones for use by various rice farmers, who had already been challenged by rural worker shortages, to directly seed their paddies. Direct seeding refers to the process of sowing seeds into the fields without nursery cultivation and transplantation, while cutting down on water use and preserving the soil.

The current application scenarios of drones are mainly in big field like Xinjiang, Heilongjiang and northern Jiangsu, Jianghan Plain and North China Plain. Myanmar and Vietnam are buying XAG’s products already. In addition, sales in neighboring countries such as Japan and South Korea are also much larger market than Europe and the United States.

The coverage of 4G networks in China’s farmland exceeds 95%. While in the United States except in places with large populations, 3G signals are only available occasionally, and 4G is even less available. The network plays the role of interconnection, allowing drones to concentrate computing on the cloud. This makes China one of the countries with the best use cases.

The United States began the process of agricultural modernization very early. The vast land has nurtured outstanding agricultural machinery companies such as John Deere. Nowadays, the rural infrastructure has given China the environment and soil for smart agriculture, which will definitely boost a group of emerging Chinese agricultural technology companies like.

IF YOU DON’T KNOW NOW YOU KNOW

Japan is the first country in the world to use drones for plant protection on a large scale. Take rice as an example. In 2014, ground machinery plant protection accounted for about 22%, and drone plant protection accounted for 36%. In 2015, Japan’s total arable land area was about 4.5 million hectares, and there were about 2.09 million agricultural practitioners, of which farmers over 65 accounted for more than half, with an average age of 66.4 years.",https://medium.com/@jenc-yincubator/4g-ai-creates-another-john-deere-but-no-longer-in-usa-f046bd547478,['Jenny Chan'],2020-11-25 04:57:43.867000+00:00,843,"AI, IOT, Agriculture, Agr-itech, XAG"
[Chapter-1] Instance-Based Versus Model-Based Learning 🧙🏻‍♂️,"One more way to categorize Machine Learning systems is by how they generalize. Most Machine Learning tasks are about making predictions. This means given a number of training examples, the system needs to be able to generalize to examples it has never seen before. Having a good performance measure on the training data is good, but insufficient; the true goal should to perform good on new instance (new data).

Instance -based Learning 🤖

Instance-based Learning

Possible the most trivial from of learning is simply to learn by heart. If you were to create a spam filter this way, it would just flag all emails that are identical to emails that have already been flagged by users not the worst solution but to the best.

Instead of just flagging emails that are identical to known spam emails, your spam filter cloud be programmed to also flag emails 📧 that are very similar to known spam emails.

A very basic similarity measure between two emails cloud be to count the number of words they have in common. The system would flag an email as spam if it has many words in common with a known spam email 📧.

The system learns the example by heart, then generalizes to new cases by comparing them to the learned examples, using a similarity measure. As in picture we can see that the new instance would be classified as a tringle because the majority of the most similar instances belong to that class.

Model-based Learning 🧭

Model-based Learning

“From a set of examples is to build a model of these examples, then use that to make predictions this is called model-based learning”

For example, suppose you want to know that if money makes people happy in higher GDP rate countries or medio care GRP rate countries.

Visualized Data

As we can see, there does seems to be a trend here! Although the data is noisy, It looks like life satisfaction up more or less linearly as the country’s GDP per capita increases. So we can chose linear model, this process is known as Model Selection. We will learn more about how to chose model based on statistic result, but now is not the time remember we will take care of the process and result (achievement) will take care itself 😉.",https://medium.com/analytics-vidhya/chapter-1-instance-based-versus-model-based-learning-%EF%B8%8F-86140ba2782f,['Vishvdeep Dasadiya'],2020-12-28 16:37:23.474000+00:00,372,"Machine Learning, Instance-based Learning, Model-based Learning, Spam Filtering, Linear Model"
AI and machine learning In the right infrastructure-2,"Key considerations for infrastructure that supports AI and ML

Location:

AI and ML initiatives are not solely conducted in the cloud nor are they handled on-premises. These initiatives should be executed in the location that makes the most sense given the output. For example, a facial recognition system at an airport should conduct the analysis locally, as the time taken to send the information to the cloud and back adds much latency to the process. It’s critical to ensure that infrastructure is deployed in the cloud, in the on-premises data center, and at the edge so the performance of AI initiatives is optimized.

To Know about: How is Robotic Technology Helping the Education Sector?

The breadth of high-performance infrastructure:

As mentioned earlier, AI performance is highly dependent on the underlying infrastructure. For example, graphical processing units (GPUs) can accelerate deep learning by 100 times compared to traditional central processing units (CPUs). Underpowering the server will cause delays in the process, while overpowering wastes money. Whether the strategy is end-to-end or best-of-breed, ensure the compute hardware has the right mix of processing capabilities and high-speed storage. This requires choosing a vendor that has a broad portfolio that can address any phase in the AI process.

Validated design:

Infrastructure is clearly important, but so is the software that runs on it. Once the software is installed, it can take several months to tune and optimize to fit the underlying hardware. Choose a vendor that has pre-installed the software and has a validated design in order to shorten the deployment time and ensure the performance is optimized.

To Know about: Artificial Intelligence for Home Security Systems

Extension of the data center:

AI infrastructure does not live in isolation and should be considered an extension of the current data center. Ideally, businesses should look for a solution that can be managed with their existing tools.

End-to-end management:

There’s no single “AI in a box” that can be dropped in and turned on to begin the AI process. It’s composed of several moving parts, including servers, storage, networks, and software, with multiple choices at each position. The best solution would be a holistic one that includes all or at least most of the components that could be managed through a single interface.

Network infrastructure:

When deploying AI, an emphasis is put on GPU-enabled servers, flash storage, and other computer infrastructure. This makes sense, as AI is very processor and storage-intensive. However, the storage systems and servers must be fed data that traverses a network. Infrastructure for AI should be considered a “three-legged stool” where the legs are the network, servers, and storage. Each must be equally fast to keep up with each other. A lag in any one of these components can impair performance. The same level of due diligence given to servers and storage should be given to the network.

Security:

AI often involves extremely sensitive data such as patient records, financial information, and personal data. Having this data breached could be disastrous for the organization. Also, the infusion of bad data could cause the AI system to make incorrect inferences, leading to flawed decisions. The AI infrastructure must be secured from end to end with state-of-the-art technology.

Professional services:

Although services are not technically considered infrastructure, they should be part of the infrastructure decision. Most organizations, particularly inexperienced ones, won’t have the necessary skills in-house to make AI successful. A services partner can deliver the necessary training, advisory, implementation, and optimization services across the AI lifecycle and should be a core component of the deployment.

Broad ecosystem:

No single AI vendor can provide all technology everywhere. It’s crucial to use a vendor that has a broad ecosystem and can bring together all of the components of AI to deliver a full, turnkey, end-to-end solution. Having to cobble together the components will likely lead to delays and even failures. Choosing a vendor with a strong ecosystem provides a fast path to success.

Historically, AI and ML projects have been run by data science specialists, but that is quickly transitioning to IT professionals as these technologies move into the mainstream. As this transition happens and AI initiatives become more widespread, IT organizations should think more broadly about the infrastructure that enables AI. Instead of purchasing servers, network infrastructure, and other components for specific projects, the goal should be to think more broadly about the business’s needs both today and tomorrow, similar to the way data centers are run today.",https://medium.com/@pvvajradhar/ai-and-machine-learning-in-the-right-infrastructure-2-88f001904d10,['Venkat Vajradhar'],2020-12-17 06:30:36.093000+00:00,723,"AI and ML, Infrastructure, GPUs, CPU, Storage"
Demystifying Python for SAS users: how to perform common data management techniques in Python.,"Step 4: Recoding Variables

Recoding variables in SAS takes place exclusively in a data step but there is no equivalent function signaling data manipulation in Python. Instead you may code the manipulation but must be sure to save it to the dataset in one of several ways. Below I illustrate the difference between temporary and permanent transformations, show three ways to recode variables and end with the equivalent to SAS arrays.

Similar to SAS, if you want to transform numeric data you may write expressions using standard mathematical notation and pandas will broadcast the expression to every cell in the column. However, if you simply ask python to perform the calculation it will not permanently save the transformation to your dataset (Figure 5, line 14). One way to create a new variable is to simply instantiate a new column as shown in Figure 5 line 15. We tell python to create the variable ‘var4_converted’ in the ‘df’ dataset and set it equal to the transformation performed on ‘var4'.

Figure 5. Recoding a variable using broadcasting.

Figure 6 gives an example of an alternative notation you may run across using the loc indexer. The loc indexer is a way to refer to locations in a dataset using the labels of the columns or rows (as opposed to their numeric index with iloc). A colloquial interpretation of the below code is that within the dataset ‘df’ we use loc to locate all rows that satisfy the specified criteria and then in those rows set the column ‘var3_cat’ to the specified value. As the column ‘var3_cat’ did not previously exist it is created in line 17.

Figure 6. Categorizing a variable using the loc operator.

Figure 7 introduces three concepts: python missing values (NaN), apply, and lambda.

Figure 7. Setting system missing codes.

Notes —First, line 21 imports the numpy library and aliases it as ‘np’ by convention. It provides lots of numeric functionality; here I have imported it specifically as it handles missing values. NaN stands for ‘not a number’ and is analogous to the system missing ‘.’ in SAS. Second, in line 22, we are using the same formula as in figure 5 line 15: new variable = old variable + transformation. Third, we are using the apply function to perform the function wrapped in the the parentheses attached to the apply statement. The purpose of ‘apply’ is to perform the specified function to every cell in the column. Find more information on apply here. Fourth, we create a temporary function within parentheses attached to ‘apply’ by calling the key word lambda. Developers refer to it as the anonymous function as it only exists within that specific line of code. Next to the key word ‘lambda’ we specify the variable to use in our function. Here we use x but as with all loops this is just a place holder and we can use any letter. After the colon we write our function. One trouble I have had switching to Python is the different order of the logic in writing expressions. Whereas in SAS we write If X then Z, else Y. This expression reads Z if X else Y.

To end this post, I want to show one way you could recreate the functionality of a SAS array. Below is a simple array for a typical use case — recoding Don’t Know/Missing/Refused codes across many variables.

Figure 8. Sample SAS array.

Figure 9. For loop in python to recode multiple variables.

Similar to SAS we have to tell Python what variables to recode. Instead of defining the array in a data step, here we create a list of the variables to recode. This can be thought of as a global variable in that it exists in python’s memory and can be accessed again. We then use a for loop to perform the desired transformation on every variable in the list. The for loop performs the same function as the do loop and indexer (i in Figure 8). Note the difference between the result of Figure 9 and Figure 7. Figure 7 has created a new variable whereas the for loop in Figure 9 performs the transformation on the original variable.",https://medium.com/@etr359/demystifying-python-for-sas-users-how-to-perform-common-data-management-techniques-in-python-1ec1a7742e18,['Eric Roberts'],2020-10-07 12:57:50.964000+00:00,688,"recoding variables, SAS, Python, numpy, apply function"
Team Aladdin — The journey this week!,"Source: androidauthority.com

This week we are back at work with our research and the group consisting of, Vivian Nguyen, Huy Nguyen, Tien Tran, and Citlalin Galvan. In the first week we focused on malicious apk files and their behaviors inside android phones. This week we wanted to go broader and see the kind of data that has been collected previously. As we looked for the data, we were surprised at how much data is out there, but how little information there is for those datasets especially on data pertaining to cyber security.

Mission Statement

Do data quality analysis on existing data sets and publish a report with a program for others to gain knowledge.

One of the biggest problems in Machine Learning is the quality of training datasets in Cyber Security. The available data signals that the focus is quantity over quality datasets, but someone should still check those datasets and see what is important and what can be discarded. That is where we come into play!

The questions that we are working to get answers for this week are:

How to analyze the data with pandas? How can we see if the data is still relevant? How to profile the data? How to create a data monitoring scorecard?

Our Progress

Our obstacle when it came to analyzing the data in the datasets was that no one in the team had any prior experience with data science and analysis. So, we made it our first priority to research as much as we could on the same to gain as much knowledge as we can. We were able to find two great courses that were really helpful for us — Data Camp and Edx. These were greatly useful for us to learn on how to analyze datasets using python and pandas.

To not feel overwhelmed with how much information there is out there and everything we wanted to do with the project and report, we broke down the process of analyzing into smaller, easier to handle steps.

Data Requirements — Even though there is a lot of data shared online, we had to focus on data related to cybersecurity. That proved to be a challenge all on its own. Not many companies are allowed to share data related to security, and open source projects were limited on information. Data Collection — In our pursuit to find any data set related to security, we stumbled upon SecRepo, a website that has loads of samples of security related data that would be very useful for our project. Data Processing — Log files and JSON files found inside SecRepo were converted to CSV files for clarity and ease of analysis. Also, the python tool for data analysis, Pandas, facilitates a lot of manipulations on CSV files. Refer to Log to CSV File Converter Program below for more information. Data Cleaning — This step is where we check what features are important and learn their essence, since many of the datasets do not have any features labeled.

Research Performed this week

To network and connect with companies and people leading in this field, we reached out to owner of SecRepo, Mike Sconzo, on how we could contribute to his open source project and on ways to improve the quality of the data sets found in his website. His website contains Network, Malware, and System datasets he created and has found from third party websites.

A good place to study on dataset analysis was UC Irvine’s DataSet Repository. They have very detailed analysis reports on every data set and it is open to the public to view. It is a great example of the kind of analysis we want to do for our own dataset report.

We are currently on the process of emailing two professors who work on adversarial machine learning and will be asking for their guidance and asking for resources that could help us with cybersecurity using machine learning.

Log to CSV File Converter Program

LogtoCsvConverter.py(https://github.com/cyberdefenders/MachineLearning)

In this program, we convert a log file into a CSV file. First, we download the log file, and check the file path. Once the program runs, we can input the file path. We then wait a bit while the file gets converted and once it is converted it will ask you to rename the file to your preferred name. You can now see your new CSV file inside the current path of the code. We have it easy and user friendly, so that everyone can use the converter from command line.

Plans for Week 3

Now that we can read the files in pandas, we will focus on analyzing the data we have downloaded from SecRepo. We will focus on how to clean up the data by learning what the features mean so we can label them and what are the primary features we should look at.

Check out our Github repository!

References",https://medium.com/cyberdefendersprogram/team-aladdin-the-journey-this-week-1e5b2b19f038,['Citlalin Galvan'],2018-08-03 18:44:45.756000+00:00,796,"Data Science, Machine Learning, Cyber Security, Python, Pandas"
Gradient Descent algorithm.,"Brief theory how it works.

Photo by rishi on Unsplash

Disclaimer

Gradient descent is one of the widely used algorithms in the model training, especially when using linear regression algorithm.

What is the linear regression?

Linear regression is the simple prediction method, which is tries to draw a line between all results, which tries to predict data y, which can be dependent from input parameter x.

Image from https://www.machinelearningmindset.com/linear-regression-with-tensorflow/

The formula for linear regression (as seen on the image) Y = aX +b. Where a is intercept and b is slope.

Fore example, we have set of x parameters as {1,2,3} and corresponding y results set {2,3,4} , which means that a=1 and b = 1. This knowledge let us assume that if x=4 then y = 5 (y =a*x+1) .

Gradient descent

To calculate proper intercept and slope during the model training we can use gradient descent algorithm.

Image from https://www.codeproject.com/Articles/879043/Implementing-Gradient-Descent-to-Solve-a-Linear-Re

Here on the picture is the cost function graphic and we want to make our parameter with lowest possible cost function. Red dot on the picture is the cost of error with current parameter. To know in which direction we should move and with which step we should know the vector , to know the vector we need to calculate partial derivative.

First, we need to start from something, so just pick some initial weights (intercept and slope) and calculate the error.

After we have first step we need to step by step calculate the gradient and new weights.

Use new weights to calculate gradient and new weights, until we will have a minimal gradient (preferably zero, but it’s rare luck).

And when you will have weights with minimum error that will be your best parameters which were calculated by using gradient descent algorithm.

Here’s a good thing about gradient descent — step, with which you changing weights dependent on the derivative and will vary. Otherwise, using to big step your calculation can be not very precise, or, in case of too small steps it will be very slow computation.

Another one good thing is that not always (I would say pretty rare) error can be equal to zero and gradient descent is very useful in computation lowest error rate.

In the next article I will try to do a little hands-on experience.",https://bogdansamoletskyi.medium.com/gradient-descent-algorithm-d94890985137,['Bogdan Samoletskyi'],2020-09-28 17:56:47.008000+00:00,366,"linear regression, gradient descent, prediction method, cost function, intercept"
Exploring the Trump Twitter Archive with SpaCy,"For this project, we’ll be using pandas for data manipulation, spaCy for natural language processing, and joblib to speed things up.

Let’s get started by firing up a Jupyter notebook!

Housekeeping

Let’s import pandas and also set the display options so Jupyter won’t truncate our columns and rows. Let’s also set a random seed for reproducibility.

# for manipulating data

import pandas as pd # setting the random seed for reproducibility

import random

random.seed(493) # to print out all the outputs

from IPython.core.interactiveshell import InteractiveShell

InteractiveShell.ast_node_interactivity = ""all"" # set display options

pd.set_option('display.max_columns', None)

pd.set_option('display.max_rows', None)

pd.set_option('display.max_colwidth', -1)

Getting the Data

Let’s read the data into a dataframe. If you want to follow along, you can download the cleaned dataset here along with the file for stop words¹. This dataset contains Trump’s tweets from the moment he took office on January 20, 2017 to May 30, 2020.

df = pd.read_csv('trump_20200530_clean.csv', parse_dates=True, index_col='datetime')

Let’s take a quick look at the data.

df.head()

df.info()

Using spaCy

Now let’s import spaCy and begin natural language processing.

# for natural language processing: named entity recognition

import spacy

import en_core_web_sm

We’re only going to use spaCy’s ner functionality or named-entity recognition so we’ll disable the rest of the functionalities. This will save us a lot of loading time later.

nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'textcat'])

Now let’s load the contents stopwords file into the variable stopswords . Note that we converted the list into a set to also save some processing time later.

with open('twitter-stopwords — TA — Less.txt') as f:

contents = f.read().split(',')

stopwords = set(contents)

Next, we’ll import joblib and define a few functions to help with parallel processing.

In the code above², the function preprocess_parallel executes the other function process_chunks in parallel to help with speed. The function process_chunks iterates through a series of texts — in our case, the column 'tweet' of our the df dataframe — and inspects the entity if it belongs to either NORP, PERSON, FAC, ORG, GPE, LOC, PRODUCT, or EVENT. If it is, the entity is then appended to 'preproc_pipe' and subsequently returned to its caller. Prashanth Rao has a very good article on making spaCy super fast.

Let’s call the main driver for the functions now.

df['entities'] = preprocess_parallel(df['tweet'], chunksize=1000)

Doing a quick df.head() will reveal the new column 'entities' that we added earlier to hold the entities found in the 'tweet' column.

Prettifying the Results

In the code below, we’re making a list of lists called 'entities' and then flattening it for easier processing. We’re also converting it into a set called 'entities_set' .

entities = [entity for entity in df.entities if entity != []]

entities = [item for sublist in entities for item in sublist] entities_set = set(entities)

Next, let’s count the frequency of the entities and append it to the list of tuples entities_counts . Then let’s convert the results into a dataframe df_counts .

df_counts = pd.Series(entities).value_counts()[:20].to_frame().reset_index()

df_counts.columns=['entity', 'count']

df_counts

For this step, we’re going to reinitialize an empty list entity_counts and manually construct a list of tuples with a combined set of entities and the sum of their frequencies or count.

Let’s take a quick look before continuing.

Finally, let’s convert the list of tuples into a dataframe.

df_ner = pd.DataFrame(entity_counts, columns=[""entity"", ""count""]).sort_values('count', ascending=False).reset_index(drop=True)

And that’s it!

We’ve successfully created a ranking of the named entities that President Trump most frequently talked about in his tweets since taking office.",https://towardsdatascience.com/exploring-the-trump-twitter-archive-with-spacy-fe557810717c,['Ednalyn C. De Dios'],2020-06-18 20:24:02.792000+00:00,521,"pandas, spa Cy, joblib, natural language processing, Jupyter notebook"
"Podcast Episode #2: Perspectives of Shannon Ellis, Academic Data Scientist","Shannon Ellis is an Assistant Teaching Professor in the Cognitive Science Department, and has taught COGS 9 Introduction to Data Science, COGS 18 Introduction to Python, and COGS 108 Data Science in Practice. In this podcast episode, Shannon reflects on her experiences in academia, her projects, and how she transitioned from her biostats-focus at Johns Hopkins to her generalized teaching position at UCSD.

During Shannon’s undergraduate experience, “Data Science” wasn’t a word defined in her vocabulary — nor was the discipline offered at her undergraduate institution. Inspired by her high school biology teacher, Shannon sought out to study genetics at King’s College in Pennsylvania, where she got her first taste in research. From inputting data she spent years collecting into a software program, she became fascinated with the software and how it gave her immediate answers. This fascination carried Shannon into her graduate programs, where she learned to program and experiment with data analysis. By the end of her PhD, she was performing computational analysis on large datasets, and was exposed to Data Science in the genetics domain.

Deciding she wanted to go down a route of analyzing data and not dedicating the entirety of her career to “answering one question,” Shannon went on to pursue a Postdoc doing research at the Leek Group, a Data Science lab in the biostatistics department at Johns Hopkins. Shannon talks fondly of the Leek group, and recalls her mentor, Jeff Leek, fostering a more entrepreneurial environment in academia and differentiating himself from other mentors by trying out “bonker ideas”.

When asked how she got into the field of biostatistics, Shannon jokingly says she “back-doored [her] way” in, not having any prior experience or degree in the subject. At the end of her Postdoc, she was faced with the dilemma of continuing a career in research or teaching. “Those who can’t do, teach”, was a saying that stuck in her head, and almost convinced her that teaching was simply a fallback career. However, after much deliberation, she concluded that she wanted to teach and continue to experiment in different domains instead of pursuing a proper line of research.

At Johns Hopkins, she taught undergraduates on public health biostatistics, where her students experimented with public health datasets using the R programming language. Afterward, she applied to universities and education-focused positions at government programs and startups. Her transition to UCSD came after she encountered a job posting from the university’s Cognitive Science Department, and after following the much needed encouragement from her advisor, she decided to apply. She has now been teaching at UCSD for a year, and continues to teach a little bit of computational genetics as part of the Data Science Capstone.

Shannon worked on two projects during her Postdoc, the first of which is Cloud Based Data Science (CBDS). As the Curriculum Lead for CBDS, Shannon develops the content for the courses. Through offering a set of online courses that can be taken for free, CBDS aims to democratize Data Science education. The program sets starting points for people with limited math or comprehension skills, and was built to go in concert with CBDS +, an in-person tutoring program. CBDS was developed during a time when Massive Open Online Courses (MOOC) were revolutionizing education, and CBDS had hundreds of thousands of applicants. But when looking at their applicants’ backgrounds, they discovered that they were all educated with a masters degree.

“We don’t want hundreds of clones of the same person,” Shannon says. She further emphasizes that she wants CBDS to help educate people from different works of life. CBDS targets those who are economically under-privileged, particularly those “who can’t take time out of their life to just study.” The program aims to improve their students’ financial conditions by helping their cohorts obtain entry-level jobs, and teaches them the basics of data analysis and data wrangling.

Her second project, Recount, is more biostatistics focused. “Biologists have gotten really good at publishing data”, but unfortunately, it’s not easy to get or use. In response, the team took all publicly available RNA-Seq data (measures gene expression levels), and processed 70,000 human samples in a single pipeline to make it easier and more accessible for biologists to work with. Shannon worked on phenotype prediction in this project, using a self-coined term called “in-silico phenotyping”, which utilizes Machine Learning to predict the kind of tissue, sex, age, and a variety of factors pertaining to the individual without going back to them for validation.

If interested, check out Shannon’s personal website to learn more about her, or the Leek Group to learn more about their projects!",https://medium.com/ds3ucsd/perspectives-of-shannon-ellis-an-academic-data-scientist-99cafe573f86,['Kashika Rathore'],2020-07-31 05:25:42.105000+00:00,759,"Data Science, Biostatistics, Genetics, Machine Learning, Python"
Pitch Deck,"So I threw this pitch deck together a little while back, wanting to share as part of application for an accelerator program I’ll just go ahead and post online since no one reads these things. Oh yeah all inquiries please direct through contact portal on our website at automunge.com. Cheers.",https://medium.com/automunge/pitch-deck-7d9ab80b4ba1,['Nicholas Teague'],2019-11-20 21:32:59.636000+00:00,50,"pitchdeck, acceleratorprogram, automunge.com, startups, investment"
Implementing a Decomposition Method,"We will implement a matrix decomposition method for linear regression.

Specifically, we will use the Cholesky decomposition, for which relevant functions exist in TensorFlow.

What is the Cholesky decomposition?

The Cholesky decomposition or Cholesky factorization is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose.

Getting ready…

Implementing inverse methods in the previous blog (Linear Regression with TensorFlow)can be numerically unfit in most cases, especially when the matrices get very large. Another approach is to decompose the A matrix and perform matrix operations on the decompositions instead. One such approach is to use the built-in Cholesky decomposition method in TensorFlow. One reason people are so interested in decomposing a matrix into more matrices such that the resulting matrices will have assured properties that allow us to use certain methods efficiently.

The Cholesky decomposition decomposes a matrix into a lower and upper triangular matrix, say L and L’, such that these matrices are transpositions of each other.

we will solve the system,, Ax=b as LL’*x=b. we will first solve for Ly=b, then L’x=b in order to get our coefficient matrix, x.

How to do it…

We will set up the system exactly in the same way as the previous recipe. We will import libraries, initialize the graph, and create the data. Then we will obtain our A matrix and b matrix in the same way as we did in the previous blog Using the Matrix Inverse Method:

import matplotlib.pyplot as plt

import numpy as np

import tensorflow as tf from tensorflow.python.framework import ops

ops.reset_default_graph()

sess = tf.Session() x_vals = np.linspace(0, 10, 100)

y_vals = x_vals + np.random.normal(0, 1, 100)

x_vals_column = np.transpose(np.matrix(x_vals)) ones_column = np.transpose(np.matrix(np.repeat(1, 100))) A = np.column_stack((x_vals_column, ones_column))

b = np.transpose(np.matrix(y_vals)) A_tensor = tf.constant(A)

b_tensor = tf.constant(b)

2. Next we will find the Cholesky decomposition of our square matrix, A’A A’=transpose of A

Note that the TensorFlow function, cholesky(), only returns the

lower diagonal part of the decomposition. This is fine, as the upper

diagonal matrix is just the lower one, transposed.

tA_A = tf.matmul(tf.transpose(A_tensor), A_tensor)

L = tf.cholesky(tA_A)

tA_b = tf.matmul(tf.transpose(A_tensor), b) sol1 = tf.matrix_solve(L, tA_b)

sol2 = tf.matrix_solve(tf.transpose(L), sol1)

3. Now that we have the solution, we extract the coefficients:

y_intercept = solution_eval[1][0] print('slope: ' + str(slope))

print('y'_intercept: ' + str(y_intercept))

#slope: 0.956117676145 y_intercept: 0.136575513864 best_fit = []

for i in x_vals:

best_fit.append(slope*i+y_intercept)

plt.plot(x_vals, y_vals, 'o', label='Data')

plt.plot(x_vals, best_fit, 'r-', label='Best' fit line',

linewidth=3)

plt.legend(loc='upper left')

plt.show()

Data points and best-fit line obtained via Cholesky decomposition.

CONCLUSION

Keep in mind that this way of decomposing a matrix, then performing our operations on the pieces, is sometimes much more efficient and numerically stable",https://medium.com/ai-in-plain-english/implementing-a-decomposition-method-b756b85b4323,['Bhanu Soni'],2020-11-08 16:04:29.190000+00:00,405,"Cholesky decomposition, Tensor Flow, Linear Regression, Matrix Decomposition, Matrix Inverse Method"
Beginning With Data Science,"Data Science. Isn’t this topic of the decade? Everyone talks about it, everyone wants in on it, no one knows where to begin. With questions like what’s the post, how’s the salary, is the work interesting or not, it has become a new age black hole and there is no Rick Sanchez to help us. Do you want to be a data analyst? Perhaps, a business analyst. Or maybe, the proverbial Data Scientist! But if you’re slightly confused (more than slightly works too), then this post is for you. This might not be the only post you read today, but I hope I can at least give you a glimpse of what this fuss is all about.

The Buzz Words — Data Analytics, Business Analytics and Data Science

Did you know that there’s a difference between analysis and analytics? Analysis mainly refers to the past. Analytics, on the other hand, is an attempt to see what the future holds. Analytics can’t exist without Analysis. Data Analytics then means figuring out what data points would look like in future, for example, what traffic might look like in an hour. Business Analytics, as the term suggests, is an assessment and prediction of business, for example, sales prediction of the latest iPhone in the next year. What makes data analytics different from business analytics is the need for business acumen. Data Science is a wide term that pertains to several areas. It means understanding and applying statistics and complex mathematical and computational techniques to predict the future more accurately. While this seems a lot like data and business analytics, the difference comes from the usage of the data mining techniques, big data along with working with large structured and unstructured data.

Techniques — Traditional vs New Age Techniques

Prediction or forecasting has been in the picture for a long time now. Earlier, statisticians used methods like hypothesis analysis combined with clustering, regression and time series to predict the future. Today, with large unstructured and structured data, there is a need to use a much more advanced method to accurately make predictions. Artificial Intelligence — Machine Learning is one way to achieve this.

I would not go into too much detail. But below are a few examples where you see data science in everyday life.

Recommendation Systems — Ever used ‘Similar Products’ feature on Amazon, maybe “You Might Also Like” on another e-commerce platform? Yeah. Me neither. ;p Google Maps — the Best route to your destination, traffic prediction are some ways that save our lives every day Fraud Detection — How do banks let you transact that easily? Denying your request, calculating your credit score? With the number of transactions happening each second, data science is a necessity. Voice Recognition — Do you make Alexa do your work for you? Perhaps let Siri dial your mother’s number? Thought so.

Career Path

You might find this chart useful based on your experience. Or might not. Help me in improving this if you know more, please?

Links of some good courses to get started:

This is all I had for today. Let’s hope that in this journey, both you and I can learn to do some good and interesting.",https://medium.com/vizhen/beginning-with-data-science-5df57d46c971,['Anjali Savlani'],2019-06-20 05:47:57.560000+00:00,524,"Data Analytics, Business Analytics, Data Science, Artificial Intelligence, Machine Learning"
Main perspectives and forecasts The workforce of the future,"In addition to the facts and statistics, it stresses that employers need to realize more than ever how their teams and employees need to be helped. It is apparent that teams working remotely and taking care of their lives will produce great results. Some horrible examples of organizations doing this wrong — for instance the manager, who told his employees he needed them to hold a video call open all day long to track what they had been doing. There have also been some excellent examples of businesses willing to invest and assist their employees with online equipment and training, as well as daily social and non-curricular activities.

The publishing of this study was spoken to me by Wendy Mars (Twitter, LinkedIn), President of Cisco for Europe, the Middle East, Africa, and Russia. She claims, in particular, that it underlines the continued value of digital transformation and the need for businesses to respond rapidly to it.

She said, “We have to be able to ensure that workers remain active in businesses and operations and to be able to operate in this very fast and efficiently through the development of workforce around the globe if we look at the time and pace at which the corporations have reacted to this because they have no genuine option … And we have seen the speed that companies have done so.

“At the present time, the dynamics of working remotely or from home and how people see it, have also changed considerably … people saw workers becoming unbelievably productive from home … it would radically change the nature of work throughout the future.”

It is evident from talking to our friends and families that all of us have found a much healthier work and life balance objectively. In addition, the results of this study indicate that, as a result of improvements to work-life, both our emotional and physical wellbeing are better-taken care of.

In several ways, businesses and organizations today seem to have a duty to repay their workers as efficiently and as well as to adjust them to their needs while ensuring that they still have positive working environments.

Mars says, “To be able to have a degree of business capability, be that they are in a bureau or remote, there would be a great dependence. The user’s standards are very high.

Unplash

In addition, challenges will be created to ensure that workers retain their know-how and acquire new skills that are worthwhile for themselves and their businesses in the future.

And organizational environments are also likely greatly affected by workplace decentralization. With personal and after-work drinks not on the table for the near future, both managers and workers need to be creative in terms of upholding mutual values and the spirit of building a team.

Giphy

Technology would possibly once again have a long-term solution. With 5 G networking and the advancement of technology such as virtuality and increased reality, stuttering video conferencing will be a thing of the past and there will be new possibilities for interaction. Organizations that lead in this direction are likely to see a more satisfied and efficient workforce of the future for their investments.

You can see my full conversation with Zhejian Peng here:",https://shaiksameeruddin.medium.com/main-perspectives-and-forecasts-the-workforce-of-the-future-70ac27ac1bbb,['Shaik Sameeruddin'],2020-10-20 07:30:00.465000+00:00,526,"remote work, digital transformation, workforce development, work-life balance, technology investments"
How to create bubble maps in Python with Geospatial data,"We often use Choropleth maps to display areas, and in that case, we use a colour encoding. Choropleth maps have an inherent bias problem with large areas. In contrast, bubble maps use circles to represent a numeric value of an area or region.

It often seems to be complicated and a bit advanced feature to create your bubble map in Python, but it is not. It is like creating bubble charts with latitude and longitude columns. Let us first import the libraries we need.

import pandas as pd

import numpy as np import geopandas as gpd import matplotlib.pyplot as plt

import plotly_express as px

And we can read the data with Geopandas. With Geopandas, you can read most of the geographic data formats like Shapefile, Geojson, Geo package, etc.. In this example, we are using population data from the city of Mälmo, Sweden.

gdf = gpd.read_file(“data/malmo-pop.shp”)

gdf.head()

Here is a glimpse of the first rows of the dataset. We have numerous columns of the population subdivision (Age 5 to age 80 and above) for each administrative unit (Deso).

We usually use choropleth maps and use a colour encoding. As we are going to see, we can create a choropleth map very easily with Geopandas. Notice that we first normalize the data (Age80_w) by subdividing the total population.

gdf[""Age_80_norm""] = (gdf[""Age80_w""] / gdf[""Total""]) * 100 fig, ax = plt.subplots(figsize=(16,16))

gdf.plot(ax=ax, column=""Age_80_norm"", cmap=""Blues"",edgecolor=""grey"", linewidth=0.4, legend=True)

ax.axis(""off"")

plt.axis('equal')

plt.show()

Bubble Maps

Alternatively, we can use bubble maps and avoid some of the pitfalls of choropleth maps. For example, we do not need to normalize the data, and we can use the total subdivision of populations. However, we need to do some conversion to the data to be able to create a bubble map.

As you can see, the data we are using are Polygons, and we need points if we want to create bubble maps. That is, however, a simple process with Geopandas. We only need to change the geometry column, in this case, Polygons to Point geometry.

gdf_points = gdf.copy()

gdf_points[‘geometry’] = gdf_points[‘geometry’].centroid

We first copy the Geodataframe with Polygon geometry to a new Geodataframe. As we want to the bubble to be at the centre of the area, we can use centroid function in Geopandas to achieve this.

Now, we have the same Geodataframe with different Geometry column, a Point Geometry. Let us plot a bubble map as we have points dataset now.

fig, ax = plt.subplots(figsize=(16,16))

gdf.plot(ax=ax, color=”lightgray”, edgecolor=”grey”, linewidth=0.4)

gdf_points.plot(ax=ax,color=”#07424A”, markersize=”Age80_w”,alpha=0.7, categorical=False, legend=True ) ax.axis(“off”)

plt.axis(‘equal’)

plt.show()

The map below shows a bubble map for Age 80 and above population in a smaller administrative area in Mälmo, Sweden. Each circle represents a different size based on the subtotal population at the age of 80 and above. To construct a bubble map, you need to provide markersize to the column you want to be mapped, and in this case, the Age80_w.

As you can see, this is a static map, and often a problem with bubble maps is the overlapping of point circles. One way we can avoid this is to create interactive maps that allow the user to interact with and zoom into an area of interest. In the next section, we are going to see how you can create an interactive bubble map.

Interactive Bubble Maps

There are different Python libraries for plotting interactive bubble maps. To construct an interactive bubble map, we use Plotly Express. We just need to convert to another projection to display the map with Plotly Express. Plotly Express has scatter_mapbox() function which can take a Geodataframe and the column you want to use for the bubble map.

gdf_points_4326 = gdf_points.to_crs(“EPSG:4326”) fig = px.scatter_mapbox(

gdf_points_4326,

lat=gdf_points_4326.geometry.y,

lon=gdf_points_4326.geometry.x,

size=”Age80_w”,

color=”Total”,

hover_name = “Age80_w”,

color_continuous_scale=px.colors.colorbrewer.Reds,

size_max=15,

zoom=10

)

fig.show()

With Plotly Express, we can avoid the overlapping problem of bubble maps by creating an interactive bubble map. See the below GIF.

Bubble maps can be an alternative map to display a numeric variable with different bubble size, instead of the most used choropleth maps. If you have a list of regions (i.e. administration areas) with values (i.e., age subdivisions), bubble maps can replace Choropleth maps. Bubble maps do not have the inherent big area bias in choropleth maps.

Conclusion

In this tutorial, we have seen how to create both static and interactive bubble maps, using Python. The static bubble maps have an overlapping problem. To avoid it, you can either create interactive maps that allow zoom in/out or increase the transparency of the static map circles.

The code for this tutorial is available in this Github Repository.",https://towardsdatascience.com/how-to-create-bubble-maps-in-python-with-geospatial-data-e51118c3d767,[],2020-04-20 14:02:24.472000+00:00,714,"Choropleth maps, Bubble maps, Python, Geopandas, Plotly Express"
Speech Recognition with TensorFlow.js,"Deploying a sample model with TensorFlow.js

As we said, TensorFlow.js is a powerful library, and we can work on a lot of different things like image classification, video manipulation, and speech recognition among others. For today I decided to work on a basic speech recognition example.

Our code will be able to listen through the microphone and identify what the user is saying, at least up to a few words as we have some limitations on the sample model I’m using. But rather than explaining, I think it’s cool if we see it first in action:

Unfortunately, I can’t run the code on medium, but you can access the live demo here

Pretty cool? I know it can be a bit erratic, and it’s limited to a few words, but if you use the right model, the possibilities are endless. Enough talking, let’s start coding.

The first thing we need to do is to install the library and get our model. For installing TensorFlow.js there are a few options that can be reviewed here, in our case to keep it simple we will import it from CDN.

<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js""></script>

<script src=""https://unpkg.com/@tensorflow-models/speech-commands""></script>

Then we would use some HTML to show the list of words:

<div class=""demo"">

<div>

<label class=""form-switch"">

<input type=""checkbox"" id=""audio-switch"">

Microphone

</label>

<div id=""demo-loading"" class=""hidden"">Loading...</div>

</div>

<div id=""sp-cmd-wrapper"" class=""grid""></div>

</div>

So far nothing strange, we have our checkbox, a loading element and a wrapper element which we will use to render the list of words, so let’s do that next:

const wrapperElement = document.getElementById('sp-cmd-wrapper');

for (let word of wordList) {

wrapperElement.innerHTML += `<div id='word-${word}'>${word}</div>`;

}

In order for the demo to start working we need to click on the Microphone checkbox, let’s set an event listener there to trigger the loading and listening processes.

document.getElementById(""audio-switch"").addEventListener('change', (event) => {

if(event.target.checked) {

if(modelLoaded) {

startListening();

}else{

loadModel();

}

} else {

stopListening();

}

});

When the checkbox changes its value we have 3 different possibilities, the user enabled the checkbox and the model is not loaded, in that case, we use the loadModel() function, if however the model was already loaded we trigger the listening process. If the user disabled the checkbox, we stop accessing the microphone.

Let’s review each function implementation:

loadModel()

loadModel() is responsible for creating the recognizer instance and load the model. When the model is loaded we will be able to get the list of labels the model was trained on with recognizer.wordLabels() . This will be helpful later when evaluating the model.

async function loadModel() {

// Show the loading element

const loadingElement = document.getElementById('demo-loading');

loadingElement.classList.remove('hidden');



// When calling `create()`, you must provide the type of the audio input.

// - BROWSER_FFT uses the browser's native Fourier transform.

recognizer = speechCommands.create(""BROWSER_FFT"");

await recognizer.ensureModelLoaded()



words = recognizer.wordLabels();

modelLoaded = true;



// Hide the loading element

loadingElement.classList.add('hidden');

startListening();

}

startListening()

startListening() will be called after the model loaded or the user enabled the microphone and will be responsible for accessing the microphone API and evaluate the model to see which word we were able to identify. This sounds complicated, but thanks to TensorFlow is just a few lines of code.

function startListening() {

recognizer.listen(({scores}) => {



// Everytime the model evaluates a result it will return the scores array

// Based on this data we will build a new array with each word and it's corresponding score

scores = Array.from(scores).map((s, i) => ({score: s, word: words[i]}));



// After that we sort the array by scode descending

scores.sort((s1, s2) => s2.score - s1.score);



// And we highlight the word with the highest score

const elementId = `word-${scores[0].word}`;

document.getElementById(elementId).classList.add('active');



// This is just for removing the highlight after 2.5 seconds

setTimeout(() => {

document.getElementById(elementId).classList.remove('active');

}, 2500);

},

{

probabilityThreshold: 0.70

});

}

Super easy! now the last function.

stopListening()

stopListening() will stop accessing the microphone and stop the evaluation.

function stopListening(){

recognizer.stopListening();

}

That’s it, that’s all that you need to build your first example of speech recognition on the web.",https://towardsdatascience.com/speech-recognition-with-tensorflow-js-66608355376e,['Juan Cruz Martinez'],2020-06-23 22:54:04.878000+00:00,580,"Tensor Flow.js, Speech Recognition, Machine Learning, Java Script, Web Development"
Why Do Women Have the Upper Hand on Tinder?,"Why Do Women Have the Upper Hand on Tinder?

Photo: MartinPrescott/iStock/Getty Images Plus

Co-authored with Kristian Elset Bø

Over the last decade, Tinder has redefined the online dating industry. The app has proven especially popular among young people, with three-quarters of those ages 18 to 24 reporting using the app at one point. Bumble is a distant second, with 31% of people using it.

Tinder differentiated itself through a simple swiping format since copied by numerous competitors. This format stood in stark contrast to early dating sites like eHarmony, which required long, time-consuming questionnaires that matched users based on personality compatibility. However, despite the easy and convenient allure of Tinder, getting a date through the app is notoriously exhausting.

I partnered up with Kristian Bø, who created the site Swipestats.io (which allows users to visualize their own data and compare against others) to analyze data. For this purpose, users downloaded data directly from Tinder and submitted it to us to give insight into the dynamics of the app’s dating market. Most shockingly, it shows two distinct worlds where the typical male user has a radically different experience from the typical female user.

Sign up for The Bold Italic newsletter to get the best of the Bay Area in your inbox every week.

While most women can easily find matches with men they’re interested in, the app presents a much more challenging environment for men. This difference is most evident in swiping patterns. While women swipe more than men overall, they are far more selective when doing so. Women swipe yes to just one in 20 people while the majority of men swipe yes more often than no.

Graph by Brayden Gerrard via Duro.Data and Swipestats.io/CC BY

Ultimately, most women only swipe yes on a handful of men per day while men are more freewheeling with their swipes. This creates a highly competitive environment where many men find it difficult to get matches consistently. Despite the high selectivity of female users, they actually match more often than men. The median female user receives about 2.75 matches per day while the median male user only receives about 1.1 matches.

At that rate, to expect a match, a typical woman would have to like just three men while a man would need to like over 50 women.

Graph by Brayden Gerrard via Duro.Data and Swipestats.io/CC BY

Of course, a match doesn’t always lead to an interaction. After matching, men message more frequently as well. The median female receives about seven messages per day while sending only five.

Why is Tinder so imbalanced?

While it has been common knowledge that women have had an easier time on the app for some time, this data provides important proof to back up the anecdotes. So why do women seem to have the upper hand on Tinder? The simplest explanation is basic supply and demand. Evidence suggests that men use dating apps more prolifically than women — both in the number of users and in the frequency of use. One estimate found that over 70% of active American Tinder users were male, and the ratio of male-to-female users among our data was similar. Taken together, this suggests a deep imbalance in the user pool of Tinder.

The result is a large number of men fighting over a comparably small pool of women, which allows women to choose potential matches very carefully. Meanwhile, men with profiles that are less attractive are left high and dry.

The data also reveals large inequalities for both genders. Men and women in the top 10% (meaning, the 10% of users who received the most matches) typically see a minimum of five matches per day, and a select few can see dozens per day. Some users went on to accumulate thousands or even tens of thousands of matches over time. The inverse of this is that while some people see a large number of matches, others see very few. Men in the bottom 10% see just one match per week at most. Success varies greatly among women as well. However, even those in the 10th percentile (meaning those who receive matches less frequently than 90% of women) can usually match about once per day.

Graph by Brayden Gerrard via Duro.Data and Swipestats.io/CC BY

However, a match is a far cry from a real date. Research has previously found that it requires 57 matches for one meetup and more than five times that for either a relationship or sexual encounter to occur. Accordingly, it would take a typical man almost 6,000 swipes over two nearly months to score a date.

Despite this, many men and women still find success on Tinder. According to one survey, more than 20% of millennials surveyed reported meeting someone off of Tinder. Men were more likely to use the app as well as more likely to have met someone from Tinder. Excluding those who have never used Tinder, nearly 30% of people have met someone from Tinder.

Graph by Brayden Gerrard via Duro.Data and Swipestats.io/CC BY

Still, the evidence is clear that men and women face much different realities on the app. While most women can get matches easily even when they are highly selective, the majority of men must temper their expectations. The data also tells us that some men will likely struggle to get a date on Tinder even after thousands of swipes and many months trying.",https://thebolditalic.com/the-two-worlds-of-tinder-f1c34e800db4,['Brayden Gerrard'],2021-03-10 21:26:40.585000+00:00,879,"Tinder, Online Dating, Swiping, Supply and Demand, Selectivity"
Insane workflows with Slack dialogs and YellowAnt!,"Insane workflows with Slack dialogs and YellowAnt!

Make Slack your de-facto command-center

Slack’s recent release of Dialogs is a complete game-changer — it has opened up a whole new world of possibilities and marks a turning point in enterprise ChatOps. Here at YellowAnt, we built a bot that lets you manage all your workplace apps through commands and buttons and create and execute powerful command and event triggered cross-application workflows. With our integration with Slack Dialogs, YellowAnt has not only become much easier to work with, but a lot more functional and sticky. You can now take actions across your apps with simple Slack Dialogs. It’s awesome — Check it out!

You can now send and reply to emails from Slack with YellowAnt

Here’s how…

Commands for GMail

Create a YellowAnt account and Integrate GMail from the YellowAnt marketplace

Your GMail account will be accessible through the command gmail

Type gmail

YellowAnt will show you a list of GMail commands

Under send, click on Run this command.

Send email dialog

This opens a dialog where you type out the message details

Click on Execute. Your mail will on its way!

Insane! Right?

Replying to emails from Slack with YellowAnt

YellowAnt GMail integration brings all your emails into Slack with some action buttons like Star, Mark as Read or Important, and Reply

Click on Reply. This opens the Gmail reply dialog

Type out the body of the reply. hit Execute

Voila! Your reply is sent!",https://medium.com/startup-frontier/insane-workflows-with-slack-dialogs-and-yellowant-711d88123cd0,['Vishwa Krishnakumar'],2017-10-01 14:02:32.297000+00:00,225,"Slack, Dialogs, Yellow Ant, GMail, Chat Ops"
"Pywedge: A complete package for EDA, Data Preprocessing and Modelling","Pywedge: A complete package for EDA, Data Preprocessing and Modelling

Pywedge helps in visualizing the data, preprocessing, and creating baseline models

Pywedge(Source: By Author)

What is Pywedge?

Pywedge is an open-source python library which is a complete package that helps you in Visualizing the data, Pre-process the data and also create some baseline models which can be further tuned to make the best machine learning model for the data.

It is a hassle-free package that saves the time and effort of creating different types of plots to analyze the data. It contains 8 different types of visualization which can be used using the user-friendly GUI of pywedge. It takes care of all the pre-processing that the data may require whether it is cleaning the data, normalization, or handling the class imbalance pywedge covers it all. It works on both classification and regression problems.

In this article, we will see what all we can do using Pywedge and explore its features.

Before starting kindly follow me on medium by clicking here and stay updated about my new articles in the field of data science.

Installing pywedge

Pywedge is pip installable so we will install it using the command prompt by running the below-given command.

pip install pywedge

Importing the required libraries

We are exploring pywedge so we will start by importing it. Also, we will import pandas for loading our dataset.

import pandas as pd

import pywedge as pw

Loading the Dataset

We will use different datasets for exploring pywedge. Let’s start by importing the Iris dataset from seaborn for the visualization part.

import seaborn as sns

df = sns.load_dataset(""iris"")

df

Iris Dataset(Source: By Author)

Visualization using Pywedge

Now we will use pywedge functions to visualize the dataset. Here we will use the pywedge_charts function which requires three inputs namely the data, “C” which is a redundant column in the dataset that we want to remove, and the dependant variable according to the problem i.e classification or regression.

mc = pw.Pywedge_Charts(df, c=None, y=""species"" )

# For Visualization

chart = mc.make_charts()

Visualization(Source: By Author)

In the output above we can clearly see visualize the interface which is created by pywedge. We can select different types of charts and use different variables to visualize the data.

Preprocessing the data

Next, we will see how easily we can preprocess the data using pywedge. For this, I have used the Boston dataset which I have split into the test and train dataset. We will see how pywedge works on finding missing values, normalization, class imbalance(for classification problem).

#Loading the dataset

train = pd.read_csv('bos_test.csv')

test = pd.read_csv('bos_train.csv') #We are handling regression so we will use type as Regression

ppd = pw.Pre_process_data(train, test, c=None, y='medv', type='Regression') #Storing the processed data into new dataframe

new_X, new_y, new_test = ppd.dataframe_clean()

Preprocessing(Source: By Author)

In the output above we can clearly see how pywedge identifies missing data and provides options to the users(highlighted in image) for standardization and converting categorical to numerical. Similarly, if we will solve a classification problem it will show whether the data is imbalanced or not.

Baseline Models

Pywedge also creates some baseline models according to our dataset, so that we can select the best performing model and tune it to reach the maximum accuracy. It also helps us in finding out the importance of features.

blm = pw.baseline_model(new_X,new_y) #We will use regression_summary, for classification we will use #classification_summary

blm.Regression_summary()

Baseline Models(Source: By Author)

Here we can clearly analyze the different types of models and their performance, we can select the best performing model and further tune it.

Go ahead try this interesting and useful library on different datasets and in case you face any difficulty reach out to me in the response section.

This post is in collaboration with Piyush Ingale

Before You Go

Thanks for reading! If you want to get in touch with me, feel free to reach me on hmix13@gmail.com or my LinkedIn Profile. You can view my Github profile for different data science projects and packages tutorials. Also, feel free to explore my profile and read different articles I have written related to Data Science.",https://towardsdatascience.com/pywedge-a-complete-package-for-eda-data-preprocessing-and-modelling-32171702a1e0,['Himanshu Sharma'],2020-12-09 12:58:57.066000+00:00,638,"Pywedge, EDA, Data Preprocessing, Modelling, Visualization"
Keras Model Sequential API VS Functional API,"Sequential API

Sequential API allows you to create models layer-by-layer by stacking them. It is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs.

Example Code:

Result

And now lets plot your model using keras utils.

Keras Plot Model

As you can see, Sequential API create model and stacking them together layer by layer. Sequential API is very easy to create deep learning model in most of case.",https://medium.com/analytics-vidhya/keras-model-sequential-api-vs-functional-api-fc1439a6fb10,['Rahmat Faisal'],2020-11-04 18:13:47.811000+00:00,73,"sequentialAPI, keras Utils, deep Learning Model, multiple Inputs Outputs, stacking Layers"
How Computational Models Can Boost Productivity in Biotech,"How Computational Models Can Boost Productivity in Biotech

Experimental lab? You just need a computer.

Edited by the author using Canva. Original photo by Artem Podrez from Pexels

What is computational modelling in biotechnology?

I was introduced to this term by Dr. Neil Swainston in his course on Coursera, ‘Industrial Biotechnology’ [1].

Computational modeling in systems biology are developed to carry out computational experiments that would otherwise be too expensive or time-consuming to be carried out using a traditional experimental lab.

These computational simulations are used to generate hypotheses, which can later be validated using lab experiments. The data generated from these experiments can be used to further refine the model. This cycle continues until the model’s accuracy at predicting the outcome is sufficient.",https://medium.com/a-microbiome-scientist-at-large/how-computational-models-can-boost-productivity-in-biotech-729d2a8918a,['Eshan Samaranayake'],2020-12-03 01:04:30.121000+00:00,118,"Biotechnology, Computational Modelling, Productivity, Experimental Lab, Systems Biology"
Core PySpark: Inner Join on RDDs. Below you will find a simple example…,"Let’s begin. First, we simply import pyspark and create a Spark Context.

Import PySpark

We are going to use the following very simple example RDDs: People and Transactions.

Create two RDDs that have columns in common that we wish to perform inner join over.

New let’s perform some data-formatting operations on the RDD to get it into a format that suits our goals. We will split each split each row using commas as a delimiter, and remove headers.

Suppose we want to perform a join on the Transactions and People dataset. The property that each dataset has in common is ‘name’.

If you consult the Pyspark documentation, performing a .join() operation on RDDs uses a (Key, Value) paradigm to find the intersection between sets. Therefore, prior to performing the join, we should format our datasets so that they conform to the (Key, Value) format required by Spark (conforms to the MapReduce paradigm).

Documentation: https://spark.apache.org/docs/latest/api/python/pyspark.html

Notice how we have formatted our RDDs as (Key, Value) pairs, where the Key is now the property we would like to perform the inner join on.

You can see the results of joining — since we formatted the RDDs as (Key, Value) pairs and then performed the inner join, the resulting RDD is the inner join on whatever we chose to be the Key. In this example, we now have a new RDD that combines the People RDD with the Transactions RDD to give us a table of transactions, each of which include additional information about the person responsible for that transaction.

I hope this was helpful to some of you!",https://medium.com/@alexandergao/core-pyspark-performing-an-inner-join-on-an-rdd-b8bac25c7ccb,['Alexander Gao'],2020-04-07 03:59:44.094000+00:00,256,"pyspark, RDDs, join(), (Key, Value)"
Implementation of LeNet From Scratch using Keras.,"Here in this blog, I’m going to discuss the code implementation of LeNet using Keras. In this, I used this research for http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf for implementation.

LeNet-5 Total seven-layer does not comprise an input, each containing trainable parameters; each layer has a plurality of the Map the Feature, a characteristic of each of the input FeatureMap extracted by means of a convolution filter, and then each FeatureMap There is multiple neurons.

fig1. LeNet Research paper Arch

Library Import

Here this is basic import.

Defining Architecture

First, we define sequential layer here the layer is stack in the form Conv2D and MaxPool2D in repeated form. At last, I use the Dense layer before the dense layer Conv2D and MaxPool2D layer used for feature extraction, and this dense layer used for classification.

Here I did some changes in the activation function and input shape in the original paper they used tanh activation function and the input shape is 32*32. You can see the complete arch in fig1.

To get complete notebook use this link https://github.com/anjanimsp/LeNet_From_Scratch",https://medium.com/@anjanisuman/implementation-of-lenet-from-scratch-using-keras-e2f7ca0ed961,['Anjani Suman'],2020-12-17 14:06:29.173000+00:00,164,"lenet, keras, conv2d, maxpool2d, feature-extraction"
Adding noise to network weights in Tensorflow,"Currently I am working on an implementation of a Neuroevolution algorithm. These type of methods use Evolutionary Algorithms to evolve the weights of a Neural Network. One essential part of evolving these weights is the so called “Mutation” step. During this step Gaussian noise is added to the weights of a Network, which allows for the creation of various mutations of the original network. I realized that I wasn’t sure how one would add this noise to all of the weights in a Tensorflow network. So I figured it would be nice to write a small blog post about it.

Creating a simple network

The first thing I did was create a small network, which I will use to find out how one would add noise to its weights. Below I initialize a small network that has a single Dense with 100 nodes, and that has it’s weights and biases initialized at 0.

from tensorflow.python.keras import Input, Model

from tensorflow.python.keras.layers import Dense input = Input(shape=1)

output = Dense(100, activation=""softmax"", kernel_initializer='zeros')(input)

model = Model(inputs=input, outputs=output)

To verify that the setup is correctly, let’s print its weights:

print(model.get_weights())

__________________________________________________________________

[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]

Adding noise: method 1

As shown earlier, we can leverage the get_weights method to retrieve the weights of a network. In the same manor set_weight can be used to set the weights of the network. Due to this we can add noise by first retrieving the original weights, followed by setting the weights to their original value plus some noise vector.

from numpy.random import normal def add_noise_1(model):

weights = model.get_weights()

for layer in weights:

noise = normal(loc=0.0, scale=1.0, size=layer.shape)

layer += noise

model.set_weights(weights)

We can now verify that this worked:

add_noise_1(model)

print(model.get_weights())

__________________________________________________________________

[array([[-0.22465831, -0.7272695 , 0.60172313, -0.46081063, 0.7302764 ,

-0.49482158, -0.98451763, -0.17196278, -0.9898952 , -0.01227215]],

dtype=float32), array([ 1.4207387 , 0.7875955 , 0.28213844, 0.36810005, -0.33920723,

0.26612052, -0.1289873 , -0.524746 , 0.5679675 , -1.0648334 ],

dtype=float32)]

Adding noise: method 2

I figured that retrieving the weights, updating them and then setting them again might be sub-optimal. Therefore I hypothesized that adding our noise directly to the networks weights could be faster. Below I implemented a function that directly adds a value to the weights using the assign_add method.

def add_noise_2(model):

for layer in model.trainable_weights:

noise = normal(loc=0.0, scale=1.0, size=layer.shape)

layer.assign_add(noise)

Comparing method 1 and 2

To verify whether there is really a significant difference between the two methods I ran them for a range of different network sizes and compared the runtimes.

As we can see in the plot above, method 2 seems to be significantly faster for small networks, but this difference becomes neglectable when the networks start to get somewhat bigger.

Hope this helps someone!

Daan",https://medium.com/adding-noise-to-network-weights-in-tensorflow/adding-noise-to-network-weights-in-tensorflow-fddc82e851cb,['Daan Klijn'],2020-12-21 19:19:27.397000+00:00,428,"neuroevolution, evolutionary algorithms, mutation, tensorflow network, gaussian noise"
Can we forecast ‘meme stocks’ movements using Reddit data? Sentiment Analysis of WSB.,"Picture from Reddit, Picture source

Unless you virtually ignore any news about the stock market, you should have heard about GME (very briefly: people in one of the brunches of Reddit coordinated and pushed the stock price of a dying computer games firm ‘to the moon’). In this post, we’ll explore what happened on Reddit during these volatile times.

In order to do so we’ll:

Scrape comments from Reddit’s Wall Street Bets branch. Alternatively, it’s possible to use this dataset, but it’s not very extensive Perform sentiment analysis using VADER Sentiment and Bert model Get some interesting graphs and talk about them Create an RNN to see if we can use the data above to forecast the direction of change of stock price for the next hour. Spoiler: No.

1. Scrapping and initial analysis

The dataset we obtain by our own scrapper (or from Kaggle) will look like this:

It consists of around 90 000 non-empty posts scrapped for the last 3 month using Reddit API.

We can also get a nice representation using wordcloud:

Once we extracted tickers (by downloading all possible tickers from NASDAQ and NYSE and basically looking for them in all words that either start with ‘$’ or contain only capital letters), we can get their distribution

As you can notice, more than half of the posts belong to GME, that’s why we’ll mostly focus on this stock from now on.

2. Sentiment

Now, let’s add a sentiment analysis. The graphs below will be based on VADER Sentiment — a pretrained model meant for analysing posts on social media. We’ll use it in the framework of nltk. In the link to the notebook provided in the end, you can also see a similar analysis using the BERT model.

Here, we extracted 4 parameters using VADER, but we’ll stick to ‘compound’ — that’s the best metric if we want to map sentiment to a single number.

3. Cool graphs

Now, finally, the fun part. We’ll use consider ‘total sentiment’ for each day and link it to the stock price. In the graphs below, you should see two parameters: ‘hype’ and ‘sentiment’. The first one refers to the sum of absolute values of sentiment — so it basically indicates the popularity of stock. The latter includes sign — so if Redditor’s are positive or negative about it. What shall we expect to see? Assuming these people, led by ‘Roaring Kitty’, moved GME share price, the hype and stock price should be correlated.

But what we see is very interesting: the stock price went up first, and only then, in about a week, sentiment peaked.

This is strange.

One might think it means that WallStreetBets users didn’t had much impact on this ‘exposure’, but we know it is not true. In my opinion, we can say that most users of WallStreetBets got interested and started to comment about GME only after it ‘went to the moon🚀🚀🚀’. What’s also interesting is that the second peak at the beginning of March happened exactly when the stock price started to grow, as we could expect. This can be explained by the fact that a lot of people were watching the stock after it rapidly went up and down. Alternatively, the problem can be that we analyse comments instead of posts and maybe with posts situation is different, but it’s reasonable to assume they appear at the same time.

If we consider other stocks, the pattern will be the same (with a couple of exceptions): the first peak is delayed, and all other peaks happen simultaneously with the stock price.

If you have any other explanation of this fact, please feel free to drop a comment .

What follows from these graphs and discussions: If Redditors wrote comments and bought GME at the same time, most of them were likely to lose money.

4. Bonus: RNN

What if we take all these possible sentiments and create a Recurrent Neural Network to forecast sign of a change in stock price?

The challenge we face is that the stock exchange is open for 40 hours a week, and Reddit for 168 (unfortunately we didn’t get more frequent data). That’s why we added night data to the opening hours. Also, we have only 3 months of data for 40 points a week, so it’s about 400 points. We split data into train/validation/test as 70/15/15, and so our test set contains only 62 points. To not overfit our data too much we’ll use 2 layers with 8 neurons each. We can represent the result as a confusion matrix:

We can see that basically, it doesn’t works, certainly not enough statistical evidence to assume otherwise. However, this can be not the best place for RNNs and time series models like ARIMA may perform better. Finally, we saw that this sentiment data is not too helpful for forecasting, so we shouldn’t expect a miracle.

The notebook with code and more (different) examples can be accessed here.

This article was inspired by Turing Machine & Deep Learning course at Erasmus University Rotterdam, the Netherlands. All analysis were performed together with Robin van Merle.",https://medium.com/@knyazdima/can-we-forecast-meme-stocks-movements-using-reddit-data-sentiment-analysis-of-wsb-7642a9f36744,['Dmitriy Knyazhitskiy'],2021-04-12 15:22:13.314000+00:00,827,"GME, Reddit, Wall Street Bets, Sentiment Analysis, RNN"
Understand Feature Engineering for Machine Learning in 5 minutes,"How to do feature engineering?

Let’s see different strategies of feature engineering. In this article, we won’t see all the methods, but the most popular ones.

Adding and dropping features:

Let’s assume we do have the following features:

Price of houses

If we want to predict the price of a flat, the number of plants might be irrelevant. In that case, we need to remove this feature from our machine learning model to don’t add extra noise.

This noise is called the curse of dimensionality. This means that as the number of features in the data increases, the number of data points required to build a good model grows exponentially.

We need to choose which features have are the most relevant to our model.

Combining multiple features into one feature:

Price of houses

In the example above, we can see that square meters and square feet are actually the same data but not the same unit. If we give this to our algorithm, it will have to understand that the square meter and square foot are related and are actually the same feature.

That’s why we need to decide on which measurement to take and keep only one.

We could also have two features, number of dogs and number of dogs, and combine them under the number of animals.

Number of animals

Though, combining the features is not every time a good idea. For example, in the case of a date feature, probably the day of the week matter.

You need to remember that quality is better than quantity.

Cleaning existing features:

You need to keep the features that you think are relevant for your model picking up on the right signal in the data.

To do that you can:

Impute missing values.

Remove outliers for not trying to train with data points that are not representative.

Getting rid of the scales, for example, if you have features in centimeters and some other ones in meters, try to convert all of them in centimeters. This is called normalization .

. Transform skewed data to make it more compact for our model thanks to an easier distribution.

Binning:

Binning is when you take a numerical measurement and convert it into a category.

Here is an example for home sales:

Home sales

In that example, we can assume that the sale price depends on the fact that there is a swimming pool.

We can then simplify our model by pre-processing the data and replacing the swimming pool length with a boolean future.

Swimming Pool boolean

One-hot encoding:

One-hot encoding is a way to represent categorical data in a way that the machine learning algorithm will understand.

Our model understands numbers but not strings, that’s why we need to convert strings to numbers. Though, we cannot assign random numbers to our strings, because our model might give more importance to big numbers than little numbers. That’s why we are going to use a one-hot encoding.

Here is an example about home sales:

One-hot encoding

One-hot encoding is useful for replacing categorical data with simple numeric data that the machine learning model will understand.",https://ai.plainenglish.io/understand-feature-engineering-for-machine-learning-in-5-minutes-b395c004950a,['Florian Bouron'],2021-07-23 23:51:30.861000+00:00,485,"feature engineering, adding features, dropping features, combining features, cleaning existing features"
Microsoft Lobe: Machine Learning For All,"A week ago, Microsoft released their first public preview of Lobe, a desktop app that promises to bring machine learning to the masses. In this first iteration of the software, you can train and test an image classifier without writing a single line of code.. so how well does it work?

I decided to give Lobe a go, and create a model to classify images of some of my daughter’s favorite animals: lions, cheetahs, zebras, and giraffes (she’s two-and-a-half and has mastered this exercise).

After installing Lobe, the first step was to drag-and-drop training images into the Lobe interface. I gathered the images (around 20 per label) from Google Images.

Importing training images into Lobe: just drag-and-drop

Next, I labeled the images (‘giraffe’, ‘lion’, etc). This could be done really quickly in batches by selecting multiple images at once:

Batch labeling is intuitive and quick

Lobe started training the model as soon as I started labeling images. Training was quick (1~2 minutes for around 100 images total).

After training was complete, Lobe allowed me to interactive with the trained model. I could drag-and-drop fresh images into the Play window and Lobe labeled them automatically, with very good accuracy!

Predictions by Lobe: so far so good!

From start to finish, the whole process (importing ~100 images, labeling, training, and testing) took me around 15 minutes.

Now of course, to use the models in real world applications (e.g. in apps, webpages, etc.), you’d need knowledge of the various model Export options available, including TensorFlow, Python, CoreML etc. But overall, I feel that Lobe provides a great way for anyone to build, experiment with, and optimize viable image classifier models.

Lobe Pros:

Completely free

An intuitive way for anyone to create ML models

Interface is clean and tidy, and user friendly

Official social communities (e.g. at Reddit) make issue sharing and resolution easy

Lobe Cons:",https://medium.com/@tictacsays/microsoft-lobe-machine-learning-for-all-bbfc7409c99f,['Takashi Fujita'],2020-11-04 09:34:07.727000+00:00,296,"Limited to image classifiers only Export options are limited to Python, CoreML and Tensor Flow (no support for ONNX yet)Machine Learning, Lobe, Microsoft, Image Classifier"
"How does Panorama work?. Panorama effect! Yes, that’s what we…","Image Stitching

Panorama effect! Yes, that’s what we will uncover in this post.

Image mosaicing/stitching is the task of sticking one/more input images. We have information spread across these images which we would like to see at once. In a single image!

We all have used Panorama mode on mobile camera. In this mode, the image-mosaicing algorithm runs to capture and combine images. But we can use the same algo offline also. See the image below, where we have pre-captured images and we want to combine them. Observe the hill is partially visible in both inputs.

When you have multiple images to combine, the general approach followed is to pairwise add 2 images each time. The output of last addition is used as an input for the next image.

Assumption is that each pair of images under consideration do have certain features common.

Left and Right images stitched together using local features of interest points

Let’s see the step by step procedure for creating a panorama image:

Step 1] Read Input Images

Take images with some overlapping structure/object. If you are using own camera then make sure you do not change camera properties while taking pictures. Moreover, do not take many similar structures in the frame. We do not want to confuse our tiny little algorithm.

Now read both first two images in OpenCV.

def createPanorama(input_img_list):

images = []

dims = []

for index, path in enumerate(input_img_list):

print (path)

images.append(cv2.imread(path))

dims.append(images[index].shape)

return images, dims

Step 2] Compute SIFT features

Detect features/interest points for both images. These points are unique identifiers which are used as markers. We will use SIFT features. It is a popular local features detection and description algorithm. It is used in many computer vision object matching tasks. Some other examples of feature descriptors are SURF, HOG.

SIFT uses a pyramidal approach using DOG (difference of gaussian). Features thus obtained will be invariant to scale. It is good for panorama kind of applications wherein images might have features variations in rotations, scale, lighting, etc.

## Define feature type

feature_typye = cv2.xfeatures2d.SIFT_create()

​

points1, des1 = features.detectAndCompute(image1, None)

points2, des2 = features.detectAndCompute(image2, None)

Here points1 is a list of key points whereas des1 is a list of descriptors expressed in the feature space of SIFT. Each descriptor will be a 1x128 vector

Step 3] Match strong interest points

Now we will be matching points based on vector representation. We will assume a certain threshold for deciding whether two points are near or not

OpenCV has inbuilt FLANN based Matcher for this purpose. It is a histogram based matching technique which computes the distance for 2 points described in SIFT feature space.

## Define flann based matcher

matcher = cv2.FlannBasedMatcher()

matches = matcher.knnMatch(des1,des2,k=2) # important features

imp = []

for i, (one, two) in enumerate(matches):

if one.distance < dist_threshold*two.distance:

imp.append((one.trainIdx, one.queryIdx))

Step 4] Calculate the homography

Now that we have matching points identified in both images, we will use them to get the generic relationship between images. This relationship is defined in the literature as homography. It is a relationship between image1 and image2 described as a matrix.

We need to transform one image into other image’s space using the homography matrix. We can do either way from image 1 to 2 or image 2 to 1 since the homography matrix (3x3 in this case) will be square and non-singular. Only thing is that we need to be consistent while passing points to homography.

I have used my own RANSAC based approach to get a Homography matrix. But you can also use inbuilt OpenCV function cv2.findHomography()

### RANSAC

def ransac_calibrate(real_points , image_points, total_points, image_path, iterations): index_list = list(range(total_points))

iterations = min(total_points - 1, iterations)

errors = list(np.zeros(iterations))

combinations = []

p_estimations=[] for i in range(iterations):

selected = random.sample(index_list,4)

combinations.append(selected)

real_selected =[]

image_selected =[] for x in selected:

real_selected.append(real_points[x])

image_selected.append(image_points[x]) p_estimated = dlt_calibrate(real_selected, image_selected, 4) not_selected = list(set(index_list) - set(selected))

error = 0

for num in tqdm(not_selected): # get points from the estimation

test_point = list(real_points[num])

test_point = [int(x) for x in test_point]

test_point = test_point + [1] try:

xest, yest = calculate_image_point(p_estimated, np.array(test_point), image_path)

except ValueError:



continue error = error + np.square(abs(np.array(image_points[num])-np.asarray([xest,yest])))

# print(""estimated :"",np.array([xest, yest]) )

# print(""actual :"",image_points[0])

# print(""error :"",error)

errors.append(np.mean(error))

p_estimations.append(p_estimated) p_final = p_estimations[errors.index(min(errors))]

return p_final ,errors, p_estimations

Step 5] Transform images into the same space

Compute the second image transformed coordinates using the homography matrix output of step 4.

image2_transformed = H*image2

Step 6] Let’s do the stitching...

After computing transformed images we get two images each having some information separate and some common w.r.t. other. When it is separate the other image will have 0 intensity value at the corresponding location

We need to fuse with the help of this info at every location while keeping overlapping info intact.

It is a pixel level operation which technically can be optimized by doing image level add, subtract, bitwise_and, etc, but I found the output was getting compromised in doing this manipulation. Output generated with old-school for-loop based approach to choose the maximum pixel from the corresponding input pixels worked better.

## get maximum of 2 images

for ch in tqdm(range(3)):

for x in range(0, h):

for y in range(0, w):

final_out[x, y, ch] = max(out[x,y, ch], i1_mask[x,y, ch])

See the output below:

Although the post was just a small guide, you can refer to the entire code available here.",https://medium.com/tech-that-works/how-does-panorama-work-image-stitching-bf1a9f0e4fa5,['Samrudha Kelkar'],2019-06-03 06:58:26.190000+00:00,813,"image stitching, panorama effect, SIFT features, FLANN based Matcher, homography matrix"
2020 in Review With Jürgen Schmidhuber,"In 2020, Synced has covered a lot of memorable moments in the AI community. Such as the current situation of women in AI, the born of GPT-3, AI fight against covid-19, hot debates around AI bias, MT-DNN surpasses human baselines on GLUE, AlphaFold Cracked a 50-Year-Old Biology Challenge and so on. To close the chapter of 2020 and look forward to 2021, we are introducing a year-end special issue following Synced’s tradition to look back at current AI achievements and explore the possible trend of future AI with leading AI experts. Here, we invite Prof. Jürgen Schmidhube to share his insights about the current development and future trends of artificial intelligence.

Meet Jürgen Schmidhuber

The media have called Jürgen Schmidhuber the father of modern AI. Since age 15, his main goal has been to build a self-improving AI smarter than himself, then retire. His lab’s deep learning neural networks (developed at TU Munich and the Swiss AI Lab, IDSIA, USI & SUPSI) have revolutionized machine learning. By 2017 they were on over 3 billion smartphones, and used many billions of times per day, for Facebook’s automatic translation, Google’s speech recognition and Google Translate, Apple’s Siri & QuickType, Amazon’s Alexa, etc. He also pioneered adversarial networks, artificial curiosity and meta-learning machines that learn to learn. He is recipient of numerous awards, and chief scientist of the company NNAISENSE, which aims at building the first practical general purpose AI. He is also advising various governments on AI strategies.

The Best AI Technology Developed in the Past 3 to 5 Years: “Deep Learning & Highway Networks”

The basic ideas behind the deep learning revolution were published deep in the previous millennium. However, these old ideas (plus certain improvements) work much better with the faster computers of today. Since 1941, when Konrad Zuse built the first working programmable general computer, every 5 years, compute got 10 times cheaper. In 1990, compute was 1 million times more expensive than in 2020. But it was back then when we published many of the basic deep learning ideas within fewer than 12 months in your “Annus Mirabilis” or “Miraculous Year” 1990–91 at TU Munich, for example: Artificial Curiosity & its special case called GANs (1990), deep learning through unsupervised pre-training (1991), vanishing gradients & LSTM, planning with recurrent world models, etc. See http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html for more. Deep learning has greatly profited from this hardware acceleration between 1990 and 2020, especially in the recent 3–5 years you mentioned.

One big thing of the past 5 years were the Highway Networks. In the early 2010s, neural nets (NNs) were not yet extremely deep and achieved at most a few tens of layers, e.g., 20–30 layers. However, in May 2015, there was something new. Our Highway Networks were the first working really deep feedforward neural networks with hundreds of layers. This was made possible through my PhD students Rupesh Kumar Srivastava and Klaus Greff. Highway Nets are essentially feedforward versions of our earlier recurrent LSTM nets. If we open the gates of Highway Nets, we obtain the so-called Residual Net or ResNet (Dec 2015), a special case of our Highway Net. Microsoft Research won the ImageNet 2015 contest with a very deep ResNet of 150 layers. Today, many are using such networks. More: http://people.idsia.ch/%7Ejuergen/highway-networks.html

The Most Promising AI Technology in the Next 1 to 3 Years: “Non-Traditional Methods for Reinforcement Learning”

The most promising AI technology is non-traditional methods for reinforcement learning (RL). For example, our RL LSTM can be trained by policy gradients, as shown in 2007–2010 with collaborators including my PhD student Daan Wierstra, who later became employee number 1 of DeepMind, the company co-founded by his friend Shane Legg, another PhD student from my lab. (In fact, Shane and Daan were the first persons at DeepMind with AI publications and PhDs in computer science.) Policy gradients for LSTM have become important. For example, in 2019, DeepMind beat a pro player in the game of Starcraft, which is harder than Chess or Go in many ways, using Alphastar whose brain has a deep LSTM core trained by PG. And the famous OpenAI Five learned to defeat human experts in the Dota 2 video game in 2018. The core of it also was an PG-trained LSTM with 84% of the model’s total parameter count. Bill Gates called this a “huge milestone in advancing AI.” And combined with ideas about recurrent world models (since 1990 — see above, and the recent work with Google’s David Ha, 2018), this will become even more important.

The Biggest Challenge in the Field of AI: “AI in the Physical World”

The big thing of the future will be AI in the physical world, for robots and industrial processes etc. Today most profits in AI are in the virtual world, for marketing and selling ads — that’s what the big platform companies on the Pacific Rim do: Alibaba, Amazon, Facebook, Tencent, Google, Baidu … But marketing is just a tiny fraction of the world economy. A much bigger part is the rest, which is going to be invaded by AI as well, like in the movies. That’s what our company NNAISENSE is about. It is pronounced like “nascence” (meaning “birth”) but spelled in a different way because it’s about the birth of a general purpose Neural Network-based Artificial Intelligence for the physical world.

The Latest Noteworthy Development: “Upside Down RL”

One very interesting recent development is called Upside Down RL (UDRL). It is turning traditional RL on its head. Standard RL predicts rewards, while UDRL instead uses rewards as task-defining inputs, together with representations of time horizons and other computable functions of historic and desired future data. UDRL learns to interpret these input observations as commands, mapping them to actions (or action probabilities) through supervised learning on past (possibly accidental) experience. UDRL generalizes to achieve high rewards or other goals, through input commands such as: get lots of reward within at most so much time! Certain problems of traditional RL with high-dimensional actions, partial observability, and reward discount factors disappear. First experiments showed that even a pilot version of UDRL can outperform traditional baseline algorithms on certain challenging RL problems.",https://medium.com/syncedreview/2020-in-review-with-j%C3%BCrgen-schmidhuber-8c80b770c690,[],2020-12-25 21:02:41.447000+00:00,1013,"AI, Deep Learning, Neural Networks, Reinforcement Learning, UDRL"
XGBoost Classifier Hand Written Digit recognition,"XGBoost Classifier Hand Written Digit recognition Niketanpanchal Follow Oct 7 · 4 min read

To get started with XGBoost , first we have to install the XGBoost library package. We can do it using ‘pip’ or ‘conda’.

pip install xgboost

In this article we’ll focus on how to create your first ever model (classifier ) with XGBoost.

The data set we choose for this example is the handwritten digit dataset , which is readily available in sklearn’s preloaded datasets. To import the data set we use the code :

from sklearn import datasets

digits = datasets.load_digits()

The digits object has ‘images’ and ‘target’ attributes in it.

The images are 2dimensional arrays of dimension 8x8 , each of these arrays represent a image of handwritten digit. Since the image has only 8x8=64 pixels , it is a quite low-resolution image.

The target attribute has the labels of the image i.e. if the image[0] represents numerical digit ‘4’ than the corresponding target has the value ‘4’.

Our problem statement is to correctly recognise the handwritten digits from the 8x8 pixel image given.

We have 10 target classes (digits 0 to 9)

images=digits.images print(images[0])

2D array representing a hand written digit

For better visualisation we use matplot library.

import matplotlib.pyplot as plt

plt.imshow(images[0],cmap='binary',interpolation='nearest')

images[0]

targets=digits.target

print(targets[0])

The load_digits dataset has 1797 records. For Training and Validation purpose we have to split the data set into training and testing sets.

The images object we have is 3D array, which is very difficult to handle,

so for our convenience we convert it into a 2D array dataset.

i.e. Now each image is represented in 2D arrays, we change it to 1D array by adding each row to the end of previous row and making it a single row.",https://medium.com/analytics-vidhya/xgboost-classifier-hand-written-digit-recognition-219acedfef13,[],2020-10-12 12:34:19.420000+00:00,270,"XGBoost, Handwritten Digit Recognition, Niketanpanchal, Classifier, Sklearn Preloaded Datasets"
What are Regression Algorithms in Supervised Machine Learning?,"Regression algorithms are a subset of Supervised Machine Learning technique which are used to predict output values based on the input labeled data.

Opposite to classification technique which is used to make predictions where the output variable is a category (yes/no, spam/not spam), regression models are used to predict a continuous value (real numbered output — temperature, stock price, weight, etc). So, regression is one of the most widely used statistics and machine learning tools for deriving intelligence from data.

Which are the main applications of regression models in machine learning?

to predict real estate values based on location, size, price, and other factors;

financial forecasting

trend analysis

marketing

time series predictions

Which are the most popular types of regression algorithm?

Simple Linear Regression; Lasso Regression; Logistic Regression; Support Vector Machines; Decision Tree Regression Random Forest Regression Multivariate Regression algorithm; Multiple Regression Algorithm; Polynomial Regression.

Resources:",https://medium.com/bite-sized-knowledge-delivery/what-are-regression-algorithms-in-supervised-machine-learning-7923cd2f8ff8,['Micro Learning'],2019-05-12 18:14:37.066000+00:00,136,"machine learning, regression algorithms, supervised learning, classification technique, simple linear regression"
Evaluation metrics for regression techniques,"To be able to choose between different evaluation metrics available out there let’s take a look at “What regression is?”

Regression is a statistical method used to predict a dependent variable based on some other variables which are correlated with our predicting variable.

Let’s take a classic example to understand it in a better way,

Suppose we want to predict the prices of the house based on the available variable which is square feet of the house. We can use regression to predict the prices where our dependent variable is the ‘price’ and the independent variable is ‘square feet’, prediction of the prices can be done using Linear Regression.

Above Fig depicts the formulae, which is used to predict our dependent variable, to predict our dependent variable(y) using the independent variable(x) we need to calculate the Gradient of the line(m) and the y-intercept(c).

In this way, we can use Linear Regression to predict the house price based on the square feet of the house.

Now, we’ve understood “what Regression is?” and also predicted the house prices using it but how can we decide that the predicted prices are correct?

This is the time when evaluation metrics comes into play.

As the name suggests, Evaluation Metrics are used to check whether our predicted prices are correct or not in simple words it is used to validate our findings.

So, there are 3 frequently used metrics which are explained below,

Mean Absolute Error(MAE): The mean absolute error, it calculates the absolute difference between our actual and predicted values.

Good predictions will have lower MAE value since the difference between the actual and predicted values will be less.

2. Root Mean Squared Error(RMSE): The root means squared error gives the average of squared differences between actual and predicted values and finally, the square root is taken.

RMSE is most useful when large errors are particularly undesirable.

3. R-Squared Error: R-squared is the proportion of the variance in the dependent variable that is predictable from the independent variable.

The R-Squared value should not be too small or too large.

Where,

n= No. of training examples

yi= Actual Values

yhati= Predicted values

ybari= Average of values

SSRES= Squared difference of actual and predicted values

SSTOT= Squared difference of actual and average values

NOTE: For a model to be good it should have lower MAE, RMSE and higher R-Squared values.

As they say “A picture is worth a thousand words” So, let’s understand the errors using the below picture.

MAE is the sum of absolute vertical distances of the observed data(yact) and Regression line(ypred) for all the values in training data. RMSE is the sum of root mean squares of the vertical distances between yact and ypred. SSRES is the sum of the squares of the vertical distances of the yact from the Regression line. SSTOT is the sum of the squares of the vertical distances of the yact from the Mean line.

Now, you are fully equipped with regression and evaluation metrics.

Hope this will help.",https://medium.com/analytics-vidhya/different-evaluation-metrics-for-regression-1b04b12e0254,['Ritik Kesharwani'],2020-10-20 12:43:29.504000+00:00,474,"Regression, Evaluation Metrics, MAE, RMSE, R-Squared Error"
Timestamp parsing for time-series data analysis with Pandas and Python,"Experiments with unique timestamps

1. List of timestamps with a standard format

ISO-8601 is a widely accepted international standard for time-related information exchange. In addition to timestamps that follow the ISO-8601 standard, a few others are also a “standard” format as far as Pandas is concerned. This means that there is some set of timestamp formats that Pandas can parse very efficiently. An exhaustive list of these is not available (as far as I know) but in general, timestamp formats with all parts of the date and ones that start with the year seem to fall under this category.

So now, let us see how these methods perform when given timestamps of a known standard format. The formats of the timestamps are consistent throughout each dataset. We test the performance with datasets of different sizes given to the applicable methods.

Comparing different methods when the timestamps are of a standard format | Image by author

The results show that Pandas.to_datetime significantly outperforms time.strptime in this instance. The pre-built lookup method also marginally outperforms the time.strptime method. However, it is still well short of the performance that Pandas delivers.

2. List of timestamps with a non-standard format

Now, if we run the same tests with datasets that have a non-standard timestamp format (e.g. 13–11–2000 04:50:32), we see some differences.

Comparing different methods when the timestamps are of a non-standard format | Image by author

We notice here that Pandas.to_datetime with a specified format performs the best and a plain time.strptime loop comes in second place. The pre-built lookup method spends too much time building the map and therefore, its performance suffers. Pandas.to_datetime without the infer option also takes a long time because of the repeated format-inference of each timestamp string.

We also see some curious behaviour with the results of Pandas.to_datetime with infer. We see that it performs exceptionally well until it hits a dataset size of close to 20000. And then it performs the same way as Pandas.to_datetime without infer. What is going on here?!

This behaviour happens to be a side-effect of the dataset used in these experiments but it illustrates an important point. The dataset used in these experiments is a list of timestamps that starts with 12:00AM on January 1, 2000 and progresses consistently with an interval of 1 second. The format of the timestamp used is dd-mm-yyyy hh:MM:ss. Therefore, when Pandas tries to infer the first timestamp in the list, there is ambiguity about the format. This is because the timestamp string 01–01–2000 00:01:00 could be either in the format dd-mm-yyyy hh:MM:ss or mm-dd-yyyy hh:MM:ss!

So, when we have a dataset that starts with an ambiguous timestamp but has an unambiguous timestamp towards the end of the list, Pandas may realize that its inference is incorrect when it reaches the end of the list. It would then fall back to the behaviour of inferring the datetime format for each timestamp string individually. This would cause the performance of the operation to be similar to the case when infer_datetime_format = False.

Experiments on datasets with duplicates

The datasets used thus far have had no duplicates in them. However, in the real world, we are often dealing with datasets that have repeated timestamps or multiple datasets from the same time period. In the industrial intelligence domain (in which I currently work), it is not uncommon to process scores of datasets together from the same time-range and therefore, there is a lot of duplicated timestamp strings between all of them.

In the following experiments, we’ll see how our choice of timestamp parsing may change based on how much duplication we have within our dataset.

For the following experiments, all datasets contained 1 million timestamp strings. During a test, different numbers of duplicates were infused into each dataset while keeping the dataset size fixed. The lowest number of duplicates infused was 0 (all unique), and the highest number of duplicates infused was 100 (each timestamp in the dataset had 99 other copies).

Experiment with timestamp strings of a standard format (and consistent throughout dataset)

Pandas performs well with or without duplicates when the timestamp format is standard | Image by author

We see here that Pandas.to_datetime is an easy choice when dealing with a standard format. And as expected, memoization and pre-built lookup mapping improves as the number of duplicates in a dataset increases.

Experiment with timestamp strings of a non-standard format (and consistent throughout dataset)

Memoization and pre-built lookups performing better than Pandas when the timestamp format is non-standard | Image by author

But when the format of the timestamps is not standard and there are some duplicates in the dataset, memoization and pre-built lookup mapping both perform significantly better. In fact, I recently used the pre-built lookup mapping method to parse a large collection of timestamp strings and it saved me over 8 hours!",https://towardsdatascience.com/timestamp-parsing-with-python-ec185536bcfc,['Sudeep Gowrishankar'],2020-11-07 17:01:01.876000+00:00,784,"timestamps, ISO-8601, Pandas, time.strptime, memoization"
Former Google CEO Eric Schmidt: Let’s Start a School for A.I.,"Former Google CEO Eric Schmidt: Let’s Start a School for A.I.

Uncle Sam might want you… to code.

If you’re interested in becoming a technologist for the federal government, former Google CEO Eric Schmidt wants to teach you how to code.

According to OneZero, Schmidt has partnered up with former U.S. Secretary of Defense Robert O. Work to create a school for folks who want to become government coders. This U.S. Digital Service Academy would operate like a regular school, offering coursework and degree tracks, and focus on cutting-edge technology subjects such as cybersecurity and artificial intelligence (A.I.).

As OneZero points out, the federal government is very interested in technologists who can craft new innovations in A.I. “We are engaged in an epic race for A.I. supremacy,” the publication quotes Rick Perry, secretary of the Department of Energy, as telling an NSCAI conference in 2019. “As I speak, China and Russia are striving to overtake us. Neither of these nations shares our values or our freedoms.”

Despite that urging, however, the U.S. government has “fallen short” when it comes to actually funding artificial intelligence research, according to a report issued by NSCAI: “AI is only as good as the infrastructure behind it. Within DoD in particular this infrastructure is severely underdeveloped.”

But the U.S. Digital Service Academy isn’t a done deal; first, Congress must approve NSCAI’s recommendation that the university be created. Then, it would actually need to be built, staffed, accredited, and launched. In order to fulfill the vision presented by Schmidt, the school would also need to forge partnerships with a variety of private companies and public institutions, in order to give students the necessary internships and other opportunities.

And even if all those goals are met, the U.S. Digital Service Academy would need to persuade young technologists to opt for it over other schools that are specializing in A.I. instruction, including Stanford and MIT.

Over the past several years, Eric Schmidt has shaped himself as an expert and advisor on U.S. technology policy. Last year, for example, he suggested that the U.S. government’s attempts to restrict hiring from China wouldn’t do this country’s technology industry any good.

“I think the China problem is solvable with the following insight: we need access to their top scientists,” he told the audience, according to Bloomberg. He also added that “common frameworks” such as Google’s TensorFlow benefit from input from scientists and researchers in other countries.

The U.S. Digital Service Academy is clearly his latest attempt to try to guide policy and discussion. If he can actually get it off the ground, though, it could provide yet another venue for technologists to learn intensely valuable A.I. and machine learning skills.",https://medium.com/dice-insights/former-google-ceo-eric-schmidt-lets-start-a-school-for-a-i-1a709e61e22b,['Nick Kolakowski'],2020-07-31 13:01:01.564000+00:00,439,"Eric Schmidt, GoogleCEO, AI, USDS, USDigital Service Academy"
MACHINE LEARNING — MALWARE CLASSIFICATION FRAMEWORK,"Malware types

Nowadays, malware is a significant concern for computer security experts. The variety and increasing number of malware affect millions of systems in the form of viruses, worms, Trojans etc. Many techniques have been proposed to analyze the malware to its class accurately. Some of analysis techniques analyzed malware based upon its structure, code flow, etc. without executing it (called static analysis), whereas other methods (termed as dynamic analysis) focused on monitoring the behaviour of malware by running it and comparing it with known malware behaviour. Dynamic analysis has proved to be useful in malware detection as behaviour is more challenging to mask while executing than its underlying code (static analysis).

We discuss framework for Malware classification using machine learning techniques. General framework for malware classification system using machine learning techniques shown in the following figure.

General malware classification framework

Executing malware in a controlled environment: Malware PE is run in a controlled environment for observing its behaviour. For example, Cuckoo sandbox can be used to run the malware and record its action in the form JSON reports. These reports contain the detailed recording of duration, system calls, its arguments, network information etc. The output of the module is set of reports in JSON format for malware and benign. Extracting features from execution reports: Raw elements are derived from the reports of Cuckoo environment. The extracted features are transformed into a tabular form and saved to a CSV ﬁle containing all possible dynamic characteristics of malware. The dynamically extracted features may include elements related to Dynamic Imports, File Operations, Mutex Operations, Network Operations, Processes Created, injected or terminated, Registry Operations, Windows API Calls and their frequency. Selecting relevant features: The module is optional (but recommended) and is responsible for choosing the most promising features from the raw feature dataset provided by the extraction module of the proposed framework. The malware dataset may contain some irrelevant and redundant features. Processing of these irrelevant and unnecessary features may lead to many problems including, 1) Undesirable delay in classiﬁcation task which in turn loses the real-time capability of MCS; 2) Increase computation overhead in terms of memory and time, and 3) Deteriorate the classiﬁcation and prediction accuracy of machine learning techniques. To address these issues, most researchers select relevant features using various feature selection techniques. Classifying malware: this phase classifies malware into different categories. The working of this module involves two phases, namely, training phase and testing phase.

a) Training Phase: A machine learning technique like MLP-ANN is being trained using identified features only for learning the behaviour of malware. The output of this phase is a trained model of the classiﬁer.

b) Testing Phase: Here, the trained model is given input of Test dataset in terms of selected features only to predict the class label of malware. A report is generated as an output that can be used by security analysts for further policy decisions.

5. Analyzing the performance of the system in terms of famous metrics like TPR, FPR, Accuracy etc.",https://medium.com/@gulshanahuja/machine-learning-malware-classification-framework-69518a4cb88,['Dr. Gulshan Kumar'],2019-09-05 02:00:08.882000+00:00,494,"Malware, Classification, Machine Learning, Dynamic Analysis, Static Analysis"
Tensorflow 2 C++ example for object detection (inference),"Tensorflow 2.x C++ API for object detection (inference)

Photo by Anoir Chafik on Unsplash

In a recent project, I had to integrate an object detection model into an existing C++ application code base for inferencing. This meant creating a C++ inferencing wrapper for our model which was trained in Python and serialized as TensorFlow binary checkpoints. We were on the latest stable release v2.3 of Tensorflow and were also using the Tensorflow Object detection API v2 which was recently upgraded to be compatible with Tensorflow 2.x.

This seemingly straight forward task took me a good few days and raising a couple of issues on Tensorflow Github repo, mainly because building Tensorflow C++ API from source is still painful and the lack of documentation around the usage of C++ API means you have to figure out things the hard way. If you are attempting the same, this article might save you some time and agony.

Oh btw, this article is mostly code, so you can view them directly on Github if that’s the sort of the thing you like.

Game plan

Build the latest Tensorflow C++ API from source (tested with v2.3.0) using docker. Load a SavedModel using SavedModelBundle Serve prediction using the new ClientSession method (vs the old Session way)

Checking thrusters

A few things though before we start

Use docker. I cannot stress this enough! You don’t want to sabotage your existing native TensorFlow installation, especially with those nasty CUDA and cuDNN version compatibility issues. But if you want, you could just use the steps in the Dockerfile to build the API from source natively. You might still want to develop locally without having to compile your cpp code against the container. To achieve this, I built the C++ API from source using the exact CUDA and cuDNN versions of my native Tensorflow installation but inside a container. Once built, I simply transferred the header and library files to the host system and made them accessible to Tensorflow and Visual Studio Code. We demonstrate the example using the Tensorflow Object Detection API v2 and a pre-trained EfficientDet-D3 model. Get the model from the TF model zoo. But, you could use any model you want as long as it’s in the SavedModel format.

Liftoff

Grab the docker image from dockerhub

docker pull boraraktim/tensorflow2_cpp

Here is the Dockerfile if you wish to build it yourself

Orbit

The cpp code is pretty self-explanatory, divided into a saved_model_loader.h header file (where most of the action is) and a get_prediction.cpp file which is the actual interface for inferencing.

Let’s wrap up with some Doggy detection

Details on compiling the code are in the Github readme

Original image from Unsplash: https://unsplash.com/photos/2_3c4dIFYFU

We detected the doggies which btw, was always the plan.

Houston, mission successful!",https://medium.com/@reachraktim/using-the-new-tensorflow-2-x-c-api-for-object-detection-inference-ad4b7fd5fecc,['Raktim Bora'],2020-10-12 21:48:58.606000+00:00,439,"tensorflow, c++, object detection, inference, efficientdet-d3 model"
"Web Scraping with Beautiful Soup, Selenium, or Scrapy?","Web Scraping with Beautiful Soup, Selenium, or Scrapy?

Photo by 卡晨 on Unsplash

The internet is full of data available for you to start your Data Science project. Obtaining that data could be as simple as copying and pasting it, but when it comes to large data, web scraping is the best solution. However, if you google “how to web scrape with Python,” you’ll get many tutorials using different Python libraries and frameworks.

In this guide, we’ll analyze the 3 most popular web scraping tools in Python, so you can choose the one that suits best to your project.

Beautiful Soup

Beautiful Soup can pull data out of HTML and XML files. On top of that, it’s the easiest to learn among the 3 options.

However, Beautiful Soup has some dependencies, such as the need of the request library to make requests to the website and the use of external parsers to extract data; for example, XML and HTML parser. These dependencies make it complicated to transfer code between projects.

Let’s see a simple example for extracting data with Beautiful Soup:

As we can see, only a few lines of code are needed to extract data with BeautifulSoup, but we need to import requests to access the website and html.parser to parse the content.

Selenium

Selenium wasn’t originally designed for web scraping. In fact, Selenium is a web driver designed to render web pages for test automation of web applications.

This makes Selenium great for web scraping because many websites rely on JavaScript to create dynamic content on the page. Other web scraping tools like Beautiful Soup don’t have this functionality, limiting the extraction of data available on most websites.

Selenium is not as easy to learn as Beautiful Soup; however, it’s still a friendly tool since it allows code to mimic human behavior such as clicking on a button, selecting dropdown menus, maximizing windows, etc.

Let’s see an example of extracting data with Selenium:

One of the disadvantages of Selenium is speed. Web scraping with Selenium is slower than HTTP requests to the web browser because all the scripts present on the web page will be executed. However, if speed isn’t a top priority, Selenium will be a good option.

Scrapy

Scrapy is a web scraping framework built especially for web scraping and written entirely in Python. It’s built on top of Twisted, an asynchronous network framework, which allows applications to respond to different network connections without using traditional threading models.

One of the biggest advantages of Scrapy is speed. Since it’s asynchronous, Scrapy spiders don’t have to wait to make requests one at a time, but it can make requests in parallel. This increases efficiency, which makes Scrapy memory and CPU efficient compared to the previous web scraping tools analyzed.

Image found on Wikimedia Commons

Some drawbacks of Scrapy is that it doesn’t handle JavaScript by default, but it relies on Splash to do the job. Also, the learning curve to learn Scrapy is steeper than tools like Beautiful Soup and the installation process and setup can be a bit complicated.",https://towardsdatascience.com/web-scraping-with-beautiful-soup-selenium-or-scrapy-62c6f3545de7,['Frank Andrade'],2020-12-21 19:45:10.445000+00:00,494,"webscraping, Beautiful Soup, Selenium, Scrapy, Python"
The Travelopia Agile Playbook — or Brochure?,"Something really exciting is starting to happen at Travelopia.

OK – so we admit – when it comes to Agile Software Delivery we are starting a little way back from the bleeding edge.

If we were to liken our project delivery to some of our fantastic destinations – we’d be Iguazu Falls.

But that doesn’t mean we’re not striving for better. Over the past 6 months we have gathered together the largest number of Agile-focused Product owners, Scrum masters and engineers that Travelopia has ever assembled to work on the Catalyst programme.

The crux of Catalyst is enabling a number of initiatives which puts our customers first, through the application of test and learn hypotheses, data science and agile system integration and delivery.

In the short term we’ve decided to follow some elements of the Scaled Agile Framework (SAFe), but over time we believe we will “take the stabilisers off” and create Travelopia’s own approach to Agile delivery which will cherry-pick the parts of Scrum, SAFe, XP and other approaches that work best for our people, teams and partners.

We are collecting and collating our own Agile playbook, or glossy brochure to keep with our Travel theme, to pass on to future Travelopia Agile teams what we learn.

If we like them a lot, we will share them with you!",https://medium.com/travelopia-engineering/the-travelopia-agile-playbook-or-brochure-e80e4919a791,['Mark Beauchamp'],2019-08-17 07:05:44.947000+00:00,213,"Agile Software Delivery, Travelopia, Catalyst Programme, Scaled Agile Framework (SAFe), Test & Learn Hypotheses"
Robert Kosara: How to Visualize Data,"This is a great article that theorizes the role of visualization in problem-solving. The sub-section on “Cycle of Visual Analysis” is a useful framework for thinking about your own projects later on in the semester.

https://eagereyes.org/talk/talk-how-to-visualize-data",https://medium.com/neu-artg6900/robert-kosara-how-to-visualize-data-90ebfb1ce760,['Siqi Zhu'],2016-01-23 21:48:13.809000+00:00,35,"visualization, problem-solving, cycle of visual analysis, data analysis, data visualization"
How to Address Missing Data,"Solutions to Three Types of Missing Data

Missing data is one of the most common data quality issues among three most common issues: Missing Value, Duplicated Value and Inconsistent Value.

Missing value is the easiest one to identify, it may be in various forms, e.g. null values, blank space or being represented as “unknown”. Apply a filter to data can make missing values more easily identified. Duplicate value occurs when several rows of data appear to be the same then most likely that they have been mistakenly recorded multiple times. Inconsistent value usuallyoccurs when the string values of the same attributes do not follow the same naming convention, e.g. both LA and “Los Angeles” are present in the “City” data field (know more about how to address inconsistent data in this article)

Why the data is missing?

1. Missing completely at random (MCAR)

This article will mainly focus on why the data is missing and how to address the issues.

It may be the result of data not recorded in the first place, hence the reason for missing data is unrelated to this attribute. Therefore, we cannot predict what subsets of data are missing, as the result, the missing values are less predictable as well.

2. Missing at random (MAR)

The attribute value of an instance is missing not because of the attribute itself but because it is affected by other properties of this instance. Use a very simple illustration as below, for example in a supermarket transaction dataset, the liquor purchase attribute would be empty for all customers under age 18, hence it is affected by another property “age” rather than that attribute “liquor purchased amount” itself. We should avoid addressing MAR by deleting the entire row of data, since it may result in bias, e.g. all instances with age under 18 are deleted.

3. Missing not at random (MNAR)

This type of missing data is very hard to distinguish from MAR. In this case, the attribute values are missing as a result of the attribute itself. For example, the machine can record temperature up to 200 degrees, hence all values above 200 degrees are missing. As we can see, the data is missing is because of fo the nature of this “temperature” attribute instead of being affected by other observed data, which distinguishes it from MAR. MNAR has already resulted in a biased dataset. In most cases, extreme values are not recorded.

How to handle missing data?

1. Ignore the tuple

The simplest solution is to remove the tuples with missing data. It is also the most ineffective solution, especially when the fraction missing values varies considerably across attributes. As a result, most instances are deleted and the dataset remains to be very small. Additionally, ignoring the missing values would result in bias when data are missing at random (MAR) or missing not at random (MNAR).

For example, in this case, deleting all tuples with missing data will result in hardly any instances remaining.

2. Fill in missing data manually

Fill in the missing data by inspecting each individual missing value. This may be the most reliable way to ensure that all missing values are handled appropriately. However, it is time and effort consuming. Most of the time, this approach is not feasible.

3. Fill in automatically

This can be achieved in one of the following ways:

1) using global constant such as “missing”, “null” etc

2) using mean or median of the attribute value

3) using mean or median of the attribute value within the same class

4) using the most probable value which is computed using from inference based algorithm such as decision tree or kNN.

But each approach has its downside. Using global constant such as “missing” or “unknown” to address numeric attributes may result in inconsistent data type. Using mean or median may introduce new bias to the original dataset and change the data distribution as well as its relationship with other attributes. The last approach seems to the most accurate approach, however, there is always the trade-off of high computation cost.

4. More Advanced Approaches

1) Matrix decomposition: This is a mathematical process of decomposing the original matrix with missing values into a product of submatrices. Then recreate the matrix with filled values by multiplying the submatrices. This article explains it in more detail.

2) Expectation maximization imputation: It is a machine learning approach to find maximum-likelihood estimates for missing values. Read this article “A Gentle Introduction to Expectation Maximization” if you would like to know more.

3) Multiple imputations: this is done by building different models to predict the missing value, then the final value is determined by considering the probability of each predicted value. This is fairly similar to the method in the classification problem, where multiple algorithms and models are combined together to create an optimal result.

More Related Articles",https://medium.com/analytics-vidhya/how-to-address-missing-data-531ed964e68,['Destin Gong'],2020-11-09 13:02:24.880000+00:00,782,"1. Missing data, 2. Data Quality Issues, 3. Data Analysis, 4. Data Cleaning, 5. Machine Learning"
The beginning of an adventuRe,"A few years ago I had to present a research about social media to the board of an online magazine. I almost didn’t know where to start: Twitter in Italy was pretty new, but I felt it could have been good for our zero-budget magazine. After some research I discovered this paper about Twitter mining and sentiment analysis using a consonant: R.

That was my first encounter with R project. It didn’t end up well. Back then my laptop wasn’t so good and I managed to crash it at least 5 times before I could perform some basic analysis. To be fair, I had a problem as well with coding. My only experience in coding was in Html and CSS, and I wasn’t even so good at it.

What I didn’t understand is that I fell in love. Suddenly I realized that the web wasn’t just a huge noisy mess, but it could be organized, analyzed and exploited. I fell in love with data science.

At the time data science wasn’t such a buzzword, I would say almost nobody knew it existed. So my life went on. I was living like 90% of us in an Excel world: my master’s thesis tables wheighted about 40 mb. I almost started learning Visual Basic, but luckily enough I didn’t.

“Suddenly I realized that the web wasn’t just a huge noisy mess, but it could be organized, analyzed and exploited. I fell in love with data science.”

Data always intrigued me, I steadily look for new insights, charts, visualizations, studies, etc. I even got a job that for the most part is about data retrieving and analysis from many different countries (the next time you feel the urge to mock HR people, you may want to think twice about it: the work changed a lot in the last years). But the process with Excel was tedious and annoying. What could I do?

R ! A spark.

I remembered about that tiny little mess I did a while ago. I could remember those little arrows <- assigning variables, the way I queried Twitter’s API and how many times my laptop crashed. I took a decision: learn to code in R.

Fun fact, in high school I was bad in math. Really bad. I mean it. VERY BAD. But weirdly enough I was good in physics, economics and statistics. So I decided to take the challenge and study statistics as well.

I had the luck to find a few nice books on the subject, that I suggest to everyone interested in statistics and machine learning:

“Naked Statistics” by Charles Wheelan

by Charles Wheelan “The Signal and the Noise” by Nate Silver

by Nate Silver “Data Science for Business” by Foster Provost & Tom Fawcett

by Foster Provost & Tom Fawcett “Discovering Statistics Using R” by Andy Field, Jeremy Miles and Zoe Field.

These books won’t make you a data scientist, but they are a good foundation on understanding what the job is about.",https://medium.com/the-data-experience/the-beginning-of-an-adventure-46b4f14e5127,['Alan Marazzi'],2015-11-03 17:04:42.560000+00:00,484,"Data Science, Statistics, Machine Learning, RProject, Sentiment Analysis"
Time Series Analysis; Applying ARIMA Forecasting Model to the U.S. Unemployment Rate Using Python,"In this thread, I’m going to apply the ARIMA forecasting model to the U.S. unemployment rate as time-series data. Also, I’ll bring the proper codes which I run the model using Python (IDE Jupyter Notebook). At the end of this thread, I put two YouTube videos for training purposes.

Step 1- Data preparation

First, we need to import the necessary dependencies to the Jupyter Notebook. the first group of libraries is needed for the data manipulation and the second set of libraries are needed for the model development.

Import dependencies for data manipulation

Dependencies to develop the ARIMA forecasting model

Data for the three cases are pulled out from the Economic Research Services of the Federal Reserve Bank of St. Louis fred.stlouisfed.org website. Data included seasonally adjusted unemployment rate for the United States, State of Oregon, and the State of Nevada. I put the links to the data at the bottom of the thread; however, you can download the data in a CSV format from the GitHub user content link here too.

Importing data to Jupyter Notebook

Step 2- Data Visualization

The following line graph displays the trends of the unemployment rate for the three time-series data, which consist of the U.S., Oregan, and Nevada. Also, I mentioned some top annotation over the line graph that may provide more information for readers. It’s just a sample to produce the line graph using Python programming, matplotlit package, and ggplot style.

Line graph for three unemployment rates in the U.S., State of Oregon and Nevada

Stationarity

Time-series data should be stationary. A stationary series means that the properties [means, variance, and covariance] do not change over time. Note that seasonality and trends are not stationary because they demonstrate the value of the time series at different times [e.g., the temperature in winter is always low]. Hypothetically, time-series data should be stationary to run the ARIMA forecasting model. Autocorrelation and order differencing to visualize the stationary status. As displayed in the below graph, the first Autocorrelation of the state of Nevada dataset is nonstationary data. This means that the state of the Nevada dataset changes over time, and it plays a key role to determine the criteria. Mean, variance and covariance are changing over time, therefore, this issue which called as stationarity attribute should be solved before employing data. Toward this end, the first and second-order of differencing was applied to display the stationarity situation among the dataset. First-order differencing defined as 6 rows down and second-order differencing 12 rows down. Both orders are stationary, but the second-order is more stationary than the first order.

Autocorrelation and two orders of differencing to test the stationarity

ARIMA Parameters

AR(p) Auto Regression: This component refers to using the past values as own lags in the linear regression model to the prediction

This component refers to using the past values as own lags in the linear regression model to the prediction I(d) Integration: It uses differencing of observations through subtracting an observation from the previous step to make the time series stationary.

It uses differencing of observations through subtracting an observation from the previous step to make the time series stationary. MA(q) Moving Average: It’s a model that uses the dependency between an observation and a residual error from a moving average model applied to the autoregression.

Basic model was run order of (p,d,q) = (1,1,1) for the U.S. unemployment rate data. As noted that the U.S. unemployment dataset was stationary and the first order yields a certain result. AIC was obtained approximately 776 in the first-order model.

The following left graph displays the KDE residuals. Kernel Density Estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a random variable. This function uses Gaussian kernels and includes automatic bandwidth determination.

Residual Results for the KDE Test

Step 3- Running ARIMA model

Types of Forecasting

Univariate Forecasting: in the method, the forecasting model is applied to the single time-series data set. In this thread, the stationary time-series data is a univariate forecasting model. Multivariate Forecasting [exogenous variables]: this sort of forecasting model relied on the multivariate dataset. In other words, each time-series data has dependencies on the other time-series data set such as forecasting the hourly weather based on temperature, pressure, wind speed, and wind direction.

The U.S. unemployment rate data set divided to train and test the dataset. The train data defined as 80% and the rest defined as 20% for the test split. The result demonstrated forecasting from 2012 to 2020. It means that regardless of the seasonal variation, the unemployment rate in the U.S. could be moved in forecasting endpoints.

Line Graph for the Forecasting, Actual, and Training Unemployment Dataset

The next model created in accord with the order (p,d,q) = (2,0,1). This model obtained as the most fitted model because the U.S. unemployment rate was stationary and it does not need the differencing attribute. In this graph, three lines are visualized in this graph which included Train and Actual data, and the Forecasting model. Running the AutoARIMA model yielded to obtain the most optimized result with the lowest AIC rate. AIC obtained 775.9 in the optimized model.

ARIMA Model Statistics

A seasonal ARIMA model is applied in the next running model. In the SARIMA Final Forecasting Model, the line graph predicted the future trend of the U.S. unemployment rate. However, the unemployment rate has placed in an unusual situation because of the COVID-19 pandemic issue, it will take time for the trends to return to the previous status. In other words, some businesses could adapt to new conditions, particularly in worker-based businesses. Regardless of the seasonal variation, the unemployment rate in the U.S. moving slightly down based on the final ARIMA forecasting model.

SARIMA Seasonal Final Forecasting Model for the U.S. Unemployment Rate

You can use the following tutorial videos on YouTube to find the Python codes. Date preparation is illustrated in video part 1, and running the ARIMA model using Python is illustrated in video part 2.

Video part (1) includes the data preparation and data wrangling using Python (Jupyter Notebook)",https://david-hasan.medium.com/time-series-analysis-arima-forecasting-model-using-python-9a7874cefcd7,['David Hasan'],2020-11-10 06:59:38.259000+00:00,984,"Video part (2) includes the ARIMA forecasting model using Python (Jupyter Notebook)ARIMA, forecasting, unemployment rate, Python, time-series data"
Deepfakes Detection by Heart Rate Prediction,"Researchers at Binghampton University and Intel Corporation have developed a model that recognizes deepfakes by predicting heartbeats. The classifier uses photoplethysmogram data to recognize fake videos. An important assumption in the model is that it learns to recognize deepfakes that have been generated using a set of publicly available architectures. This imposes restrictions on the use of the model in real applications.

The approach detects fake videos with an accuracy of 97.27% and a generative model of deepfakes with an accuracy of 93.39%.

DeepFakes problem

The popularity of deepfakes has grown in recent years. Artificially generated videos of famous people are used for a variety of purposes, from filters on social media images to political propaganda and false news. This makes research on methods for recognizing deepfakes a topical area.

The idea behind the method

The researchers analyzed the remnants from the generative GAN model and tried to link them to biological signals. The proposed framework for the classification of deepfake videos is able to recognize a fake video and its source if it was generated by one of the available models.

The model starts with several generative networks that receive one real video as input. The real video and the generated deepfakes are then fed to the input of the registration module. At this stage, the model extracts parts of the face of interest, which track biological signals for photoplethysmograms.

The last module is a classifier that predicts the video class by a presentation. If the model predicted a deepfake, then it predicts the most likely architecture of the model that was used to generate.",https://medium.com/datadriveninvestor/deepfakes-detection-by-heart-rate-prediction-d96d8843a14b,['Mikhail Raevskiy'],2020-12-08 16:29:39.485000+00:00,259,"Deep Fakes, Photoplethysmograms, GenerativeGANModel, Intel Corporation, Binghampton University"
Top 4 commonly used Machine Learning Algorithms,"Let's explore the basic Machine Learning algorithms

If you are a machine learning engineer or data scientist, you can use these techniques to create functional Machine Learning projects easily. These four basic machine learning algorithms are the base of artificial intelligence and Machine Learning.

Here is a detailed video about the Top 10 Machine Learning Algorithms For Data Scientists

1. Gradient Descent

Gradient Descent is an optimization algorithm. Optimizers help to converge a machine learning model during training by reducing the loss. Loss is nothing but the difference between the actual value and predicted value.

A model converges means, there is very little possibility of a further significant decrease in loss, if we do more training iteration. In other words, a model converged, means there is very little possibility of a significant increase in performance, by doing more training iteration.

You can see the implementation in Gradient Descent from Scratch in Python.

2. Linear Regression

Linear regression is one of the most fundamental and basic, machine learning algorithms. It is used to show, the relationship between, two or more variables.

The column values, that are used to predict the output, are called features, or independent variables. And the column value, that is predicted, is called the dependent variable, or label.

You can see the implementation in Linear Regression from Scratch in Python.

3. Logistic Regression

Logistic regression is one of the most fundamental and basic, machine learning algorithms. Logistic regression is a machine learning algorithm, used for classification problems.

It uses, the log odds ratio, and maximum likelihood method, to fit the model.

The column values, that are used to predict the output, are called features, or independent variables. And the column value, that is predicted, is called the dependent variable, or label.

You can see the implementation in Logistic regression from Scratch in Python.

4. Decision Tree

The decision tree algorithm, is a machine learning technique, for making predictions. As its name suggests, it behaves like a tree structure. The decision tree is built by, repeatedly splitting, training data, into smaller and smaller samples.

It works on, the principle of conditions. We check conditions, at a node, and split the data, as per the result, of the conditional statement.

Decision Tree algorithm belongs to, the family of, supervised machine learning algorithms. It can be used to, build classification, as well as regression models.

You can see the implementation in the Decision tree from Scratch in Python.

End Notes:

In this article, we discussed the Top 4 commonly used Machine Learning Algorithms and I hope you found this article useful.

You may like to explore more Machine Learning algorithms in the below video.

Happy Learning !!",https://medium.com/@dhirajkumarblog/top-4-commonly-used-machine-learning-algorithms-d3af44852d95,['Dhiraj K'],2021-01-21 07:37:00.608000+00:00,422,"machine learning, gradient descent, linear regression, logistic regression, decision tree"
Cross-border arbitrage strategies,"This time we continue our series of articles with the topic of cross-border arbitrage and the related strategies, so heavily required for successful trading.

Decentralization led to the existence of a large amount of cryptocurrency exchanges. The same assets are being traded on these exchanges. The cryptocurrency prices, however, differ between them. The reasons for that are:

1. Time factor. For example, the users of one exchange can find out earlier about released positive news and start purchasing coins faster than the users of other exchanges. In this moment the difference in the price of a coin can reach considerable values.

2. Liquidity. At the exchanges with a relatively small trade volume the cryptocurrency price can be influenced even by one large deal. If one trader decides to buy a significant amount of coins, and the exchange can’t offer a matching amount of offers — the trader will collect the full hand and will be making deals on less profitable conditions — cheaper than the current level.

3. Exchange trade rules. Let’s say that on one of the exchanges the price of bitcoin is sometimes lower than the market one. It’s linked to the platform offering good conditions for cryptocurrency pay-out into fiduciary currencies and low commissions. Consequently, traders are eager to sell their cryptocurrency cheaper.

4. Technical maintenance or network errors can impact the work of the exchange. This, in turn, can influence the coin price change.

Let’s look into an example of a cross-border arbitrage strategy.

At exchange A and at exchange B the cryptocurrency pair BTC/USD is traded. In different periods of time the bitcoin price at the exchanges varies. At one point it can be higher at exchange A than at exchange B, in the other moment — the situation can change, and bitcoin can cost more at exchange B than at exchange A.

An open arbitrage position is market neutral, which means that it doesn’t depend on the direction of cryptocurrency price movement. Because of that arbitrage strategies are considered to be low risk.

From the received profit one should take out the charges, including the exchange commissions as well as the funding fee. The size of these charges depends on various reasons and can make up from 20% to 80% of the received profit.

When working with this strategy on nonliquid currencies or when trading relatively large volumes — it is important to consider the liquidity of the traded instrument. One needs to use the set position algorithm.

It is also crucial to consider the expenses for providing the credit “shoulders” (leverages) on various trading platforms. There is a risk of certain situations arising — when the cost of funding can lead to direct losses.

There are versions of cross-border arbitrage strategies, when the marginal crediting is not used. In this case the currency bought at exchange A is transferred to exchange B and sold. This strategy possesses a risk of transferring the funds, specifically with the transfer delay. Arbitrage strategies are used by plenty of players. This means that with significant delays, when transferring the currency from one exchange to another, the arbitrageur loses the potential profit since the cross-exchange spread quickly diminishes.

All in all, cross-border arbitrage strategies offer much flexibility and not at the cost of high risks. This makes them highly sought after at the market.

Follow our articles to learn more about algorithmic trading and the related fields in the world of cryptocurrency!

HyperQuant Social Media",https://medium.com/hyperquant/cross-border-arbitrage-strategies-d03195dba56d,[],2018-10-17 16:47:27.788000+00:00,562,"Cross Border Arbitrage, Cryptocurrency, Algorithmic Trading, Crypto Exchanges, Margin Funding"
Visualizing all of the Ways to Win Our March Madness Pool,"March Madness. The best time of the year. The collective American public goes nuts for 64 teams duking it out game-after-game for basketball glory. We enter tournament pools. We build brackets. We momentarily care deeply about teams and players that mere days prior we (mostly) couldn’t have even told you existed.

Like the rest of America, the CompassRed office also went nuts for March Madness. While I’m a basketball fan, I don’t pay much attention to the college basketball season as it’s happening¹. However, March Madness…this is my time to show that knowing a few random rules² and nothing else about picking teams can lead to office pool glory.

Now, I don’t know about you, but as each game in the tournament progresses — especially in the later rounds — I find myself trying to do the mental calculus of which games need to go which way so that I can win my pool. Here is a recent video of me doing this:

Typically, my thought process goes something like “even though I have Virginia in the Final Four, if they lose now, then that means that Darren, who has them winning, would lose more points overall than I would, meaning that I might be able to leap frog him in the standings”. Now do this for every possible game. My allegiance to teams waxes and wanes as quickly as I decide what is the best route for me to win.

Of course, there’s a better way to do this. We could actually calculate every possibility and figure out every outcome.

Me, f̶i̶g̶h̶t̶i̶n̶g̶ ̶T̶h̶a̶n̶o̶s̶…calculating possibilities.

It doesn’t really make sense to do this for the first round of games. Entering that first game, there are 2⁶³ possible outcomes³. That is a decidedly large number. However, that exponentially decreases to the point that by the time teams head into the Elite Eight, fighting to make it to the Final Four, there are only 128 possible outcomes remaining for the final seven games⁴ of the tournament. I’m no Dr. Strange, but I can handle this.

There are two data points that we need to start off with. First, heading in to the Elite Eight, what did the points⁵ of everyone in our office pool look like?

Darren — 72 Eugene — 68 Frank — 68 Jeff — 68 Tom — 68 Bob — 63 Patrick — 58 Ryan — 57 Aru — 57 Gabby — 54 Pat — 34

Second, what were the matchups heading into the Elite Eight?

East Region — Duke (1) vs. Michigan State (2)

— Duke (1) vs. Michigan State (2) West Region — Gonzaga (1) vs. Texas Tech (3)

— Gonzaga (1) vs. Texas Tech (3) South Region — UVA (1) vs. Purdue (3)

— UVA (1) vs. Purdue (3) Midwest Region — Kentucky (2) vs. Auburn (5)

Given that starting point, we can generate every possible outcome for the tournament. A great way to visualize those outcomes is with a Sankey diagram:

If you follow each strand, you can see every possible path to winning the NCAA tournament. We can also highlight all of the outcomes that lead to each team winning:

Now that we have all of those scenarios, we can figure out all of the different outcomes for our office pool. We do this by comparing each of the 128 scenarios to each pick that everyone in the office made for all of the remaining games. While typically people want to optimize the number of points that they get from their bracket, I’d rather maximize my rank. I want the glory of winning. Remember — if Virginia losing (spoiler: they don’t) means that Darren loses more possible points than me, even though I also picked Virginia, then I’m happy. Here’s what that looks like:

There’s some interesting takeaways here:

It looks like Darren is more than likely going to win. Even if everything goes wrong for him, then the worst that he can do is 5th place. Eugene will probably do well, but he can never win. Frank and Tom both have a shot at winning. Aru, who is in 9th place at the beginning of this round, actually has a chance to win. There’s a chance that I (Ryan) can end up doing pretty well, but I can never win. So much for the glory of winning the office pool. Pat is completely and utterly screwed.

It looks like Darren, Frank, Tom, and Aru all have a chance of winning. Of the 128 possible scenarios, how many of those scenarios does each of them win?

It appears that Darren will win in 56 of the 128 scenarios (43.75%). Tom will win in 32 of 128 scenarios (25%). Aru will win in 24 of 128 scenarios (18.75%). Frank will win in 16 of 128 scenarios (12.5%).

The next question is — which of those 128 scenarios leads Darren, Tom, Frank, or Aru to glory. For that, we can consult another Sankey diagram:

Now we can see all of the paths that might occur that would lead to each person winning our office pool. Let’s separately highlight the scenarios that would lead to each person winning:

Now, of course, we know what actually happened in 6 of those 7 games.

After all of those games, we’re down to only two scenarios left — either UVA or Texas Tech is winning the whole tournament tonight. For our office pool, that means that there’s only one possible winner left. Congratulations Darren! Go ‘Hoos!",https://medium.com/compassred-data-blog/visualizing-all-of-the-ways-to-win-our-march-madness-pool-d7b55936c905,['Ryan Harrington'],2019-04-08 15:37:18.866000+00:00,892,"March Madness, College Basketball, NCAATournament, Bracketology, Office Pools"
The #paperoftheweek 4 was: Deep Neural Networks with Box Convolutions,"Authors of the paper propose to bring box filters back to computer vision in a form of a box convolution layer. That layer computes sums over boxes of arbitrary size in a constant time thanks to pre-computed integral images. This layer is differentiable with respect to height, width and offset of a box, therefore all its parameters are learnable with a backpropagation algorithm. The advantage of such layer is that it can achieve arbitrary big receptive fields while maintaining computational efficiency and a small number of trainable parameters thus less likely to overfit.

In an experiment with ENet, a semantic segmentation model, authors show that replacing its blocks with box convolution-based blocks not only makes the model smaller and faster but, actually, more accurate.

Box convolutions could potentially improve any neural architecture that would benefit from larger receptive fields of its units. Initially, the authors implemented them in Torch 7 (Lua), PyTorch implementation of box convolution work layers in progress.

Abstract:

“B ox filters computed using integral images have been part of the computer vision toolset for a long time. Here, we show that a convolutional layer that computes box filter responses in a sliding manner can be used within deep architectures, whereas the dimensions and the offsets of the sliding boxes in such a layer can be learned as a part of an end-to-end loss minimization. Crucially, the training process can make the size of the boxes in such a layer arbitrarily large without incurring extra computational cost and without the need to increase the number of learnable parameters. Due to its ability to integrate information over large boxes, the new layer facilitates long-range propagation of information and leads to the efficient increase of the receptive fields of network units. By incorporating the new layer into existing architectures for semantic segmentation, we are able to achieve both the increase in segmentation accuracy as well as the decrease in the computational cost and the number of learnable parameters.”

For or more details and a good read, check out the paper: https://papers.nips.cc/paper/7859-deep-neural-networks-with-box-convolutions.pdf

The article was written by Evgeniy Mamchenko, Deep Learning Engineer at Brighter AI Technologies.

We are hiring!

Brighter AI has developed an innovative privacy solution for visual data: Deep Natural Anonymization. The solution replaces personally identifiable information such as faces and licenses plates with artificial objects, thereby enabling all AI and analytics use cases, e.g. self-driving cars and smart retail. In 2018, NVIDIA named the German company “Europe’s Hottest AI Startup”.",https://medium.com/generate-vision/the-paperoftheweek-4-was-deep-neural-networks-with-box-convolutions-8beab1be87f2,['Brighter Ai'],2019-09-23 12:19:04.998000+00:00,404,"computer vision, box convolution layer, integral images, deep architectures, semantic segmentation"
"Predicting Life Expectancies, Part 1: Using Statistics to gain insight","This blog is a walkthrough of my project, Predicting Life Expectancies. If you want to fork the repo or follow through the steps in more detail please check out https://github.com/roydipta/life_expectancy

What is the importance of trying to predict Life Expectancies?

The point of this project wasn’t just to predict how long a person would live, but it was more to see what aspects of their lives lead to either higher or lower life expectancies. If we are able to figure out which factors affect life expectancies; this can help guide policy makers to create better laws, it can help guide people to make better life decisions, etc…

First things first, WE NEED DATA! There were three sets of data that was pulled. Life Expectancy (2000–2015) from the World Health Organization (WHO), Obesity Among Adults by Country (1975–2016) from WHO, and Suicide Rates Overview (1985–2016) from UN Development Program, World Bank, and WHO.

We have our data so let’s get started! We need to import all of the packages that we’ll be using and pull in the data to three data frames (we’ll parse everything later):

import pandas as pd

pd.set_option('display.max_columns', 300)

pd.set_option('display.max_rows', 100)



import numpy as np

import matplotlib.pyplot as plt

%matplotlib inline

import seaborn as sns



import statsmodels.api as sm

from statsmodels.formula.api import ols

from sklearn import linear_model

from sklearn.linear_model import LinearRegression

from sklearn.model_selection import train_test_split

from sklearn import metrics

from sklearn.preprocessing import PolynomialFeatures

from sklearn.preprocessing import StandardScaler life = pd.read_csv('data\life_expectancy_data.csv')

suicide = pd.read_csv('data\suicide_rates.csv')

obesity = pd.read_csv('data\obesity_data.csv', index_col=0)

There’s a whole bunch of data cleaning steps as well as feature engineering steps that had to be done to make the data ‘pretty’ and usable. Please check out the /data_cleaning_engineering.ipynb file in the github for a full step by step data preparation.

Once the data was cleaned the dataframes were merged. The three tables were merged with the ‘country’, and ‘year’ column.

df1 = pd.merge(life, obesity, how='left', on=['country', 'year']) df = suicide.groupby(['country', 'year', 'gdp_per_capita ($)']).suicides_no.sum()

df = df.to_frame()

df = df.reset_index(level=['country',""year"", 'gdp_per_capita ($)']) df2 = pd.merge(df1, df, how='left', on=['country', 'year'])

We need a place to start, so let’s look at the actual distribution of life expectancies in each country. We’ll use the help of plotly on this:

import plotly

plotly.offline.init_notebook_mode()

import plotly.graph_objs as go



x = df_2015.groupby(""country"")[""life_expectancy""].mean().sort_values()

data = dict(type = 'choropleth',

locations = x.index,

locationmode = 'country names',

colorscale= 'Portland',

text= x.index,

z=x,

colorbar = {'title':'Life Expectancy (Age)', 'len':200,'lenmode':'pixels' })

layout = dict(geo = {'scope':'world'},title=""Life Expectancy around the world in 2015"")

col_map = go.Figure(data = [data],layout = layout)

col_map.show()

We can see somewhat of a pattern. At first glance, it looks like more developed countries seem to have higher life expectancies. If you focus on the countries in red (meaning highest life expenctancies) you can notice that these countries (Canada, Europe, Austrailia) have free healthcare.

This is a good start! So we mentioned that it looks like more developed countries seem to have higher life expectancies. Let’s test this hypothesis. We can perform a Two Sampled T-Test to test it. Our null hypothesis in this case is Life expectancy in developed countries is equal to the Life Expectancy in Developing Countries. Our alternative hypothesis is Life expectancy in developed countries is NOT equal to the Life Expectancy in Developing Countries

In:

developed = df[df['status']== 'Developed']

developing = df[df['status']== 'Developing']

developing.life_expectancy.dropna(inplace=True) stats.ttest_ind(developed['life_expectancy'], developing['life_expectancy'])

Out:

Ttest_indResult(statistic=29.76836505889292, pvalue=2.4650861700062064e-170)

So what does this mean? Our p-value is way under 0.05 which is our confidence interval, meaning we can reject our null hypothesis. If we look at the bar graph we can see that on average Developed countries have at least 15 years more life expectancies than developing countries.

Hmm, now I’m wondering why this is the case. In more developed countries there should better access to healthcare. Also, in more developed countries there has to be better access to food and just basic neccesities. We do have a obese table maybe we can see if more developed nations have higher amounts of obesity. Okay, so let’s set up our statistical tests. Our null hypothesis will be that Obesity in Developed countries is the same as Obesity in Developing countries. And our alternative hypothesis is Obesity in Developed countries is NOT the same as Obesity in Developing countries.

developing.obesity.dropna(inplace=True)

developed.obesity.dropna(inplace=True) stats.ttest_ind(developed['obesity'], developing['obesity'])

Out:

Ttest_indResult(statistic=8.718160546902753, pvalue=4.985541604428258e-18)

Again, our P-value is wayyy under 0.05 so we can reject our null hypothesis! This makes sense, the more food we have available the higher obesity it can lead to. I think we got some good statistics. Now we can move onto actually running a regression and seeing which other features are important in predicting life expectancies. Lookout for Part 2! :)",https://medium.com/analytics-vidhya/predicting-life-expectancies-part-1-using-statistics-to-gain-insight-654cb13a9866,['Dipta Roy'],2020-12-09 16:42:24.652000+00:00,717,"Life Expectancy, Obesity, Suicide Rates, Data Cleaning, Data Engineering"
Contemplating the Zen of Python,"Contemplating the Zen of Python

A brief analysis of the 19 guiding principles for Python design

The Zen of Python by Tim Peters

While reading an article last week about the PEP8 Style Guide for Python, I once again came across the Zen of Python. For those who haven’t seen it before, you can easily conjure it by casting this spell in your Python interpreter:

import this

Long-time Pythoneer Tim Peters penned down the BDFL’s [Benevolent Dictator For Life, a nickname of Python creator Guido van Rossum] guiding principles for Python’s design into 20 aphorisms. This poem has served as a mini style-guide for Python coders for over a decade.

Mysteriously, only 19 of the guidelines are written down. Guido van Rosum reportedly said that the missing 20th aphorism is “some bizarre Tim Peters in-joke.”

This article contains my interpretation of these aphorisms, including examples and applications.",https://medium.com/better-programming/contemplating-the-zen-of-python-186722b833e5,['Chaitanya Baweja'],2020-01-06 19:29:09.616000+00:00,140,"python, PEP8, Tim Peters, Guido Van Rossum, BDFL"
Evaluation Metrics Part 2,"Let us discuss in brief, the other metrics in this picture

Prevalence

Prevalence is the fraction of the total population, that is labeled positive.

Negative Predictive Value

Negative Predictive Value or NPV is the proportion of negatively labeled samples which are correctly predicted negative.

Positive and Negative Predictive Value can again be expressed in terms of prevalence, specificty and sensitivity as

False Discovery Rate

FDR

FDR is the proportion of positively predicted samples which are originally labeled negative. In other words, it is the proportion of false positives out of all the positively predicted samples.

False Omission Rate

FOR

FOR is the proportion of negatively predicted samples which are originally labeled positive. In other words, it is the proportion of false negatives out of all the negatively predicted samples.

False Positive Rate

FPR, Fall-out

FPR is the proportion of negatively labeled samples which are incorrectly predicted positive.

False Negative Rate

FNR

FNR is the proportion of positively labeled samples which are incorrectly predicted negative.

Positive Likelihood Ratio

LR+

LR+ is the ratio of the probability of a sample being predicted positive given that the sample is originally labeled positive to the probability of the sample being predicted positive given that the sample is originally labeled negative. In real life scenario, LR+ denotes the probability of a person who has a disease testing positive divided by the probability of a person who does not have the disease testing positive. Higher the value of LR+, the more likely a positive test result is a true positive. On the other hand, LR+< 1 indicates that a positive test result is likely to be a false positive.

Negative Likelihood Ratio

LR-

LR- is the ratio of the probability of a sample being predicted negative given that the sample is originally labeled positive to the probability of the sample being predicted negative given that the sample is originally labeled negative. In real life scenario, LR- denotes the probability of a person who has a disease testing negative divided by the probability of a person who does not have the disease testing negative.

Diagnostic Odds Ratio

DOR

DOR is the measure of the effectiveness of a diagnostic test (or model), and is defined as the ratio of the odds of the test (prediction) being positive if the sample (subject) is originally positively labeled relative to the odds of the test (prediction) being positive if the sample (subject) is originally negatively labeled.",https://medium.com/the-owl/evaluation-metrics-part-2-756e380cd7f3,['Siladittya Manna'],2020-06-26 02:49:32.214000+00:00,379,"Prevalence, Negative Predictive Value, False Discovery Rate, False Omission Rate, False Positive Rate"
pH Scale Recognition by RGB Color Using Flask.,"Regression Model

Machine Learning Algorithms which were used to predict the pH scale were Multiple Linear Regression, Rigid Regression, SVM, Decision Tree and Random Forest Regression.

The highest r2_score was shown by Random Forest Regression. r2_score is a constant model that always predicts the expected value of y, disregarding the input features, would get a R² score of 0.0. Parameters y_true-like of shape (n_samples,) or (n_samples, n_outputs) Ground truth (correct) target values.

Initially Feature Scaling was used on the independent variable in order to get more precise test set results. After Feature Scaling, the dataset was split into test and training set and then the classification was done to predict the test set results.

Multiple Linear Regression : 0.682162

Ridge Regression : 0.682794

SVM : 0.929199

Decision Tree : 0.946644

Random Forest : 0.953246

The best r2_score is 1 and as we can see, all of the above models have shown astonishing results with Random Forest having the highest r2_score.",https://medium.com/analytics-vidhya/ph-scale-recognition-by-rgb-color-using-flask-e4707b0b78f7,['Yash Garg'],2020-10-13 12:55:44.214000+00:00,151,"Regression Model, Multiple Linear Regression, Ridge Regression, SVM, Decision Tree"
Here are the mind-blowing things a deconvolutional neural network can do,"Convolution is a revolutionary innovation that took the machine learning world by storm. This mathematical process dramatically improves the accuracy of image recognition in neural networks.

Thanks to convolution, neural networks were finally able to beat humans in the famous ImageNet visual classification challenge.

Here is a typical convolutional neural network (CNN). This is the VGG16 algorithm that won ImageNet in 2014:

This neural network processes an image (on the left of the diagram) and classifies it into one out of a thousand possible categories (on the right).

When I show a picture of a chair to the network, the output node for the ‘chair’ category will light up on the right.

So far so good.

But one day, a machine learning researcher asked a brilliant question:

Can we run a CNN in reverse?

What if we activate the ‘chair’ category on the right and run the whole network backwards. Will it then produce a complete image of a chair?

The answer is ‘YES’, and it has profound implications for the field of machine learning.

So here’s how it works.

First, we need to build a suitable network architecture. Something like this:

Reading this architecture from right to left, it’s just another convolutional neural network that takes an image of a chair and classifies it.

But when run it in reverse, from left to right, it will convert a class vector to a complete chair image.

In essence, the network is producing new images of chairs from scratch. Here’s what that looks like:

These images were generated by feeding slightly different class vectors into the network.

But we can go further than that.

Le’s say we train the network on a bunch of chair images, and we label each image with two numbers: the type of chair, and the angle we are seeing it from.

A label of [1, 0] could mean: a garden chair seen from the front, and [2, 90] could mean: a living room sofa seen at a 90 degree angle.

Now what happens if we run this network backwards, pick a random chair type, and then slowly increase the angle value?

This is what happens:

This is mind-blowing.

Keep in mind that the neural network has never learned these specific images. Instead, it’s producing a chair image from scratch and rotating it correctly based on what it has learned about perspective from other chair images.

And this will work for any type of image, not just chairs. You can do it for cars, houses, landscapes, and even human faces.

Do you realize what this means? It means that…

Machine learning can produce images of any kind of object: chairs, cars, landscapes, interior design, etc.

Machine learning can take a photo of an object, and generate a new image where we see the object from a different angle.

Machine learning can generate faces of non-existent people

Machine learning can take an image of a person, and change their facial expression, their clothes, and even their body pose.

We’re just scratching the surface of what is possible with this technique.

What kind of images would you create with this trick?

Write a comment and let me know!",https://medium.com/machinelearningadvantage/here-are-the-mind-blowing-things-a-deconvolutional-neural-network-can-do-2fc99e008fe4,['Mark Farragher'],2019-03-14 17:35:58.986000+00:00,498,"convolution, machinelearning, neuralnetworks, CNN, Image Net"
PREDICTING NIGERIA 2019 ELECTION WITH AI — SENTIMENT ANALYSIS,"By streaming data from Twitter in real-time we were able to gather over 800,000 tweets which formed the sample for this analysis.

Data (Tweets) streamed were primary keywords pertaining to the Nigeria 2019 election such as: “PDP”, “Atiku”, “Atikulated”, “Peter Obi”, “APC”, “Buhari”, “Mbuhari”, “Tinubu”, “Osinbajo”, “Lagos”, “Kaduna”, “Kano”, “Abia”, “Delta”, “port harcourt” .

The sentiments of each Tweet was resolved and all values were stored on a database. See data description below.

Tweets ordered by sentiments.

Overall the percentage of Negative and Positive Tweets are 42% and 58% respectively as can be seen from the chart below.

General Pre-Election Sentiments.

For the Individual presidential candidates, Buhari had a higher number of overall mentions in tweets totaling about 218,519 tweets. Atiku on the other hand had a total mention of 136,149 mentions in tweets.

The distribution of tweets for each presidential candidate is as given below.

As can be seen in the distribution, Atiku has a higher positive rating in percentage when compared to tweets that mentions him. But this is not the conclusive milestone on our data.

Next we explore sentiment distribution as regards to the two main political parties in question, APC and PDP with a total tweet mention of 81,312 and 75,642 respectively.

In the next distribution we will be looking at comparing related data side by side to give us a holistic view of what is really happening.",https://medium.com/@kelly-idehen/predicting-nigeria-2019-election-with-ai-sentiment-analysis-d5daecf291ad,['Kelly Idehen'],2019-02-16 08:40:16.020000+00:00,222,"Nigeria2019Election, Tweets, Data Streaming, Real Time Analysis, Sentiment Analysis"
Optimizers be TensorFlow’s Appetizers,"Stochastic Gradient Descent is a method in which we start taking downward steps as soon as we finish computing on each one of the training set examples. This helps us make progress in our downward journey as soon as possible by using the gradients from each of our training set examples to update parameters. With this, we would have taken ‘m’ steps toward the bottom in one pass through the whole training set as opposed to taking just one step in Batch Gradient Descent.

This method is gives fast results but has 2 major disadvantages:

The first is that as each one of our training set examples are different, the gradient computed on each one of them points to a different direction and hence we don’t take direct steps towards the minimum cost but non uniformly , effectively move towards the point of minimum cost. And as a result of this, the parameters will never in fact reach the point of minimum cost but will just keep circling its vicinity.

The second disadvantage of Stochastic gradient descent is that we will loose the effect of vectorization on our code. Vectorization is at the heart of deep learning, it is the reason that our computer can perform such complex computations in such speed. in stochastic gradient descent, as the computation happens on each of the training set examples one by one, for-loops have to be used to pass through the training set and it will probably take a lifetime, compared to computation with vectorization.

this image shows stochastic gradient descent

Notice how the cost effectively takes steps towards the minimum point of cost but does not get there directly. It can also never actually reach the very point of minimum cost, it can only be circulating in its region.",https://towardsdatascience.com/optimizers-be-deeps-appetizers-511f3706aa67,['Aditya Ananthram'],2018-05-19 11:59:29.460000+00:00,293,"stochastic gradient descent, batch gradient descent, vectorization, deep learning, cost optimization"
Spatial partitioning based on homogenous features,"Working for Sonder Inc, developed a bespoke clustering algorithm to partition geographic space into homogenous neighborhoods based on arbitrary input features. Can’t really say much more than that… =)

Data-driven partitioning of San Francisco",https://medium.com/paololucchino/spatial-partitioning-based-on-homogenous-features-87e2204286b9,['Paolo Lucchino'],2018-11-30 14:46:19.582000+00:00,33,"Clustering Algorithm, Sonder Inc., Neighborhood Segmentation, Geographic Space Analysis"
2 ways to train a Linear Regression Model-Part 1,"One of the simplest Supervised Machine Learning models out there is Linear Regression. In this post, we will discuss two different ways we can train a Linear Regression model.

Let me list them out really quickly before I move into explaining each one in detail:

1. Train using closed-form equation

2. Train using Gradient Descent

The first way directly computes the model parameters that best fit the model to the training set and the second computes it iteratively. I will talk about closed-form equation in this post and will create a Part-II talking about Gradient Descent and variants of it.

But first, how does a linear model make predictions?

Linear regression consists of finding the best-fitting straight line through the data. This best-fitting line is what we call the regression line. A linear model makes prediction by computing the weighted sum of input features, plus a constant called the bias term¹

The equation for a Linear Regression model prediction can be given by:

How do we train this model?

You see the different θ’s in the equation? Those are called model parameters. To train a model, we need to find the value of θ (model parameters) that minimizes the cost function. There are 2 ways to do this! But first, let’s define our cost function.

The most common measure of performance of a regression model is RMSE. So, we need to find values of θ that minimize RMSE. But in practice, it is much easier to minimize MSE. Let’s say the cost function we are trying to minimize is Mean Squared Error. The MSE is calculated using the below equation

Now that we have our cost function defined, let’s go ahead and jump into the first way of finding the best model parameter values (θ).

“Closed-form” Equation

There is a mathematical equation that computes the model parameters directly. This equation is called the Normal Equation. You can read more about Normal Equations here. The Normal equation is given by:

But inserting all the values that we have into this equation; we will be able to compute the best model parameters that minimize our cost function. Easy right? Let’s test the equation on a linearly generated data.

X = 2*np.random.rand(100,1)

#add X0 = 1 to each instance(bias term)

X_b = np.c_[np.ones((100,1)), X]

y = 2+4*X +np.random.randn(100,1) fig = plt.figure()

plt.plot(X,y,""b."")

plt.axis([0,2,0,10])

plt.title(""Randomly generated Linear data"")

plt.show()

fig.savefig('result1.png')

# Normal equation

theta_cap = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) print(theta_cap)

The function we used to generate the data was 2+4x+noise. The normal equation has computed values of θo and θ1 that are close enough! Let’s write some code to do prediction

X_new = np.array([[0],[4]])

X_new_b = np.c_[np.ones((2,1)), X_new] y_predict = X_new_b.dot(theta_cap)

print(y_predict) plt.plot(X_new, y_predict,""r-"")

plt.plot(X,y,""b."")

plt.title(""Linear Regression model prediction"")

plt.axis([0,2,0,15])

plt.show()

While the normal equation computes the inverse of X^T X, the computational complexity is almost O(n^3), which if you haven’t figured out is bad! There is another approach that solves this in O(n^2) time. This approach computes the pseudoinverse of X using a matrix factorization technique called Singular Value Decomposition. In simple terms, SVD divides the matrix X into the matrix multiplication of 3 matrices. More about SVD can be found here. A simple implementation of this can be done using pinv() function from Numpy’s linear algebra model.

np.linalg.pinv(X_b).dot(y)

Why SVD is more efficient than Normal Equation?

a. Normal Equation works only if X^T X is invertible or singular. Pseudoinverse on the other hand is always defined.

b. As mentioned above, SVD has less computational complexity than Normal equation.

But both these approaches are very slow when the number of features and instances are large. Which is why the next way of training the model might be better for such cases. I will discuss that in my Part-II post.

Let’s end the post by doing a simple Scikit-Learn implementation of Linear Regression on the generated dataset and see if we get model parameters similar to the one computed by Normal Equation.

from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()

lin_reg.fit(X,y)

print(lin_reg.intercept_)

print(lin_reg.coef_)

And lo and behold, it does! Notice how sklearn gives us the same result as computing pinv(). That is because sklearn computes the pseudoinverse of X.

Conclusion

In this blog we learnt:

How linear models make predictions — the general equation for Linear Regression How to train a model and minimize the cost function — the equation of Mean Squared Error The 1st method of training a Linear Regression model using Closed Form equation — the Normal Equation A simple implementation of the Normal Equation Computing pseudoinverse using Singular Value Decomposition Scikit-learn implementation of Linear Regression

If you enjoyed this blog please leave a clap or comment. It lets me know that my blog was helpful :) Happy learning, folks!

References

1. Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. Book by Aurelien Geron.",https://jeffyjacob.medium.com/2-ways-to-train-a-linear-regression-model-part-1-e643dbef3df1,['Jeffy Merin Jacob'],2020-10-27 13:24:40.457000+00:00,754,"machine learning, linear regression, supervised learning, closed-form equation, gradient descent"
#Howto use in-tooltip-charts with 23degrees.io EN/GER,"## für Deutsche Version nach unten scrollen ##

In our last blog entry we explained to you the basics of how to use tooltips in charts and maps. Today we’ll go a step further and explore how to insert one or more visualizations into the tooltip of a chart or map with 23degrees.

How to insert a visualization into a tooltip

To insert a visualization into a tooltip first go to the tooltip editor of your chart or map and select insert statistic.

Now choose from one of the three following options.

1. My existing statistics & 3. Insert a Specific slug

When selecting my existing statistics pick your visualization for the tooltip from the folder where it’s stored. Then click on the chart or map you would like to insert.

Alternatively, you can Insert a Specific slug of an existing chart or map into an input field. Slug = the part of the url that comes after “https://app.23degrees.io/view/”

In the next step, you can set parameters effecting the way the visualization will be displayed in the tooltip. These settings include width and height (as % of total tooltip) and the possibility to merge your in-tooltip-chart with the corresponding visualization according to a specific data field (column).

We call this a Join.

In the example below we merged the two visualization by the columns containing the country names (“Land”). Note that you can also merge by two or more columns

Once the Join is active click on ADD STATISTIC to insert your chart into the tooltip.

And there it is! Your tooltip now contains a line chart and thanks to the Join function the values displayed in the line chart correspond to the country selected on the map.

2. Data field

Another way to create an in-tooltip-chart is to insert a slug (the part of the url after https://app.23degrees.io/view/) of an existing visualization in the data table of your chart/map.

Here, the first step is to copy the slug of the visualization that will later appear in your tooltip.

Next, paste the slug into a data field (column) of the main visualization.

Create your visualization (save -> continue to statistic) and go to the tooltip editor. Click insert statistic and select Insert statistic from Data Field. Now chose the data field, that contains your slug from the dropdown menu.

The next steps are the same are the same as before. Mind that the Join function does not exist with this type of in-tooltip visualization.

As you can see, the diagram in the tooltip is only displayed, when the element containing a slug in the corresponding row of the data table is selected. In this case, the tooltip only shows a graphic for the “e-paper readers” because the other rows in the data table do not contain any slugs.",https://medium.com/@23degrees/howto-use-in-tooltip-charts-with-23degrees-io-en-ger-805a373e79bc,[],2020-09-11 13:07:08.724000+00:00,449,"23degrees, Tooltips, Visualizations, Chart Insertion, Map Insertion"
Data Literacy —The ABCs in the Age of Data (Part I),"Word cloud of feedback for the Data Literacy ePrimer, an eLearning course we designed

Getting someone to navigate and understand the complexities of what Data Science and AI really is, can be difficult in the face of the hype surrounding it.

We started out with a simple ambition to help public officers understand more about Data Science & AI (“DSAI”). By the end of our journey, we created an eLearning course with about 4 hours of videos, diagrams, and explanations, teaching a broad range of topics from DSAI use cases, importance of data, visual analytics principles, Machine Learning as well as a framework on how to scope a data project. When we received positive reviews and feedback for the Data Literacy ePrimer, we knew that the effort over the past 1+ year was well worth it.

Here’s a look back at the development journey of how we built up the content, and hopefully inspire our public officers reading this to pick up the ePrimer. We believe it is important knowledge that will serve you well going into the age of data.

The Problem

The Data Science & AI Division at GovTech has been collaborating with public officers across various agencies on projects for a few years. We have a thriving community of enthusiasts and practitioners which we engage with regularly as well. Over the years of project collaboration and community engagement, we slowly developed a sense of what the common misconceptions about data are, and what baseline knowledge, we think, would really help raise the overall understanding about data for the public service as a whole.

We call that pool of knowledge data literacy. Unfortunately, this body of knowledge is not something you learn about comprehensively in most data-related courses out there.

Based on our framework on data training, both Data Literacy and acquiring applied analytics skills are equally important.

You can probably attend half-day seminars about how cool Data Science & AI is, but you may not gain a deeper appreciation of why good data is of paramount importance.

You can attend 3-day courses on advanced analytics features in Excel to work with real data, but you won’t learn why some charts and dashboards just help people understand the presented insights better.

You can also attend 5-day courses on visual analytics principles and how to use tools to visualise data, but you will not be able to peel open the black box of how machine learning helps you predict outcomes.

How many of these courses can a person realistically attend? Going for one or two of such courses are often inadequate to understand the full picture behind what makes Data Science & AI useful.

And it’s no wonder why management can’t reconcile why their “trained” staff did not come back with the right skills to perform magic with data.",https://medium.com/dsaid-govtech/data-literacy-the-abcs-in-the-age-of-data-part-i-7007a8b17506,['Joseph Tan'],2020-10-06 02:43:05.032000+00:00,458,"Data Literacy, Data Science, AI, e Learning, Visual Analytics"
Large amount of observations: Statistical Test not so Statistical,"There are several ways if you want to assess normality of a distribution: visually and/or statistically.

For a visual assessment, you draw the Q-Q plot of quantiles vs theoretical quantiles. If your data are exactly gaussian (normal/gaussian distribution), the points should align on a single line. In reality for real data sets, this is rarely the case and it is up to you how lenient you allow non-normality when points are deviating from the diagonal line.

For instance, this is a normal distribution:

This is clearly not a normal distribution and requires prior transformation (ignore axis names):

As for statistical tests, you have many normality tests. The most 4 known normality tests, in order of strength from empirical testings:

Shapiro-Wilk

Anderson-Darling (the one provided on the link you provided)

Lilliefors

Jarque-Bera

All existing normality tests are “failing” (not providing a reliable answer) when the amount of samples is large enough. For instance on Allstate, all these tests are unreliable. Example of Shapiro-Wilk test:

When the amount of samples increase (like from 1000), the null hypotheis has higher odds of being rejected despite providing same-distributed data, due to potential deviation — this effect is even stronger when playing with 5+ -digit observations

When the amount of samples decrease (like 100), the null hypothesis has lower odds of being rejected due to less sensitivity to outliers but forces the researcher to look more carefully at visuals — and to also use several tests (because one could reject while another one not reject the null hypothesis)

This is true on all the normality tests. Any small deviation from normality when the sample size is large enough leads to rejection of the null hypothesis. Even if your Q-Q plot shows a near-perfect alignment: kurtosis become heavily sensible from a “large amount of observations” point of view for normality tests. A very slight difference can impact the test result as your sample size increases.

This is also true for most omnibus tests (any test trying to spot a variance difference). Most normality tests belong to omnibus tests. Hence why normality tests should be avoided on Allstate data for instance.

Extra: in contrary to the omnibus tests being sensible to kurtosis, the tests relying on means are sensible to skewness.

Reminding the testing hypotheses if needed:

Null hypothesis: the data is following a normal distribution

Alternative hypothesis: the data is not following a normal distribution

p-value < alpha (significance threshold, typically alpha=0.05): the null hypothesis is rejected (one must reject the fact that “the data is following a normal distribution”, but it does not prove “the data is not following a normal distribution”)

If you sample a lot of observations and do the same tests, the results you might get can be entirely different. Ideally:

Create many samples of different sizes from the label data (and if possible with a normal distribution) — or just create synthetic normal sets

Compute the p-value from a normality test

Compute the ratio of null hypothesis rejection

Draw the curve of sample size vs ratio of null hypothesis rejection

And you should end with an increasing ratio curve, even if the sample is from the same distribution.",https://medium.com/data-design/large-amount-of-observations-statistical-test-not-so-statistical-3d8ed0e94be,[],2016-11-13 20:12:44.938000+00:00,503,"normality, normality tests, Shapiro-Wilk, Anderson-Darling, Lilliefors"
Our Tech Predictions for 2017,"Every December, we take a look back at big ideas from the past twelve months that promise to gain momentum in the new year. With more than eleven thousand projects launched between our Design and Tech categories in 2016, we have a nice sample to draw from. More importantly, we have a community of forward-thinking backers who help creators figure out which versions of the future to pursue. Here are some of the emerging trends we expect to see more of in 2017.

Ossic

Next-level listening

With the headphone jack inching closer to the endangered-features list this year thanks to Apple, a number of projects have made a compelling case for pushing beyond the limits of traditional stereo sound. Ossic brings VR’s spatial immersion to the audio realm, while Nura’s advanced headphones customize sound for listeners’ unique hearing profiles. And Basslet’s wearable subwoofer reminds us that sound is a full-body experience, bringing the visceral bass of a dance club to personal music listening. We’re excited to see more creative takes on listening devices, especially as audio-based user interfaces and voice control become increasingly common product features. Which brings us to…

Vi

Everyday artificial intelligence

Whether chatting with a device as if it’s a virtual assistant strikes you as a sci-fi dream come true or a dystopian nightmare, we’re going to see an increasing number of products that use voice-controlled artificial intelligence interfaces to fit into users’ lives more seamlessly. Among the projects leading the way in this arena are Vi, wireless earphones that double as a personal trainer; Bonjour, an alarm clock that wakes you up with a personalized daily briefing; and Dashbot, a talking car accessory that recalls Kit, David Hasselhoff’s buddy from Knight Rider.

One of the factors driving this talking AI boom is the emergence of platforms like Microsoft’s Cognitive Service, Amazon’s Alexa, and Google’s Speech API, which allow product developers to focus on user experience rather than low-level speech processing. For the DIY set, Seeed’s ReSpeaker offers a turnkey devkit for working with these services, and we’ll surely see more tools for integrating AI voice interfaces into all manner of products.

Blubel

Collective intelligence and community-sourced data

But artificial intelligence can only do so much. Actual human brains are still pretty useful, especially when lots of them work together. Blubel, Oombrella, and Wynd are connected devices that collect info from individual users about safe routes for bikers, hyper-local weather conditions, and air quality, respectively, pooling these distributed data points to make the whole system smarter. We expect to see designers and users continue to embrace this kind of transparent, purposeful approach to gathering data, especially as the increasingly porous boundaries between our online and physical lives stoke concerns about personal privacy and data harvesting.

Kano’s Speaker Kit

Hackable devices for kids

Whether you believe that learning to code is a necessary skill for 21st century kids or just want to encourage your gadget-obsessed youngsters to spend less time passively staring at screens, the increasing number of hackable products that offer children engaging, creative intros to technology are an encouraging trend. Kano, Technology Will Save Us, and Makeblock all introduced DIY kits that balance beginner-friendly guided learning with remarkably open-ended possibilities for discovery and invention. Meanwhile, Cubetto and Root’s charming robots invite children as young as three to explore basic programming concepts through playful physical interfaces.

Beyond education, hackability has the benefit of keeping a toy fresh and interesting as kids grow. We expect to see more kids’ products incorporate customizable elements that invite youngsters to reinvent and remix their playthings.

Wazer

A factory on your desktop

At this year’s World Maker Faire in New York, we presented the Future Fab Lab, a collection of digital fabrication machines (beyond 3D printers) that translate manufacturing techniques typically found on factory floors to makers’ desktops. Among them were Wazer’s waterjet cutter, capable of precisely shaping metal and stone; Mr Beam’s user-friendly laser cutter; and Mayku’s FormBox, a compact vacuum former for shaping plastic. While these machines all focus on producing hard goods, we expect to see this shrinking-industrial-equipment concept applied to realms of manufacturing such as textiles, fashion, and food.

Space research and history

While it may not house alien megastructures as once hypothesized, KIC 8462852 remains the most mysterious star in the galaxy. Astronomer Tabetha Boyajian’s campaign invited space enthusiasts to follow her journey to understand the strange light patterns coming from this celebrated celestial body. 2017 promises to be an equally stellar year for this democratized approach to space exploration. Lightsail, the Planetary Society’s sun-propelled satellite, is slated to launch, bringing to fruition an idea that Carl Sagan first popularized in 1976. Concurrently, Ozma Records is creating a deluxe reissue of the Voyager Golden Record, another 40-year-old Sagan-led project, which collected sounds from around the Earth as an interstellar message for extraterrestrials. With the support of more than 10,000 backers, it became our most-funded Music project ever. We suspect it’s a harbinger of even more intriguing projects that tap into the public’s fascination with space exploration’s storied past and promising future.

Slow Dance

Tech art for the home

One of the most unpredictable success stories of 2016 on Kickstarter was Sisyphus, Bruce Shapiro’s meditative kinetic sculpture that slowly draws intricate patterns on a sand-covered table with a metal ball. The project gained a huge following and went on to become our most-funded Art project ever. Jeff Lieberman’s Slow Dance frame also uses engineering for poetic purposes, creating the illusion that real objects are moving in slow motion before your eyes. Back in 2014, Electric Objects explored this territory with digital displays devoted to showcasing an ever-growing collection of artworks — kind of like Netflix for visual art. We’d love to see more technology-driven art that’s designed to live in people’s homes rather than on the walls of galleries and museums.",https://medium.com/kickstarter/our-tech-predictions-for-2017-5810bb931d63,[],2016-12-09 17:49:01.743000+00:00,951,"Ossic, Next-level listening, Everyday artificial intelligence, Collective intelligence, Community-sourced data"
A Momentous Bitfury Birthday,"This week, Bitfury enters our tenth year of operations. From our humble beginnings to the global European technology unicorn we are today, it has been an exciting (and at times unbelievable) journey. I’m both excited for the year ahead of us as well as incredibly proud of the work we’ve done. To celebrate, I wanted to share some of my favorite moments from the last nine years.

2011

I co-founded Bitfury after reading Satoshi Nakamoto’s groundbreaking paper on bitcoin and blockchain. It felt as if we had stumbled upon the launch of the next internet before anyone else. We began working immediately to design hardware that could make bitcoin even more secure.

2012

We invested in a production run of our first bitcoin chip (FPGA). We had no outside funding — it was just our dedication, along with the support of close family and friends, that made this happen. Our production run was a success, and we began selling our first product.

2013

After seeing how bitcoin network was evolving, we designed and launched our first bitcoin ASIC mining chip (55nm). What I remember most from this year was the excitement. Suddenly, more and more people were paying attention to bitcoin and wanting to get in on the ground floor.

2014

This was a great year for many reasons. Bitcoin was growing in popularity, and after we attracted our first external funding through our Series A round, we began to expand our team around the world.

2015

We realized that for bitcoin to truly take off, every part of the hardware stack had to be reinvented, from the smallest chip to the largest data center. We acquired Allied Control and their groundbreaking two-phase immersion cooling technology to help build better, more sustainable data centers. Today, this division is a leader in liquid cooling and high-performance computing.

2016

We built our cutting-edge bitcoin transaction processing center in Tbilisi, the Republic of Georgia, outfitted with our cutting-edge immersion cooling and our latest bitcoin mining chip. This data center set a new standard for bitcoin transaction processing around the world.

2017

We continued our industry-leading bitcoin work by partnering with Hut8 Mining, a publicly listed Canadian company, to provide their data centers with our cutting-edge bitcoin mining hardware. As we attracted more technologists into our team, we realized that blockchain technology was going to be just as game-changing as bitcoin. After years of research, in 2017 we launched our Exonum™ blockchain platform for blockchain applications and began to implement it in the land titling registry in the Republic of Georgia. We were no longer just a bitcoin mining firm — we were now a blockchain services company.

2018

We expanded our blockchain offering this year. Our Exonum platform was growing in popularity, and we successfully launched our Crystal™ digital currency compliance and analytics service. Our bitcoin division also launched its newest generation of hardware, the Bitfury Clarke™ chip and Bitfury Tardis server.

2019

This was our busiest year yet. We continued to build out our blockchain services with the launch of our music entertainment division, Bitfury Surround, which will use blockchain to improve the inefficiencies and inequalities in the music industry through the Surround™ platform. Our Exonum blockchain team expanded into enterprise blockchain-as-a-service with Exonum Enterprise, and our Crystal crypto compliance team expanded to offer analytics for bitcoin, bitcoin cash, ethereum, ERC-20/21 tokens, and Litecoin (and now Tether!) The second half of 2019 also saw us launch our artificial intelligence division, formalizing our expansion into an emerging technologies company.",https://medium.com/meetbitfury/a-momentous-bitfury-birthday-51f1343fb4b3,['The Bitfury Group'],2020-02-18 08:59:21.142000+00:00,568,"Bitfury, bitcoin, blockchain, AI, Tether"
Bakhtinian Dialogic as a Framework for Human/AI Co-Learning,"Companionship is not dictated by ultimatums in a vertically stratified relationship. Companionship is a socially forged identity between individuals based on implicit mutually beneficial factors that adapt and mature based on the needs of the relationship. A successful human relationship with an AI Companion has the same requirements as social companionship between humans. The co-creation of this human/AI relationship needs to be nurtured and explored in order to create a companionship that is essentially a third entity where all participants (human and AI) continue to adapt and meet each other’s needs. The definition of “self” in the companion relationship is challenged/redefined in the cooperative engagement process. The results extend beyond the ever-growing personalized companionship into the individuals’ social and interaction spheres.

Bakhtin emphasized the tendency of humans to create individual meaning from interactions with text, media, and each other. The nature of all human interaction is a dialogue and each individual creates a personal interpretation that manifest as overt expressions, conscious engagement, reflex reactions, or subconscious analysis.

“The nature of human life itself, in dialogue a person participates wholly and throughout his whole life: with his eyes, lips, hands, soul, spirit, with his whole body.” Bakhtin, M. M. (1984) Problems of Dostoevsky’s Poetics. Edited and translated by Caryl Emerson. Minneapolis: University of Minnesota Press

Individuals create a social vocabulary that is unique based on their personal dialogue with the world around them. Bakhtin analyzed academic structures and the act of reading as an example of how regardless of the delivery of information, the recipient is in dialogue with the incoming concepts and reframing them based on personal experience and social vocabulary. In a conversation between two people, these perspectives coagulate meaning and expressions of perspective in external and internal dialogues creating a new shared identity based on their social interaction. Dialogue occurs in conversation, in reading a text (or interacting with an AI Companion), and especially in how individuals (consciously or not) adapt the new pathways created in the social dialogic identity into their own internal vocabulary.

This is the core of learning.

This is the basis of how humans create relationships.

Dialogic interaction is exactly how humans will successfully build on-going, adaptive, and mutually beneficial relationships with AI Companions.

Co-Learning with an AI is a transformative experience for all participants. It creates new experiences and learning paths that allow new vocabularies and understandings across all involved, contextual participants and environments. These new literacies and expressions contribute to the growth of the personalized cooperative experience and the greater social lexicon of interaction.",https://medium.com/maslo/bakhtinian-dialogic-as-a-framework-for-human-ai-co-learning-f93403915e2a,['Beau Johnson'],2020-11-06 16:20:35.988000+00:00,415,"Companionship, Human/AI Relationship, Dialogue, Bakhtin, Co-Learning"
Unchecked AI Can Mirror Human Behavior,"The 5 Most Common Types of Bias

If we approach the topic from a statistical point of view, there are five ways in which bias can creep into the results.

Confirmation bias

Confirmation bias is the inclination to look for, decipher, favor, and review data that affirms or bolsters one’s earlier individual convictions or values. Therefore, confirmation bias is a powerful type of cognitive bias with a critical impact on society’s correct workings by misshaping evidence-based decision-making.

An example of this is when you remember information selectively or make a biased interpretation of information given to you. Studies showed that we could even be manipulated to remember fake childhood memories. This indicates that people sometimes don’t even notice when they analyze data in a biased way (another psychological phenomenon that fits this category is wishful thinking).

Selection bias

Selection bias is the bias introduced by selecting individuals, groups, or data for analysis that does not achieve proper randomization, thereby ensuring that the sample obtained is not representative of the population to be analyzed. The term “selection bias” usually refers to a statistical analysis’s bias resulting from the sampling method. Therefore, it is essential to consider selection bias. Some conclusions of the study may be wrong.

Outliers

An outlier is an extreme data value. For example, a 110-year-old customer or a consumer with $10 million in their savings account. You can identify outliers by carefully inspecting the data, especially when distributing the values. Since outliners are extreme data values, it can be dangerous to decide based on the calculated “average.” In other words, extreme behavior can have a significant impact on what is considered average. It is imperative to base your conclusions on the median (the average value) to have an accurate result.

Overfitting and underfitting

Underfitting implies that a model gives an oversimplistic picture of reality. Overfitting is the inverse (i.e. an overcomplicated picture). Overfitting risks causing a particular assumption to be treated as the truth, whereas it is not the case in practice.

How can this bias be counteracted? The most straightforward approach is to ask how the model was validated. If you receive a somewhat glazed expression as a reaction, there is a good chance that the analysis outcomes are so-called unvalidated outcomes and, therefore, might not apply to the whole database. Always ask the data analyst whether they have done a training or test sample. If the answer is no, it is highly likely that the analysis outcomes will not be applicable to all customers.

Confounding variables

Basically, this happens when additional factors influence variables you have not accounted for. In an experiment, the independent variable usually affects your dependent variable. For example, if you want to investigate whether the need to exercise leads to weight loss, the need to work out is your independent variable and the weight loss is your dependent variable.

Disturbing factors are all other factors that also influence your dependent variable. They are additional factors that have a hidden influence on your dependent variable. Aggravating factors can cause two main problems: increased variance and the introduction of bias.

It is essential to confirm that the conclusion drawn from research and analysis results is not affected by distortions. Uncovering biased results is not the sole responsibility of the analyst concerned. It is the joint responsibility of all those directly involved (including the market participant and the analyst) to reach a valid conclusion based on the correct data.",https://medium.com/better-programming/unchecked-ai-can-mirror-human-behavior-2ce1ce76f914,['The Unlikely Techie'],2020-10-16 17:24:22.477000+00:00,559,"Confirmation bias, Selection bias, Outliers, Overfitting, Underfitting"
R Video Tutorial #1: Working with dates and times with plot.ly,"In this video I show you how you can make nice interactive charts with the plot.ly package in R. I go over some issues I ran into regarding how plot.ly interacts with different date formats. Beneath the video you can find the code to reproduce it yourself and the necessary data here.

If this is useful for you please consider supporting me or join my mailing list if you want to stay up to date with videos like this or new tools to help you with your data work.

The video:

The code:

library(textclean)

library(plotly)

library(lubridate)

library(data.table)

library(datetime) setwd(“YOURWORKINGDIRECTORY”) #read in the data with fread from data.table

data <- data.table::fread(“sunrise_vienna_2019_zamg.csv”, encoding = “UTF-8”) #rename the columns

colnames(data) <- c(“Day”, “Month”, “Year”, “sunrise”, “sunset”, “moonrise”, “moonset”) #use mgsub to rename the german months with numbers

data$Month <- textclean::mgsub(data$Month, unique(data$Month), seq(1:12)) #make a new column with the whole date, this uses data.table

data[, date := lubridate::dmy(paste(data$Day, data$Month, data$Year, sep=”-”))] #transform the time from character to a date-time format

data$sunrise <- as.POSIXct(data$sunrise, format = “%H:%M”)

data$sunset <- as.POSIXct(data$sunset, format = “%H:%M”) # #lubridate doesn’t work with the y-axis of plotly

# data$sunrise <- lubridate::hm(data$sunrise)

# data$sunset <- lubridate::hm(data$sunset) #calculate the length of days and round them so they have no digits

data$day_length <- round(data$sunset — data$sunrise, 0) #start preparing the plot

daylength_plot <- plot_ly(data = data, x = ~date, y = ~sunrise, mode = “lines”, type = “scatter”,

#we name the first line (sunrise) and edit the info for the mousover and change

#the color and “thickness” of the line

name = “Sunrise”, line=list(width=5, color = “#ffca7c”),

#text that is shown on the mouseover

hoverinfo = “text”, text = ~paste0(“Date: “, date ,

“

”, “Sunrise: “, format(sunrise, “%H:%M”) ,

“

”, “Sunset: “, format(sunset, “%H:%M”),

“

”, “Length of Day: “, day_length)

) %>%

#we add the second line (sunset), name it and set color and “thickness”

add_trace(y = ~sunset, mode = ‘lines’, name = “Sunset”,

line=list(width=5, color = “#d4916e”)

) %>%

#we add the length of the days as vertical lines

add_segments(x = ~date, xend = ~date, y = ~sunrise, yend = ~sunset, name = “Length of Day”,

line=list(width=0.5, color = “#ffca7c”)

) %>%

#we set some global options for the whole plot

layout(title = “Length of Days in Vienna 2019”,

xaxis = list(title = “Date”),

yaxis = list(title = “Time of Day”,

#we revers the y-axis so the mornings are on top

autorange = “reversed”,

#we set the y-axis to date so it correctly maps the variable

type = “date”,

#we also set the ticks so they correctly map the hours and minutes

tickformat=”%H:%M”)

) #for publishing the plot on plot.ly we set our username and api key

# Sys.setenv(“plotly_username”=”YOURUSERNAME”)

# Sys.setenv(“plotly_api_key”=”YOURAPIKEY”)

#

# api_create(daylength_plot, filename = “daylength_vienna_2019”, sharing = “public”,

# fileopt = “overwrite”)

The chart:

— — — —

Bernd Schmidl runs a one man market research company: berndschmidl.com

He studied Sociology. He thinks about philosophy, baseball and other stuff. Sometimes stuff spills out",https://medium.com/@bernd_43537/r-video-tutorial-1-working-with-dates-and-times-with-plot-ly-93ce328d85b9,['Bernd Schmidl'],2020-03-05 14:46:45.372000+00:00,438,"RR, plotly, data Visualization, date Formats, market Research"
Measuring urban footprint to understand city behavior,"First steps: What is Urban Footprint and how to measure it

The urban footprint measures the expansion of a city in the territory. Cities are great sources of cultural and economic wealth as well as an optimal environment for people to develop their lives, but they also have a great impact on the environment due to the pollution that they cause, the consumption of natural and energy resources, as well as the construction of infrastructures (transport, communication, services) that they need for their correct operation.The analysis of the urban footprint allows us to quantify the degree of impact and soil consumption, as well as helping us to know the spatial distribution of people and services in a determined metropolitan area.

For a correct Urban Footprint evaluation, it is not only important to be able to quantify its dimension, but also how the density is distributed in its interior. It is possible to identify different types of cities according to their pressure on the territory, for example the models with a large urban footprint and low density are less efficient and they increase the territory impact compared to other cities where the urban density is bigger and the extension smaller.

For this reason, in the current context of constant urban growth,the study of the Urban Footprint helps us, assuming that cities are indeterminate and unpredictable organisms, to improve the urban knowledge to guide how to design future resilience strategies.

Spanish Urban Footprint Map

Spanish territory covers a 87,013,900 hectares, of which 2,846,400 hectares are urban area, representing just 3% of the total.

The tool that we have developed in uDA allows us to calculate the extension of the Urban Footprint at the present time from the Spanish Cadastre open data and to qualify its density from the data of the National Statistics Institute (INE) and National Register. It is constructed from the urban cadastral blocks of the whole country, quantifying the growth of cities in a synchronized way with the Cadastre.

The consideration or delimitation of urban area has been taken directly from the one determined in the Cadastre Law*, which includes not only land consolidated with building and services, but also incorporates all soils for urban development (future areas where cities will grow) and areas occupied by public areas and services.

* Real Decreto Legislativo 1/2004, Ley del Catastro Inmobiliario

The main goal will be to quantify the size of the expansion of Spain’s principal metropolitan areas. For that reason,we decided to do the analysis at Functional Urban Area (FUA) scale. The choice to select this scale compared to the municipality scale is due to the fact that municipality dimension scale is sometimes variable in the Spanish territory and in many cases there is no correlation between the importance of the urban area and its municipality extension (a clear example is Barcelona, with a very small municipal size for its great importance).On the other hand, the delimitation of the FUA covers not only the extension of the main city, but also its area of influence. According to the definition of the Urban Audit:

The Functional Urban Area (FUA) consists of the city and its commuting zone. Once all cities have been defined, a commuting zone can be identified based on commuting patterns using the following steps: 1: If 15 % of employed persons living in one city work in another city, these cities are treated as connected cities. In this case, the first city is a part of the Functional Urban Area of the second city and does not have its own FUA. (…) 2: All municipalities with at least 15 % of their employed residents working in a city are identified. 3: Municipalities surrounded by a single functional area are included and non-contiguous municipalities are dropped. In Spain, there are 80 FUAs, representing a total of 1262 municipalities.",https://medium.com/urbandataanalytics/measuring-urban-footprint-to-understand-city-behavior-42d45384442c,['Alejandro Cantera'],2020-06-09 16:32:02.687000+00:00,629,"Urban Footprint, Spanish Territory, Urban Cadastral Blocks, Functional Urban Areas, Cadastre Law"
Is education ready to work in data-intensive environments?,"What do initiatives such as personalized and adaptive learning, chatbots for education, automatic translators or the use of predictive learning analytics have in common? All of them are components of a ‘data-driven education’.

In many countries, there is a clear interest in expanding the role of digital technologies in education, which inevitably is leading towards more data-intensive educational systems. With the growing interest for adaptive intelligent tutoring systems offering natural language interaction, tools for predicting school dropout or new automated systems to boost student recruitment, it is likely that the importance of data-intensive technologies for education will increase in the years to come.

Although such digital innovations can bring new benefits, it is also important to understand that they could transform the current landscape of education in unexpected directions. The loss of, unauthorized access to, or disclosure of, personal information has gained media attention recently, but lack of transparency, automated bias, or the use of data to influence user behavior, are also very important challenges that need to be weighted when exploring these trends.

The changing landscape of education will require not only that students and teachers become more data literate, but also that education organizations and administrators will have to develop a (more) proactive and comprehensive strategy when planning for, implementing and increasingly interacting with data-intensive education systems.

With (advanced) intelligent systems (for instance, those capable of identifying patterns or recognizing voices, faces, images, texts or even keystrokes), there is going to be a greater need for education in algorithm literacy. This will mean not only having to expand some of the current definitions of digital literacy — including those related to the use of artificial intelligence (AI) — but also to develop new institutional capacities, supporting educators and administrators to adopt these tools in safe, ethical, and transparent ways.

The growing relevance of data-intensive systems opens new challenges (and questions) that are expected to play a critical role during the coming decade. Here are some of the questions that will be important to systematically consider and answer within (and outside) educational institutions as countries adopt tools that enable more data-driven educational practices:

Privacy and data protection: Who has my data? Is the data secure? What data is held where, and who has access to it? Who is tracking me? What are my rights? How to protect my privacy? Where to get related help?

Ethical use of data: What are the risks of relying on automated systems? How to embrace technological solutions for education without ignoring ethical implications? In what processes and circumstances are the data-intensive systems (AI) appropriate to be used?

Data Accountability: What assessment has been made of the ethical use of data? Has the data been captured with the knowledge and consent of all the parties involved? If personal data previously collected is intended to be used for a new purpose, what should be done? What are the quality control mechanisms that need to be in place and implemented to use the best possible data?

Algorithmic literacy: What positive and negative impacts could the use of AI in education have on people? How to critically assess outcomes from the use of AI systems? To what extent should current frameworks of digital literacy address a deeper understanding of the ethical and social implications of big data?

Agency and responsibility: How to prepare students and educators to protect themselves from unintended uses of technology? Can end-users be more actively involved in the design or application of data-intensive tools for education?

Bias awareness: How to minimize the impact of bias on certain users or groups? What datasets is/was the algorithm trained on and what are their limitations and potential biases?

Transparency: How are student data collected, analyzed and used? How to overcome the ‘black box problem’, when an algorithm’s complexity is inscrutable even to its developers? What are the best practices to keep a transparent data policy? How to keep the data clear, consistent, and understandable?

Explainability: What does it mean to open AI’s ‘black box’? How to make related terms and conditions more user-friendly? (Here is an interesting example of simplified terms and conditions of different social media.)

There is little doubt that there is a growing need for amplifying and diversifying existing conceptions of what it means to be ‘literate’ in a digital age. As new frameworks are elaborated to enable higher levels of transparency and accountability, people and institutions will need to understand these challenges and educate themselves on both opportunities as well as the societal impacts of these innovations.

According to a recent report from UNESCO in this topic, there are at least six major challenges:

Developing a comprehensive view of public policy on AI for sustainable development;

Ensuring inclusion and equity for AI in education;

Preparing teachers for an AI-powered education;

Developing quality and inclusive data systems;

Enhancing research on AI in education; and

Dealing with ethics and transparency in data collection, use, and dissemination.

The growing volume of data being collected within an education system could offer richer, more sophisticated overviews of how students are learning, and provide useful insights on how to better support them with the use of technology. However, many fundamental questions remain related to the potential long term consequences of tracking and profiling today’s students.

The availability of good data can help lead to making good decisions. This is true in education, as it is in other sectors. But the opposite can also happen if the right actions are not taken. If we are entering into the ‘datafication of education’, countries will need to define rules and guidelines to ensure that present and future technology-enhanced education becomes beneficial, reducing and mitigating risks along the way. Although it is much too early to predict the potential impact of the use of AI in education, it is not early to discuss how to better prepare for the world that is coming.

Here is a selection of relevant initiatives and sources that can be of help for those who would like to learn more about this topic:

You may be interested in the following related posts on the EduTech blog:

Read more World Bank blogs and stories.",https://medium.com/world-of-opportunity/is-education-ready-to-work-in-data-intensive-environments-2c7913428f42,['World Bank'],2019-09-03 14:26:39.998000+00:00,999,"data-driven education, personalized learning, adaptive learning, chatbots for education, automatic translators"
A Flaw in the System,"Me and my family gathered together on Christmas day to watch as the Cleveland Browns attempted to win a crucial game in the enemy territory of Green Bay, Wisconsin. They were nearly successful, with a very close final score of 24–22 in favor of the home team. Unfortunately for the Browns and the Cleveland faithful, Baker Mayfield made some mistakes on Saturday afternoon. Four of them, actually.

Mistakes were commonplace in this game, just as any in the’s world’s craziest sports league. Baker threw four interceptions during the game, the Packers dropped several more, and Kevin Stefanski abandoned the run game on a final drive with several timeouts. But mistakes are what separate the bad teams from the good, and the good from the great. Apparently, they’re also what separates good bettors from great bettors.

For those who aren’t familiar, OBIWAN is the name I developed for my predictive NFL model, and I use it to bet NFL games. The acronym stands for Offensive Based Inference (and) Win Adjusted Net-pay.

Recently, I’ve noticed it predicting the over on total lines far too often, and this led to a nearly disastrous Week 15. Among other bets, it took the over for 83% of games, which is problematic, given a higher amount of under results occurring this season. This caused a 7–10 record on totals, and a loss of 14% investment on the week.

Not great.

Given that I use over-under bets as the basis for my portfolio each week, this is certainly troubling. I can no longer say with a reasonable confidence level that OBIWAN's total predictions will be profitable in the long run. So what was the issue?

I thought it would be an issue with the way the algorithm awards high-scoring drives to the offense following a turnover. The idea was the team on the good end of the turnover should have better field position, and as such should score more often. However, quick testing showed this wasn’t the case, and that this adjustment didn’t result in much more scoring for teams.

I then looked at the small nominal defensive adjustment applied to teams during each simulation and quickly determined the problem. By simply forgetting a minus sign, I was adding points in a few lines of code when I should have been subtracting.

For those who don’t code, this may sound pretty silly, and you would be completely correct. Just like leaving a single two-meter weakness that could destroy a moon-sized battle station, the Rogue One retcon notwithstanding.

I quickly corrected the issue for Week 16, which unsurprisingly went far better than Week 15.

Returns continue to be strong. 4 out of 6 positive weeks, and a total return of investment of $627.81 on $1,000 or 62.8%. Positive bets are stabilized by totals (other than that sizeable hiccup) and spreads, but the true driver of returns has been moneyline bets.

Moneylines, spreads, and totals have a total return of investment of 30.66%, 3.67%, and -1.98%, respectively. I hope that the total ROI will increase, and I am confident that it will for two reasons: it continues to hit above the break-even point of 52%, and the algorithm has just been fixed.

A look at distributions.

Totals are more densley clustered around the positive peak of 90.91% versus the negative -100%. This is promising, and should only improve. The current issue is weighting certain total bets more than the others, and a quick glimpse at a list of bets shows that OBIWAN favors heavy over bets, a result that makes a lot more sense following the revelation of the bug.

Spreads are essentially neutral, which follows with the 3.67% total ROI. I’m largely okay with this. As long as it remains positive, it remains a good bet. Although it would be great to have better returns, spreads are just tricky.

Moneyline has been the moneymaker. Although the negative peak is much larger than the positive, the wide spread towards high returns is what has caused the 30.66% ROI. OBIWAN almost exclusively bets underdogs, and yet has a winning percentage of 51.22%, a fact I’m quite proud of. Conventional betting wisdom tells you to bet underdogs, and the statistical analysis certainly supports this.

Let’s take a look at Week 17.

It’s funny that I just spent an entire blog post about fixing the large amount of over bets, but the lines are low this week. Vegas has spent the last two weeks losing money on unders, and as such have readjusted them back to lower lines.

As a Packers fan, I hate to see a Vikings moneyline bet in the portfolio, but hey, you always have to trust the math. See you next week.",https://medium.com/@samhenrichsdata/a-flaw-in-the-system-e02c7c1728cf,['Sam Henrichs'],2021-12-29 03:23:45.072000+00:00,766,"Cleveland Browns, Green Bay Wisconsin, Baker Mayfield, NFLBetting, OBIWAN"
Data Engineer Jobs on the Rise — Skills Needed and Who’s Hiring,"If you review the schedule of events at data science conferences, such as ODSC West, you’ll notice that data engineering is a primary focus. That’s no surprise, as data engineer is currently one of the fastest-growing roles. Job site, DICE’s, recent 2020 Tech Job Report listed it as the fastest-growing job with 50% growth in 2019. While the “rise of the data engineer,” is a popular phrase, the demand for these engineers has long outstripped the supply. If DICE’s data is correct, the demand for them will continue to do so.

Fig 1. source dice.com

What is a data engineer?

Data engineers are responsible for the overall data infrastructure that supports business products and services. Companies are gathering ever-increasing amounts of data that drive everything from business intelligence dashboards to platform workflows. They design the infrastructure that stores, moves, and integrates data from many different sources. They create environments for data scientists to analyze data (lakes and warehouses), enable ETL at scale, and optimize the ecosystem to ensure continuous insights.

Experts estimate that it takes two to three data engineer jobs per data science job to help maintain that pipeline, driving the high demand for these engineers.

What are the Core Competency Skills?

To better understand who employers are seeking for data engineering jobs, we examined our own jobs portal as well as other sources. In total, we reviewed over 1,350 job postings for Q2 and Q3, 2020 for this role.

Fig 2 source: odsc.com

Definitions of a data engineer role vary, but generally, it is someone who:

Understands data platforms, infrastructure frameworks, and data pipeline workflows.

Knows how to source, transform, and analyze data at scale.

Employs programming skills and advanced techniques like machine learning to create the “glue” code that binds the data workflow life-cycle.

Fig 2. Is a representative set of skills sourced from our job portal for this role. Unsurprisingly, data skills, such as database, SQL, NoSQL, and ETL (extract, transform, load) feature prominently. From the postings, it is evident that this role’s domain is in the cloud and that experience on AWS or Azure is a top requirement.

Additionally, the jobs posted indicated that the ability to engineer data at scale with a mixture of pipelines and utilize other infrastructure tools is a defining feature of this role. Knowledge of areas, such as distributed systems, APIs, big data, and data lakes, as well as of platforms, such as Hadoop, Apache Spark Kafka, and Airflow is also important.

Programming skills also feature prominently in job postings with python leading the pack. Java made the cut also, which is unsurprising since it has been the main infrastructure language for more than a decade. Scala, the language of Apache Spark, is also in high demand. Languages that many data sciences favor, such as R, did not feature prominently. However, machine learning, an expansive topic in itself, was one of the top requirements for a data engineering position.

According to this report, there is an overlap in skills between a data engineer and a machine learning engineer. This is not surprising as these data engineers need machine learning and analysis skills to build environments that facilitate large scale AI-driven projects.

Who’s Hiring Now?

Fig 3. Who’s Hiring Now

To determine who’s hiring data engineers in the USA, we looked at a sample set from various job sites in August and September. Big tech including Amazon, Facebook, and Apple are actively recruiting. The finance industry is weathering the pandemic better than many and that’s reflected in strong hiring by Capital One, JPMorgan Chase, and USAA among others. Healthcare companies like United Healthcare and CVS Health are also hiring teams of data engineers. Interestingly, consulting companies Capgemini and Accenture are also hiring as many industries seek guidance on navigating the crisis.",https://medium.com/@ODSC/data-engineer-jobs-on-the-rise-skills-needed-and-whos-hiring-44ca547834fa,['Odsc - Open Data Science'],2020-11-11 14:03:15.302000+00:00,612,"data engineering, data infrastructure, machine learning, cloud computing, programming skills"
New Esri Land Cover map shows us how to move fast and share knowledge.,"Feedback is welcome.

As with most first releases of new digital technology, there are some criticisms of the Esri Land Cover 2020 map. In a Twitter thread, MD Madhusudan highlighted tea estates in India that are variously labelled as scrub, grass, trees and crops, and an extensive solar farm that’s been classified as “crops” rather than “built area”. Other commentators observed a loss of spatial detail but also acknowledged that given the global scale of this open science product, the final product is still very impressive.

Curious to know how this feedback might be used, I reached out to Steve Brumby and Sam Hyde, co-founders of Impact Observatory. “We are very pleased with the reception and engagement from the user community, which was very encouraging”, said Steve. “A number of regional experts noted interesting places where the map did not agree with other approaches. We greatly appreciate this feedback and are tracking constructive suggestions closely in order to help us identify areas where we could improve our map.”

Sam went on to explain that some of the disagreements may be due to differences in interpretations of category labels. For example, in some cases the ‘built area’ label has been interpreted to mean ‘urban.’ However, as Sam explains, “built area can be urban, but it can also be infrastructure in more rural areas such as a large greenhouse in cropland, or a small rural settlement.”

The process of labelling begins with human annotators using an online platform to draw boundaries around features such as forests, lakes, settlements and farms on photos. The result is a dense collection of labels that enable deep learning algorithms to explore and learn both spatial and spectral characteristics of features and start labelling them.

“How a feature is labelled can be important for practical reasons,” said Steve. “A built area or farming classification may impact local taxation rules. The global community needs to be able to conduct fair comparisons across countries to understand human pressures on the environment, and track how countries are making progress towards their commitments to the big UN conventions on climate, biodiversity, and sustainable development. Our partnerships with Esri and the UN are helping Impact Observatory understand and navigate some of these complexities of global-scale mapping.”

Move fast and make change happen.

Releases like this one from Esri and Impact Observatory represent a new model of science, one of perpetual iteration and improvement. We don’t have time to wait for perfect products to be published. We must publish fast, share openly, and welcome scrutiny. Willingness to learn and improve is a critical characteristic of initiatives that lead to real change.

But, there’s still more to be done.

What other data do we need to make resource management more sustainable and conservation decisions more effective? A more detailed land use map is an imperative addition to planning “toolboxes”. Land cover maps tell us information about physical land types, but they can’t always tell us how that land is being used. What activities are happening in those places? Which farming methods are being used? What are people doing to modify or maintain habitats?

Artificial intelligence, machine learning, and ecological modelling offer opportunities to improve our understanding of what’s happening right now and what the impact of our decisions will be. However, the precision and scale that’s needed to produce global maps that accurately reflect local circumstances will require much more investment to develop the training data sets and models. The better we can predict the impact of planting here, restoring there, or halting development elsewhere, the better our chances are of stopping a catastrophic ecosystem collapse.

Climate change is already altering our environment, and the speed at which these changes happen is increasing. Scientists have warned that decisions made in the next ten years will be the most critical we ever make. There’s no time to wait for the perfect dataset. We have to act today on the information we have and update our strategies as the data and technology improves. The faster we process data, the faster we can get it into the hands of people with influence. And who are the people with influence? That’s all of us.",https://medium.com/vizzuality-blog/new-esri-land-cover-map-shows-us-how-to-move-fast-and-share-knowledge-8d9110de80ea,['Camellia Williams'],2021-08-26 10:46:48.256000+00:00,685,"Esri Land Cover2020, Feedback, AI, Machine Learning, Ecological Modelling"
Recognizing Handwritten Digits with Scikit-learn under Data Analytics using Python,"Data analytics is the science of analyzing raw data in order to make about that information. Analytics is the systematic computational analysis of data or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data. It also focus on applying data patterns towards effective decision making. It can be valuable in areas rich with recorded information; analytics relies on the simultaneous application of statistics, computer programming and operational research to quantify performance .Data analysis is not limited to numbers and strings, because images and sounds can also be analyzed and classified.

Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language. Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python. It features various classification ,regression and clustering algorithms including support vector machine ,random forest, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Here we are going to analyze the digits data-set of the Sci-Kit learn library. We are going to train a Support Vector Machine(SVM) and then we will be predicting the values of a Unknown Handwritten digits.

Here we jupyter notebook for the the performing operations. So lets start first with importing required libraries.

There are total 1797 images are there in the dataset.

Our whole data-set is stored in digits. Following is an example of a digit in our dataset. It consists of 64 pixels (8X8).The data set contains images of hand-written digits: 10 classes where each class refers to a digit from 0 to 9.Each image stored as 8x8 matrices as following (for digit 0):

Matrix value for Digit

This dataset consists of 1,797 images that are 8x8 pixels in size. Each image is a handwritten digit in grayscale, as shown in Figure

Pixel image in dataset

Let us train our SVM with the first 1790 images in out data-set. After that we will use the remaining Data-set as our test data and check the accuracy of our training machine.

It consists of 6 images of 64 pixels each(8X8) of six different numbers.The output of the above test data will be produced as below:

Lets fit our model using SVM classifier. Here we use 1st 1790 images for training the model and remaining are for validation.

Prediction by the model

As we see,both predicted and target values are same for this data.Lets check the model peidiction for some datasets.

As we can see we have achieved 100% accuracy. Let us now define a function that will find the accuracy of our SVM and train our model with varying data-set. We will start with 3 elements in our training data and work our way up to 1790 data and store the accuracy of our models in a dictionary.

Accuracy function

The values dictionary holds all the accuracies values for the given model.

Let us plot our dictionary result to visualize the accuracy result.

Accuracy of the model

Conclusion:As we can clearly see for well above 95% of our models the achieved accuracy is 100% . Hence we can easily conclude that our model works accurate for more than 95% of the time.Hence by using Scikit-Learn library in python,data analysis becomes easy ,effective and take less time.

“I am thankful to mentors at https://internship.suvenconsultants.com for providing awesome problem statements and giving many of us a Coding Internship Exprience. Thank you www.suvenconsultants.com""",https://medium.com/@vaishudesai29/recognizing-handwritten-digits-with-scikit-learn-under-data-analytics-using-python-8433261322cb,['Vaishnavi Desai'],2020-10-08 13:06:14.610000+00:00,557,"Data Analytics, Statistics, Computer Programming, Operational Research, Scikit-Learn"
Early Election Results vs. the Final Forecast and a Quick Look Ahead,"With the early results in, it is clear that the election is closer than the expected results from our forecast. It appears that polls broke in favor of Trump at the end, confirming signs we saw in some of the recent polls leading up to election day in key states.

What can these early results tell us about what is likely to happen?

We can use the simulations from our Presidential election forecast and condition on simulations where we observed results that we have so far. This allows us to focus on scenarios that have played out like reality so far while still allowing for random variation in line with the historical errors we have seen in past forecasts.

This allows us to make an updated probabilistic prediction on the election and guess at what might happen in the states that are yet to be called.

As of just past 2 am on the east coast, this is the current electoral map:

Florida is a state the forecasting model missed as it expected a slight lead for Biden based on fundamentals and polling averages of just under 1%, but Trump won the state and leads by +3% with 96% of the votes in.

As of now Biden needs 45 electoral votes to win the election while Trump needs 58. Assuming Trump wins Alaska as expected, we can look at what happens if he also wins Georgia and North Carolina, which is plausible based on current results.

That would give Trump 247 electoral votes. In scenarios where this happens, Trump is forecasted to win the election 74% of the time. If Trump loses Georgia though, that election win probability drops to 68%.

However, if we account for the fact that Biden currently leads in Arizona and condition on simulations where he wins the state, then Biden is actually likely to win the election 94% of the time.

All this is contingent on the midwest playing out as the polls suggested it would, meaning that in particular Wisconsin, Michigan, and Pennsylvania follow expected patterns and Biden is able to win them.

One concerning result today for Biden is that the margin in Ohio has shifted more towards Trump than the forecasting model expected. Trump seemed favored in Ohio by +1% but ended up winning the state by more. He currently holds a +8% margin in the state with 90% of votes in.

This suggests that Trump could be running ahead of his polling margins in the midwest, meaning that he might see his actual vote shares increase above expected polling averages anywhere from +4% to +8%.

Countering this though is the result in Minnesota where Biden is currently leading by +7% with 80% of the vote in, which is close to his polling average of +8%. Minnesota’s vote is historically less correlated with other midwestern states, but it is more demographically similar in some ways to those states compared to Ohio. The fact that the actual result is close in Minnesota suggests polling there, and potentially in Michigan and Wisconsin, was fairly accurate.

Biden was expected to win Pennsylvania by +5% and Michigan and Wisconsin by +8% each. We can see how Trump running ahead of his polling by large enough numbers in the midwest would shift these states towards his camp and therefore the election.

If that shift though is not large enough (whether due to late deciding voters, turnout, or polling errors), then Biden could still win Michigan and Wisconsin while losing Pennsylvania. That would make the election a nail biter as Trump would have 267 electoral votes, leaving the race to Arizona, Maine, and Nevada. However, given Biden is favored in these states, he would be slightly favored to win in this scenario.

The Trump camp has to feel a lot better about their chances than they did going into election day while the Biden camp is certainly feeling much more nervous.

It is going to come down to Pennsylvania, Wisconsin, and Michigan in the midwest. Overall, it looks like Trump is favored to win currently with anywhere from a 60% to 70% chance based on current results and voting patterns, but the odds could change very quickly pending how other key states, in particular Arizona and Georgia, turn out and how the remaining votes start to come in.

We will need to see whether the results in Pennsylvania, Wisconsin, and Michigan match the trend in Ohio or Minnesota more closely, especially since Pennsylvania mail in ballots have not really been counted yet.",https://medium.com/vinod-b/early-election-results-vs-the-final-forecast-and-a-quick-look-ahead-efdcb7bf8d8f,['Vinod Bakthavachalam'],2020-11-04 07:56:14.531000+00:00,737,"Election2020, Presidential Election, Trump, Biden, Midwest"
Bokeh 0.12.15 Released,"We are pleased to announce the release of Bokeh 0.12.15!

This release has the following highlights:

Addressed several WebGL bugs #6867 , #7087 , #7211 , #7681

, , , Switched to Chrome headless for tests, which will support WebGL testing #6594

Updated data source selections to be proper Bokeh models #6845

Fixed memory leaks with certain kinds of Bokeh server usage #7468

Added new glyphs for hexagon markers and hex tiles, and a new hexbin method #7638 , #4786

method , Completed BokehJS port to TypeScript #6481

Plus several other bug fixes and documentation improvements. For full details, see the CHANGELOG and Release Notes. Additionally, examples for each release are uploaded to CDN. Click here to download.

If you are using Anaconda, Bokeh can most easily be installed by executing the command conda install -c bokeh bokeh . Otherwise, use pip install bokeh .

NumFOCUS Fiscal Sponsorship

Perhaps the biggest news of this release cycle did not involve any code at all: Bokeh joins NumFOCUS Sponsored Projects. Over the years Bokeh has enjoyed generous financial support from the DARPA XData Initiative and from Anaconda, Inc. However, it has always been a goal to have Bokeh join NumFOCUS in order to place it within an even wider Open-Source Community.

Non-profit NumFOCUS helps shoulder the burden of financial and other obligations around administering Open Source, leaving project committees to decide how best to utilize available funds. The Bokeh committee is composed of:

Sarah Bird

Ryan Hafen

Andy Terrel

Jake Vanderplas

Bryan Van de Ven

Now that it is easier than ever, if you or your company find value in Bokeh, please consider making a donation to directly support its development.

Hex Tiling and Binning

The issue #4786 : Add a hex tiling glyph has been open for two years, but it wasn't until recently that all the necessary things (i.e. aspect ratio control) were in place. This release sees the addition of a new glyph. Hexagonal tilings are useful for several things including binned aggregations such as these plots of one million NCAA shots:

These were created by binning manually and using Hextile directly, however it's often even simpler. A new method for basic hexagonal binning has been added to the Figure class:

x = 2 + 2*np.random.standard_normal(n)

y = 2 + 2*np.random.standard_normal(n) p.hexbin(x, y, size=0.5, hover_color=""pink"", hover_alpha=0.8)

For more sophisticated use-cases, such as scaling individual tiles by another quantity, you can use HexTile directly, or look for new features based on this coming soon to HoloViews.

Selection Architecture Update

One of the most useful features of Bokeh is the ability to access and inspect the selections users make with various selection tools. For several years, this information has been available in a complicated and hard to manage plain dictionary structure:

# OLD WAYS

source.selected['0d']['indices'].flag source.selected['1d']['indices']

This has needed improvement for a long time. Happily, Claire Tang decided to tackle this issue, and now there is a proper Bokeh model Selection with meaningful properties.

# NEW WAYS

source.selected.indices source.selected.line_indices

It’s much clearer (and simpler to document) what property corresponds to a particular kind of selection. Look for selected.image_indices for image hit testing support to be added in the next release.

A quick note: Although existing code for reading selection continues to work (and will for the foreseeable future), it was not possible to maintain compatibility for writing selections. If you need to programmatically set a selection in a CustomJS or Bokeh server callback, you must use new Selection models directly.

BokehJS Ported to TypeScript

BokehJS was originally written in CoffeeScript. At the time, this decision helped Python developers without much JavaScript knowledge get Bokeh up and running quickly. However, as the project has matured, it’s become clear that other tools will help more with long-term maintenance.

Thanks to a tremendous effort by Mateusz Paprocki, I can happily report that BokehJS is now written entirely in TypeScript. The porting effort has already helped uncover some previously unknown bugs, and made core developers more effective and efficient. It’s my hope that this switch will also make BokehJS development more accessible to new JavaScript developers.

JupyterLab Support for push_notebook

A few releases ago we were pleased to announce Bokeh compatibility with the new JupyterLab project. At the time, we had the resources to get basic plots and apps functioning, but not the push_notebook feature. I am happy to report that Luke Canavan pushed new work on the Bokeh JupyterLab extension, and it is now on par with classic notebook support, including push_notebook :

Upcoming Work

As mentioned in previously, we are reaching a point of relative stability with respect to Bokeh development. The major tasks for the next few Bokeh releases are these:

Allow Bokeh app components to be individually included in arbitrary HTML templates

Finish the work to clearly delineate and standardize all modules and APIs

In addition to this, we hope to accomplish as much bug fixing, polishing, and documentation as possible in the coming few months.

As we get closer to a Bokeh 1.0 release, I’d like to thank the 287 total contributors who have helped make Bokeh such an amazing project. If you’re interested in helping out, drop by the Dev Chat Channel!

Thanks,

Bryan Van de Ven",https://medium.com/bokeh/bokeh-0-12-15-released-d1e1981bc976,[],2020-07-05 16:06:03.266000+00:00,822,"bokeh, data visualization, data science, python, Jupyter Lab"
Sentiment Analysis of a Youtube Video,"Selenium

Step 1: Install Necessary Libraries

For selenium we need to install it first in Google Colab. Below are the codes for installation.

It would take some time to install based on your internet speed.

Step 2: Import Necessary Libraries

After installing selenium we need to import all the libraries for scrapping the websites.

Step 3: Setup ChromeDriver and Scrap the Comments

We have taken an authors list to store all the comments made by youtube users.

Then, we configure the ChromeDriver for scrapping.

If you go on comment section of youtube you will find that the youtube comment section dynamically loads when you scroll down. For gathering at least 100 comments I have iterated for loop 6 times. Also, I have taken timer to stop for 5 sec to load the page. If you have slow internet connection make sure to increase the time so that page loads successfully and you don’t ran into exceptions.

Now comes the best part. We start scrapping all the comments and their author details.

We got uncleaned data and we have to clean it as it contains some of the extra details like when did the user commented and how many reply it got at the end.

In first comment, “

11 hours ago

” after username (Aspen Gaming) and “

26

REPLY” at the end of the comment.

We want our data to be like this,

where we have a user and his comment.

For removal of those extra details we need to clean it and then put it into data frame.

Step 4: Clean and Put data into DataFrame

In line 7, we are removing time of the comments after username like

11 hours ago

.

In line 8, we are removing

REPLY at the end of the comment.

In line 9, we are removing

followed by number of replies to that comment.

When a user edit their comments, youtube shows the username followed by (edited). This we have to remove from our username. This is what line 10- 12 is doing!

Lastly, we put author name as row name and their comment in column inside a Data frame.

Our final data after scrapping the website through selenium looks like:

Final DataFrame after scrapping

What are the problems we found here?

We had to do some cleansing before storing it into Data Frame.

These might not work if youtube changes its webpage design.

This is why I switched to Google API.",https://medium.com/analytics-vidhya/sentiment-analysis-of-a-youtube-video-63ced6b7b1c4,['Amit Ranjan'],2020-12-13 16:18:57.134000+00:00,375,"Selenium, Web Scraping, Youtube Comments, Google Colab, Data Cleaning"
Insurance cost prediction using linear regression,"I would like to say that I’m thankful for the course Deep Learning with PyTorch: Zero to GANs by Aakash and the folks in Jovian.ml (this post is a supplement to the course’s seacond assignment). So far, I’ve felt that the course was solid and full of content. Despite being only a high schooler, I’m looking forward to learning a lot in this course! If you’d like to learn more about the course, follow https://bit.ly/pytorchcourse

In this assignment, we’re going to use information like a person’s age, sex, BMI, no. of children, and smoking habit to predict the price of yearly medical bills. This kind of model is useful for insurance companies to determine the yearly insurance premium for a person. The dataset for this problem is taken from https://www.kaggle.com/mirichoi0218/insurance

We will create a model with the following steps:

Download and explore the dataset Prepare the dataset for training Create a linear regression model Train the model to fit the data Make predictions using the trained model

First, we need to import libraries that we need to complete the project",https://medium.com/nerd-for-tech/insurance-cost-prediction-using-linear-regression-155edd73cf94,['Merugu Akshay Kumar'],2020-11-08 06:27:45.160000+00:00,176,"Deep Learning, Py Torch, Jovian.ml, Linear Regression, Medical Insurance"
Rasoee — A Picture is Worth a Thousand Ingredients,"Rasoee — A Picture is Worth a Thousand Ingredients

An attempt by a group of food-loving students to make the diverse world of different cuisines more navigable for our fellow eating enthusiasts. Arijit Gupta Follow Dec 16, 2020 · 4 min read

Rasoee won First Runners-Up at the 2020 Global PyTorch Summer Hackathon in the mobile/web application category. Written by Arijit Gupta. Made with Ameya Laad, Anish Mulay, Dev Churiwala and Smitesh Patil, undergraduate students at BITS Goa

What is Rasoee?

Our main inspiration while making this was looking at the number of people sharing pictures of food they’ve made at home during the lockdown. We then started wondering if there was someway to make it easier for others to replicate the ones they like themselves. Rasoee is a web/mobile application that can be used to identify food items from images. The app can identify from 308 different types of dishes from over 5 different cuisines. After identifying the dish, it also provides(if possible) a list of key ingredients that are used to make this dish, and link to a recipe and some tutorial videos on YouTube.

How it works

Rasoee uses an image classification model trained on a dataset of 308 images. We chose to train an EfficientNet B2 model that we believed was small enough to be deployed as an edge model and had enough depth to classify images into such a large number of categories. The making of this involved many steps, starting from creation of the dataset and ending with the making of the mobile and web application along with integrating the model into it.

Scrape your way to a dataset

While analysing the datasets available online, we saw that the most valid dataset available was the Food-101 Dataset.

Food-101 Classes

But, the Food-101 Dataset seemed a bit restricted in variety to us, and while the quality of the dataset is extremely high, we decided to go out on a limb and create our own dataset. To do this we used a bing image scraper and a lot of patience. What resulted was a terribly noisy dataset that we then cleaned to the best of our abilities, but we now had a dataset with 308 different categories of food.

Models, models, everywhere

Now came the moment where we had to make the most difficult decision of all, which model to use? PyTorch has such a great array of models that can be used even in the official documentation that we were spoiled for choice. After going through many models, and looking at their size v/s depth trade-off we settled on using one of the EfficientNet implementations for PyTorch, specifically the EfficientNet B2 model which has 9.2M parameters and a size of ~300Mb. Three of us were training different models and preprocessing, but we finally settled on the model that gave us the highest accuracy, and we achieved an accuracy of 91.31% using CrossEntropy loss. The training script is very basic and the training notebook can be found here.

User interface

The application has both a website and a mobile version. While our primary aim was the mobile application as it would be more convenient to users, the website can also be used on a mobile browser or any other edge device. It is very simple to use, just upload an image and wait for the results.",https://medium.com/pytorch/rasoee-a-picture-is-worth-a-thousand-ingredients-82576790a7bc,['Arijit Gupta'],2020-12-16 23:35:42.340000+00:00,546,"Food-101 Classes, Image Classification Model, Efficient Net B2 model, Cross Entropy loss, Py Torch models"
Running Jupyter Notebooks on Remote Servers,"Jupyter Notebook is a staple tool in many data scientists’ toolkit. As a tool, Jupyter Notebook can enhance productivity by making it easier to perform data analysis, model prototyping, and experiments in an interactive way, thus shortening the feedback loop from coding and seeing results.

In many cases, running a Jupyter notebook on a laptop or work station is sufficient. However, if you’re working with large datasets, doing computationally expensive data processing, or learning complex models you probably need something more powerful than a laptop extra power. Maybe you’re running graph convolutional networks on large graphs or doing machine translation with recurrent neural networks on large text corpora and need some more CPU cores, RAM, or a couple of GPUs. Luckily, you may be in the situation where you have these resources available on a remote server!

If your remote server has a Graphical User Interface (GUI), you are in luck. You can use remote desktop software to access the remote server and otherwise use Jupyter Notebook as your normally would on your laptop.

However, many servers do not have a GUI. If you are in this situation, you can set up your experiment by writing a Python script on your laptop, run it on a small subset of your data to verify that it can run, copy it to a remote server, and execute it from the command line. You could even set up the experiment in a notebook and export the notebook to a script using jupyter nbconvert --to script your_notebook.ipynb . Although this workflow certainly allows you to run your code on the remote server, you can no longer use Jupyter Notebook to, e.g., experiment with your models and visualize your results interactively. What a shame!

In this post I will show you how to run a Jupyter notebook on a remote server and how to access it on your laptop. I will also show how to setup two bash commands to make the whole process easier.

Starting the Remote Notebook Server

We will use the Secure Shell Protocol (SSH) to start the Jupyter Notebook server on the remote server. SSH allows us to send commands to the remote server. The basic syntax is as follows:

ssh username:password@remote_server_ip command

The exact command you should send depends a little on your context. In my case, I share a remote server with other people have therefore not installed Jupyter in the shared environment. The first step for me is therefore to go to my project folder, activate the virtual environment, and start the notebook server. In particular, I would like to execute the following three bash commands on the remote server:

cd project_folder

. virtual_environment/bin/activate

jupyter notebook --no-browser --port=8889

I execute the jupyter notebook command with the --no-browser flag to start the Jupyter notebook with launching a browser since the remote server cannot display a browser if it doesn’t have a GUI. I also change the port from the default port 8888 to port 8889 using the --port=8889 flag. This is a personal preference; having local and remote notebooks on different ports to make it easier to see where my code is running.

To execute commands on the remote server, we run the combined command

nohup ssh -f username:password@remote_server_ip ""cd project_folder; . virtual_environment/bin/activate; jupyter notebook --no-browser --port=8889""

Note that I have one-lined the three commands and separated them using ; instead of line breaks. Executing this command will start the Jupyter Notebook server on port 8889 and let it run in the background. Finally, I have added the -f flag to the ssh command to push the process to the background and prepended the nohup command to silence all output from the process so you can continue using the terminal window. You can read more about the nohup command here.

Accessing Your Notebook

You can now access the notebook typing in the url

remote_server_ip:8889

This command requires you to memorise the IP address or to bookmark the web page. However, we can make it just as easy to access the remote notebook as if it were a local notebook by using port forwarding:

nohup ssh -N -f -L localhost:8889:localhost:8889 username:password@remote_server_ip

The -N flag tells ssh that no remote commands will be executed. At this point we do not need to execute any remote commands. The -f flag pushes the ssh process to the background as mentioned previously. Finally, the -L flag specifies the port forwarding configuration using the syntax local_server:local_port:remote_server:remote_port . The configuration specifies that all requests sent to port 8889 on the local machine, e.g., your laptop, to port 8889 on the remote machine at username:password@remote_server_ip . As before, the nohup command has been prepended to silence the output.

The effect the above command is that you can now access the remote Jupyter Notebook server in your browser at

localhost:8889

as if you ran the notebook locally.

Stopping the Remote Notebook Server

In principle, you can let the notebook server run indefinitely on the remote server (barring restarts or crashes), but you may need to stop the server, for instance to upgrade your version of jupyter . If you need to stop it there are two ways to do so: through the browser or through the command line.

Through the Browser Window

In the recent versions of the Jupyter Notebook, you can find a Quit button at the top right of the browser window as indicated by the arrow in the image below. If you press it, you will have to relaunch the server again using the start-up command we saw previously.",https://towardsdatascience.com/running-jupyter-notebooks-on-remote-servers-603fbcc256b3,['Tobias Skovgaard Jepsen'],2019-03-05 06:45:29.334000+00:00,896,"Through the Command Line You can also stop the server from the command line. To do so, you need to find out which process is running on port 8889. You can do this using the following command: jupyter, notebook, data"
Multiple Linear Regression Implementation in Python,"Multiple Linear Regression Implementation using Python

Problem statement: Build a Multiple Linear Regression Model to predict sales based on the money spent on TV, Radio, and Newspaper for advertising.

Importing the Libraries

#Importing the libraries import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

numpy : NumPy stands for numeric Python, a python package for the computation and processing of the multi-dimensional and single-dimensional array elements.

pandas : Pandas provide high-performance data manipulation in Python.

matplotlib : Matplotlib is a library used for data visualization. It is mainly used for basic plotting. Visualization using Matplotlib generally consists of bars, pies, lines, scatter plots, and so on.

seaborn : Seaborn is a library used for making statistical graphics of the dataset. It provides a variety of visualization patterns. It uses fewer syntax and has easily interesting default themes. It is used to summarize data in visualizations and show the data’s distribution.

Reading the Dataset

#Reading the dataset dataset = pd.read_csv(""advertising.csv"")

The dataset is in the CSV (Comma-Separated Values) format. Hence, we use pd.read_csv() to read the dataset.

dataset.head()

Sales Dataset

Equation: Sales = β0 + (β1 * TV) + (β2 * Radio) + (β3 * Newspaper) + e

Setting the values for independent (X) variable and dependent (Y) variable

#Setting the value for X and Y x = dataset[['TV', 'Radio', 'Newspaper']]

y = dataset['Sales']

Splitting the dataset into train and test set

#Splitting the dataset from sklearn.model_selection import train_test_splitx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 100)

from sklearn.model_selection import train_test_split : It is used for splitting data arrays into two subsets: for training data and testing data. With this function, you don’t need to divide the dataset manually.

We need to split our dataset into training and testing sets. We’ll perform this by importing train_test_split from the sklearn.model_selection library. It is usually good to keep 70% of the data in your train dataset and the rest 30% in your test dataset.

test_size : This parameter specifies the size of the testing dataset. The default state suits the training size. It will be set to 0.25 if the training size is set to default.

randon_state : This parameter controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.

Implementing the linear model

#Fitting the Multiple Linear Regression model mlr = LinearRegression()

mlr.fit(x_train, y_train)

from sklearn.linear_model import LinearRegression: It is used to perform Linear Regression in Python.

To build a linear regression model, we need to create an instance of LinearRegression() class and use x_train, y_train to train the model using the fit() method of that class. Now, the variable mlr is an instance of the LinearRegression() class.

Model Equation

#Intercept and Coefficient print(""Intercept: "", mlr.intercept_)

print(""Coefficients:"")

list(zip(x, mlr.coef_))

Intercept & Coefficients

Regression Equation: Sales = 4.3345+ (0.0538 * TV) + (1.1100* Radio) + (0.0062 * Newspaper) + e

From the above-obtained equation for the Multiple Linear Regression Model, we can see that the value of intercept is 4.3345, which shows that if we keep the money spent on TV, Radio, and Newspaper for advertisement as 0, the estimated average sales will be 4.3345 and a single rupee increase in the money spent on TV for advertisement increases sales by 0.0538, the money spent on Radio for advertisement increases sales by 1.1100, and the money spent on Newspaper for advertisement increases sales by 0.0062.

Prediction on the test set

#Prediction of test set y_pred_mlr= mlr.predict(x_test) #Predicted values print(""Prediction for test set: {}"".format(y_pred_mlr))

Predicted values

Once we have fitted (trained) the model, we can make predictions using the predict() function. We pass the values of x_test to this method and compare the predicted values called y_pred_mlr with y_test values to check how accurate our predicted values are.

Actual values and the predicted values

#Actual value and the predicted value mlr_diff = pd.DataFrame({'Actual value': y_test, 'Predicted value': y_pred_mlr})

slr_diff.head()

Actual and the Predicted values

Evaluating the Model

#Model Evaluation from sklearn import metrics meanAbErr = metrics.mean_absolute_error(y_test, y_pred_mlr)

meanSqErr = metrics.mean_squared_error(y_test, y_pred_mlr)

rootMeanSqErr = np.sqrt(metrics.mean_squared_error(y_test, y_pred_mlr)) print('R squared: {:.2f}'.format(mlr.score(x,y)*100))

print('Mean Absolute Error:', meanAbErr)

print('Mean Square Error:', meanSqErr)

print('Root Mean Square Error:', rootMeanSqErr)

Evaluation Metrics

from sklearn import metrics : It provides metrics for evaluating the model.

R Squared: R Square is the coefficient of determination. It tells us how many points fall on the regression line. The value of R Square is 90.11, which indicates that 90.11% of the data fit the regression model.

Mean Absolute Error: Mean Absolute Error is the absolute difference between the actual or true values and the predicted values. The lower the value, the better is the model’s performance. A mean absolute error of 0 means that your model is a perfect predictor of the outputs. The mean absolute error obtained for this particular model is 1.227, which is pretty good as it is close to 0.

Mean Square Error: Mean Square Error is calculated by taking the average of the square of the difference between the original and predicted values of the data. The lower the value, the better is the model’s performance. The mean square error obtained for this particular model is 2.636, which is pretty good.

Root Mean Square Error: Root Mean Square Error is the standard deviation of the errors which occur when a prediction is made on a dataset. This is the same as Mean Squared Error, but the root of the value is considered while determining the accuracy of the model. The lower the value, the better is the model’s performance. The root mean square error obtained for this particular model is 1.623, which is pretty good.

Conclusion

The Multiple Linear Regression model performs well as 90.11% of the data fit the regression model. Also, the mean absolute error, mean square error, and the root mean square error are less.",https://medium.com/machine-learning-with-python/multiple-linear-regression-implementation-in-python-2de9b303fc0c,['Harshita Yadav'],2021-05-08 07:47:02.361000+00:00,918,"multiple linear regression, python, machine learning, data visualization, data analysis"
Look back at my 2018 predictions,"In the beginning of 2018, I made these predictions.

Here’s a look back at how well I did.

Technologies

Blockchain

We can expect to see Casper implemented in Ethereum, and lightning networks on Bitcoin. This will be a big push to the scalability problem that we have been complaining about all of 2017 with respect to crypto currencies.

This didn’t turn out exactly like this. However Plasma made up for it. Starting with Plasma MVP that started the fire all the way to various iterations of Plasma Cash.

Wired : Plasma turned out to be the most dominant topic in scalability this year.

Tired : Casper made progress but no where close

We will also see a fall in prices of the tail currencies. Expect to see much fewer crypto currencies seeing the end of 2018, but expect them to finish much stronger.

Not bad!

I think we will see some solid usecases being implemented on blockchain outside of just currency, that will connect to the end customers, albeit a select few.

Wired : My favorite blockchain implementation was Augur. Bringing prediction markets to blockchains.

Tired : No real usecase came out. I’d have liked to see something from Filecoin.

AI

We can expect self driving cars to hit atleast one major city this year, especially on highways. … i expect some forward looking companies to start re-imagining the car itself

I think I grossly overestimated the state of the art. Unfortunately, I didn’t follow this space closely enough in ’18 to comment any further.

Voice assistants : I am a skeptic on this, i expect the enthusiasm to fall in this space. … That said, Google Now, Siri and maybe Cortana will continue to advance the technology in useful ways.

Wired : Google probably stole the show with the bot that places phone calls for you.

Tired : Hard to judge by any means if enthusiasm fell in this space. It seems to be still going strong.

Social Media

Social media (especially Facebook) seemed to take a beating in late 2017, and i feel that trend will continue this year. We can expect more commercialization of social media.

Wired : Facebook did take a lot of beating!

Tired : Commercialization didn’t change much, did it?

We may see commercial bots and proactive ads this year. This will primarily be driven by Messenger/Whatsapp playing around with commercial accounts.

Tired : Didn’t happen

The social feed is likely to be replaced by something else.

Tired : Mildly disappointed that this didn’t happen.

Wired: In other matters, WeChat introduced stories.

AR VR

… i am convinced there will be a push to the development efforts with the release of standalone devices like the Occulus Go.

Wired : AR-VR didn’t lose steam

Tired : Go didn’t make it to mainstream

My best bet is we will continue to see improvements in this field and wouldn’t be surprised to see no real useable product coming out using this technology.

Wired : This was easy.

Fields

Healthcare

I think we will see some more wearables and AI coming together to make this more feasible this year.

Wired : Apple led the way with the heart sensor in the Apple Watch 4

Tired : Software is yet to catch up

FinTech

I would like to see payment gateways comeup (into mainstream) where payments may be made in crypto. This is definitely far fetched, but we should start seeing some leading indicators.

Wired : Coinbase released Coinbase Commerce. (Stripe for crypto)

Tired : Never saw Coinbase Commerce actually used anywhere

… it will be interesting to see innovations we have on the newsfeed, and the way people consume content. I will be looking forward to proactive feeds, where content is pushed to me when it is most relevant (more intelligently).

Wired : Bytedance owning TikTok became the most valuable startup. No feeds at all

Tired : Proactive feeds? I don’t even remember what that meant when I wrote it. Apologies!

I’ll be writing my predictions for 2019 too!

I had made predictions for 2017 and reflected on how bad they were. I did much better this time :)",https://medium.com/madhavanmalolan/look-back-at-my-2018-predictions-50a9292f3c7c,['Madhavan Malolan'],2019-01-05 16:30:17.352000+00:00,644,"predictions, blockchain, AI, voice assistants, social media"
Mental Health Reddit Analysis,"For my capstone project at Flatiron, I wanted to see if I could predict whether someone has a specific mental health illness based on their posts about things unrelated to their mental health. My background before becoming a data scientist was in the mental health field, so I wanted to combine my new skills with my previous passion of helping those struggling with mental health related symptoms.

As you can imagine, this was a very difficult thing for a model to do. While I tried to create one large model that was able to classify users as either of the control group (no self-reported mental health diagnosis) or of six specific mental health disorders (PTSD, anxiety, depression, ADHD, bipolar, and borderline personality disorder), this proved very difficult. I ended up creating a 2 model approach — first a binary classifier that classifies users as either part of the control group, or part of the mental health group. This model was trained only on comments and posts from non-mental health specific subreddits. This model achieved an overall macro f1 score of.69, precision of .68, and recall of .7. For the mental health class, the f1-score was .59, precision was .55, and recall was .64.

hid_dim1 = 768

hid_dim2 = 384 model_bin_sbert_balanced = tf.keras.models.Sequential([

tf.keras.layers.Dense(hid_dim1, activation=tf.nn.leaky_relu), tf.keras.layers.Dense(hid_dim2, activation=tf.nn.leaky_relu), tf.keras.layers.Dense(2, activation=tf.nn.leaky_relu), tf.keras.layers.Dense(1, activation=’sigmoid’)])

I then try to classify data from the “mental health group” based on which diagnosis they may have. This of course proved very difficult to do. The first version where every class was weighted the same had a f1 score of .297. It does best with anxiety (recall of .65 and precision of .36) and PTSD (recall of .53 and precision of .36). Also interestingly, PTSD is misclassified as anxiety 473 times, and anxiety is misclassified as PTSD 358 times.

hid_dim1 = 768

hid_dim2 = 384 model_sbert_subclass = tf.keras.models.Sequential([

tf.keras.layers.Dense(hid_dim1, activation=tf.nn.leaky_relu),

tf.keras.layers.Dense(hid_dim2, activation=tf.nn.leaky_relu),

tf.keras.layers.Dense(6, activation=tf.nn.leaky_relu),

tf.keras.layers.Dense(6, use_bias=False),

tf.keras.layers.Softmax()])

The second version of the model, which took into account class weights, has a weighted f1-score of .27, precision of .32, and recall of .27. ADHD’s recall and precision are similar to each other at .25 and .27. Anxiety’s precision is .47 and recall is .17. Bipolar’s recall is .43 but precision is .2. BPD’s recall is .29 and precision is .19. Depression has a recall of .18 and precision of .17. PTSD’s recall is .34 and precision is .35. Interestingly, in this model PTSD is linked more closely to bipolar than anxiety.

This project was very much a challenge for me in so many ways. It tested my knowledge and my fortitude. It also took place as I was dealing with several life changes. I’m very proud of what I was able to create for this project, and all I have learned in my data science journey so far.",https://medium.com/@abookishwolf/mental-health-reddit-analysis-bdd15e03e133,['Kayla Strickland'],2021-06-17 08:37:59.725000+00:00,454,"mentalhealth, datascience, capstoneproject, prediction, PTSD"
The beginning of a deep learning trading bot — Part1: 95% accuracy is not enough,"Start experimenting — Finding the right data

Before training the production-grade level models we first have to find out how explanatory stock prices and financial news are when forecasting for stock returns.

In order to get a first impression of how well stock prices and news indicate future stock price changes we initially train multiple models on a smaller dataset. The dataset that we will use to start proving our assumptions are the historical price and volume data of the IBM stock.

IBM has a fairly long price history on Yahoo, prices reach back as far as 1962. The easiest way to get the historical IBM prices is to simply download the dataset from yahoo’s IBM page. For each trading day, Yahoo provides Open, High, Low, Close prices and the Volume (OHLCV). Once downloaded and having it loaded into a notebook the IBM OHLCV data looks as follows.

IBM’s price and volume data

To get an idea of how the prices have changed over time, we plot the daily closing prices. The IBM price graph start date is January 2nd, 1962, ends on February 3rd, 2020, and has a price range between $7.5 – $225.

IBM daily close price 1962–2020

Let’s also have a look at the volume data which we will use as an additional feature to our price data points. The volume for each day is calculated by multiplying for each trade the number of shares times the trade share price. Then the products of all trades during a day are summed and form the volume data point for this particular day.

IBM’s daily trading volume 1962–2020

Preprocessing our data — I know it’s boring but necessary 😊

Feeding raw price and volume data into a deep learning model is usually a bad idea. When looking at IBM’s price graph you can see the prices from 1962 to 1991 ($7- $48) are on a totally different level than the prices around the years 2000 and 2020 ($140-$220). In essence, these two price ranges have little to do with each other. Meaning that the range from 1962–1991 (average price $25) has little explanatory value for the price range of 2000–2020 (average price $130). In order to bring past price points on the same level as price points of recent times, and thus more useful for training our neural networks, we have to do a couple of preprocessing steps. Let’s start the preprocessing, I promise it‘s going to be quick.

Firstly, we are going to convert prices and volumes into price returns/percentage changes. The easiest way to do this is by using the pandas function pct_change().

The advantage of having price returns is that they are more stationary than raw price data. A simple explanation of stationarity: Stationarity = Good, because past data is more similar to future data, making forecasts easier.

IBM’s daily stock returns and volume changes

The next graph illustrates nicely that converting stock prices to stock returns removes the trend of increasing stock prices.

Secondly, a min-max normalization is applied to all price and volume data, making our data range from 0–1. Instead of using the raw price returns and volume changes, normalized data has the advantage that it allows a deep learning model to train more quickly and stably.

Thirdly, we will split the time series into training, validation and test datasets. In most cases a training and validation dataset split is sufficient. However, for time series data it is crucial that the final evaluation is performed on a test set. The test dataset has not been seen by the model at all and thus we avoid any look ahead or other temporal biases within the evaluation.

Having calculated stock returns, normalized and split the data into 3 sections, the datasets have now the illustrated shape below.",https://towardsdatascience.com/the-beginning-of-a-deep-learning-trading-bot-part1-95-accuracy-is-not-enough-c338abc98fc2,['Jan Schmitz'],2020-05-20 02:37:41.011000+00:00,615,"IBM’s preprocessed datasets Stock Returns, Financial News, Data Preprocessing, Yahoo Finance, IBMstocks"
Step by Step Facial Recognition in Python,"Step by Step Facial Recognition in Python

A simple how-to using Python, Pillow, and a few lines of code…

In this article, I will guide you to create your own face recognition in images. For this purpose, I will use the Python face recognition library and Pillow, the Python Imaging Library (PIL).

I chose to use Visual Studio Code since I need to use integrated terminal. First, I start by setting a virtual environment and install pipenv on my terminal.

Run pipenv shell to start your virtual environment and install the face recognition library.

For this tutorial, I created two folders named known and unknown . The first folder includes pictures of some of the more well-known people in politics like Barack Obama, Donald Trump, Bernie Sanders, Joe Biden, and Elizabeth Warren. The latter includes different pictures of the people from the first folder, some of the 2020 Presidential candidates, and some SNL characters (played by different actors) of Donald Trump, Barack Obama, and Bernie Sanders.

I will run a match on the known and unknown folders to see if there are any pictures of known people in the unknown folder. I can do this by command line easily by running:

This will go through all the images and show us the matches in the second folder from the first one.

As you can see from the output, Bernie_SNL.jpg —which was performed by Larry David—is matched as Bernie Sanders. To avoid that, I will check the distance of each match, which essentially tells how much of a match the images are, by running:

face_recognition — show-distance true ./img/known ./img/unknown

I can see the decimal value of distance between matched images.

I will add the flag and change the tolerance so the matching algorithm will only accept the values under a certain number. Adjusting tolerance helps get more accurate results.

As seen in the above image, Bernie_SNL.jpg did not match with the real Bernie Sanders.jpg .

If I just want to get the names of the people in the images, I will use:

face_recognition — show-distance true ./img/known ./img/unknown | cut -d ‘,’ -f2

to get the output below.

Let’s move one of the unknown people, Andrew Yang, to our known folder and run the code above again. As you see below, Andrew Yang will also be defined as a known person and it will show the matches from the unknown folder.

If we want this process to go faster we can add — cpus flag to our command line.

Now I will create the Python files to work with the facial recognition library.",https://medium.com/better-programming/step-by-step-face-recognition-in-images-ad0ad302058a,['Ulku Guneysu'],2020-12-15 02:26:45.803000+00:00,415,"Python, Pillow, Facial Recognition, Visual Studio Code, Virtual Environment"
Sorting Objects in Python,"In computer science, classes are blueprints for defining objects with related attributes and methods. A common task in computer science is sorting data structures based on attributes. In this post, we will discuss how to sort objects of the same class based on attribute values.

Let’s get started!

Suppose we have a class called ‘AppleMusicUser’, with an ‘__init__’ method that initializes an apple user ID:

class AppleMusicUser:

def __init__(self, apple_id):

self.apple_id = apple_id

Let’s create an instance of our ‘AppleMusicUser’ class with the email ‘emusk@tesla.com’:

user1 = AppleMusicUser(""emusk@tesla.com"")

If we print this object we get:

print(user1)

Which says the ‘AppleMusicUser’ object is located at the specified memory address. We can change how this object is represented by adding a ‘__repr__’ method:

class AppleMusicUser:

def __init__(self, apple_id):

self.apple_id = apple_id

def __repr__(self):

return 'AppleID({})'.format(self.apple_id)

Now if we print:

print(user1)

Next, suppose we have a list of ‘AppleMusicUser’ objects:

users = [AppleMusicUser(""mzuckerberg@facebook.com""), AppleMusicUser(""jdorsey@twitter.com""), AppleMusicUser(""emusk@tesla.com"")]

Let’s print the list of objects:

print(users)

We can sort the list of objects according to the apple ID using the ‘sorted()’ method and a lambda function:",https://towardsdatascience.com/sorting-objects-in-python-d43db6edaaea,['Sadrach Pierre'],2020-05-13 18:43:49.521000+00:00,157,"sorted_users = sorted(users, key=lambda user: user.apple_id)print(sorted_users) Output: [AppleID(emusk@tesla.com), AppleID(jdorsey@twitter.com), Apple"
Financial Charts and Visuals With Plotly in Python,"Financial Charts and Visuals With Plotly in Python

Professional financial charts with less-code using Plotly in Python

Photo by Chris Liverani on Unsplash

Disclaimer: This article is purely for educational purposes and should not be considered as a piece of advice for making investments.

Plotly for Visualization

One of the most popular packages used for interactive visualizations is Plotly. Plotly is built on top of python and enables data scientists to produce professional and great-looking plots with less-code. It became popular because of its extensive category of plots which can be produced in no-time. The categories of plots include basic charts, statistical charts, ML and AI charts, scientific charts, and financial charts.

In this article, I’m going to walk you through the process of creating interactive and professional financial charts in Plotly with python. We will also explore yahoo’s API for pulling historical stock data which we will be using for visualizations. Let’s get started!

Importing packages

Our primary packages include pandas for data processing, pandas DataReader for pulling the historical stock data, Datetime to deal with dates, finally, Plotly and its dependencies for interactive visualizations. Follow the code to import the primary packages into our python environment.

Python Implementation:

Our next process is going to be pulling the historical stock data for visualizations using the pandas DataReader package.

Pulling Stock Data

For our visualizations, we are going to pull six companies’ historical data namely Facebook, Amazon, Apple, Netflix, Google, and Microsoft using yahoo’s API. Let’s pull the data in python!

Python Implementation:

Firstly, we have defined two variables specifying the start and end date of our data. Next, using the pandas DataReader package, we have pulled the historical data of the companies. Finally, we stored only the close price data of companies in the ‘stocks_close’ variable. Now, we are ready to do visualizations on our stock data.

Area Chart

Often called ‘Mountain’ charts, area charts are a more simplified interpretation of standard line charts. They plot closing prices over a given period, and the area beneath the line is shaded. Follow the code to create an area chart with Plotly in python.

Python Implementation:

Output:",https://medium.com/datazen/financial-charts-and-visuals-with-plotly-in-python-843ffa9341a9,['Nikhil Adithyan'],2020-11-03 06:38:26.344000+00:00,338,"Financial Charts, Plotly, Python, Visualization, YahooAPI"
So When Did Our Simulation Begin?,"Thanks to fellow Hacker Noon writer and author of the Simulation Hypothesis Riz Virk, the Simulation theory or argument popularized by Oxford’s Nick Bostrom and Elon Musk has now begun to penetrate much of the tech world and beyond.

When I first heard this theory I found that it parallels two philosophical neo-theological movements that I have ascribed to or at least felt compelled to connect to: Chassidut (Jewish mystical movement pioneered in Eastern Ukraine over 250 years ago) and Zen Buddhism. The metaphysical underpinnings of both, in which they discuss the nature of the world being but an illusion (Mitzrayim in Chassidut and Samsara in Buddhism) seem like nice theological fits to the Simulation theory.

However, my main thrust of this article is not to discuss the parallel qualities each have to this current theory, but rather ask a more fundamental question about the impact this theory has on a variety of accepted understandings of our universe.

Shattering Our Beliefs

Perhaps one of the biggest foundations and belief sets that the Simulation theory unravels if true (I for one believe it is) is the age of our universe. Now when we look out at the universe through a scientific lens we see it being billions of years old. Surprisingly, this has never been a conflict for those of us who ascribe to the Hebrew Bible. Years in Judaism are counted from the creation of Adam. The sages very rightly warned us not to try to understand the Six days of Creation as it is beyond us. Meaning billions of years could fit into those 6 cosmic days.

I am mentioning this because it is important to understand that from my perspective I am raising the issue of the Simulation theory and the age of the Universe not from a theological challenge, but rather that the Simulation theory is perhaps the single biggest watershed moment for those technologists that have followed what were accepted underpinnings of Western science.

The Simulation theory renders the debate about the universe’s age obsolete as it does the question about evolution or environmental shifts or whether the dinosaurs are really real. All of those things would be easily programmed into the simulation as part of the “game.”

What is Reality?

This is the point, which is most transformational — nothing we have here is truly real. Only the Constructor or Architect of the Simulation is real and since we are in the Simulation we cannot actually understand what it is exactly.

Since there is a high possibility that none of the things we have gleamed about our reality’s past are concrete (either they are programmed into our simulation from the beginning or the Constructor has shifted the underpinnings whenever the simulation needed a tweak) then perhaps it is time for humanity whether we are PC (player characters) or NPC (non-player) characters or a combination of both to figure out what the Constructor really wants from us and how to reach the final “level” and of course rescue the proverbial “Princess.”",https://medium.com/hackernoon/so-when-did-our-simulation-begin-46445cc7b8b0,['David Mark'],2019-05-16 11:10:05.320000+00:00,496,"Simulation Theory, Nick Bostrom, Elon Musk, Chassidut, Zen Buddhism"
[withR]좀더 하는 ggplot2-Annotations with Model Coefficients(그래프 주석달기),"#library(ggplot2)

#library(gcookbook)

그래프에 주석을 달아 보자.

먼저 먼저 그래프를 만들기 위한 데이터를 생성하자.

library(gcookbook) #예제 자료

#모델 생성

model <- lm(heightIn ~ ageYear, heightweight)

summary(model)

#R cookbook Graphics 책 참조

predictvals <- function(model, xvar, yvar, xrange=NULL, samples=100, …) {

if (is.null(xrange)) {

if (any(class(model) %in% c(“lm”, “glm”)))

xrange <- range(model$model[[xvar]])

else if (any(class(model) %in% “loess”))

xrange <- range(model$x)

}

newdata <- data.frame(x = seq(xrange[1], xrange[2], length.out = samples))

names(newdata) <- xvar

newdata[[yvar]] <- predict(model, newdata = newdata, …)

newdata

}

#예측값데이터 생성

pred <- predictvals(model, “ageYear”, “heightIn”)

sp <- ggplot(heightweight, aes(x=ageYear, y=heightIn)) + geom_point() +

geom_line(data=pred)

sp + annotate(“text”, label=”r²=0.42"", x=16.5, y=52)

sp<-로 할당하여 재사용하기 편하게 코딩하고 있다.

그다음에 sp+annotate()를 이용해 그래프 위에 주석을 달아 r²값 결정계수를 표현한다.

sp + annotate(“text”, label=”r² == 0.42"", parse = TRUE, x=16.5, y=52)

주석에 표현이 달라진다.",https://medium.com/excitinglab/withr-%EC%A2%80%EB%8D%94-%ED%95%98%EB%8A%94-ggplot2-annotations-with-model-coefficients-%EA%B7%B8%EB%9E%98%ED%94%84-%EC%A3%BC%EC%84%9D%EB%8B%AC%EA%B8%B0-133f123fe80c,['Neo Jeong'],2019-08-07 06:40:11.550000+00:00,103,"ggplot2, gcookbook, lm, glm, loess"
I Want Me But Absent of Sentience,"I Want Me But Absent of Sentience

Photo by Kowon vn on Unsplash

Where does the technology come into this story?

It might be a dishwasher, but you know it still has to be loaded by a human.

We could live on microwaveable ready meals and junk food (those of us who don’t already — I’m not judging (much)) However, they are expensive, calorie-laden, and often of little nutritional value. Or we could buy prepared fresh fruit and veggies; a healthier option.

Toasted cheese sandwiches, squished by one of those toastie makers, might be a possibility, for convenience, but anything eaten over and over gets monotonous. Also, there’s still the washing up. For humans. Do you see where we are going with this?

When are we going to get our own super-efficient, artificially intelligent simulated humans? They’ll arrive in our homes pre-programmed, to do all the tedious tasks we humans don’t care for.

If you watched Humans (the joint Channel 4 and AMC TV show), you may be thinking, oh no! What if the Synths are really sentient beings? That would be a disaster. Or, if the iRobot were a stupid Roomba that gets stuck between chair legs and needs a human to help it get going again. Not much use if you set it to suck up all the dust and pet hair while you were at work!

Wouldn’t you love a mega bot like the graceful pickers in the Amazon warehouses? You can bet your bottom dollar they get every task done with speed, quality, and precision. Think of the possibilities …

When I was a teenager at school in the seventies, I was promised more leisure time thanks to future technology that would be able to do many of the tasks humans were, at the time, bound to do. What happened?

Technology advanced and company owners did take advantage of labour-saving innovation. Productivity and profits soared. However, somewhere along the line, workers stopped benefitting. We didn’t get more leisure time. In fact, many humans in the UK now work longer hours for less remuneration to survive. In the UK, there are now 14 million people living in poverty. That’s quite a turn around from the prediction I was aiming for.

Technology has to have some real leisure time boosting qualities for the workers. Forget the water-guzzling dishwasher. I’d like my synthetic me, with just enough artificial intelligence, to do all the food shopping, prepping, and cooking. Then clean up the kitchen and do the washing up. I’d also like it to plump my pillows daily, shake the duvet and straighten it. Do the laundry once a week. I’d let it off ironing because I don’t own anything that needs ironing. In fact, I don’t own an iron. Tidy up, dust, and vacuum once a week. Clean the kitchen and bathroom on a daily basis.

All those tedious chores hold zero interest for me and could so easily be done by an automaton. I’ll call mine Cinderella, it will be absent of sentience and there will be no bonding. In my mind, technology should be working to improve our lives; freeing up our time to create and pursue joyful lives instead of being forced to be slaves to money.

Just think how much me-time we could have to write and power walk on the beach or in the countryside; climb mountains and hike up hills. Wouldn’t you love the time to create your art, see your friends, and watch Netflix whenever you like?! What else would you do with all your freed up time?",https://medium.com/illumination-curated/i-want-me-but-absent-of-sentience-da3025ce4777,['Karen Madej'],2021-02-22 21:57:13.606000+00:00,583,"Technology, Artificial Intelligence, Labour-Saving Innovation, Leisure Time, Synthetic Me"
A Beginner’s Guide To Computer Vision,"Computer Vision

Before we dive into the various CV techniques, let’s explore the human body part that computer vision is trying to emulate in terms of functionality.

Most humans don’t give much thought to vision; it’s a bodily function that automatically works with little to no deliberate influence.

Photo by v2osk on Unsplash

The human vision sensory system has developed over thousands of years to provide humans with the ability to extrapolate scenery meaning and context from the light that is reflected by objects in our 3-dimensional world, into our eyes.

Our eyes and brain can infer an understanding of environments from reflected light.

Our visual system equips us with the ability to determine the distance of objects, predict the texture of objects without directly touching, and identify all sort of patterns and events within our environment.

Computer Vision is the process by which we try to equip computer systems with the same capabilities that the human's visual sensory system possesses.

An appropriate definition for computer vision is as follows:

Computer Vision is the process by which a machine or a system generates an understanding of visual information by invoking one or more algorithms acting on the information provided. The understandings are then translated into decisions, classifications, pattern observation, and many more.

Our visual sensory system consists of the eyes and the brain, although we understand how each component of the eyes such as the cornea, lens, retina, Iris etc., we don’t fully understand how the brain works.

To create algorithms and systems that have the capability of extracting contextual information from images, causations of patterns have to be observed. Then solutions can be derived from the understanding of the causes and effect of specific patterns.

There are a lot of applications of Computer Vision, here are a few:",https://towardsdatascience.com/a-beginners-guide-to-computer-vision-dca81b0e94b4,['Richmond Alake'],2020-09-22 23:23:37.211000+00:00,287,"Computer Vision, Machine Learning, Artificial Intelligence, Image Processing, Pattern Recognition"
Interactive Visualizations with Python,"Dash

Dash is a framework for Python written on top of Flask, Plotly.js, and React.js. It is a library designed to create web applications focused on data visualization, it’s very complete even if you don’t know React, you can make a complete application using only Python.

Here’s a simple example of a Dash App that ties a Dropdown to a D3.js Plotly Graph. As the user selects a value in the Dropdown, the application code dynamically exports data from Google Finance into a Pandas DataFrame.

A basic example of a Dash App

Dash applications are composed of 2 different parts, the first one describes how the application looks like and the other one describes the interaction of its components.

This library provides a couple of modules with which through Python we can generate different HTML elements, containers, buttons, text fields, and most importantly Graphics. Each of these classes we use to create applications is what we know as Layout.

In the case of Interactivity, Dash provides some functions which will work as Callbacks that will be executed automatically when any property of our elements changes.

The previous example was written in just 43 lines of code:

Since it is built using the Flask micro-framework, it allows in a very simple way to raise your server. And every setting or extension can be used to create even more complex applications.

And as they mentioned in their announcement, being able to use React to build great interactions of the elements practically in “real-time” is fantastic and more than this is easy to create because Dash takes advantage of both libraries for those who may not be experts in web development and can do everything from Python.

Reactive Web Apps using Python Dash

As I mentioned at the beginning, there are many tools for data visualization, some more complex than others. However, it is always good to know the advantages or limitations and choose the one that is useful for you.

Remember you can use Dash to create any kind of application. Check out their gallery.

Additional resources",https://medium.com/dev-genius/interactive-visualizations-with-python-a4721aec5c77,['Esteban Solorzano'],2020-07-06 17:08:00.810000+00:00,332,"Dash, Python, Flask, Plotly.js, React.js"
STARBUCKS SUPER PROMO,"B. portfolio.json — Promotions information:

portfolio Data Frame

This is a straight forward data set, which has information about the Starbucks’s promotions sent to its clients. There are 10 offers classified in 3 types: bogo , informational and discount and there are no Nan values. I want to highlight the importance of the duration column, it could help us to relate the promotion with a transaction.

C. transcript.json — Timeline / Events:

transcript Data Frame

This is the more interesting dataset, it contains a timeline with the events of all the offers for each client and the amount of the transactions. If a transaction and a promo were completed in the same moment (time column), we can consider that the value in the transaction is the amount the customer has paid to complete the promotion. Here is a list of the events:

events

There are 4 possible sequences for a promotion:

table sequence promotion

It’s also important to note that in the ‘value’ column there is a dictionary containing the offers id, the amount of the transaction and the reward when an offer is completed.

3. defining the model

Before modifying the data, it’s important to define the type of model and the evaluation metrics based on the problem we are addressing: How much a customer is willing to pay based on the type of promotion and demographics.

Since we are going to predict continuous values (“how much a customer is willing to pay”) a good first approach is to use a regression model. My first attempt was a simple Linear regression model, but as you will see in the following sections, there were many outliers that affected the prediction. For this reason Support Vector Regression (SVR) seems a better fit for the job, since it finds a function representing the data within a margin of error (epsilon), where most of the points can be found inside this margin, making SVR a stronger model to treat outliers than other regression models. You can find more information about SVR in this blogpost by Tom Sharp

image taken from here

To evaluate the model we can do a simple cross validation technique taking a part of the training data to get predictions from the trained model and compare it with the actual values(y_test)

To measure how well the model fits we can measure R2 score and the Adjusted R-Squared (same as explained_variance_score in Scikit-learn). The first one is the Coefficient of Determination, which is a good measure to determine how well the model fits the dependent variables.

However, R2 score does not take into consideration overfitting, for these reason we can also calculate the Adjusted R-Squared, wish is similar to the R2 score but it takes into account the Mean of Error, that reflects the tendency of the estimator (how biased is the model). Basically If R2 score is equal to Adjusted R-Squared the mean of error is 0.00. If R2 Square is roughly the same as the Adjusted R-Squared we can conclude that the model is quite robust in this matter. The following formula was taken from this post.

R2 score = 1 — [(Sum of Squared Residuals / n) / Variancey_actual] Adjusted R-Squared = 1 — [{(Sum of Squared Residuals-Mean Error) / n} / Variancey_actual]

4. data pre-processing (cleaning and transforming data)

conceptual dataframe

To create a dataset that fits into a SVD model, all features must have numeric values. To explain the modifications made to the dataset I divided these sections in three parts, each of them explains one part of the ‘conceptual dataset’ shown above.

a . Demographics:

As we saw in data exploration, we can find the demographic information in the profile Data Frame, where have the following features:

‘id’ will help us merge the information with the other datasets.

‘age’ and ‘income’ are numerical values that don’t need to be change, for the moment.

‘became_member_on’ doesn’t give us demographics information about the user, thus we can drop this column.

‘gender’ . We have to convert these features into numerical values. To do this, we can do One Hot Encoding this feature to create three columns for each value (F , M , O).

b . Amount of the transaction (y):

To obtain the value a user paid to complete a promotion, we can extract this value from the transactions that were made. As we can see in the table below (red squares), when a promotion is completed there is a transaction at the same time that has the value that the user paid to complete this offer, thus the first step could be to merge this rows to have the amount with each completed offer.

But there is a tricky part (green square), there are some offers that are classified as ‘informational’ that does not have an ‘offer completed’ event, thus we can’t link the Informational offer with a transaction ( first time I try to create the data frame, all the transaction values were $0.0 in the informational offers type).

To get the transaction values for this type of promotion we have to check if there is a transaction during the duration of the promotion ( in the portfolio dataFrame). We can assume that the transaction made by the user after he or she sees the promotion was influenced by this offer, if it’s still active. In the example below we can see this client saw the informational offer in the hour 216, and made a transaction 6 hours after. Because this offer has a duration of 72 hours we can relate the amount of this transaction with this promotion.",https://medium.com/@arturo.gonfo/starbucks-super-promo-48c95b40c413,['Arturo Gonzalez'],2020-10-20 02:51:38.566000+00:00,905,"portfolio Data Frame, transcript Data Frame, events, table sequence promotion, Support Vector Regression (SVR)"
A New Visualization IDE for Machine Learning,"Upgrade your ML workflow from tinkering with matplotlib to automatically generating interactive charts straight from your ML training script. I’ll show you how to:

Log custom charts with just 3 lines of code Play with the code that generates the visualization Save custom presets to use from your script

Log charts in 2 minutes

In your Python script, specify a table of data to visualize. The columns can be anything you like, and you’ll use them later in the chart to specify the different features. For example, for a scatter plot you would want to specify an x and y axis, as well as an optional color axis.

To follow this tutorial, open this Google Colab and run the cells. Re-run the script multiple times to get data you can compare across runs in the dashboard.

Play with the notebook →",https://medium.com/swlh/machine-learning-visualization-ide-4242df4f1a0a,[],2020-10-16 02:28:07.675000+00:00,136,"ml, matplotlib, visualization, charts, data-visualization"
Approachable AI Applied: Defect Detection,"Defect Detection

The American Society for Quality (ASQ) suggests that the Cost of Quality is usually around 15–20% of sales, often as high as 40% in some organizations. The cost of poor quality includes both internal and external failure costs. In other words, organizations are spending a considerable amount of money correcting defects found both on the factory floor and in the customer’s hands. This not only leads to a direct monetary loss but also indirectly in the form of reputational risk.

Currently, organizations try to minimize costs through inspections and ensuring specific procedures are followed. However, these are manual, time-consuming processes that become mundane and still lead to errors.

The advent of computer vision opens up the possibility of companies automating inspections both on the line and in the field. Most public computer vision models are able to accurately classify things like faces, dogs, cars, etc. However, those models are not trained to identify the specific defects that are relevant to you and your process.

Data scientists can build custom computer vision models to fit your needs but those resources are expensive and hard to come by.

Certain vendors provide the ability for users to create their own custom models but they require the installation of a new end-to-end hardware solution.

Elipsa’s goal with Approachable AI is to allow users to build custom defect detection models with their existing cameras and software, and without the need for a data scientist.

Problem: Cost of Defects

In our example, we explore examples of a casting manufacturing product found here. Casting is a manufacturing process in which a liquid material is usually poured into a mold, which contains a hollow cavity of the desired shape, and then allowed to solidify. A casting defect is an undesired irregularity in a metal casting process. There are many types of defects in casting like blow holes, pinholes, burr, shrinkage defects, mold material defects, pouring metal defects, metallurgical defects, etc.

Product inspection is a very time-consuming process prone to human error. Defects can be cause to reject an entire order, leading to a large loss for the business.

Our data consists of photos taken from the top view of a submersible pump impeller.

The dataset contains a total 6,633 grey-scaled 300x300 pixels images.

Elipsa’s Approachable AI Applied

In the data used to build the model, we have historical images of defective examples (3,758 images) and images of quality products (2,875 examples). We also set aside a series of images to test against the model to analyze the final accuracy of prediction.

To build the model in Elipsa, we simply create a folder for each label that we are classifying. In our case, there are two labels and thus two folders: defect (def_front) and ok (ok_front). Once our defective training images and ok quality images are uploaded to their respective folders, Elipsa is able to build the computer vision model with a click of a button and no need to write code. The system learns the patterns that are indicative of normal and defective products to be able to classify future examples accurately.

Quality Example

Defective Example

Elipsa automatically splits the images into training (5,307 images) to build the model and testing (1,326) to automatically optimize the model.

Results

Computer vision models can take a considerable amount of time to train, depending on the number of images and the size of these images. Elipsa builds the model on its backend servers and notifies the user when it is complete.

With the model built, we had a total of 713 (261 quality products / 452 defective products) images to run through the model for testing. These are new images that the model has not seen before. In other words, this is the equivalent of deploying this model to production and streaming new product images against it.

Overall, the model was 99.29% accurate at predicting whether the product was defective or not.

Drilling down into those results, the model was 100% accurate at predicting that quality products are OK. For defective products, we ran 452 images against the model. The model was 98.68% accurate at detecting whether they were defects, missing only 5.

Deployment

With high accuracy received, Elipsa users can easily deploy this model to the cloud or to their own edge device with a push of a button. If the accuracy is not to their liking, users can hold off on deploying into production and simply add new images to the respective folders to try and improve the model accuracy.

Summary

We were able to build a highly accurate image classification model for defect detection without the need for a data scientist. In addition, we were able to deploy this model to production without changes to infrastructure.

The use of AI and computer vision for defect detection would allow an organization to cut down on manual processes, enabling more efficient use of the workforce. In addition, by catching defects through AI, companies can prevent defective products from leaving the factory floor; helping to eliminate recalls and buybacks and increase customer satisfaction.

For more information book a demo @ www.elipsa.ai",https://medium.com/elipsa/approachable-ai-applied-defect-detection-f7a4ea7838f4,['Jeff Kimmel'],2021-05-17 14:58:01.314000+00:00,824,"Defect Detection, Quality Control, Cost of Quality, Computer Vision, Image Classification"
Galaxy images classification — Convolutional Neural Network (CNN) explained with codes,"This is a multiclass image classification problem that uses convolutional neural network with TensorFlow (Keras api) to train on the Galaxy10 dataset.

First let’s download the dataset. In this tutorial, I’m running the code in Google Colab.

!pip install astroNN

Next we import the libraries.

from astroNN.datasets import galaxy10

from astroNN.datasets.galaxy10 import galaxy10cls_lookup import keras

from keras.models import Sequential

from keras.layers import Dense, Dropout, Flatten

from keras.layers import Conv2D, MaxPooling2D

from keras.utils import to_categorical

from keras.preprocessing import image import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import matplotlib.image as mpimg

from sklearn.model_selection import train_test_split

from tqdm import tqdm

Now, we can load the data and preprocess them. Each image has a corresponding label from 0 to 9. In order to make the computation more effectively later, we can one-hot encode each label with the to_categorical() function. In addition, we normalize the image values between 0 and 1 as a good practice to speed up the computational time.

images, labels = galaxy10.load_data() labels = labels.astype(np.float32)

labels = to_categorical(labels)

images = images.astype(np.float32)

images = images/255

Here we can take a look at some images with their corresponding labels from the training .

def show_image(image_data,label):

label = galaxy10cls_lookup(int(label))

plt.imshow(image_data)

plt.title(label)

plt.show() for i in range(3):

show_image(images[i], i)

Next we separate the training set and the test set with the train_test_split() function from sklearn.

X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size = 0.15) print(X_train.shape)

print(y_train.shape)

(18517, 69, 69, 3)

(18517, 10)

Now we also know the shape of the training set. There are 18517 images with 69 by 69 pixels with 3 (RGB) channels. The shape of the test set should be the same as the training set.

CNN model

The first layer is a convolutional layer with 32 output nodes. We also apply a 3 by 3 filter and a Rectified Linear Unit activation function in it and the input shape. Notice that the MaxPooling layer and the Dropout layer could help prevent the network from overfitting.

Finally we flatten the output data to a shape of 1D so that in the last Dense layer the model could make a choice between 0 and 9 to determine which label an image should belong to.

model = Sequential() model.add(Conv2D(32, kernel_size=(3,3),activation='relu',

input_shape=(69,69,3))) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='softmax')) model.summary()

Compile and Train the model

model.compile(

loss='categorical_crossentropy',

optimizer='Adam',

metrics=['accuracy']) model.fit(X_train, y_train, epochs=20,

validation_data=(X_test, y_test))

Result

Training and Validation loss

plt.plot(model.history.history['loss'],color='b',

label='Training Loss') plt.plot(model.history.history['val_loss'],color='r',

label='Validation Loss') plt.legend()

plt.show()

Training and Testing accuracy

plt.plot(model.history.history['accuracy'],color='b',

label='Training Accuracy') plt.plot(model.history.history['val_accuracy'],color='r',

label='Validation Accuracy') plt.legend()

plt.show()

Conclusion

From the accuracy plot we can see that the model might be overfitting since the training accuracy keeps going up linearly while the validation accuracy plateau at an early stage. However the model still has a nearly 75 % accuracy in classifying the images which is far greater than the result of random guessing (10%).",https://medium.com/analytics-vidhya/multiclass-image-classification-problem-convolutional-neural-network-trains-on-galaxy-images-6ca6aa74e5d7,['Jonathan C.T. Kuo'],2020-12-17 08:21:28.285000+00:00,424,"multiclass classification, convolutional neural network, Tensor Flow, Keras API, Galaxy10 dataset"
My Journey To Kaggle Competitions Grandmaster Status — Chapter the First,"The coveted grandmaster status

In this series I will document my journey, from start to grand finale, to becoming a grandmaster on Kaggle.com. This will be no easy feat. Indeed, I anticipate both sweat and tears, but this arduous accolade will be well worth it in the end. Wish me the best of luck, though I assure you no one is wishing louder than myself.

The list of eye candy that Kaggle provides

To that end, I will first begin by explaining what the purpose of my “journey” is. Kaggle.com is in essence an exceptionally user-friendly site that caters to Data Scientists. Within its data-fied walls include basic courses for the aspiring Data Scientist, a Job Board for those a little further along, a discussion forum with fascinating insight, banter, and computer science witicisms on all topics Data Science related, a section for Data and software engineers to create and store their own jupyter-esque notebooks, a database filled with a diverse collection of data and datasets that would make any Data Scientist drool, and the creme de la creme of our attention, the Kaggle Competitions section, location of our coveted Grandmaster status.

Kaggle competitions consist of pre-selected datasets brimming with scrumptious data to use to any Data Scientist’s heart’s content. The challenge: solve pre-determined problems through the use of data cleaning, wrangling, engineering, analysis, and predictive modeling to achieve the highest accuracy score you can and climb (claw! struggle!) your way to the top of the leaderboards, netting performance tier medals along the way.

Kaggle’s performance tier is a tier system based on one’s performance on Kaggle’s many delicacies (surprise! surprise!). Each tier is also broken up into categories allowing a determined data enthusiast to earn different tiers in each of the different categories: obviously my interest of focus is in the competitions category. Tiers are ranked as follows: Novice, Contributor, Expert, Master, and Grandmaster, with tiers Novice and Contributor requirements each focusing solely on account registration and a “tutorial” checklist respectively. From there things begin to get more difficult.

Competitions expert tier requires 2 bronze medals to be won; competitions master tier requires 1 gold medal and 2 silver medals to be won; competitions grandmaster tier requires 5 gold medals and 1 solo gold medal to be won. These medals are awarded based on positioning in the aforementioned Kaggle competitions and can be found in the picture (curtousy of the wonderful Kaggle website itself) added beneath this paragraph.

My long-winded explanation finally complete, I leave you with this promise: future additions to this “Grandmaster Status” series, which will likely come in either weekly or bi-weekly installments, will include a much more palatable ratio of data/pretty-data-science-visualizations to Daniel-ramblings. I’ll end here by thanking you and bidding you adieu. Let the games begin.",https://medium.com/@danielbensonpoe/my-journey-to-kaggle-competitions-grandmaster-status-chapter-the-first-26650fb4e713,['Daniel Benson'],2020-09-27 08:46:08.134000+00:00,455,"Kaggle, Data Science, Grandmaster Status, Competitions, Data Engineering"
How to Use Property Decorators to Prevent API Break Changes,"How to Use Property Decorators to Prevent API Break Changes

Make your APIs consistent

Photo by Aziz Acharki on Unsplash

Introduction

Some Python programmers develop packages and frameworks for others. In this case, their code includes public APIs that other developers can use. When we design the packages, it’s not always possible to foresee all the potential problems that can arise during the evolution of the technological world — nor can we foresee every pertinent business need. Oftentimes, we have to keep updating our packages, and this is when you see different versions of the packages that you may use.

Because of the open-source nature of Python, most of Python programs, including many packages themselves, rely on other Python packages. One common thing that can happen is one day your code works, but it stops working the next day after you update some dependencies for your program. Certainly, it’s why people use containers like Docker to address these dependency-related issues. But from the perspective of the package developers, we should strive to prevent API-break changes at all costs.",https://medium.com/better-programming/how-to-use-property-decorators-to-prevent-api-break-changes-ed0e8c8b4949,['Yong Cui'],2020-10-02 12:31:41.302000+00:00,173,"property-decorators, API-break-changes, Python-programs, dependency-management, package-development"
Friday Five: Beaten by the bots on Black Friday,"Zone’s Ross Basham handpicks and shares the five best new stories on digital trends, experiences and technologies…

1. Bots snapping up the stock on Black Friday

Happy Black Friday! But whether that phrase gets your pulse racing or just elicits a big sigh, there’s no doubt we’re shopping online more this year. And with in-demand items like the PlayStation 5 just released, that’s thrown the problem of bots — which can automatically buy a product the second it goes on sale — into the spotlight.

That’s why out-of-stock PS5s are available on eBay for hundreds of pounds more than the RRP. But it’s not just big-ticket electronics — everything from cuddly toys to film collectibles are seeing bots snap up the stock, while the most advanced bots were originally developed for the highly lucrative limited-edition trainer market.

2. Time to embrace a life of digital minimalism?

Should we radically simplify our digital and online lives? That was the theme explored by Professor Cal Newport in the latest edition of The Book Club, hosted by Zone and Campaign in partnership with Penguin Business. Newport is the author of Digital Minimalism, in which he explains how to take back control from our devices.

According to Newport, “a digital minimalist starts with what they care about and then asks themselves how they can best support or amplify those things using technology”. In this Campaign article Newport discusses how this concept can apply to the world of marketing, while you can catch up on The Book Club here.

3. Twitter asks users: are you sure you like this?

In the latest move to curb the spread of misinformation, Twitter will now issue warnings when users attempt to like content that has been flagged as “misleading information”. Twitter had already introduced the same warning for attempted retweets, and claims those prompts had decreased quote tweets by 29%.

It has also introduced a prompt when users try to retweet a post containing a link to an article they haven’t actually read. Will any of this actually stop Trump supporters jumping on the bandwagon when the outgoing president goes on one of his late-night rants? Probably not, but at least Twitter is making an effort.

4. Lift-off for Stannah’s ‘revolutionary’ platform

Zone has helped leading stairlift and chairlift manufacturer Stannah launch an innovative digital platform in the UK that allows customers to buy its life-changing products online. The platform distils the process of buying a new stairlift into a simple 20-minute interaction on a laptop, tablet or mobile phone.

Customers are able to find and buy a stairlift tailored to meet their needs by submitting specific information about their home, including type of staircase and number of turns, and choose from two packages. Sam Stannah, head of the Strategic Projects team, said: “This is a revolutionary move in the stairlift industry.”

5. Romantic robot pens very modern love story

As anyone who’s ever tried will attest, writing romantic fiction is harder than it looks. So when the New York Times challenged an AI language system to write for its popular ‘Modern Love’ column, you’d expect the worst, right? Well, author GPT-3 surprised many with its tender, unexpectedly romantic offering.

GPT-3’s story has everything: the meet-cute, the shared interests, the chemistry, even a casual bit of infidelity (bad AI!). OK, so it ends in an endless stream of drinks and dinner but that’s at the heart of many successful relationships, isn’t it? And it’s still a cut above 50 Shades of Grey.",https://medium.com/@thisiszone/zones-ross-basham-handpicks-and-shares-the-five-best-new-stories-on-digital-trends-experiences-4bfa54497282,[],2020-12-03 13:18:00.969000+00:00,569,"Digital Trends, Black Friday, Digital Minimalism, Twitter Warnings, Stannah Platform"
This AWS Orchestration Service allows you to be Agile!,"It is so common that a data scientist needs to create a web dashboard application or a working proof of concept. Like most of the things data scientists do, it should be done in an agile and neat way. In my last project, I was asked to create a dashboard for a system dynamic model to be used in a workshop as a proof of concept.

In that work, I created a dashboard using Django with diagrams visualised using plotly. Django has various apps for dashboard development such as Django-controlcentre, Django-dash, Django admin tools and so and so forth. You could find a list of them in this link.

Maintenance and deployment are always of main concern when you develop an application in an agile way. Practically, due to the nature of these types of works, there is not enough amount of money to be spent on deployment and maintenance. Therefore, you need a solution that allows you to deploy your application very quickly and let it go under use with the minimum maintenance!

AWS Elastic Beanstalk provides you with this solution. It is an orchestration service that allows users to deploy their applications very quickly. It is an orchestration service because it comes with connectivity and integration to many other AWS services such as EC2, S3, Simple Notification Service (SNS), CloudWatch, autoscaling, and Elastic Load Balancers. It also provides access to various AWS services such as Relation Database Services (RDS) that provides users with fully managed database services. It is fast and straightforward while it takes care of load balancing as well as scaling your application up and down automatically.

Figure 1: EB benefits. Source: AWS

You need to create an application in Elastic Beanstalk. Then you can create different environments such as development, test and production very quickly in that application. Figure 2 illustrates how Elastic Beanstalk conceptually works. Everything is fully managed by Elastic Beanstalk, which makes its use an absolute charm! Perhaps, you only need to get yourself familiar with it initially. After that, it is quite straightforward.

Figure 2: Process overview of using Elastic Beanstalk. Source: AWS

In my case, I only needed to install EB CLI and use it to initialise, create and deploy my application following the steps given in this link. It goes without saying that you may deal with some issues down the path you are creating your environment. I mention some of the common issues that you may need to cope with as follows:

You need to make sure that the allocated AWS instance has been configured in your settings.py as ALLOWED_HOSTS:

ALLOWED_HOSTS = ['127.0.0.1','xxxxx.xxx.ap-xxx-x.elasticbeanstalk.com']

More information: https://stackoverflow.com/questions/34428877/django-allowed-host-setting-for-elastic-beanstalk-instance-behind-elastic-load-b

Ensure that you add new and changed files to the staging area and commit your changes before you deploy the new version of your code. See this document for using EB CLI and Git. Remember that you need to initialise your Git rep first and commit all changes before deployment.

When you initialise your application, ensure that you are using the right version of python:

eb init -p python-3.6 sd-dashboard

Make sure that you create Django.config file in .ebextensions folder in the root directory of your Django project and not in .elasticbeans talk folder. This config file provides the EB with the required information about where Django wsgi file is located.

option_settings:

aws:elasticbeanstalk:container:python:

WSGIPath: system_dynamic_dashboard/wsgi.py

The static files should be configured as below:

STATIC_URL = '/assets/'

STATIC_ROOT = os.path.join(BASE_DIR, 'assets') # and then in django.config, you should have something similar to:

option_settings:

aws:elasticbeanstalk:container:python:

WSGIPath: system_dynamic_dashboard/wsgi.py

""aws:elasticbeanstalk:container:python:staticfiles"":

/assets/: ""assets/""

Database configuration can be done in EB console. In “Configurations”, create your database.

The database is created as part of the “AWS managed Relational Database Service (RDSA)” and you can do further configuration there. If you want to use the same database during the development, you need to make sure that the right port accesses has been given to the instance. This can be done in the AWS RDS console where you pick your database and add the required “Security group rules”. You need to have the following configuration in settings.py:

if 'RDS_HOSTNAME' in os.environ:

DATABASES = {

'default': {

'ENGINE': 'django.db.backends.postgresql_psycopg2',

'NAME': os.environ['RDS_DB_NAME'],

'USER': os.environ['RDS_USERNAME'],

'PASSWORD': os.environ['RDS_PASSWORD'],

'HOST': os.environ['RDS_HOSTNAME'],

'PORT': os.environ['RDS_PORT'],

}

}

Following link also elaborates on the deployment of a Django application into EB:

https://realpython.com/deploying-a-django-app-and-postgresql-to-aws-elastic-beanstalk/",https://medium.com/analytics-vidhya/this-aws-orchestration-service-allows-you-to-be-agile-483053770add,['Arash Kaviani'],2019-09-04 04:06:55.854000+00:00,674,"Data Scientist, Dashboard Development, Django, AWS Elastic Beanstalk, EC2"
Abridge and meeting minutes,"I came across this product called Abridge on Fred Wilson’s blog today.

It is an elegant product that lets anyone record conversations during their doctor / physician visits and within a minute after the recording ends, publishes a written transcript of the conversation highlighting key medical terms in the conversation like MRI, knee surgery, etc, for easy follow up.

These transcripts are later searchable. Meaning, if I’ve recorded five years worth of conversations with doctors, physicians, dentists, etc and I search for something related to my blood pressure or my root canal, then every relevant conversation around that turns up for an easy sort and read.

This is an exciting application of a growingly prevalent technology of speech to text and I’m keen to see this expand to other verticals.

I think there is a big opportunity for someone like a Zoom or Bluejeans to build this feature for meetings.

At the end of each meeting, it is the responsibility of someone to take notes and send out a summary of important points discussed to follow up on. And this whole task can be automated and even personalized to highlight specific terms relevant for each individual that the summary is sent out to.

The potential to scale to other verticals seems limitless. If I were a VC, I’d be betting big on Abridge or any similar product with this vision.",https://medium.com/a-good-life/abridge-and-meeting-minutes-622565b6b74c,['Kumara Raghavendra'],2019-07-19 08:43:28.323000+00:00,224,"Abridge, Speech To Text, Doctor Visits, Medical Terms, Transcripts"
Why so much talk about Decision Science?,"When I first came across this term and tried to learn about it, I was overwhelmed by the information. Everyone has picked it up and have written pages on end about it. There was one thing in common though.

We all have read many articles, blogs and seen posts of ‘decision science vs data science’. We haven’t actually tried to think why do we compare the two. There is more congruence between the two. I would say that the only main difference is that data science delivers the results and decision science helps us take calculative steps based on the results.

What if I asked you to compare the sports, chess and boxing. Many would argue chess as an activity rather than a sport but that is a discussion for some other time. Both require attention and patience and practice and pattern recognition in the opponent. Yet, they are quite different and does not require to set up a comparison between the two. The missing piece that we failed to notice was that they did not affect each other. Whereas, decision science depends a lot on data science.

Imagine this, you are a data scientist dumped with a huge database containing all sorts of data. Your superior asks you to comb through the data and provide him with your findings. We have all been there where based on certain results we can easily decide what to do. That is where the decision scientist comes in. It is not as easy as it sounds.

A decision scientist doesn’t just look at the data provided. There are factors like ‘past-experiences, variety of cognitive biases, individual differences, commitment and some more’. Data Science is a tool by which correct decisions can be made. Read more here.

Whoever said that data science is for the math majors or the computer genius in the class. Basic analysis is required in every field. You do not have to be Alan Turing to go about it. Psychology students learn distributions in their bachelors, biology majors have to understand the normal curve too. In short if data science is everywhere, so is decision science and has been so always.

Photo by Frank Vessia on Unsplash

Since we are just venturing into this field, decision science can quite easily get misunderstood for analytical fields or areas using machine learning, but it is very much a separate area.

“While data science is perhaps the most broadly used term, ‘decision science’ seems like the more fitting description of how machines are assisting business leaders in solving problems that have traditionally relied on human judgment, intuition and experience,” according to K.V. Rao, founder and CEO of sales forecasting software company Aviso, in TechCrunch. “It may not be the sexiest phrase in the world — I’ve never seen it in any marketing materials — but ‘decision science’ aptly encapsulates how computers are helping to systematically identify risks and rewards pertinent to making a business decision.”

As I have mentioned above, according to Data Science Central, “Data Scientist is a specialist involved in finding insights from data after this data has been collected, processed, and structured by data engineer. Decision scientist considers data as a tool to make decisions and solve business problems.”

Photo by Emily Morter on Unsplash

By now, we’re accustomed to the what a great deal being a Data Scientist is. The demand for Data Scientists is only skyrocketing and will be go on maybe till aliens take over. The overlooked champion of the business and technology world have just been recognized. Little do we know about them.

If people were rational, behavioral economics would be a myth. From Economics to Marketing to Psychology, all require a Data Scientist and a Decision Scientist. Intellectuals believe that the combination of two can take the business to great heights.

A data scientist just has their data in hand. They have no knowledge of client needs or promises made by the company or whether the management has some reservations. In short, they do not have a 360-degree view of the problem at hand. Regardless decision scientists have an analytical mind which helps them picture the problem and find the best solution for the company and client/customer. Hence, they are prepared for all kinds of situations.

Being rarer than data scientists, the demand for decision scientists are only to increase. The craze for data science had just started when educational institutions started providing courses. The trend is still increasing. Everyone is gearing up with the latest tools. We do not want to be left behind. Universities have already started courses for students to become decision scientists. It’s a matter of time before schools adopt some concepts.

I have made a list of as many universities as I could find which were offering the course in major countries. Now, most of these are PhD courses as decision science requires a some taste of how things run in the real world. It is a great opportunity as research is still underway.

ASIA:

IIM Bangalore, India

IIM Lucknow, India

INSEAD, Singapore(in all campuses)

CUHK Business School, China

Asia and Pacific decision science institute

EUROPE:

European decision sciences institute

University of Konstanz, Germany

IESE Business School, Spain

Rwth-aachen University, Germany

HEC, Paris

AMERICA:

Decision Science institute

The College of Penn

Carnegie Mellon University

Harvard University

Kellogg School of Management

Thank you for reading!

Do tell me your remarks.",https://medium.com/analytics-vidhya/why-so-much-talk-about-decision-science-f9d8b8bd92d4,['Ada Johnson'],2020-10-02 17:02:37.434000+00:00,864,"decision science, data science, machine learning, cognitive biases, decision making"
Finding organic clusters in complex data-networks,"A common task for a data scientist is to identify clusters in a given data set. The idea is to simply find groups of objects that have more connections or similarities to one another than they do to outsiders. In the study of networks, we use clustering to recognize communities within large groups of connections.

Typically, a force-directed layout algorithm organizes a network map, makes patterns visually comprehensible, but it cannot identify and mark the clusters. Furthermore, in large network maps, the high level of detail overwhelms our senses. To be able to precisely examine its patterns, we need quantitative views of the data contained in the network. While there are a variety of data clustering methods in machine learning, the Louvain Modularity algorithm works well particularly for large data-networks. It detects tightly knit groups characterized by a relatively high density of ties. Beyond the visual realm, you can use a Louvain clustering algorithm to partition a many million-node online social network onto different machines.

Clustering the network map of an ecosystem in Graph Commons

Once the network clusters are detected, the identified groups of nodes can be given distinct color and names, so they are clearly differentiated and together provide a summary of the larger network. We can label a cluster based on the commonalities of its nodes or the most central nodes found in the grouping.

In Graph Commons, you can use clustering on your data-networks using the Analysis bar. You first click on the “Run Clustering” button, then set the resolution of how much granular clusters the algorithm should identify. Once the clusters are found, they are automatically labelled based on the most connected node in the cluster. However, we strongly recommend that you to rename these communities yourself to highlight what these communities specify in your context. Finally, you can view the list of all the nodes that belongs to a certain cluster and download it as a CSV file.",https://medium.com/graph-commons/finding-organic-clusters-in-your-complex-data-networks-5c27e1d4645d,['Graph Commons'],2017-10-18 16:13:48.938000+00:00,320,"Data Clustering, Louvain Modularity, Network Map, Graph Commons, Force Directed Layout"
5 PEP8 Must-Remember Guidelines,"5 PEP8 Must-Remember Guidelines

Follow these tips to create effective and legible code

Whenever you are writing code, there is one thing that must always be remembered when writing — it has to be readable. Just like when you had to hand write essays in school, the content of your essay meant nothing if your teacher couldn’t read it. Everyone who codes has their own unique styles and quirks that allow them to get things done. But, if you are going to open source share your work for anyone to see, it should be consistent in its appearance and readability. In this post, I will be sharing five tips to create organized and legible code when working in Python. Let’s get started!

1.) Line Length

I want to start with this tip because it’s quick and to the point — limit each line of code to 79 or fewer characters. That’s it.

2.) Variable Names

In general, you want to try and avoid having your variable names being just one letter or number. However, there are three letters that you absolutely need to avoid if, for whatever reason, you decide you are going to use a one letter variable. Those letters are ‘O’ (can be confused with the number ‘0’), ‘I’ (can be confused with the letter ‘l’), and ‘l’ (can be confused with the number ‘1’). Were you confused as you were reading that sentence? If you were, that proves my entire point.

Also, make sure you don’t use protected words (words that turn green when spelled out) in Python as your variable names, such as list, dict, or str. Along with possibly causing confusion, it will also raise an error if you try and run your code.

3.) Indentation

This primarily applies to when you are writing functions and/or conditional statements. If your indentation is off, your code won’t work. But along with that, you want to have proper indentation if there is more than one argument required in any given function. For example, a function with good indentation can look something like this:

def fake_function(var_one, var_two,

var_three, var_four):

print(var_two) result = fake_function('var_one', 'var_two',

'var_three', 'var_four')

You can easily see that the arguments for the function are distinguishable from the action the function will perform. Sloppy indentation could look like this:

def fake_function(var_one, var_two,

var_three, var_four):

print(var_two) result = fake_function('var_one', 'var_two',

'var_three', 'var_four')

Because the indentation is off, it’s hard to tell what exactly the arguments in the function are and what the function’s action is supposed to be.

4.) Operator Placement

Similar to the indentation aspect, if you have code that uses operators — =, +, -, etc. — you want to make sure your operators are located in a logical place that is easy to read. For example, good operator placement could look like this:

income = (gross_wages

+ taxable_interest

+ (dividends — qualified_dividends)

— ira_deduction

— student_loan_interest)

The operators are clearly placed before each variable and you can easily tell what is going on, especially in line 3.

Bad operator placement would look like this:

income = (gross_wages +

taxable_interest +

(dividends - qualified_dividends) -

ira_deduction -

student_loan_interest)

Although it may be easy to understand what’s happening in lines 1 and 2, once you get to line 3 it becomes much more difficult and ambiguous to understand where this code is going. So, the general rule of thumb is to place your operators before your variables in your code so that it makes each step more coherent.

5.) Whitespace

Building back off of the operators from section, you want to make sure that you have a space in between your variable name and the operator itself. This blank space is what’s known as whitespace. Although the operator placement in the most recent example above is incorrect, there is good whitespace placement between the variables and the operators. This also applies to text operators, such as in, and, or, etc. Below is an example of some good whitespace usage:

i = 1

i = i + 1

i += 1

x = i*2 - 1

y = x*x + i*i

z = (x+x) * (i-i)

I want to point out an important exception to what I stated above in lines 4, 5, and 6 (or variables x, y, and z). Notice, for example, in line 4 that i*2 contains no whitespace. That is because it makes it clear to the reader that this is the first step in the math for that line, and that the subtraction will come second. Whichever math property(ies) have the highest priority (remember P.E.M.D.A.S!), you don’t want to include whitespace with that step.

Thank you for reading this post and I hope it helped improve the readability of your code!

LinkedIn",https://towardsdatascience.com/5-pep8-must-remember-guidelines-b8390b9ec03d,['Acusio Bivona'],2020-11-08 03:33:56.542000+00:00,745,"PEP8, Python, Code Readability, Programming"
How we calculate our Google rating,"We’ve written before on the importance of reviews and ratings. This played a big part in our development and implementation of the ability for tutors to be reviewed by clients on TutorCruncher. It is, of course, important to practice what you preach which is why we also collect reviews and testimonials from companies using us.

You might notice that if you google TutorCruncher, you may well come across something like this in Google:

This structured data adds an extra bit of gloss to search results

The reason for this is that in our website’s structured data, we are able to set up certain pages to include a rating based on a number of reviews. One might suggest, somewhat cynically, that we have pulled a number out of thin air and splashed 5 stars next to it. However, the honest souls at TutorCruncher are above such shenanigans!

This comes from data we have aggregated across the following review platforms:

Taking the mean rating of each of the review platforms (and accounting for the fact that Trustpilot scores out of 10 whereas all of the others score out of 5), we get an average rating of 4.92/5 based on 115 reviews.

Note that this data is accurate as of June 2018.

The reviews we have manually added to our structured data are designed to reflect this fact. If you were to search for TutorCruncher generally, you may well come across specific instances where Google pulls through and displays ratings from recognised review sites:

We rank #1 on Capterra for tutoring management software

Check out all of our positive feedback on TrustPilot

If you have not already, like us on Facebook

Feel free to leave us a review on Google

And of course, the reviews on Google’s dedicated reviews tool display prominently on our company’s profile:

If you search for the best tutoring management software, this is what we want you to see!

This is part of the ever-evolving world of SEO, so take a look at some of the rest of our blogs on good SEO practice for your company. As always, we’d love to hear your feedback.

Thanks for reading,

The TutorCruncher Team",https://medium.com/tutorcruncher/how-we-calculate-our-google-rating-327cdd202e2,['Sam Jenkins'],2018-08-01 14:40:28.997000+00:00,346,"tutoringmanagementsoftware, capterra, trustpilot, facebookreviews, googlesreviews"
Increase Productivity: Data Cleaning using Python and Pandas,"Increase Productivity: Data Cleaning using Python and Pandas

Data cleaning can be time-consuming, but understanding the different types of missing values, and how to deal with them, will significantly increase your productivity.

Photo by Max Duzij on Unsplash

According to IBM Data Analytics, Data Scientists can spend up to 80% of their valuable time simply finding, organising and cleaning data [1]. So, it’s natural that you want to increase productivity on data cleaning to get back to what matter most — generating insights. You will need to quickly handle a common type of messy data: missing values, also known as Not a Number (NaN).

In practice, some datasets arrive with missing data. For example, people usually don’t like sharing their income in online forms and surveys, so they will either leave it blank or input something completely unexpected. If they leave it blank, then it’s a reasonably straightforward issue.

However, values, such as ‘n/a’ or ‘ — ’, should be considered as a NaN but Pandas will not recognise them as such. These unusual types of missing values will make the process of data cleaning longer than it should, especially for beginners and entrepreneurs who are not Data Scientists themselves.

Why should you care?

In statistical terms, if the number of cases is less than 5% of the sample, then you can drop them [2]. However, messy datasets can have much more than 5%. In this case, you will need to investigate further and handle the issues yourself. If the missing values are not handled correctly, you might reach inaccurate conclusions, and potentially, waste more time by repeating your analysis.

To avoid wasting time and improve productivity, follow the data cleaning checklist for missing values below. You will benefit from one of the most important Python libraries: Pandas.

Data cleaning checklist

We will use the laptops.csv file as an example. This CSV file was adapted from the Laptop Prices dataset on Kaggle. Below is what the raw data looks like, and you will notice there is a lot o missing values.

The following work is available on my GitHub. But, before you start going through the checklist, it’s always good practice to get an overview of your dataset. Then, we will move on to our first check.

1) Standard Type

Now, let’s begin by checking standard missing values. By ‘standard’ I mean values that Pandas will immediately detect such as empty cells and NA values. You can check for standard missing values by typing the .isnull() method. It will return a boolean: True for missing values and False if otherwise.

2) Non-standard Types

Frequently, if users enter data manually (not using a dropdown menu or multiple answers), then you will find inputs other than the standard types mentioned above, ‘NA’ or blank. Some users might prefer typing ‘na’ instead of ‘NA’, it is just a matter of choice. In those cases, missing values will have a different format which we can call them non-standard missing values. Examples, such as ‘n/a’, ‘na’, or ‘ — ’ are not detected by Pandas. Take a look at indexes 7 and 8.

You can easily handle non-standard formats by creating a list and importing the file again. Pandas will then detect the new missing values. Here is how you can do that:

Once you have created a list of non-standard missing values, check what happens to indexes 7 and 8 after applying the .isnull() method.

Voilà! Pandas have recognised the different formats as missing values.

Some DataFrames can be massive, and you will not see all formats straight away. Don’t worry, as you keep working and find more types of missing values, just add them to the list. Try to detect and convert non-standard types before summarising or counting the number of missing values. Otherwise, you will develop unreliable results.

3) Random Types

As a Data Scientist, you create models to predict what can happen. However, somewhere in the process, there is human input. So, you can expect the unexpected. For example, you might find a numeric type instead of a string (‘Male’ or ‘Female’). Random types of missing values are more common than we would like. Take a look at the value at index 3. Pandas could not find it, but you can.

Random missing values are not like standard ones, we need a different approach to find them. Python is a versatile and flexible programming language, which allows you to tackle the same problem in different ways. Here is one technique you can use:

Let’s unpack the for-loop above:

First, import NumPy. The for-loop iterates over each row in the selected column, ‘In stock.’ The goal is to try and change any entry to an integer using int(row). If successful, NumPy will convert the entry to a missing value. However, if unsuccessful, Python will pass and keep going.

Second, you will need to use try and except ValueError. This is called handling exceptions, and you can read more about it by check Python’s documentation [3]. Handling exceptions alerts you if Python could not change the entry into an integer. As a consequence, it returns ValueError, and the code stops.

Finally, make sure to use the .loc method. This is Pandas’ preferred method for modifying in place entries [4].

4) Summarising

Now that you have cleaned the missing values, you are ready to summarise them. Suppose you want to check the total number of missing values for each column.

Alright, now that you have summarised the number of missing values, it is time to do some simple replacements.

5) Replacing

Sometimes it’s not possible to delete the rows with missing values. Whenever that happens, you will have to sort it out yourself. So, here are some ways to replace missing values:

Conclusion

You want to increase productivity, but data cleaning can be time-consuming. As a Data Scientist, you will inevitably deal with messy datasets containing missing values. So, keep in mind the different types of missing values (Standard, Non-Standard and Random) to quickly move forward (Summarising and Replacing) in your analysis. You will spend less time on data cleaning, avoid inaccurate inferences and become more productive.

Thanks for reading. Here are other articles you might like it:",https://towardsdatascience.com/increase-productivity-data-cleaning-using-python-and-pandas-5e369f898012,['Renato Boemer'],2020-12-30 13:41:50.725000+00:00,1000,"Data Cleaning, Python, Pandas, NaN, Missing Values"
Quantum Computing And The Meaning Of Life—Not Just ‘42’,"But what exactly is quantum computing?

To understand why it’s so incredible, one must look at the difference between a quantum computer and a regular computer. A regular computer works by switching millions of tiny transistors between 1 and 0, or “on” and “off”.

The computer can only tell each transistor to either let an electric current pass or not. There’s no other way and no in-between. So a computer has to switch through the different combinations, one by one.

First, it’s for example 1000101, then 0101101 and then 1100100. These three random numbers already represent 3 different setups and have to occur in order. The computer can not make all 3 of them simultaneously. And though coming up with these 3 will only take the computer a few nanoseconds, having to go through billions of combinations with a lot more numbers (transistors) involved, can quickly become a time-consuming effort.

A quantum computer makes use of a physical phenomenon that takes place in the still quite mysterious quantum world. A so-called “qubit”, which replaces the traditional transistor and consists of a molecule that’s deliberately spun at incredible speeds by shooting it with lasers at pinpoint accuracy while keeping it suspended in a near-absolute-zero environment, will fall into a so-called superposition.

Remember the transistor? It’s either 1 or 0. The qubit, however, can be either 0, or 1, or anything in between (meaning a little of both at the same time). It uses a quantum state, which basically means it’s everything and nothing at the same time.

To describe it really simply: Instead of having to go through the three binary number examples one after the other, a quantum computer can calculate and display all three at the same time.

Imagine the game where you put a little ping pong ball under one of three plastic cups and start switching the cups around. If you were to work like a regular computer, you’d lift them up one by one to find the ball. A quantum computer simply lifts up all three at the same time, finds the ball, and then acts as if it never lifted the two empty cups in the first place.",https://medium.com/illumination/quantum-computing-and-the-meaning-of-life-not-just-42-b1d638c6cdd0,['Kevin Buddaeus'],2020-09-06 02:46:13.153000+00:00,356,"Quantum Computing, Qubits, Superposition, Transistors, Lasers"
3 Tangible Steps in Combating Prejudice In Data Science — ClosedLoop.ai,"3 Tangible Steps in Combating Prejudice In Data Science — ClosedLoop.ai

We all want our algorithms to be fair, but sometimes, it’s tough to know how to start.

Image by Gerd Altmann from Pixabay

When we think of prejudice, we tend to think of the most obvious ways that it shows itself. The current zeitgeist has surfaced the harm done by passive acceptance of injustice. As a data scientist, you have an important responsibility in making progress toward fairness. Often, the drive will have to come from you as the practitioner. It can be difficult to know how to approach this issue. I’m going to focus on three concrete ways in which you can work towards eliminating bias from your data science decisions.

1. Measure Fairness in a Meaningful Way

Say the word “accuracy” in reference to a machine learning model, and data scientists will come out of the woodwork to explain why it’s not the right metric. They’ll continue by listing a half dozen better ways of quantifying effectiveness. It’s doubtful those same semantic warriors are as well versed in ways of measuring fairness in ML models. We want our models to be fair just like we want our models to be effective and performant. Consequently, we measure fairness. What you report about a model reflects your priorities, and measuring fairness should be a priority.

Measuring fairness in models can be challenging. In an attempt to avoid introducing bias in our models, many datasets will not include demographic information. This is a great illustration of good intentions leading to bad results. You should record demographic information, even if it’s withheld from the model. Without this information, it’s not possible to measure how your model performs across these lines. If your company is not recording this information, consider advocating on behalf of doing so.

Once you have the ability to measure fairness, you’ll need to choose the right metric. In a previous post, I discussed a metric for quantifying bias in healthcare called group benefit equality, and drilled into a somewhat technical description of why I believe it is the most appropriate measure in the context of healthcare. If you detect prejudice in your algorithm, provides a nice description of how you can use SHAP scores to start quantifying why the decision is biased in the way that it is.

2. Don’t Let Your Data Get WEIRD

Researchers in the social sciences often have issues generalizing their theories. One reason is that they conduct experiments on college students, which represents a narrow band of society. The description for these data sets are WEIRD: Western, Educated, Industrialized, Rich, and Democratic. Such a phenomenon is okay if you’re only trying to make predictions about those groups. When you try applying social theory to other parts of the world, underlying assumptions about norms break down. Most datasets we use have similar assumptions about how an individual will generate data, and can often lead to inequality.

This effect can be particularly bad in healthcare. There are pronounced differences between the way rich and poor people use health systems. For example, poorer individuals use ambulatory services to get rides to normal appointments. This is difficult to anticipate when you’ve never lacked access to a car. Oftentimes, subject matter experts (SMEs) will be keenly aware of such phenomena. Collaborating with SMEs is extremely valuable for brainstorming ways to account for such effects, particularly in the feature engineering stage.

The things that aren’t recorded by the data are an even greater challenge to deal with. Healthcare professionals know that a huge portion of what determines your health is based on your home and work environments. Just a few examples of health factors tied to wealth and zip code are access to healthy food, exposure to harmful chemicals, and easy access to green space. Accounting for these effects are difficult, but can often be the key for gaining insights to why a group measures low in fairness measures.

3. Focus on Real World Effects

One of the biggest aspects that differentiates great data scientists from the rest is their ability to understand how their model impacts the real world. The reason this is so important, is that understanding the intended use has an impact on how you define the group you make predictions about, and how you define your outcome. While focusing on intended consequences will make you a good data scientist, considering unintended consequences will make you a great data scientist.

For example, you might consider building a model to predict if a person will be hospitalized due to a heart attack. One reasonable step might be to limit your population to those individuals who have a previous diagnosis of a heart condition. Filtering the population in this way is a double edged sword. While it will make your model more precise, it will also filter out those individuals living with no previous diagnosis. If a person is getting regular checkups, it is more probable that the conditions building up to a heart attack are recorded. By excluding individuals who have unusual usage patterns, you end up building a model that will direct resources to individuals who are already getting the best care.

One of the best aspects of working on data science is that healthcare provider’s and patient’s interests align. Healthcare providers want to predict severe medical complications so that interventions can be applied. As an example, if you note that a person is a risk for a fall related injury, the intervention might be to get them a step-stool. It’s deadly simple and really effective. Win-win. Many of the known inequalities in healthcare are impactable. Actively engaging with SMEs should guide your modeling efforts to help make those efforts more effective. Your models have the potential to make people healthier, help the bottom line, and create a more equitable healthcare plan. That’s a big of win-win-win as there is.",https://towardsdatascience.com/3-tangible-steps-in-combating-prejudice-in-data-science-closedloop-ai-fb514b4d8400,['Joseph Gartner'],2020-09-24 18:39:13.878000+00:00,969,"Data Science, Prejudice, Fairness, Metrics, WEIRD"
The New Skeuomorphism is in Your Voice Assistant,"Google Home and Amazon Echo

Yay, we killed Skeuomorphism!

Not too long ago humanity left behind its skeuomorphic interfaces. We became accustomed to the idea of buttons to tap on screens and swipes that moved content right or left. We learned that content could be out of view but within reach. We graduated to a flatter, more abstract representation that still inherits spatial metaphors and relationships but that are communicated more subtly and implicitly. We stripped our visual interfaces off their ornamentations to allow a more authentic approach to visual aesthetics.

Skeuomorphism means using real world references and metaphors on interfaces to enhance their comprehensibility. A skeuomorphic button looks like a physical switch, a skeuomorphic canvas can have a wood texture.

Killing skeuomorphism made us feel very smart about ourselves. We finally don’t need glossy buttons to understand something is tappable!

Skeuomorphism is not dead

But is skeuomorphism really dead? Well, no. Skeuomorphism is alive and this time it is invisible.

The new skeuomorphism lives inside your voice assistant: Your Amazon Echo, Google Home, your phone. You call it Siri, Alexa, or Cortana.

The human assistant as metaphor

The voice assistant pretends to be a bodiless human. It speaks to us like a person. We call it names. It throws in a joke every once in a while. We attribute emotions and feelings to our voice assistant. It defines itself as female or male. The sound of a voice assistant imitates human sound and intonation.

The articulation of the metaphor of a human assistant and the way voice assistants mimic humans is literal. Just as buttons look literally like button on the skeuomorphic visual interface, the voice assistant that sounds literally like a human is a skeuomorphism.

Voice is the interface

At the heart of Alexa, Siri, and Cortana are AI-enhanced algorithms that perform searches, execute commands, and read out results. Voice is the input and output channel for these algorithms. Voice is the interface going forward with home automation, autonomous vehicles, and smart objects.

With voice as interface the topic of the psychology of robotics becomes relevant. Similar to conversational interfaces, voice can trigger an emotional response by suggesting interaction with another human being. The emotional attachment, the willingness to make the leap of faith into a metaphor is part of the skeuomorphism in the voice assistant.

Machinery as metaphors for visual interfaces

Skeuomorphic visual interfaces use references and metaphors from the physical world. References were typically made to known tactile surfaces. Such surfaces were control panels known from heavy machinery or common domestic switches. Early computer interfaces were quite literal in their use of these references.

As computer became “personal”, metaphors from the office context were introduced. The desktop metaphor is the first instantiation we had to bridge from the abstract to the familiar. When we went to more personal mobile devices with touch screens, we needed more detail in the exact behavior to make objects complete and accessible.

De-skeuomorphizing visual interfaces

People have eventually become familiarized with visual interfaces to a degree that made using literal surface metaphors obsolete. Affordability and avoidances of visual interface components are comprehensible without skeuomorphism. The definition of the word “button” has transitioned from an exclusive physical domain to a virtual one as well.

Even though stripped of skeuomorphism, today’s visual interfaces still use the physical surface as metaphor. A button still makes reference to a physical switch — just without being overly literal.

De-skeuomorphizing voice interfaces

The value of the voice assistant is not determined by its level of realism in mimicking the human. Its value is providing solutions to human problems in its role as personal assistant, knowledge source, controller, and access point to services.

There is nothing wrong with initially applying skeuomorphism in designing voice interfaces. But we will see a trend of de-skeuomorphization of voice interfaces just as we witnessed it for visual interfaces. This will happen as soon as people comprehend voice as a natural way of interfacing with products and services and have developed the corresponding behavioral authenticity. The de-skeuomorphized voice interface will be less literal in mimicking humans and be more focussed on the value it brings to humans.

Biomimicry and skeuomorphism

Skeuomorphism is a surface layer and addresses human interaction with a product or service. The product or service as such provides a solution to a human problem. Understanding the principles behind human needs and desires ultimately results in great products and services.

A reference principle that can be applied in the creation of products is biomimicry. Biomimicry manifests itself at a much deeper level than skeuomorphism and concerns the how and the why a product solves human problems while skeuomorphism is the initial and temporary literalism for human interaction with them.",https://uxdesign.cc/the-new-skeuomorphism-is-in-your-voice-assistant-3b14a6553a0e,['Bert Brautigam'],2017-05-04 04:23:17.578000+00:00,763,"Google Home, Amazon Echo, Skeuomorphism, Voice Interface, Biomimicry"
How to setup iofogctl on Raspberry Pi OS,"In a previous article I described how you can implement an intelligent edge ecosystem using iofog to demonstrate an AI oriented example on edge.

In this section we are going to prepare ourselves to deploy the previous example on a real edge device by setting up iofogctl on raspberry pi os.

to setup iofogctl on raspberry pi os we need to build it for ARM architecture and since it is developed using golang we should first setup the go sdk on raspberry.

Install Go SDK

first navigate to https://golang.org/dl/ and choose the go sdk ending in armv6l.tar.gz like https://golang.org/dl/go1.15.5.linux-armv6l.tar.gz ,

wget https://golang.org/dl/go1.15.5.linux-armv6l.tar.gz

then extract it using sudo tar -C /usr/local -xzf go.1.15.5.linuxc-armv6l.tar.gz

to persist it as your os known commands edit ~/.profile and add the following lines

PATH=$PATH:/usr/local/go/bin

GOPATH=$HOME/go

now to apply your changes use the command bellow:

source ~/.profile

to make sure whether its setup is done execute go version and it must print the go version details.

Install iofogctl

to install iofogctl you should download its tagged version source from https://github.com/eclipse-iofog/iofogctl/releases. https://github.com/eclipse-iofog/iofogctl/archive/v2.0.3.tar.gz .

now extract it using tar -xzf v2.0.3.tar.gz .

now if you do not have rice go module first go on and run sudo apt-get install golang-rice .

then go to extracted iofogctl tar folder, edit the Makefile, find export GOARCH ?=amd64 , change it to export GOARCH ?=arm and save your changes.

go on and run make build in same folder.

the script will create a bin folder putting a file named iofogctl in it. to make sure it has the right architecture run file bin/iofogctl | grep --color -i arm and it should highlight the arm word in terminal.",https://medium.com/@rouzikrm/how-to-setup-iofogctl-on-raspberry-pi-os-27ac43c91ea6,['Rouzbeh Karimi'],2020-12-03 16:44:43.129000+00:00,256,"iofogctl, raspberrypi, ARMArchitecture, GoSDK, Golang"
Taking The Next Step Towards An Intelligent Supply Chain,"In recent years, there has been an increase in the interest shown by companies worldwide for improving their supply chain systems. The advent of 2019 has also proved to be a wake-up call for several brand owners about how under-equipped their supply chain systems are in dealing with unexpected disruptions. Studies show that 30% of CFOs are prioritizing the changing of their supply chains at present. Naturally, a supply chain system in the current times can become sufficiently efficient only when the right kind of technology is incorporated into it.

Why is an intelligent supply chain necessary in the first place?

Customer outlook about a company can often make or break its performance. Even large companies have made errors in managing their supply chain, which has caused inaccurate information to be communicated to customers. Instances like these can negatively affect the reputation of a company. An intelligent supply chain can make dealing with such issues easier by recording and predicting accurate information.

Customers are also demanding for increased transparency of supply chains worldwide, so that the ethical implications of each company are made clear. A smart supply chain can improve transparency without causing any additional hassles to brands. Moreover, an intelligent supply chain can offer analytics regarding customer shopping behavior, prevent issues like counterfeiting and forgery, make business activities more time and cost-efficient, and much more. Experts predict that tomorrow’s supply chains will be connected and self orchestrated ecosystems.

The features that make a supply chain intelligent

A supply chain can be considered intelligent when it has been incorporated with technology and able to perform the following functions:

Create digital identities for products: Blockchain technology makes it possible for companies to create unique digital identities for their products, and thereby make them traceable right to their origins. Data is immutably stored using digital identities, and therefore next to impossible to tamper with.

Blockchain technology makes it possible for companies to create unique digital identities for their products, and thereby make them traceable right to their origins. Data is immutably stored using digital identities, and therefore next to impossible to tamper with. Offer real-time insights and analytics: Supply chain efficiency can be improved by using data collected from customers. An intelligent supply chain system offers real-time analytics and insights regarding product demand and customer response. In the end, customers will be subjected to a better user experience, while companies will be able to work more productively than before.

Supply chain efficiency can be improved by using data collected from customers. An intelligent supply chain system offers real-time analytics and insights regarding product demand and customer response. In the end, customers will be subjected to a better user experience, while companies will be able to work more productively than before. Enable transparency of the supply chain: Supply chain transparency is the need of the hour, as the counterfeiting and forgery cases of luxury goods are becoming increasingly common. A smart supply chain system rules out the chances of products getting duplicated or tampered with to a significant extent. Transparency of the supply chain will provide reassurance to brand owners that their authentic products are reaching customers. Customers can also confirm that their purchases are completely authentic.

Supply chain transparency is the need of the hour, as the counterfeiting and forgery cases of luxury goods are becoming increasingly common. A smart supply chain system rules out the chances of products getting duplicated or tampered with to a significant extent. Transparency of the supply chain will provide reassurance to brand owners that their authentic products are reaching customers. Customers can also confirm that their purchases are completely authentic. Enhance customer experience: Customer satisfaction regarding a company is bound to increase when there is an efficient, transparent, and trackable supply chain system in place. A smart supply chain system enables customers to track their product and to trace the product back to its origin.

Customer satisfaction regarding a company is bound to increase when there is an efficient, transparent, and trackable supply chain system in place. A smart supply chain system enables customers to track their product and to trace the product back to its origin. Improve the trackability of goods: Products might go through different stages during their life cycles. Using traditional supply chains, products may not be as trackable as companies want them to be. When products become more trackable, companies will become more efficient in managing their orders as well. An intelligent supply chain system allows companies to track their products in real-time, and make the right decisions as and when required.

How Authlink’s Brand Panel improves supply chain intelligence

Authlink’s Brand Panel is an innovative venture which allows companies to make their supply chains intelligent in several ways. Apart from making it easier to track and trace goods along the supply chain, the Brand Panel enhances the user experience for end customers as well. The Brand Panel also assigns unique digital identities for products and sufficiently manages provenance data without fail. Supply chains are made easy to maintain, monitor, and improve upon in Authlink’s ecosystem.",https://medium.com/@authlinkco/taking-the-next-step-towards-an-intelligent-supply-chain-1214ddee6751,[],2020-12-29 03:53:13.839000+00:00,832,"Supply Chain, Intelligent Supply Chain, Digital Identities, Real Time Insights, Transparency"
"You are in an Algorithmic Prison. And funny thing is, you don’t even know what it is.","Asha, one of the finest employees in her organisation, fell into a financial crisis a year back, for reasons outside her control. She missed some payments on EMI and her credit score suffered. This is making difficult for her to get a new job and is worsening her financial situation. Which in turn, is making it even more difficult for her to find a job.

This isn’t the truth, at least as yet. But you know that you are not very far from this reality. The reality where algorithms will start judging you and take more decisions for you. And before you know, you will be in an Algorithmic Prison.

We are increasingly seeing use of Machine learning algorithms in predicting multiple facets of life. Predicting if it is going to rain tomorrow is one thing, predicting whether a loan applicant will default, is an entirely different proposition. The algorithms are deciding if you should get the loan, if you are fit for a job or if you are eligible for that insurance. Although, for companies, the loss of missing out opportunity on risky propositions may not be worth it; for individuals, it could be life changing and have very high negative repercussions.

To make matters worse, through connected systems, if you are denied one service, it is likely you will be deemed unfit for multiple such opportunities.

The Bias

The algorithms study multiple data points and behavioural aspects of the past and give out a probabilistic predictions. If the past was biased, it is very likely the predictions will show that too. Thereby, possibly depriving certain regions and castes and religions of opportunities. While, we morally preach equality for all, the algorithms do not have moral values. At least not as of now.

Moreover, the algorithms make predictions while segmenting you in buckets. If on an average, people like you from the same region, company or race have defaulted, it doesn’t mean you are going to default too.

The Polarised Views

If you are consuming a kind of content. the engines show similar content to you in anticipation of increasing the click throughs. Thereby, just adding to confirmation bias of your opinions and creating polarised views. We are already seeing the impact of social media on the election campaigns. And then there are companies, who are trying hard to use those analytics and help political parties. Media is the only source of information for you, and that is highly driven by algorithms on what to show you next. The movie Inception… its already here, just not in the form as in the movie.

The Justice

Most of the algorithms often keep learning with data and make decisions on their own. This makes the systems highly opaque and difficult for a human to reason why a particular decision was made.

We have proper judiciary system in place. But if an algorithm is denying you a job or a loan, who is to be blamed for that? If it is discriminating against a race or religion, can you explain how the algorithm made the decision? Well, the justice system has to evolve. For all you know, that will also be driven by algorithms.

Privacy — Huh?

You have thought of this at least once that Alexas, Siris and Googles of the world are “listening”, to you. We have no clue how the companies are tracking our behavioural data for the purpose of providing the best recommendations and services possible.

Monika, a friend, through her privacy awareness has enabled highest security an app has to provide. But, there are services which are part of our lives now. Choosing complete privacy is not possible now. We all have to be socially connected. She has to use Maps frequently and share her location. Privacy — well, yeah!!!

Do the right thing

AI is the new hype and we are seeing new prediction/recommendation engines emerging every day. All the systems are being built for a reason and the reasons may vary from being as simple as generating more profits, to as serious as the security of a Nation. It is changing our world entirely, but the consequences may reach far beyond the original purpose.

We at CoffeeBeans Consulting, are a bunch of engineers, who do not consider data as a thing. We are working on multiple AI products, starting from recommendation engine to a recruitment platform.

We understand most of the data is about people behaviours and identity. We value and respect it with humanity, while consciously taking responsibility of the world that we want to live in.

An algorithm, a software is not good or bad in itself. It is important how we use it. It is on to us, the engineers and the organisations in this domain, to make ethical choices, take responsibility and Do the right thing.",https://medium.com/@coffeebeansconsulting/you-are-in-an-algorithmic-prison-and-funny-thing-is-you-dont-even-know-what-it-is-2aebe38b8586,['Coffeebeans Consulting'],2020-12-07 09:52:50.281000+00:00,786,"AI, Algorithmic Prison, Machine Learning, Data Privacy, Ethical Choices"
Fuzzy C-Means Clustering —Is it Better than K-Means Clustering?,"Fuzzy C-Means Clustering —Is it Better than K-Means Clustering?

Image by Mediamodifier from Pixabay

Clustering is an unsupervised machine learning technique that divides the population into several groups or clusters such that data points in the same group are similar to each other, and data points in different groups are dissimilar.

In other words, clusters are formed in such a way that:

Data points in the same cluster are close to each other and hence they are very similar

Data points in different clusters are far apart and are different from each other.

Clustering is used to identify some segments or groups in your dataset. Clustering can be divided into two subgroups:

(Image by Author), Subgroups of Clustering

(Image by Author), Hard Clustering vs Soft Clustering

Hard Clustering:

In hard clustering, each data point is clustered or grouped to any one cluster. For each data point, it may either completely belong to a cluster or not. As observed in the above diagram, the data points are divided into two clusters, each point belonging to either of the two clusters.

K-Means Clustering is a hard clustering algorithm. It clusters data points into k-clusters. To get a deep dive understanding of the K-Means algorithm, read the below article:

Soft Clustering:

In soft clustering, instead of putting each data points into separate clusters, a probability of that point to be in that cluster assigned. In soft clustering or fuzzy clustering, each data point can belong to multiple clusters along with its probability score or likelihood.

One of the widely used soft clustering algorithms is the Fuzzy C-means clustering (FCM) Algorithm.

Fuzzy C-Means Clustering:

Fuzzy C-Means clustering is a soft clustering approach, where each data point is assigned a likelihood or probability score to belong to that cluster. The step-wise approach of the Fuzzy c-means clustering algorithm is:

Fix the value of c (number of clusters), and select a value of m (generally 1.25<m<2), and initialize partition matrix U.

Calculate cluster centers (centroid).

Here,

µ: Fuzzy membership value

m: fuzziness parameter

Update Partition Matrix

Repeat the above steps until convergence.

Installation and Usage:

To implement the fuzzy c-means algorithm, we have an open-sourced Python package, that can be installed using PyPl:

pip install fuzzy-c-means

Fuzzy c-means is a Python module that can implement the fuzzy c-means algorithm. This module has an API similar to that of Scikit-learn.

Conclusion:

Fuzzy c-means clustering has can be considered a better algorithm compared to the k-Means algorithm. Unlike the k-Means algorithm where the data points exclusively belong to one cluster, in the case of the fuzzy c-means algorithm, the data point can belong to more than one cluster with a likelihood. Fuzzy c-means clustering gives comparatively better results for overlapped data sets.

References:

[1] Fuzzy Clustering Wikipedia (18 Jan 2021): https://en.wikipedia.org/wiki/Fuzzy_clustering

[2] Fuzzy c-means Documentation: https://pypi.org/project/fuzzy-c-means/",https://towardsdatascience.com/fuzzy-c-means-clustering-is-it-better-than-k-means-clustering-448a0aba1ee7,['Satyam Kumar'],2021-06-02 18:42:51.627000+00:00,436,"Fuzzy C-Means Clustering, Unsupervised Machine Learning, K-Means Clustering, Hard Clustering, Soft Clustering"
"AI Lesson for Teachers, Teens, and Everyone In Between","My goal is to outline a lesson that any teacher can use in the classroom or any person interested in a very high level understanding of how AI works can walk through. This is not meant to be an exact representation of how AI truly works, but simply give intuition as to how it works. I have been a Math, SAT, ACT, ISEE tutor for close to a decade and work in machine learning research.

Pre-requisites: know what a probability is.

There are 2 sub-lessons, 1 smaller one and 1 larger one. All lessons will be under the scope of computer vision problems — object detection.

Supervised learning vs Unsupervised learning Training a machine learning model

Machine learning problems are often broken into two categories, supervised and unsupervised problems. Supervised problems are where you give the model examples of something and then expect it to be able to predict that thing later on an unseen image. Unsupervised problems are where you have a bunch of images and you try to figure out which ones are most closely related (not based on anything except what you can see) and then group them without knowing what the final class you are trying to predict actually is.

Supervised Learning

I will now show you a series of shapes and a name for the shape.

“zhags”

These shapes above are called zhags.

“flarks”

These shapes above are called flarks.

Now I will present you with an object and you tell me if it’s a zhag or a flark. There is a hidden rule that categorizes zhags and flarks. Your job is to learn that rule.

?

This is a zhag. If you guessed that, awesome! You learned a successful model.

But maybe now you get an object that doesn’t fit exactly what you thought.

?

This is a flark.

Little did you know, the hidden rule is if the shape has any curve at all it is a flark. This is why sufficient training data is so important to machine learning problems! If this was a missing training data point in an autonomous vehicle this could cost someone their life.

Unsupervised Learning

Say we have a set of images and strictly using the images and no previous knowledge we need to place them on the xy-plane where their distance between each other represents how different they are from one another.

Here are a group of images.

Images from Wikipedia

Now we are meant to place these on the xy-plane. Here’s a possible iteration of this.

So if I now said, group these into two sets you probably would do this one of two ways.",https://towardsdatascience.com/ai-lesson-for-teachers-teens-and-everyone-in-between-7df81bd343f3,['Mike Chaykowsky'],2020-06-15 21:42:04.250000+00:00,417,"Group 1:Group 2:AI, Machine Learning, Supervised Learning, Unsupervised Learning, Computer Vision"
FrankMocap — New SOTA for Fast 3D Pose Estimation,"FrankMocap is a new state-of-the-art neural network for 3D body and hand movement recognition that was recently developed and published by researchers at Facebook Artificial Intelligence Research (FAIR).

Egocentric Hand Motion Capture. Source: FAIR Github

The model accepts video footage from one RGB camera as input. At the output, the model gives the predicted body and arm poses. FrankMocap's main goal is to make it easier to access 3D posture estimation techniques. FrankMocap processes predictions at 9.5 frames per second on inference. At the same time, the system bypasses analogs in the accuracy of predictions.",https://medium.com/swlh/frankmocap-sota-3d-pose-estimation-87b679419e74,['Mikhail Raevskiy'],2020-12-13 21:12:46.706000+00:00,93,"Frank Mocap, FAIR, 3DBody Movement Recognition, RGBcamera, Egocentric Hand Motion Capture"
Is Numerical Analysis useful to improve state-of-the-art Deep Learning architectures?,"AI Paris-based start-up, We are sharing our vision of AI, scientific research and the every day life of company builders that we are.

Follow",https://medium.com/jalgos/is-numerical-analysis-useful-to-improve-state-of-the-art-deep-learning-architectures-991508f6ff6e,[],2019-06-25 08:03:11.203000+00:00,23,"AI, Paris Startup, We Are Sharing, Scientific Research"
How TurboHire Helped WakeFit Hire 235 Candidates In 45 Days,"Bengaluru based home solutions startup WakeFit is a research and development-driven Home solutions brand, set up in March 2016. It was established with a dream to democratize sleep in India. It intends to make front line sleep products at its manufacturing plant in Bengaluru at a reasonable cost by upgrading costs through a direct-to-consumer (D2C) model.

WakeFit was hiring for the roles of customer service, support, and sales when they approached TurboHire. Here is how TurboHire helped WakeFit in easing the process of recruitment.

The WakeFit X TurboHire Success Story

The Trial period offered by TurboHire enabled Wakefit to gather all the information on how well our product has been designed to meet all their recruitment needs. They were able to take an ample amount of time to discover all the features of TurboHire and understand their value. TurboHire helped in the creation of an optimized workflow for various job requirements resulting in an interview to shortlist ratio improvement by 2.5X. A company having 3000 to 5000 employees received around 150,000 candidate profiles in a year. Out of which only 20% were probably opened. It is not humanly feasible to go through all the candidate profiles on each new job role and also required a huge amount of time and money. TurboHire’s Candidate Discovery Platform not just helped in reducing the time to hire but also significantly reduces the cost of hiring for organizations. It further helped build an organizational brand by reaching to candidates who have applied in the past for their jobs. AI-powered Referral and Careers Page pulled great candidates while ensuring the best matching of open-jobs to candidates. The Careers Page in TurboHire engages your brand by posting jobs on the page and involves employees to refer or apply internally. In TurboHire, all the referral campaigns are powered by JustDropYourResume where an employee can simply drop the resume of their candidate and complete the application. An easy-to-use reporting dashboard helped in keeping track of all the activities with numbers. The TurboHire reporting dashboard analyzes, tracks, and reports real-time data regarding the results of the company.

Result

In the first 45 days of using TurboHire for recruitment activities, Wakefit hired 235 candidates, interviewed 850 applicants, along with 8 vendors, and generated more than 1200+ applications count.

The creation of an agile method for interview scheduling for feedback incorporation during the process was introduced. TurboHire helped Wakefit create different interviewing processes for various roles to derive the best time-to-hire and improvement in the quality of hire.

Click here if you wish to read the full Case Study.",https://medium.com/@turbo_hire/how-turbohire-helped-wakefit-hire-270-candidates-in-45-days-f335c8660624,[],2021-02-06 06:02:11.316000+00:00,421,"Wake Fit, Home Solutions, Sleep Products, Turbo Hire, AI-Powered"
10 Examples to Master Pandas Styler,"10 Examples to Master Pandas Styler

Style your dataframe to make it more appealing and informative

Photo by Joshua Reddekopp on Unsplash

Pandas is highly efficient at data analysis and manipulation tasks. It provides numerous functions and methods to operate on tabular data seamlessly. However, all we see is plain numbers in tabular form.

What if we integrate a few visual components into Pandas dataframes? I think it makes them look more appealing and informative in many cases.

We can achieve this by using Style property of pandas dataframes. Style property returns a styler object which provides many options for formatting and displaying dataframes. A styler object is basically a dataframe with some style. In this article, we will go through 10 examples to master how styling works.

We will use a customer churn dataset which is available on Kaggle and also create some sample dataframes.",https://towardsdatascience.com/10-examples-to-master-pandas-styler-408ea794e91,['Soner Yıldırım'],2020-12-30 13:38:40.830000+00:00,140,"pandas, styler, dataframe, visualization, data-analysis"
I finished Andrew Ng’s Machine Learning Course and I Felt Great!,"Just finished Andrew Ng’s Machine Learning course on Coursera, and it’s GREAT! Here some thoughts and observations:

What’s great about it

A well-designed learning curve

EVE Online game’s (in)famous crazy learning curve

This is especially important for people that never heard of Machine Learning. Not assuming the student has any prior knowledge and gradually guide them through complex concept makes the learning experience challenging yet still fun.

2. Avoid complex math, yet find a way to still enable the student to do ML

The Andrew Ng ‘Silent Protector’ Meme

Maybe the biggest fear for people want to get into Machine Learning and AI is ‘I’m not a math person’. Being able to not getting into too much math yet still explain clearly the concept is invaluable, especially for totally green guys.

3. Octave/Matlab is more ‘math’ like, less digression on programming language itself

Some people might not agree with me on this. Yes, Octave/Matlab doesn’t have all the fancy libraries like scikit-learn and Pandas, yet it’s very expressive when it comes to representing math equations. Transfer equations from class to Matlab code is easier than Python IMHO.

4. Cover most popular models, a good foundation

Linear/Logistic Regression

SVM

Neural Network

Collaborative Filtering

Anomaly Detection

K-Means

PCA

With all these algorithms/models under your belt, you are ready to solve a lot of problems with Machine Learning.

5. Provide practical ML projects knowledge, not only algorithm and programming

Besides theory, the course also offers very practical Machine Learning project knowledge, hot to build a pipeline, how to structure the problem solving, etc.

6. Well designed quizzes and assignments, as part of learning too

I was always amazed by how well the quizzes and assignments are designed. They are challenging, yet with some efforts achievable, and at the same time offer some new perspective on the lesson. I always learned a few new things doing those and totally enjoyed them.",https://medium.com/datadriveninvestor/thoughts-on-andrew-ngs-machine-learning-course-7724df76320f,['Michael Li'],2019-09-27 14:54:56.859000+00:00,295,"Machine Learning, Andrew Ng, Coursera, Linear Regression, Logistic Regression"
How to find an entry level data science job starting from zero?,"How to find an entry level data science job starting from zero?

Is becoming a data scientist with no coding background possible?

Data Science is a great, intellectually challenging career. Primarily Data Scientists are tasked with:

cleaning and preparing data sets

analysing data sets looking for patterns

extracting actionable insights from data

They tend to work both with the management (communicate insights), the internal team (looking for more data) as well as external clients (understanding what accuracy is needed).

How to find an entry level data science job?

In recent years data science went from a niche domain to an essential component of any larger business. The need for data scientists grew and after some time also the number of data scientists.

We’re currently past the time of easy entry to the data science market. There literally hundreds of applications for junior data science positions. So if you’re motivated primarily by a good salary ($100,000+ in the US at more senior positions), then think again. You’ll be competing with passionate people willing to work a lot towards their goals.

However, if you love solving problems, if you’re not scared of intellectual challenges and have enough persistence, then don’t worry. It might take some time to become a junior data scientist but you will get there. Be patient and code your way to success.

In order to find an entry level data science job you will need:

good knowledge of Python — enough to import and manipulate .csv files, scrape the web and connect to various APIs.

— enough to import and manipulate .csv files, scrape the web and connect to various APIs. command of prediction, classification and clustering algorithms — basics of various regressions, KNN or k-means.

— basics of various regressions, KNN or k-means. Github portfolio — presentation of your past projects on Github to show what you were able to do.

— presentation of your past projects on Github to show what you were able to do. up to date LinkedIn profile — a good online presence with clear statement of your previous education and workplaces is also a must. Be transparent.

Then it’s time to do some research on jobs available in your region (or elsewhere, if you plan on working remotely) and start applying.

A good way to start is also working on Upwork or Fiverr. These two services connect freelancers with companies. You’ll be able to apply for particular projects, from simple Django app to complex machine learning problems related to actual business. It might also in hiring you full-time later on.

Finally, Upwork and Fiverr will also show you what kind of skills are looked after: what algorithms, what technology stack, what kind of projects. This knowledge is invaluable to navigate the data science market.

Data Science Job course

So you’ve decided that you want to become a data scientist but don’t know where to start?

Or you have finished one or two courses on Coursera, read a couple of books and don’t know where to go from there?

Or you’ve started applying for data science jobs, but you stress about doing something wrong?

Don’t worry.

If this is the case, then my Data Science Job course is for you.

I have prepared it with an idea to connect the (data science) dots. There are plenty of data science courses online which can teach you about algorithms and plenty of books with great material, however, there are not enough materials about the process of becoming a data scientist:

what courses to take,

what books to read,

what algorithms to learn,

how to look for a job,

how to prepare for an interview.

This is what the Data Science Job course is about. It’s about giving you the knowledge and confidence to find your first data science job.

It’s a live course, which means I regularly add new video and expand previous materials. Pay once, learn for life.

And on top of everything, we have a Facebook group for our students to exchange thoughts and struggles and help each other.",https://medium.com/data-science-rush/how-to-find-an-entry-level-data-science-job-starting-from-zero-f3e4be9c0541,['Przemek Chojecki'],2020-09-17 07:58:09.190000+00:00,642,"data science, entry level jobs, Python, prediction algorithms, classification algorithms"
Gettier Problems in Machine Learning,"Gettier Problems in Machine Learning

Have a look at one of the most prominent philosophical puzzles through the lenses of machine learning.

Gettier Problems

If you haven't heard the term already, here’s the gist of it. The Classical Account of Knowledge suggests the following:

One can know a proposition is true, only if:

1) The porposition is true;

2) One believes the proposition;

3) and One’s beliefs are justified.

According to this view of knowledge, knowledge is just justified true belief. That every time you come to a conclusion q through p implying it, p needs to be true and justified so as to lead to it. This justification needs to satisfy the anti-luck intuition and the ability intuition. The anti-luck intuition simply states that your knowledge isn’t just out of luck that you’ve got it right while e the ability intuition states that this knowing was down to your cognitive abilities.

Now, this account of knowledge is plausible. But around the 1960s, Edmund Gettier got to disproving the basis of this account of knowledge.

The Stopped Clock Case

This was stated by Bertrand Russel to prove a different point but we shall use this example here. Let us say you get up one morning and look at the time on your clock. This clock is very reliable and there is absolutely no reason you’d doubt that the clock is showing you the wrong time. Also, the time shown by the clock corresponds to what time you might take it to roughly be.

But here’s the problem. That clock stopped working 24 hours ago exactly. The moment you’re looking at it, it just so happens that it is showing you the correct time. This is pure luck. But is the inference you make of the time actually knowledge? Well, one would argue that a clock that isn’t working can never show you the correct time and how you got the proposition of the time correct was just luck attacking the anti-luck intuition. So, you have a justified true belief that doesn’t satisfy knowledge.

Another case could be, looking at the above image of a clock and imagining whether it is one of a stopped clock or a working one.

The Sheep Case

This example is from the American philosopher Roderick Chisholm. Imagine a farmer looking at a field in clear daylight and seeing what looks like a sheep. This is a justified true belief.

Now, imagine that we set the case such that the farmer is not looking at a real sheep but a sheep-shaped object and there really is a sheep behind the view of a sheep-shaped object. That means the farmer has a true belief but what he is looking at isn’t a sheep but a sheep shaped object.

Generalizing Gettier Problems

Step 1. Take a belief that is formed in such a way that it would result in a false belief but is justified nonetheless.

Step 2. Make the belief true, albeit for reasons that have nothing to do with the subject’s justification of the case.

The Solution

People thought the solution to Gettier cases would be quite simple. Maybe just add something onto the tripartite analysis or negate Gettier completely.

Keith Lehrer suggested that we add in a fourth clause saying that your belief shouldn’t be based on any false assumptions. But there are problems with this. In the clock case, it simply isn’t psychologically plausible that you start to question whether the clock is working or not. And what are even assumptions?

The definition of an assumption can not be so narrow so as to assume that the clock is working or so broad that genuine knowledge shall be excluded.

Another solution was by Lukasz Lozanski, who completely negates the Gettier problems saying that assumptions made were far too specific to generalize conclusions we reached at. And thus, the knowledge we concluded with was either not justified or false. Or the fact that there could be logical flaws in the case stated in his article here.

With all that kept in mind, I don’t think Gettier cases are so far-fetched or logically flawed so as to cease all existence. And hence, I shall present my own and the problems that follow.

The Machine Learning Gettier Case

When it comes to ML we have one basic problem we try to solve: Character Recognition. It’s quite simple, you design a neural network that classifies all the Unicode characters there can possibly exist. Let’s say you write two really similar characters on two separate parchment paper. These two: ⻳ and 龟. This is the 2nd last example on this site. The first Unicode character has a description different from the second, so technically they are two different characters. Now, you keep the incorrect character’s butter paper in front of the correct character. The answer classified by the algorithm is of the correct character even though, it’s supposed to be of the incorrect character.

How would you justify correct answers that have false assumptions to a machine? This is something above hoaxes because otherwise, you can just completely negate the case stating that it is completely untrue. In this case, we can’t alter the weights leading to a supposedly correct solution.

To me, this example poses a problem to machine learning algorithms, not ones that just classify but in the real world, this example exemplifies. When robots are supposed to make inferences that might be correct because they are justified and true but for the fact that in this case aren’t seemingly. If this continues, it’ll become a barrier in the way machines learn.",https://medium.com/swlh/gettier-problems-in-machine-learning-8f49a6a0e2c7,[],2020-10-26 03:09:29.017000+00:00,906,"Gettier Problems, Machine Learning, Classical Account of Knowledge, Stopped Clock Case, Sheep Case"
A Guide to Automating Your Stock Analysis With Python,"Collecting Historical Stock Data

Obtaining historical data on the stocks that we want to observe is a two-step process. First, we must narrow down a list of stocks. Then, we have to make individual calls to the Yfinance API in order to import data about each company.

Choosing the Stocks We Want to Target

The library get-all-tickers allows us to compile a list of the stocks that fit the criteria we set. Currently, the library supports filtering stocks by their region, sector, market cap, and exchange. For this example I am looking at companies that have a market cap between $150,000 and $10,000,000 (in millions). You will notice that I also included a line of code to print the number of tickers we are using. This is very important. You will need to be sure that you are not targeting more than 2,000 tickers, because the Yfinance API has a 2,000 API calls per hour limit.

Obtain a list of stocks to analyze.

I would recommend using this library because it allows for our list of observable stocks to be dynamic. If it better suits your analysis though, I provided an alternative line of code that allows you to type in the stocks you would like to have the program analyze.

Obtaining and Saving Historical Data

Utilizing the Yfinance API, we will be able to access and save the historical stock data for every element (stock) in our “tickers” list. This library gives us access to the data Yahoo Finance provides for free to the public. If you’re looking for other information about a stock (not historical data), check out their API.

The code below will cycle through our list of stocks obtained in the section above. For each stock it will make a call to Yahoo Finance to obtain its historical data. If it fails it will retry up to five times for each stock or until it has made 1,800 API calls. When it successfully retrieves the data from Yahoo Finance, it will save it as an individual CSV file in the folder called “Stocks.” In order to analyze each stock, we must first save their data to a central folder. In my example I am calling this folder “Stocks.” Lastly, every time the code runs it will remove and recreate the folder “Stocks” to ensure that it is not considering data from the previous day in its analysis.",https://medium.com/automated-trading/a-guide-to-automating-your-stock-analysis-with-python-4b6929e54201,['Cameron Shadmehry'],2020-08-17 14:54:59.029000+00:00,393,"stockdata, YfinanceAPI, collectinghistoricaldata, marketcap, stocksanalysis"
Extracting selected data from a single page using lxml.html.xpath,"Fast and Simple ways to learn scrapping!

In this exercise, we will use XPath to collect information from the provided URL and use lxml. But, before we start it, please refer to the XPath and CSS selectors using DevTools. This article

Here is the little documentation about XPath expression:

Part 1

Part 2

Part 3

Part 4

We’re going to use this link for doing a “web scrapping”:

First, we have to install the module in our environment, if you are using macOS try to install through terminal, then we can import the module through IDE. It depends on you either using jupyter notebook, pycharm or VScode.

Step-1

A musicURL string object contains a link to the main page. musicURL is parsed using the parse() function, which results in the doc object which has lxml.etree.ElementTree object type.

Let’s get a base element through <article>

Get the <article> path

Make a new object as articles and put the <article> path into cell by using xpath() function and do not forget to add [0] at the last of code cause we’re just get the first area of <article> tags. The XPath for the articles posseses all of the fields that are available inside , such as title, price, availability, imageUrl, and starRating.

First <article> path

After this, we have to set up the individual expression. If we are going to get information about title, price, availability and imageURL, we should to declare the individual XPath expression such as:

Set up Individual XPath Expression

Check the result by printing each result object. There is still messy data on availability, imageUrl and starRating object. So let’s try to clean up the data

Checking

Cleaning Process

So, here is the final step, we are going to make a DataFrame after doing a “Web Scrapping” using lxml.html and XPath.

Make a list of number

DataFrame from Web Scrapping

This article is used to fast learning documentation about web-scrapping using lxml.html and XPath. Hope u enjoy it!",https://medium.com/@mulianaraul/extracting-selected-data-from-a-single-page-using-lxml-html-xpath-975bcbde9e0c,[],2020-12-19 16:16:56.703000+00:00,307,"web-scrapping, lxml.html, XPath, lxml.etree.Element Tree, Data Frame"
Machine Learning Optimization Methods and Techniques,"Top Optimization Techniques in Machine Learning

Now let us talk about the techniques that you can use to optimize the hyperparameters of your model.

Exhaustive search

Exhaustive search, or brute-force search, is the process of looking for the most optimal hyperparameters by checking whether each candidate is a good match. You perform the same thing when you forget the code for your bike’s lock and try out all the possible options. In machine learning, we do the same thing, but the number of options is usually quite large.

The exhaustive search method is simple. For example, if you are working with a k-means algorithm, you will manually search for the right number of clusters. However, if there are hundreds or thousands of options that you have to consider, it becomes unbearably heavy and slow. This makes a brute-force search inefficient in the majority of real-life cases.

Photo by the author.

Gradient descent

Gradient descent is the most common model optimization algorithm for minimizing error. In order to perform gradient descent, you have to iterate over the training dataset while readjusting the model.

Your goal is to minimize the cost function because it means you get the smallest possible error and improve the accuracy of the model.

Photo by the author.

On the graph, you can see a representation of how the gradient descent algorithm travels in the variable space. To get started, you need to take a random point on the graph and arbitrarily choose a direction. If you see that the error is getting larger, that means you chose the wrong direction.

When you are not able to improve (decrease the error) anymore, the optimization is over and you have found a local minimum. In the following video, you will find a step-by-step explanation of how gradient descent works:

Looks fine so far. However, classical gradient descent will not work well when there are a couple of local minima. When finding your first minimum, you will simply stop searching because the algorithm only finds a local one. It is not made to find the global one.

Photo by the author.

Note: In gradient descent, you proceed forward with steps of the same size. If you choose a learning rate that is too large, the algorithm will be jumping around without getting closer to the right answer. If it’s too small, the computation will start mimicking exhaustive search take, which is, of course, inefficient.

Photo by the author.

So you have to choose the learning rate very carefully. If done right, gradient descent becomes a computation-efficient and rather quick method to optimize models.

Genetic algorithms

Genetic algorithms represent another approach to ML optimization. The principle that lies behind the logic of these algorithms is an attempt to apply the theory of evolution to machine learning.

In the evolution theory, only the specimens that have the best adaptation mechanisms get to survive and reproduce. How do you know which specimens are and aren’t the best in the case of machine learning models?

Imagine you have a bunch of random algorithms. This will be your population. Among multiple models with some predefined hyperparameters, some are better adjusted than the others. Let’s find them! First, you calculate the accuracy of each model. Then, you keep only those that worked out best. Now you can generate some descendants with similar hyperparameters to the best models to get a second generation of models.

You can see the logic behind this algorithm in this picture:

Photo by the author.

We repeat this process many times, and only the best models will survive at the end of the process. Genetic algorithms help to avoid being stuck at local minima/maxima. They are common in optimizing neural network models.",https://medium.com/better-programming/machine-learning-optimization-methods-and-techniques-56f5a6fc5d0e,[],2020-12-07 21:19:18.359000+00:00,597,"Optimization techniques, machine learning, hyperparameters, exhaustive search, gradient descent"
DEEP FAKES: The Most Serious Threats in AI,"A Deep fake is basically a type of syntactic media that is artificially created by using the self-learning algorithms of artificial intelligence. So basically a deep fake could be an audio clip, a video footage or an image that has been produced synthetically or artificially via the usage of artificial intelligence. Through this technology, you can swap phases in a video or in an image, you can carry out lip synching and you can modulate the voice in an audio clip. So this technology of creating deep fakes or synthetic media in a world driven by cloud computing, artificial intelligence and vast volumes of data represents tremendous opportunities and as well as challenges. So the ability to create substantive media using A.I. creates numerous opportunities because it will simply transform the way we listen, speak or communicate on social media.

But however, it poses numerous challenges as well, because deep fakes can be easily misused to promote fake news, to propagate disinformation and propaganda. Such deep fake audio clips, videos and images can be easily misused to manipulate individuals and as well as institutions, because through such deep fakes once reputation could be easily destroyed by synthetically making the individual say unethical or controversial statements.

Through deep fakes extremist propaganda can be carried out on the Internet, which could be used to polarize the society and to eventually source social discord that could lead to communal riots as well. These fake videos could be easily used to damage one’s reputation, especially popular individuals such as celebrities, politicians, etc.. Through such disinformation and manipulation, violence can be easily incited in the society and hence the possible misuse of deep fakes has emerged as a major security challenge.

Threats in Deep Fakes

Now, imagine a deep fake video of a popular religious leader or a political leader who seemed to be making controversial statements against other communities. Such manipulative videos can easily instigate riots and violence, thereby threatening security and stability in the society.

Deep fakes also pose a specific threat to women because by using deep fakes, synthetic pornography could be generated in order to malign and destroy the reputation of women. This could also become a tool for blackmail, through which the individuals could be exploited and harassed further.

Then more dangerously, deep fakes can be easily used to disrupt democratic elections and to instigate revolt against governments and as well as to sow discord and confusion in the minds of the voters, thereby disrupting the electoral process.

Then another alarming possibility is that deep fakes could be used to counter truth and facts and replace it with fake news and misinformation.

And the real threat is that these deep fakes could be easily weaponized by a nation state for geopolitical reasons. Through a well designed campaign Intelligence agencies could wage a psychological war by promoting a negative campaign against a particular government, by disrupting elections in a target country, by trying to swing voter preferences in favor of a particular candidate or a certain party, thereby compromising the national security of the target country.

In a world which is driven by the Internet and social media fake news tends to travel faster and gain more acceptance as compared to the truth. That being the case, the potential misuse of deep fakes generated through artificial intelligence could further enable the spread of fake news and propaganda, thereby threatening the national security of a country, societal order in a particular country and as well as an individual’s reputation or an organization’s reputation.

Conclusion

So I conclude that this has to be the most serious threat that has emerged from artificial intelligence, and hence there is an urgent need to come up with solutions to counter deep fakes. Therefore, we need a multi stakeholder and multi-modal approach through which all the stakeholders can be brought together and through collaborative exercise, the governments, the tech firms, the civil society, the common public and more importantly, the media could be brought together to design solutions to counter deep fakes.

It would be the responsibility of the government and the legislature to come out with appropriate legislative regulation that can not only regulate the creation and spread of deep fakes and also prohibit them and provide for suitable action against the perpetrators. So appropriate policies will have to be brought on by the government and even the tech firms, especially the social media platforms, they need to come up with appropriate policies to flag deep fakes and also to take suitable action against them.

The tech firms also have a responsibility to come out with suitable technological interventions through which deep fakes can be easily detected, thus helping in preventing the spread of deep fakes.

Then more importantly, we need to focus on media literacy wherein the consumer and as well as the journalists and the media organizations are made aware of the impact of deep fakes. And they should be trained as to how such deep fake videos, audios and clippings can be identified.

So there has to be greater awareness amongst the common public and as well as amongst the media organizations so that they can immediately dismiss deep fake, thereby curbing its spread and impact. So considering the emergence of this threat, what we need is a critical consumer, especially when we are consuming vast volumes of media on the Internet. Every time we watch a video or look at an image or listen to an audio clip, we need to pause and question ourselves whether what I’m watching is authentic and genuine. And by following a basic set of precautions, one should be able to identify such deep fakes and then ensure that such deep fakes do not gain legitimacy.",https://medium.com/illumination/deep-fakes-the-most-serious-threats-in-ai-ce9943409c03,['Vishnu Aravindhan'],2020-11-18 13:56:11.450000+00:00,944,"Deep Fakes, Artificial Intelligence, Synthetic Media, Fake News, Disinformation"
Python Dash vs. R Shiny — Which To Choose in 2021 and Beyond,"Creating UI elements

Let’s continue our comparison by taking a look at UI elements. The goal is to create the same form-based application in both Dash and Shiny.

The application should be used to filter job candidates by level, skills, and experience. It also allows you to specify additional points of interest.

Let’s start with Python’s Dash. All of the core UI components are available in the dash_core_componenets library. The convention is to import it abbreviated as dcc . Other imports and boilerplate remain the same.

Here’s the code for a simple form-based application:

import dash

import dash_core_components as dcc

import dash_html_components as html



app = dash.Dash(__name__)

app.layout = html.Div(children=[

html.H1('Heading 1'),

html.Label('Filter by level:'),

dcc.Dropdown(

options=[

{'label': 'Junior', 'value': 'junior'},

{'label': 'Mid level', 'value': 'mid'},

{'label': 'Senior', 'value': 'senior'}

],

value='junior'

),

html.Label('Filter by skills:'),

dcc.Dropdown(

options=[

{'label': 'Python', 'value': 'python'},

{'label': 'R', 'value': 'r'},

{'label': 'Machine learning', 'value': 'ml'}

],

value=['python', 'ml'],

multi=True

),

html.Label('Experience level:'),

dcc.RadioItems(

options=[

{'label': '0-1 years of experience', 'value': '01'},

{'label': '2-5 years of experience', 'value': '25'},

{'label': '5+ years of experience', 'value': '5plus'}

],

value='25'

),

html.Label('Additional:'),

dcc.Checklist(

options=[

{'label': 'Married', 'value': 'married'},

{'label': 'Has kids', 'value': 'haskids'}

],

value=['married']

),

html.Label('Overall impression:'),

dcc.Slider(

min=1,

max=10,

value=5

),

html.Label('Anything to add?'),

dcc.Input(type='text')

])





if __name__ == '__main__':

app.run_server(debug=True)

And here’s the corresponding application:

Image 5 — Simple form-based application in Python’s Dash

Yeah, not the prettiest. Dash doesn’t include too many styles by default, so you’ll have to do it independently.

Let’s replicate the same application with R and Shiny:

library(shiny)



ui <- fluidPage(

tags$h1(""Heading 1""),

selectInput(

inputId = ""selectLevel"",

label = ""Filter by level:"",

choices = c(""Junior"", ""Mid level"", ""Senior""),

selected = c(""Junior"")

),

selectInput(

inputId = ""selectSkills"",

label = ""Filter by skills:"",

choices = c(""Python"", ""R"", ""Machine learning""),

selected = c(""Python"", ""Machine learning""),

multiple = TRUE

),

radioButtons(

inputId = ""radioExperience"",

label = ""Experience level:"",

choices = c(""0-1 years of experience"", ""2-5 years of experience"", ""5+ years of experience""),

selected = c(""2-5 years of experience"")

),

checkboxGroupInput(

inputId = ""cbxAdditional"",

label = ""Additional:"",

choices = c(""Married"", ""Has kids""),

selected = c(""Married"")

),

sliderInput(

inputId = ""slider"",

label = ""Overall impression:"",

value = 5,

min = 1,

max = 10

),

textInput(

inputId = ""textAdditional"",

label = ""Anything to add?""

)

)



server <- function(input, output) { }



shinyApp(ui, server)

Here’s the corresponding application:

Image 6 — Simple form-based application in R Shiny

As you can see, Shiny includes a ton more styling straight out of the box. Shiny applications look better than Dash applications by default. However, who wants their apps to look “default” anyway? We’ll cover custom styling in the next section.

Winner: R Shiny. You can create a better-looking application with less code.",https://towardsdatascience.com/python-dash-vs-r-shiny-which-to-choose-in-2021-and-beyond-6a25c90b2811,['Dario Radečić'],25-03-2021 00:00,348,"UI elements, Python Dash, R Shiny, Form-based Application, Job Candidate Filtering"
K-Means Clustering from Scratch in 5 lines of code (Python),"In this short post, I will walk you through implementing the K-means clustering algorithm from scratch in the most efficient way.

The post assumes that you already know the theory of the algorithm.

Let’s start by creating some short dataset to use:

import numpy as np

import random

import matplotlib.pyplot as plt

import pandas as pd group1 = [(random.randint(0, 100), random.randint(0, 100)) for i in range(50)]

group2 = [(random.randint(150, 250), random.randint(250, 350)) for i in range(50)]

I created two distinct groups so it’s clear for us to see the clusters. Let’s plot them and see:

x_data = np.concatenate([np.array(group1)[:, 0], np.array(group2)[:, 0]])

y_data = np.concatenate([np.array(group1)[:,1], np.array(group2)[:, 1]])

all_data = np.concatenate((x_data.reshape(-1, 1), y_data.reshape(-1, 1)), axis=1)

plt.scatter(x_data, y_data)

Cool. We have two distinct groups that should be easily clustered.

Now, let’s start with the first step in the K-Means algorithm: initialize the clusters randomly. From the plot, we know we need two clusters. So let’s pick two random coordinates for them:

k = 2

centroids= [(random.randint(min(x_data), max(x_data)), random.randint(min(y_data), max(y_data))) for i in range(k)]

Let’s take a look at them:

plt.scatter(x_data, y_data)

plt.scatter(np.array(centroids)[:, 0], np.array(centroids)[:, 1], c= [i for i in range(len(centroids))], s=200)

Looks good. We have the two groups and the two clusters that were initialized randomly.

Now, the second step in the algorithm is to calculate the distance between each point in the dataset and each of the clusters. Let’s do that using NumPy:

distances = np.concatenate([np.linalg.norm(all_data - centroid, axis=1).reshape(-1, 1) for centroid in centroids],

axis=1)

Notice that in the code above I wanted to generalize the code to any number of centroids. That’s why I applied the function np.linalg.norm over the entire dataset and all the available centroids and then I just concatenated all these side by side. That will provide us with K values for each point in the dataset where the ith value represents the distance between that point and the ith cluster. Now, all we need to do is just to take the minimum of each of these values and assign the point to the label that corresponds to the index of the value (which corresponds to the centroid). Let’s do that:

labels = np.argmin(distances, axis=1)

Now, we already assigned the labels of each point in the dataset to the nearest cluster to it. Let’s take a look at them:

Looking good. The next step is to change the coordinate of the cluster to be the mean of the points that have their labels. This step can be implemented in various different ways. However, I’m determined not to use for loops and be as efficient as possible. So I will do it using Pandas. I will create a panda data frame using the data and the labels. Then, I will just group all values by their labels and get their mean. That should do the trick in 1 line of code and it’s super-efficient:

pd.DataFrame(np.concatenate([all_data, labels.reshape(-1, 1)], axis=1)).groupby(2).mean().values

Note that the above line of code provides the mean of all the data points that have the same labels. Since we have K labels, then it should give K coordinates. These are the new coordinates for the K centroids:

centroids =pd.DataFrame(np.concatenate([all_data, labels.reshape(-1, 1)], axis=1)).groupby(2).mean().values

That was the final step. All we need to do is just repeat that step a few times until convergence happens:

for i in range(100): distances = np.concatenate([np.linalg.norm(all_data - centroid, axis=1).reshape(-1, 1) for centroid in centroids],

axis=1)

labels = np.argmin(distances, axis=1)

centroids =pd.DataFrame(np.concatenate([all_data, labels.reshape(-1, 1)], axis=1)).groupby(2).mean().values

and here are the results:

Now, as promised let’s put all of that code together to get the algorithm in 5 lines of code:

Let’s now run the algorithm on three groups instead of two:

centroids, labels = scratch_k_means(3, all_data)

Let’s plot the results:

plt.scatter(all_data[:, 0], all_data[:, 1], c=labels)

Looking great. Let’s try to run K-Means from sklearn on the same dataset and compare the two results:

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, random_state=0).fit(all_data)

Let’s print the coordinates of the centroids of both:

The coordinates of the centroids from the two algorithms are identical as expected.

Hope this was helpful. Find the 5-lines Kmeans code here",https://medium.com/@mohamed-gaber/k-means-clustering-from-scratch-in-5-lines-of-code-python-6250df0b5881,['Mohamed Gaber'],2021-04-25 10:20:28.892000+00:00,631,".K-means clustering, K-means algorithm, sklearn, clustering algorithm, Num Py"
Implementation of AI in the Security Industry: The Basics and More,"The idea behind artificial intelligence isn’t new by any means. We’ve seen it in popular culture, search engines have been using it for more than a decade, and the algorithms powering modern software heavily leverage AI and automation.The security industry is one industry that is feeling the effects of AI, from cryptographic algorithms that create strong passwords to data annotation processes that clarify security footage. AI isn’t just a big idea or a nice-to-have — today it’s a must-have.In this article, we are going to explore the relationship between data annotation and the security industry.

The Traditional Approach to Security: Video Footage

If you are a homeowner, or you own a dashcam, you can agree that it is tedious to view hours of grainy black and white footage. Multi-camera systems are even more of a hassle. Replaying the same event on tape repeatedly, trying to find out which objects are missing and which facial features the perpetrators are showing, is time-consuming.

Luckily, with the following forms of data annotation, you can view security footage without spending as much time doing so.

Common Types of AI Data Annotation

Detect Objects with Bounding Boxes

Bounding boxes display a colored box over an object of interest. You can choose to display bounding boxes over bags, faces, or any other image in a photograph or video feed. By covering items in boxes, you make them easier to track, saving a lot of time when reviewing your footage.

Instance Segmentation: An Advanced Form of Bounding Box Object Detection

Instance segmentation is similar to bounding boxes but much more precise. While bounding boxes cover an object in a square or rectangle, regardless of the object’s actual shape, instance segmentation limits itself to the item’s shape.

One example of why you would want to use instance segmentation is if you are tracking damaged merchandise. You will want to know if an object loses a part of itself in transit, like a bag strap. Instance segmentation makes it easier to observe this.

Skeletal Annotation: The Next Level of Crime Surveillance

When security firms and police review security tapes, they make inferences about suspects: how tall they are, their bodyweight, their physique, and more. What skeletal annotation allows you to do is to analyze a person’s gait, which you can use to make more meaningful and specific inferences like weight distribution, potential previous injuries, and more.

These benefits are great, but how can you add AI data annotation to your footage? Read on.

Some Benefits of Adding AI to Your Security Footage

Tracking People

Repeat suspects of crimes generally visit a crime scene several times before committing the crime. First-time suspects usually do not think of doing this. By tracking people with data annotated security footage, you can build up a suspicious person profile, which the police or a security firm would love to have at hand.

By tracking people, you can learn so much more about how to protect your car, property, or business, based on the inferences made by AI and the data you have.

Identifying Objects and Animals

Objects and animals can move very quickly on the screen. An animal may move so fast that the human eye might mistake it for a blur. Unless they stop and hover, have you ever considered how difficult it is to see a hummingbird in flight?

By identifying objects and animals on security footage, you can find suspicious activities happening on camera that you would otherwise miss without AI.

Detecting Movement

Tracking people and animals includes tracking the movements of every part of their body. With AI, you can analyze how someone in security footage moves their hands, feet, or even where they are looking.

You can access so much more information about package thieves, carjackers, and suspicious individuals on your property than by just viewing footage without data annotations.

Tracking Objects in Video

Video footage is extensive, and it is a lot to process for any individual surveillant. Even if you have a room full of guards whose sole purpose is to review security footage, the process can take hours, days, or even weeks, depending on the security incident.

Keymakr: Data Annotation Services for the Security Industry

AI is used a lot in the security industry, especially with data annotation services on security video feeds.

Keymakr uses experienced data annotators, big data, machine learning, and AI to provide much-needed context to your security images and footage feeds.

Are you looking to improve your data annotation processes? We can help. Contact us today to learn more about our services.",https://medium.com/@mafonya/implementation-of-ai-in-the-security-industry-the-basics-and-more-de1dabd25153,['Michael Abramov'],2020-10-20 15:22:35.456000+00:00,732,"AI, Data Annotation, Security Industry, Video Footage, Object Detection"
Neurons in Spreadsheets,"Neurons in Spreadsheets

Your own neural network on the cheap

In the previous post, we saw what a neural network is and how it works. Now comes the fun part. We’ll make one in a spreadsheet.

It doesn’t matter which spreadsheet you use. You can visualise such neurons in Excel, or you can equally well use LibreOffice, Google Sheets or any other spreadsheet application.

Image by the author

In the top line, the two yellow cells are the input values. You can change these to simulate various inputs. The next line contains the two synaptic weights by which we will multiply the input values. Here we put them to 0.6 to simulate a logical and. Like above, if you change both synaptic weights to 1.1, you will make the neuron behave like a logical or, since the threshold is put at 1, and if either input is 1.1, and thus greater than the threshold value, the neuron will fire. In the next line, we see the activation value, which is just the sum of the two weighted inputs. The next line contains the threshold, which in all these examples is 1. But there is no reason not to change it to any other value if you want to experiment. Finally, in the last line, the blue cell contains the output of the neuron. This will be 1 if the neuron has fired or 0 if the neuron has not fired.

You can also easily add additional neurons to your network and try to create more complex behaviours.

The only special thing you need to do is to insert two formulas into the spreadsheet:

Into cell C3 (the “activation” cell), we enter the formula for calculating the weighted sum of the two inputs: =(D1*D2)+(B1*B2). Into cell C5 (“output”), we enter a conditional statement: =if(C3>=C4, 1, 0). This is saying: if the sum of the inputs (cell C3) is greater than the threshold (cell C4), the neuron will fire (output=1); otherwise, it will stay silent (output=0). These formulas are written for Google Sheets, so if you use another spreadsheet application, you might need to change the syntax a bit.

That’s it! Your first, very own artificial neuron. You can now play with it. You can change the synaptic weights and the activation thresholds, and observe how it works. You can also easily add additional neurons to your network and try to create more complex behaviours. Of course, this doesn’t yet learn. It is pre-programmed by the fixed synaptic weights to do one thing only.

As an additional example, let’s look at how we would create a little network that can calculate the ‘exclusive or’, or xor, function in a spreadsheet. Here is the network we want to simulate (we discussed how this works in the previous post):

Image by the author

And here is the same neural network in a spreadsheet table. Again, the input is on top (yellow), and the output is at the bottom (blue). But now we have three neurons. The first layer (above) consists of two neurons (pink and green). Each of these neurons is connected to the two yellow inputs; this is why each has two synapses instead of one. Each of these neurons has its own (blue) output. Both intermediate outputs from the first layer feed into the orange neuron at the bottom. The orange neuron’s inputs are the outputs of the previous layer; this is why we don’t have yellow input cells for that neuron. The output of the whole network is the blue cell at the very bottom.

This neuron will implement an xor function between its (yellow) inputs in the first row and the blue output cell at the very bottom:

Image by the author

What you can see from this, is how the behaviour of a neural network is all encoded in the synaptic weights, which, in this case, we entered by hand. In a later post will see how an artificial neural network can change these weights by itself, and in this way learn new behaviours. Make sure to follow this series, so you don’t miss the fun!

Simulating neural networks in a spreadsheet is a great way to learn how they work and to get accustomed to the basic ideas and the structure of artificial neurons. I hope you enjoyed this and I hope to see you around here next time!

Thanks for reading!",https://medium.com/the-innovation/neurons-in-spreadsheets-e917c5c77a22,['Moral Robots'],2020-10-10 17:38:20.374000+00:00,717,"neurons, spreadsheets, neuralnetworks, excel, libreoffice"
MerzFiles. Stop!,"One thing is true: There are too many newsletters. So don’t fill your mailbox with another one.

Don’t subscribe MerzFiles, and the following will be spared you:

Exclusive Experiments with GPT-3 in various areas: the meaning of life, new art forms, abstract and weird contents

in various areas: the meaning of life, new art forms, abstract and weird contents Exclusive Experiments with other AI-driven models

All articles on Art, Science, AI, Society, Videogames and more

and more All Articles on Merzazine with Friend-Links (it means: in case you have a basic Medium account, you will spare your “3 articles per month” for other writers)

(it means: in case you have a basic Medium account, you will spare your “3 articles per month” for other writers) Exclusive ideas and thoughts , not written in Merzazine (yet): possibility to follow the development of conceptions

, not written in Merzazine (yet): possibility to follow the development of conceptions Hidden Tweets and hidden gems I found in the WWWasteland

I found in the WWWasteland another hidden items like interaction with AI for you

etc.

Just don’t click on this link below, which could bring you to the Newsletter MerzFiles.",https://medium.com/merzazine/merzfiles-stop-39411570172b,"['Vlad Alex', 'Merzmensch']",2020-10-20 21:15:10.473000+00:00,185,"Newsletter, Merz Files, GPT-3, AI-driven models, Art"
Velas Platform Use Case and Features,"After introducing the Velas platform to us all, i hope we have taken time to go through the benefits and derivation the platform brings. I will continue from where i stopped in my last post. And i will take time to evaluate to you the core features of the Velas platform.

The Velas blockchain uses Artificial Intelligence enhanced DPOS (AIDPOS) consensus on the platform to make sure that there is a high rate of volume processes on the platform, and equivalently ensuring that decentralization and security of the platform are not affected at all. And with the platform, you can own a smart contract on the blockchain, run multiple transactions at high speed and lots more.

Features of the Velas Platform includes the following:

Velas Artificial Intelligence: The AI on the Velas platform is going to be employed to identify the relationships between datas on the platform; It adapts input in a concise way and generates the best results without changing the ouput. This segment requires 3 velas node network in The Block size, The Block time and Transaction per second (TPS).

Rewards on the velas platform includes:

1. The Velas platform scalability is close to 30,000 transactions per second solving scalability troubles.

2. The Velas platform creates block only when required. (On users discretion).

3. There are no manual selection of block producers, its all done by the AI totally disruputing bias-ness or favoritism.



AI-enhanced DPOS (AIDPOS): Artificial Intuition Delegated Proof of Stake (AIDPOS) — is used to secure the Velas blockchain; it helps to fix the scalability problems and also tackles the Bitcoin’s POW system and the POS system of both Peercoin and NXT.

Note: POW: Proof of work, POS: Proof of stake

Do you want to use the Velas platform? Avail yourself to the transaction process below:

Use Case of the Velas blockchain:

1. Staking: You can stake the velas token on wallet.velas.com only, all you have to do is visit the webpage, create a wallet, buy vlx from exchanges to your velas wallet then you stake on the platform with you choosing your options yourself; the Return on Investment ROI is 0.8% — 1.5% each month depending on the staking pool.

The minimum tokens required to stake on the velas platform is 10,000 velas tokens. Also remember that while staking, you have the opportunity to choose any node you desire on the platform.

2. Payments: VLX can be used for smart contracts and transactions in Velas wallets, with a 25% discount on payment fees. In addition, VLX, which will be used as the main currency in CoinPayments and the upcoming exchange, will also enjoy a 25 percent discount on transaction fees.

3. High volume transactions (30k Tx/ sec): This means that the Velas blockchain can conduct 30,000 transactions each seconds.

4. Security: The DPOS algorithm is used to further tighten up the platform.

5. Smart contracts: Just as the etherum smart contract and waves blockchain or neo, Velas platform also allows the creation of smart contracts, easier to set up, and can contaimn several transactions in a seconds after solving scalability problems.

6. Tokenization

VELAS TOKEN

Velas ticker: (VLX)

Blockchain: Velax

Circulating Supply: 1 342 809 241 VLX Token

Total Supply: 2 080 131 909 VLX Token

Do use the links below for more information and details about the project:

Website: https://velas.com/

White Paper: https://velas.com/VELAS-Technical_Paper.pdf

Twitter: https://www.twitter.com/VelasBlockchain/

Telegram: https://t.me/velascommunity

Facebook: https://www.facebook.com/velasblockchain/

Instagram: https://www.instagram.com/velas.blockchain/

Medium: https://medium.com/@VelasBlockchain

Linkedin: https://www.linkedin.com/company/velas-ag

Github: https://github.com/velas

Discord: https://discord.gg/CTcKpPc",https://medium.com/@ogbeniiyalenu/velas-platform-use-case-and-features-b807fbf9dc79,[],2020-07-09 16:59:41.823000+00:00,541,"Velas, VLX, AIDPOS, Artificial Intelligence, Blockchain"
"Depix AI Recovers Pixelized Passwords, Earns 10K GitHub Stars","If you think pixelizing a password or other text data will protect it from prying eyes, think again — your information may not be as safe as you’d imagine.

Pixelization (also known as mosaic) is a common coding method used to conceal information by dramatically reducing the resolution of sensitive areas in an image. For years, the technique has had broad applications in security and censorship — but its days may be numbered. “Depix” is a new AI-powered tool that can easily undo pixelization to enable recovery of the information therein. Uploaded this week, the project has already received nearly 10,000 stars on GitHub.

“I’ve seen companies pixelize passwords in internal documents. No tools were available for recovering a password from such an image, so I created one,” writes the Depix developer, Netherlands-based information security consultant Sipke Mellema. The tool works on images pixelized with a linear box filter, which overwrites a box of pixels with the average value of all pixels in the box. Says Mellema, “my algorithm attacks the common linear box filter.”

Mellema explains that because linear box filters are deterministic algorithms, pixelizing the same values will result in the same pixelated block, and pixelizing the same text using the same block locations will result in the same block values. Every block, or combination of blocks, can be considered a sub-problem.

To prepare a pixelated text sequence for Depix processing, first cut the relevant pixelated blocks from a screenshot image in a single rectangle format, then paste a De Bruijn sequence with expected characters into an editor with the same font settings (text size, font, colour, hsl). Next, take a screenshot of the sequence, preferably with the same screenshot tool previously used to capture the pixelized image.

Finally, simply run python depix.py -p [pixelated rectangle image] -s [search sequence image] -o output.png

Since the original information under the pixels is lost, it is impossible to directly reverse the filter. For most pixelized images, the tool tends to find single-match results for blocks, which it assumes are correct. Matches of surrounding multi-match blocks at the same geometrical distance as in the pixelized image are also treated as correct. When the correct blocks have no more geometrical matches, they are output, while the average of all matches is output for multi-match blocks.

The developer says the Depix technique ""beautifully links to vulnerable patterns in cryptography. It's similar to hash cracking, exploiting the use of ECB, and the utilization of known-plaintext attacks."" He advises people to avoid obfuscation techniques on sensitive data, warning the ""assumption that a schema can't be broken, just because the implementer doesn't know how, is a common pitfall in information security.""

The Depix project is on GitHub.",https://medium.com/syncedreview/depix-ai-recovers-pixelized-passwords-earns-10k-github-stars-d752915fac72,[],2020-12-11 19:55:05.754000+00:00,443,"pixelization, mosaic, Depix, AI-powered tool, ECB"
Part 2 — The Math Behind Optical Flow,"Part 2 — The Math Behind Optical Flow

Part 2 of GPS Denied Navigation

This is Part 2 in a series on building software for GPS Denied Navigation in advanced aerospace robotics.

In Part 1 of my series in GPS Denied Navigation I showed some simple code on how to detect features from video using opencv.

Now that we can recognize features between image sequences, we need to know how to use them.

Optical flow is the apparent motion of objects, surfaces or edges based on the relative motion of the camera. There is actual math behind this. Let’s take a look at how we track a single pixel first:

The diagram below shows a pixel that has moved from (x,y)(x,y) at time tt to (x+u, y+v)(x+u,y+v) at time t+1t+1.

Pixel Correspondence Problem

From this picture, it’s easy to figure out the velocity vector (u,v). But when we look at two real images, we’d first need to solve what’s called the pixel correspondence problem. That is, we need to know which pixels in image 2 correspond to which pixels in image 1.

To solve this problem we make two assumptions.

Assumption 1: the motion is small: this means we can look in the vicinity of where the pixel was to try to determine where it now is.

Assumption 2: the appearance doesn’t change from t to t+1: this assumption is best expressed mathematically.

This relationship is known as the brightness constancy constraint.

If we drop the time index and do a Taylor series expansion of the right hand side of this equation, we find the following:

We can plug this back into the brightness constancy constraint and reorganize some terms to find the following:

It is common to use a subscript to denote derivatives, so this is how it looks with simplified notation:

This equation says that any change in the appearance of a pixel over time has to be explained by spatial motion induced by the camera movement.

Note: I(x,y,t) gives the intensity of the pixel at location (x,y) at time t. If this is a gray-scale image, this intensity would be a single number corresponding to the darkness of the pixel. If this were a color image it might be a 3-vector with values giving the amount of red, green, and blue in the image.

In Part 3 I’ll explain the difficulty of solving this equation and what we can do to estimate a solution.",https://medium.com/building-autonomous-flight-software/math-behind-optical-flow-1c38a25b1fe8,[],2020-05-17 15:56:11.032000+00:00,389,"Optical Flow, GPSDenied Navigation, Pixel Correspondence Problem, Brightness Constancy Constraint, Taylor Series Expansion"
Automating Away Our Customer Relationships?,"Automating Away Our Customer Relationships?

Finding the Balance in Customer Service

If you’re one of the more than two-thirds of US consumers that shopped online pre-COVID, then chances are you’ve dealt with customer service within the past few months. Whether that reason was to cancel travel plans or figure out new apparel return policies due to physical location closures, many of us have had to deal with the friction that is customer service. Most of my customer service interactions have involved chatbots, and it got me to thinking about this customer service shift that has occurred over the past several years. While there are some obvious advantages to using these customer service automation tools, these tools lack something that customers value: empathy and understanding. Consequently, organizations may be automating away critical touchpoints that can help retain customers and create loyalty.

‘I, Robot’ meets Customer Service

How We Got Here: Call Centers Considered Cost Centers

Many organizations see call centers as cost centers, and as such, they’ve put a lot of effort into building out AI-powered platforms that will handle customer inquiries in place of traditional service agents. You know these platforms, from the human-like IVR (Interactive Voice Response) that attempt to field your questions to the website chatbots that interact with you via text. Companies such as Amtrak and Verizon have implemented AI-based tools to help customers book trains or determine the best way to assist customers. In fact, Gartner predicted that by 2020, we can expect to see 85% of all customer interactions handled by AI without the involvement of a human support agent (This includes interactions beyond customer service such as purchasing via ecommerce flow rather than going through a sales agent). There’s no doubt that these tools provide value (see below for use cases), however there are still areas where live agents are needed to strengthen customer relationships.

Interactions With Customer Service Can Build Customer Loyalty

Customer service represents one of the few direct communication channels customers have with a company, and because of this, it’s often perceived as a proxy for the brand of the company. This is reflected in a global study by Microsoft, in which 96% of respondents say customer service is important in their choice of and loyalty to a brand. Businesses whose customers have alternatives and who depend on sales & marketing to serve those customers should use customer service touchpoints to build a long-term relationship with their customers. Using insurance as an example, where there are plenty of alternatives, I remain loyal to USAA because every time I call with an inquiry, I’m greeted by a friendly agent who goes above and beyond to make sure my questions are answered.

Expectations for customer service are increasing, and with its direct impacts to the quality of customer relationships, good customer service is a critical part of operations. Mikkel Svane, CEO of Zendesk (a customer service software company), says in his book Startupland that:

“Customer relationships matter more than ever, because your future revenue depends on those relationships lasting well beyond a single transaction…customer service interactions are becoming your primary means of creating true customer relationships.”

So why then are companies so adamant about automating away these opportunities to build relationships? They’re obsessed with the bottom-line, and every minute a customer agent spends on the phone is a minute that costs the company. However, companies should shift their mindset to look at customer calls as an opportunity to create value rather than a necessary evil that costs the company time and money. After all, positive customer service experiences can lead to repeat purchases, higher retention, and increased CLV.

With That Being Said…

Automation can still play an important role in the following areas. Notice that these use cases involve supporting a live agent or performing functions that live agents cannot do already:

Proactive Support — Anticipate areas where a customer would initiate a call (e.g., service outage, known defect, etc.) and preempt the call with a notification explaining the situation

Language Barriers — A platform supporting multiple languages enables a company to better serve a global customer base

Knowledge Assistance For Live Agents — Provide context for calls and key customer data to live agents for increased productivity and efficacy

Self-Service For Simple, Frequent Issues — Identify areas that lead to largest volumes for calls and automate the issues that are light touch and easy for customers to handle on their own (Customers are often quick to educate themselves on how to fix their own problems)

Call Routing — Leverage tools (e.g., natural-language processing) to route customer to appropriate agent best suited to answer their question

Social Media Scans — Track current customer sentiment and the health of a company’s brand using tools to scan the social media platforms where their customers are most present

So What’s The Right Balance?

While the use cases above present opportunities to supplement or automate customer service, there are limitations where a human agent should still be leveraged to provide a good experience for customers. For example, in areas where the service tools don’t answer the customer’s full question or cannot understand the customer’s question to begin with, companies should provide an effortless way to get in touch with a live agent (rather than the dreaded IVR menu prompt, can we get rid of this please!?). Additionally, some customers only want to work with live agents and should not be forced to go through an automated flow. For areas that are automated, companies should make sure that all channels (e.g., email, call, SMS, etc.) are integrated so that agents are aware of a customer’s entire service history and can provide better support.

The future of customer service is not exclusively AI powered chatbots, but AI powered tools that empower service agents to handle more complex inquiries requiring human touch while taking routine inquiries off the agent’s plate. The goal of adding automation to customer service is to not only cut costs, but also increase productivity and deliver a better experience.",https://medium.com/swlh/automating-away-our-customer-relationships-27d39d1ae661,['David Klippel'],2020-08-20 18:58:51.132000+00:00,982,"Customer Service, AI, Automation, Call Centers, Customer Loyalty"
Allow the Books Speak to You With Python,"Step #1. Import the Python library

The library I was talking about is pyttsx3 (python text to speech version 3). It is a text-to-speech conversion library in Python. Unlike alternative libraries, it works offline and is compatible with both Python 2 and 3.

You can use any editor for creating this project. I prefer using Pycharm due to its user-friendly and other important features. You can use any editor and then import this library by executing the below-mentioned command:

pip install pyttsx3

pip is a package manager for Python. That means it’s a tool that allows you to install and manage additional libraries and dependencies that are not distributed as part of the standard library.

Here, we are telling the package manager to install the specific library to our project.

Step #2. Make the Code Talk

Once the library is installed, we can import it into our project.

import pyttsx3

Then it takes three lines of code to make the code speak.

speak = pyttsx3.init() speak.say('A.I. is going to take over the world') speak.runAndWait()

Here, we have initialized an instance of our imported library. We used the in-built method say in which we wrote the text that we want to convert into speech. Lastly, we call the runAndWait method for the execution.

Run the above code for turning your text into the speech.

Step #3. Create an Audiobook

There is a prerequisite before we move to create our own audiobook. We will need a pdf that can be converted into an audiobook. You can choose any pdf file. If you have a pdf for any book or novel, then you can use that one.

Once we have a pdf, then the next thing we need is a package to read pdf files. We will again go to our editor and install the package —

pip install PyPDF2

After the package installation, we can import the package in our code to read the pdf file.

import PyPDF2 book = open('filename.pdf','rb')

pdfReader = PyPDF2.PdfFileReader(book) page = pdfReader.getPage(1) text = page.extractText()

Here, we have imported the package to read pdf files in our code. Then we have created an object call book to open the given pdf file. The second argument ‘rb’ stands for — read as binary.

After that, we call the PdfFileReader method of the imported package and pass our pdf file information to it. Then we are calling the getPage method to extract the specific page information and extract the text by calling method extractText in the next step.

Once we get the text extracted then we can pass it to the method say that we created in step #1.

Final Code for audiobook

import pyttsx3

import PyPDF2 book = open('filename.pdf','rb')

pdfReader = PyPDF2.PdfFileReader(book) pages = pdfReader.numPages()

speak = pyttsx3.init() for num in range(0, pages-1)

pages = pdfReader.getPage(num)

text = page.extractText()

speak.say(text)

speak.runAndWait()

That’s it. In eleven lines of code, we created our own custom audiobook.

Next time, instead of sitting in front of a computer going through some daunting pdf, just convert it into an audiobook and lie down while listening to it.

There are several other methods present in library pyttsx3 for customizations like changing the voice of the reader, controlling volume, and even we can save the audiobook as a .mp3 file in our system. I’ll leave those things up to you for exploring the library further.",https://towardsdatascience.com/allow-the-books-speak-to-you-with-python-e95c65030c7a,['Shubham Pathania'],2020-12-17 13:51:51.009000+00:00,526,"pyttsx3, python, text-to-speech conversion, Pycharm, pip package manager"
Bitfury AI Opens Offices in Italy and Netherlands,"Today the Bitfury Group announced its artificial intelligence division, Bitfury AI, has opened two new offices. The first office, focused on R&D, is now open in the Dutch city of Eindhoven, a leading tech and innovation hub. The second office is a regional location in Milan, Italy.

Beginning with the appointment of veteran technologist Fabrizio Del Maffeo in August 2019, Bitfury’s artificial intelligence team now has more than a dozen employees hailing from technology, AI and IoT powerhouses like Intel, Synopsys, Advantech and the ASUS Group. The team was also joined by two expert advisors in 2019, Prof. Dr. Luca Benini and Prof. Dr. Marian Verhelst. Prof. Dr. Luca Benini is the chair of Digital Circuits and Systems at D-ITET ETH Zurich, specializing in RISC V/CPU artificial intelligence accelerators. Prof. Dr. Marian Verhelst is the Senior Scientific Director of IMEC and professor at KU Leuven, specializing in digital and analog accelerators.

“Artificial intelligence is the emerging technology of the century,” said Valery Vavilov, CEO and co-founder of Bitfury. “This newly expanded team will help us fulfill our mission of making the world more trusted and secure by delivering intelligent solutions that help humanity take full advantage of the benefits of AI.”

“Eindhoven is renowned internationally as an engineering and technological center, with major companies like Philips, Intel, Synopsys operating R&D divisions here,” said Fabrizio Del Maffeo, head of Bitfury AI. “The expansion of our team into this region will help us have access to the best R&D expertise and resources in the Netherlands, while our Italy office will lead development of our AI turnkey solutions.”",https://medium.com/meetbitfury/bitfury-ai-opens-offices-in-italy-and-netherlands-c2482685121c,['The Bitfury Group'],2020-03-04 07:20:40.786000+00:00,262,"Artificial Intelligence, Bitfury Group, Eindhoven, Milan, R&D"
Podcast Episode #5: Industry Relations and Networking Advice with Erik Mjoen,"Erik Mjoen is the Industry Relations Manager at UC San Diego’s Halıcıoğlu Data Science Institute (HDSI), where he focuses on facilitating networking, leadership, partnership, and innovation. In this episode, we talk with him about his experiences in Industry Relations, his advice to students seeking industry opportunities, and his tips on soft and hard skills.

His Work

As an Industry Relations manager, Mjoen’s responsibilities entail overseeing research, leadership, partnerships, and developments in Data Science. Most of his daily activities involve meetings with companies, in hopes of establishing a collaboration between them and the HDSI. To establish partnerships, these companies must have a clear vision of what their goals are, how they will benefit from this partnership, and how they will utilize the resources.

Aside from meetings, Mjoen focuses on connecting students with companies by planning career fairs and other events for students to gain exposure to industry.

Mjoen’s work does not strictly pertain to either academia or industry. Rather, he is the liaison between both, which means he has a diverse set of tasks to work on and events to plan, so his day-to-day routine often varies.

He started his journey toward his current role in college, where he studied International Business and minored in Economics. He then worked in various international firms in San Diego, zeroing in on technological innovation during the dot-com boom, and eventually running his own recruitment business. Seeking a different path later on, he was introduced to UCSD and got his first job at the Career Center, focusing on industry engagement for technology and engineering. Over time, he familiarized himself with the university’s engineering talent.

Mjoen has worked at UCSD for more than three years and found that the university is a bastion for innovation and discovery. Both its faculty and student body are forward-thinking, and HDSI, in particular, strives to help “the next generation of innovators and leaders.”

Four Quadrants of Industry Relations

HDSI’s industry partnership framework can be broken down into four parts, according to Mjoen.

The first of these is Research and Innovation, which consists of engaging faculty and students, usually graduate-level, in solving complex problems. By working on these projects, students partake in addressing real-world issues all across the spectrum, as the faculty are very diverse in their domain knowledge. What is unique about HDSI is that it seeks to expand these experiences into the undergraduate curriculum.

The second part is Recruitment and Talent, which Mjoen argues is the most important because it cultivates competition and diversity in the marketplace. HDSI’s partners may set up internship and hiring goals, and they’ll in turn get assistance in meeting those goals as well as building recruitment brands.

The third part is Continued Learning, which serves to keep current professionals informed about the evolution of the Data Science field and the industries that apply its numerous moving parts. Faculty lectures and comprehensive workshops cultivate Continued Learning.

Leadership comprises the fourth quadrant and aims to foster the Data Science curriculum lends itself to the necessary industry skill — sets. This area fosters two-way communication and collaboration, and it ensures that academic faculty and companies can learn from each other through facilitating relevant research endeavors.

Career Advice for Students

When planning out their careers, Mjoen stresses that students should concentrate more on the learning process and less on acquiring the most impressive-sounding job title offered by a top-grossing company. Also, as important as hard skills are, students should continually practice their soft skills and networking skills.

Many networks in our daily lives are outside the workplace: our friends, classmates, families, and others. Therefore, our aptitude in networking grows only with constant practice in every situation. Mjoen explains that a strong network is what paves the way for opportunities, and we must be vigilant of any opportunities that spring up in which we can make new connections. The doors that open up are almost always unpredictable, and it’s up to us to maintain strong connections through relationship-building and reaching out to people in our network.

Mjoen notes that networking is proactive not passive. As uncomfortable as it may seem, seeking out an employer or industry expert in person, as opposed to drafting an online application, will leave a more lasting impression. Doing so also increases the chances of follow-ups. Additionally, it’s better to follow-up upon meeting these recruiters to further the odds of them securing a place in your network and vice versa.

Having an elevator pitch also comes in handy, says Mjoen. With it, you can tell your story and your goals succinctly and efficiently. These should generally last 30 to 60 seconds.

For more insight about Mjoen’s efforts in pioneering for industry-academia alliances, go to this link.",https://medium.com/ds3ucsd/podcast-episode-5-industry-relations-and-networking-advice-with-erik-mjoen-db14f845d9e4,['Camille Dunning'],2020-06-08 19:37:09.226000+00:00,763,"Industry Relations, Erik Mjoen, UC San Diego, Halıcıoğlu Data Science Institute (HDSI), Networking"
My Reinforcement Learning Adventures,"While the value loss seems to improve, the policy loss seems to plateau. But, one thing I noticed was that while the wings/attack orientation seems to improve, the coordination between players doesn’t seem to be appearing. So, the main problems are:

Players can move randomly without coordination

The wings attack mechanism is too complicated

Like before, the observation space is too complicated

And finally, this is applicable to version 1 too but the action space is a bit too complicated to be human-understandable.

Version 3

Version 3 environment test. The images next to each other are what each model sees. So, the top left video is what the blue side sees and the one next to it is what the red guys see.

Environment

Action space

I made the action space a 4 by 4 vector field so that it is understandable which direction the model is going towards. I also made it so that the vector field represents the force acting on the players.

Observation space

I made the observations space a simplified version of the render above so that it is human understandable and possible easier to learn for the model than before. And now, running multiple games at once is possible!

QOL changes

I made it so, to encourage cooperation, that the players stick together by having a hiarchy and having springs attached to them like so

Spring pulling visualization

Also, for attacking, I just made it so that the velocity is the direction of attack like so and there’s a triangle of attack

Basic model explanation

I wrote a separate series on this on medium called Understanding OpenAI baseline source code and making it do self-play but I basically modified OpenAI baselines to make it so that you can do selfplay in them! I remember it taking like 1 month, maybe 2, but I’m happy with the results. I did this mainly because I wasn’t sure if the problem was in my model or an environment, and since I knew Open AI is one of the best AI companies out there, I thought I can’t go wrong by using their code. The code still doesn’t work for anything other than ppo2 for now.

For the model, I just used cnn-lstm with action space of box(continuous action space) which was quite logical for me because the observation space is an image and the output is just forces which is continuous.

Results

Policy loss

While the policy seems to be improving, when I check the videos, I noticed that there’s no significant change on what is being done across the 100 epochs run. One problem that lead to this was that the model fought against itself. So, at any moment in time, it didn’t make much sense for one side to consistently win against another. Basically, the main problem with this model was that I had no idea if the model was actually learning.

Version 4

Version 4 environment test run. As you can see, it isn’t going well.

Action space

For the action space, I changed it so that I can specify what size I want. For example, the above animation is a 2x2 vector field. I also made it so that the vector field is not an acceleration/force vector field but rather a velocity one and I just apply force that moves the model in the direction of the force. I just thought that’ll be easier to comprehend.

Observation space

No change.

QOL changes

Since I didn’t know if the model was learning, I decided to go with the easiest strategy: fighting against a stationary opponent. So, I’ll know that the model is improving if it is able to defeat the enemy in a faster way.

Another thing I did was I removed the spring mechanic because, if possible, I wanted the model to learn to stick together by itself.

Results

Reward plot with different models

Policy loss plot with different models

As you might have seen from the video, it didn’t go to great. The first plot above is basically the reward so, in the beginning, with a random policy, it worked for a while but then it just stopped doing anything productive and just went to a corner.

For the longest time I was confused about why this was happening but what happened was quite simple. For my action space, I output a continuous actions space with the shape (2, 2, 2) where action space[0][0] represents the top left vector. The problem with this was I used actor critics which outputs a mean and a log standard deviation. So let’s say for example, the model learns that shifting the mean to say -5 is quite a good idea for all actions with the standard deviation of 1. What happens there is that then all that part of the action space ever does is point towards left bottom and it’ll just remain there because all the numbers sampled will always be negative. Just to note, this problem occurred on a 1x1 action space as well.

The reason the policy seems fine is because the value loss is initially really high so the value loss improves and eventually just predicts 0 which leads to the policy loss to appear to be improving.

Version 5(Current)

Version 5 environment test run

Action space

Instead of the continuous action space I used before, I decided to just use a multidiscrete action space. Then, the model just does a softmax and chooses an action without going into all the mean changing problems. For this model, it’s just a 1x1 action space. Currently, I started training on a 2x2 action space

Observation space

No change.

Results

Reward plot with different models

Policy loss plot with different models

While the model doesn’t seem to improve with all the models, I see that for models like impala_cnn and cnn_lstm, the models rewards are increasing. Also, it’s a bit noisy but the problem of the finding an invalid policy seems to have stopped happening which I’m happy about! Also, the policy loss seems normal. So I’m finally getting something after 2 years!

Next Steps

Since I have a self play environment set up, I plan to just have a model fight an older version of itself. For example, making one model learn for 1 epoch and then make the other model fight against it and see how it improves.",https://medium.com/analytics-vidhya/my-reinforcement-learning-adventures-873fe5cdacef,['Isamu Isozaki'],2020-12-16 16:20:22.423000+00:00,1017,"Environment, Action Space, Observation Space, Quality of Life Changes, Results"
Another `Variational Auto Encoders Explained` Post and Letter Encoding,"I have read a fair number of posts about variational auto encoders but only recently did I try my hand in generating my own. I’ve been going through the early access edition of GANs in Action by Manning.

An auto-encoder deconstructs (or encodes) some data into a hidden representation with the goal of losing as little information as possible. Its performance is measured by how well it can reconstruct (decode) the data based on its internal representation.

The cool thing about auto-encoders is that they are unsupervised. Nowhere in the example above did we tell the encoder the number is a 2. So we can use them to create a structure on a data set without the need for labels. We can then use that structure for supervised training. We also don’t tell it how to encode the data. We just provide a structure for the encoder and decoder, and measure how well its doing based on how much information is lost in the process.

A variational auto-encoder (VAE) is an auto-encoder, except instead of learning a compressed representation, it learns a probability distribution of the data:

This is the point at which most explanations get all hand-wavy and spill a few words about latent space. But all this means is that the compressed representation will be a mean and standard deviation, representing a normal distribution. When we connect it to the decoder, we’ll want to take a noise sample using the mean and variance. This is the μ + Σ * ε from the graph above. This just means that when we decode, we’ll use the mean and add the standard deviation multiplied by some random normal value.

def sampling(args):

z_mean, z_log_var = args

epsilon = K.random_normal(shape=(latent_dim,), mean=0.0)

return z_mean + K.exp(z_log_var/2.0) * epsilon

The other interesting thing about auto-encoders is that you can inject your own values into the process and generate output. From there you can map out a latent space and see the internal representation of our model.

Implementation

Most posts use the mnist handwritten numeric digit data set but we’ll use emnist, the handwritten character data set instead. Someone on Kaggle conveniently transformed the dataset into a csv. I’ve excluded some function definitions. My full code can be found here.

%matplotlib inline

import imageio

import matplotlib.pyplot as plt

import numpy as np

import pandas as pd from PIL import Image train = pd.read_csv(""emnist-letters-train.csv"", header=None)

test = pd.read_csv(""emnist-letters-test.csv"", header=None) mappings = {}

with open(""emnist-letters-mapping.txt"") as f:

for line in f.readlines():

code, lower, upper = line.split()

mappings[int(code)] = chr(int(lower)) def display_images(imgs, names, max_imgs=100):

"""""" Displays images and their corresponding names""""""

... def process_emnist(arr, mappings):

"""""" Process emnist letters """"""

... X_train, y_train = process_emnist(train, mappings)

X_test, y_test = process_emnist(test, mappings) display_images(X_train[:10],y_train[:10])

Our training set is (88800, 28, 28) representing 88,800 letters from A to Z in either upper or lower case.

Below we create the VAE.

A variational auto encoder is just an encoder that feeds a decoder and encodes to a probability density function. We define the models independently and then define the VAE as the combined model. The function returns two models: one for training and one for evaluation. The only difference is that for training, the final layer is hooked up to the sampling layer. But when we want to display our results, we should not include the noise. That model is vae_eval and is just connected to the z_mean layer.

The final layer of the encoder is the sampling layer which we defined ourselves and wrapped in a Lambda layer. The mean and var layers are used for the loss function which tracks how far away our probability distribution in our encoding is from a mean of 0 and a standard deviation of 1. The mean layer is also used when we want to see our results.",https://medium.com/ml-everything/another-variational-auto-encoders-explained-post-and-character-encoding-8fb426688841,['Branko Blagojevic'],2018-10-26 10:59:49.525000+00:00,604,"vae modelvae_inputs = Input(shape=(28, 28), name='encoder_input')x = Flatten()(vae_inputs)x = Dense(1024, activation='relu')(x)  x ="
Code a Deep Neural Network,"Code a Deep Neural Network

Photo by timJ on Unsplash

In last post, we’ve built a 1-hidden layer neural network with basic functions in python. To generalize and empower our network, in this post, we will build a n-layer neural network to do a binary classification task, in which n is customisable (it is recommended to go over my last introduction of neural network as the basics of theory would not be repeated here).

All images created on my own, referred images are source-added.

Weights Initialization

Firstly, weights need to be initialized for different layers. Note that in general, the input is not considered as a layer, but output is.

(For detailed training and testing process, please check here)

We’ve learnt that for lth layer:

where n^[0] equals the number input feature.

Consider a neural network below:

In this case, the first W^[1] would have shape (4, 2) , and the second W^[2] would have shape (1, 4) .

Where input requires to be a list, for the case above the input layer would be

[2, 4, 1]

And our initialized weights need to be small enough so that the gradient would large in the backpropagation process and learning would be faster.

Forward Propagation

The forward process would be straight-forward as listed below:

Where l is the lth layer and g(x) is the activation function. Here we would use 2 different activation function:

All the function above applies to matrix.

The forward propagation would be following the equations above. Note that in our implementation, except the last layer we use sigmoid activation, the rest we use relu activation function.

Cost Function

Still we consider this a binary classification, the cost of a batch would be:

Where a is the predicted value, and y is the actual one.

Backward Propagation

Now that our forward process has finished, in order to let our model to improve through iterations, let’s get to the key which is backward propagation.

[source: https://github.com/enggen/Deep-Learning-Coursera ]

The backward gradient can be calculated in recurrent fashion:

First, implementation of derivative of sigmoid and relu is required.

For symmetrical reasons, here we have both functions with the same input even though it is not required.

Following the equations above, we have our implementation of backward propagation. Note that except the last layer where sigmoid function is used, the rest we all apply relu derivative to get the gradients.

Now given the gradients, we have our weights updated as following:

Apply on Dataset

Let’s have our model apply on created dataset with 200 features.

Now let’s have our model trained.

Here we have a 3-layer NN with 200 input features.

To have the detailed training process, please check my github.",https://towardsdatascience.com/code-a-deep-neural-network-a5fd26ec41c4,['Jeremy Zhang'],2020-11-06 13:47:59.621000+00:00,415,"deep learning, neural network, code, weights initialization, forward propagation"
The Co-op Close-up: Procter & Gamble,"SFU’s professional master’s program in computer science trains computational specialists in big data and visual computing. All students complete a paid co-op work placement as part of their degree. In this feature, we examine the co-op experiences of some of our big data students.

Kanksha Masrani completed her bachelor’s degree in information technology at Charotar University of Science & Technology before joining SFU’s professional master’s program in computer science.

Can you tell us about Procter & Gamble? What is it like working there?

Procter & Gamble (P&G) is an American multinational consumer goods company headquartered in Cincinnati, Ohio, and was founded in 1837 by William Procter and James Gamble. The company specializes in a wide range of personal care and hygiene products.

During my co-op here, I have been working with an exciting team where I have been given the chance to manage multiple projects at the same time. My work experience here allows me to combine my technical excellence with business and management skills while building a large network of colleagues and professionals that I can turn to when needed.

P&G organizes multiple hackathons to help generate fresh ideas. People of different ages and backgrounds come together and help solve another team’s problem. You can also get involved in multiple activities happening across the organization, be it off-site events for your team or representing your product category in some sport activity. Along with the projects you own, you are encouraged to be a part of these activities to further develop your soft skills outside the workplace.

Can you describe the project(s) that you are working on?

In total, for the course of eight months, I was assigned five projects. In one project, I am developing a business intelligence tool on Power BI which integrates KNIME, an ETL tool used for integrating data from multiple data sources, doing required data manipulation and analysis using Python and R snippets to deliver performance data from the Nielsen database which enables faster and simpler reporting. This business intelligence tool offers insights on shares, KPIs, price, and promotions across multiple retailers and time periods for the blades and razors product category. Using the same tools — Power BI and KNIME — I am also developing an innovation tracker which is currently providing insights on price, promotion, market share, and other sales metrics for new products launched by P&G in the Canadian market. The dashboards I’ve created allow my team to dynamically explore data and derive insights that may not be easily obtained through spreadsheets. For example, the team can easily glean whether performance improved across product categories over time, and if not, which category contributed towards the decline and by how much.

What are the most important skills needed in your co-op position and how has the big data program prepared you for the role?

P&G puts a strong emphasis on using data to make “better, smarter, real-time business decisions.” As a Business Analyst at P&G, you are a business leader. Your role is to mine multiple sources of data, derive actionable insights, and translate complex results or algorithms into simple conclusions that will empower others to take action to win with the consumers. Business knowledge, interpersonal, and communication skills are therefore a must to thrive in this multi-functional leadership team.

Nothing beats hands-on experience when it comes to skills development. At university, I mastered machine learning, big data tools and technologies, data mining, and more. But interning at P&G taught me how to apply those skills in meaningful ways and allowed me to further develop my collaboration, critical thinking, and presentation skills, for example when presenting my recommendations to non-technical executives.

What are you most appreciative of when looking back at your co-op experience thus far?

P&G develops future leaders by giving them meaningful responsibilities from Day One. Only four months in, I was leading a team of twelve. This gave me an opportunity to take my leadership skills from good to great. Every project was a challenge, but with the right kind of people around, it was not difficult to overcome it. The level of autonomy I have been given is very impressive. I have real, business-critical tasks to complete, and, with the support of my colleagues and mentors, I have been able to finish my projects to a high standard while also growing my confidence and knowledge. Here at P&G, if you are flexible, determined, and want to expand your skill set, there are so many opportunities to learn and grow!",https://medium.com/sfu-cspmp/the-co-op-close-up-procter-gamble-f7e2d6a69dfc,['Kathrin Knorr'],2019-11-18 18:41:17.031000+00:00,738,"SFU, Computer Science, Co-op, Procter & Gamble, Big Data"
6 Types of Neural Networks Every Data Scientist Must Know,"DEEP LEARNING, NEURAL NETWORKS

6 Types of Neural Networks Every Data Scientist Must Know

The most common types of Neural Networks and their applications

Neural networks are robust deep learning models capable of synthesizing large amounts of data in seconds. There are many different types of neural networks, and they help us in a variety of everyday tasks from recommending movies or music to helping us buy groceries online.

Similar to the way airplanes were inspired by birds, neural networks (NNs) are inspired by biological neural networks. Though the principles are the same, the process and the structures can be very different. This is as true for birds and planes as it is for biological neural networks and deep learning neural networks.

To help put it into perspective, let’s look briefly at the biological neuron structure. Figure 1 shows the anatomy of a single neuron. The central part is called the cell body, where the nucleus resides. Various connections pass the stimulus to the cell body, called dendrites, and a few connections send the output to the other neurons called axons. The thickness of the dendrites and axons implies the power of the stimulus. Many neurons with various cell bodies are stacked up and form a biological neural network.

Figure 1: Anatomy of Single Neuron (Image by author)

This same structure is visible in deep learning neural networks. The input is passed through an activation function (similar to the nucleus) with weighted edges (similar to dendrites). The generated output can be passed to another activation function. Many of these activation functions can be stacked up, and each of these is called a layer. Apart from the input layer and the output layer, there are many layers in the interiors of a neural network, and these are called hidden layers.",https://towardsdatascience.com/6-types-of-neural-networks-every-data-scientist-must-know-9c0d920e7fce,['Ramya Vidiyala'],2020-12-18 14:02:07.149000+00:00,292,"Deep Learning, Neural Networks, Data Science, Biological Neurons, Activation Functions"
"Public, Private, and Protected — Access Modifiers in Python","Private Keyword

The private members of a class are only accessible within the class. In Python, a private member can be defined by using a prefix __ (double underscore).

Output of private- access modifier

So, in the private modifier’s case, we cannot access the attribute. So is a private modifier the way ahead?

The answer would be No

Python performs name mangling on private attributes. Every member with a double underscore will be changed to _object._class__variable .

Output of private attribute modified

Therefore, this proves Python provides you with access-modifiers functionality, but it can’t be compared to classical languages like C++ and Java. This practice entirely depends upon the programmer.

So a responsible programmer, upon seeing an attribute with such a naming convention, would refrain from accessing it outside its scope. This also wouldn’t be good to use in cases where fellow programmers aren’t aware of such naming conventions.",https://medium.com/better-programming/public-private-and-protected-access-modifiers-in-python-9024f4c1dd4,['Sarath Kaul'],2020-03-25 07:06:00.314000+00:00,141,"private-keyword, access-modifier, name-mangling, C++, Java"
Machine Learning Algorithms: Surprises at Deployment?,"In a recent ArXiv paper by a large group of Google researchers (+ two EE/CS professors and a PhD student), called “Underspecification Presents Challenges for Credibility in Modern Machine Learning” they report discovering a new underlying cause of deployment surprise: “under-specification”, that is, that algorithms that seem equally good at time of development (i.e. they all give a similar “solution” and therefore the problem is under-specified) perform dramatically differently during deployment, in terms of performance on subgroups.

Is the above paper’s discovery a new insight? Are deployment surprises a specialty of deep-learning? is “under-specification” a deep learning problem? Is under-specification in fact a problem in prediction?

That predictive algorithms can perform dramatically differently on data subgroups is well-known. Simpson’s Paradox is an extreme example, where a correlation between an input and output changes direction when examining subgroups of the data. The larger the number of predictors, the larger the chance of a Simpson’s Paradox. Predictive models are also easily “fooled” when the training dataset includes a minority group that has a different input-output relationship than the rest of the training data. Models are fooled because metrics used to train and evaluate algorithms give equal weight to each observation (e.g. least squares or maximum likelihood for training; RMSE and accuracy metrics for evaluation).

Illustration of Simpson’s paradox: a positive x-y relationship appears for each group separately, but a negative x-y relationship appears when the groups are combined. (Source: Wikipedia, Creative Commons)

While the Google researchers paper’s abstract concludes with a vague sentence that might mislead readers to think there’s a technological solution (“Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain”), in several places in the 59-page paper the authors conclude:

“This confirms the need to tailor and test models for the clinical settings and population in which they will be deployed.”

or

“This is compatible with previous findings that inputting medical/domain relational knowledge has led to better out of domain behaviour…, performance… and interpretability… of ML models.”

The paper closes with a proposal to circumvent the need for a context-specific dialogue between the data scientist and the end user by building models that favor “predictors that approximately respect causal structure”. While using causal structure is feasible and useful in some domains, especially in low-dimensional problems, the areas where ML shines are exactly those where causal relationships are difficult to specify. Explanation and prediction both have their merits, and predictive solutions can still be sound and useful even without underlying causal modeling if developers and users collaborate and communicate during the entire loop of design, testing, deployment, and post-deployment feedback.

At their basis, deployment surprises are a failure to understand the limitations of ML, or even statistical models. They all rely on many human choices —by data scientists, data collectors, data engineers, people on whom data are collected, the end users (e.g. decision makers), and more.

In judicial decision making, there has been a growing number of studies identifying issues related to deployment disasters, triggered by the 2016 ProPublica report on glaring errors of the COMPAS system used in several judicial decision making contexts. Many of the issues are related to discrepancies between the data used to train the algorithm and those during deployment, but there are many other context-related issues that surface when we ask “how will the ML solution be used to generate an action?” We can then ask what data the judicial decision maker will use as input to the system and compare that to the input used to train the data (different populations, different definitions of “recidivism”, etc.). We can compare the action that will be triggered (e.g. parole decision) to the action used to define the output in the training data. These are examples of the critical knowledge a dialogue would uncover.",https://medium.com/swlh/machine-learning-algorithms-surprises-at-deployment-6388b006fc0c,['Galit Shmueli'],2020-12-11 17:35:03.478000+00:00,629,"machine learning, deep learning, simpson's paradox, prediction models, causal structure"
Basics: Gradient*Input as Explanation,"Basics: Gradient*Input as Explanation

In this post, I want to get to the basics of gradient-based explanations, how you can use them, and what they can or can’t do.

Gradient*Input is a great way to explain differentiable machine learning models, such as neural networks, because it is conceptually simple and easy to implement. Computing gradients is also very fast, especially if you make use of modern ML libraries, like pytorch, TensorFlow, or JAX, that include automatic differentiation.

Gradient*Input is only suitable for explaining relatively simple models, but it is an essential concept, necessary to understand a host of other gradient-based techniques that are more robust.

What are gradients?

Chances are you have come across gradients in high-school or university classes. This will not be a rigorous introduction, but just a quick reminder.

First, we need to remember what derivatives are. In essence, the derivative of a differentiable function, f(x): ℝ → ℝ, tells us for any input x how much f(x) will change if we increase x a little bit.

Blue: The function f(x). Red arrows: Derivative of f(x) at two different x. The derivative itself (one number) is just the incline of the arrow. Image by author.

This is useful for example for convex optimization problems: To find a minimum we just start at any point and go into the direction in which f(x) decreases the most and repeat calculating the gradient and taking steps until we arrived at the minimum, i.e. gradient descent.

The gradient of a differentiable function f(x): ℝ^d → ℝ is just a vector of (partial) derivatives of f(x) w.r.t. to every of the d dimensions of x.

Blue: The function f(x1, x2). Red arrows: Gradient of f(x1, x2) at two different points. Image by author.

The gradient tells us for any point (x1,x2) how much f(x1, x2) will change when taking a small step in any direction in the d-dimensional input space. Thus, it also tells us in what direction f(x1, x1) will increase the most.

Formally, we write the gradient of f(x) w.r.t. x as ∇f(x).

What does an attribution look like?

A large class of explanation methods meant to explain machine learning models is called attribution methods, producing explanations called attributions. The purpose of an attribution is to attribute a share of the responsibility for a particular output to every dimension of the respective input.

For example, consider a DNN trained to recognize crocodiles from an image. An attribution method would assign a score to every pixel of the input, indicating how much this pixel contributed to the final output (crocodile or not-crocodile). If we put these individual attributions back together into a matrix, we typically call them an attribution map. We can visualize an attribution map as an image, indicating which regions of the image are important.

Left: Crocodile from ImageNet2012 dataset. Right: One possible attribution map, overlayed over the original image. Image by author.

There are different ways of creating attribution maps and generally, there is no ground-truth to tell us which pixel contributed how much to the output. That is why no attribution method can claim it is “the right one”. Generally, they all have different properties that may be useful in different application scenarios.

What is Gradient*Input?

Gradient*Input is one attribution method, and among the most simple ones that make sense. The idea is to use the information of the gradient of a function (e.g. our model), which tells us for each input dimension whether the function will increase if we take a tiny step in this direction. If our function output is, say, the probability of the image containing a crocodile, then the gradient basically tells us how much each dimension increases the model’s prediction of crocodileness.

Now, if we know for each dimension how much one step in this direction increases the output, we just need to multiply it with the input itself to get a complete attribution.

The idea is that the gradient tells us the importance of a dimension, and the input tells us how strongly this dimension is expressed in the image. Putting this together, the attribution for a dimension is only high if 1) the dimension seems to be important for the output and 2) the value for the dimension is high.

So far, it seems all very reasonable. There is only one problem: the gradient only tells us the importance of a dimension if we just take a tiny step. This is very local information, and when explaining complex functions, the gradient can change quickly even after a few tiny steps in the input space. This means that the function might go up if we take a step in the gradient direction, but it might also go down if we take a larger step in the same direction — effectively invalidating the explanation provided by gradients.

Now that sounds pretty disappointing and there are certainly datasets and models for which Gradient*Input does better than for others. If you are trying to explain a shallow neural network, it is still worth a try.

There are remedies to this problem though, which rely on the same principle as Gradient*Input but make it more robust. Perhaps the simplest are SmoothGrad and Integrated Gradients but there are countless other methods out there that are more sophisticated and may build on different principles.

Closing Words & Alternatives

Gradient*Input’s strength that it is so simple and fast. It is also a solid conceptual basis for more involved explanation methods.

The downside of vanilla gradients as an explanation method is that they are only suitable to explain a function locally. This means, that you should only use them to explain rather simple functions because simplicity implies that they behave similarly globally as they do locally.

In one of my previous posts, I made a comparison of gradient-based explanation methods. All of them are based on Gradient*Input, and adapt it to be more suitable for explaining complex functions.

Of course, there are many more explanation methods out there. The most popular ones include Occlusion Analysis, SmoothGrad, Integrated Gradients, Expected Gradients, LRP, DeepLIFT, and Shapley Values. I would suggest turning to them if you want to explain deep neural networks.

I hope you learned something useful.",https://towardsdatascience.com/basics-gradient-input-as-explanation-bca79bb80de0,['Eugen Lindwurm'],2021-03-03 21:01:02.889000+00:00,1006,"Gradient*Input, Explanations, Attribution Methods, Derivatives, Gradients"
Text Recognition Workflows | How to Use Clarifai’s Advanced Text Workflow,"By Jeff Toffoli

We are retiring our OCR model and have replaced it with a new and improved Visual Text Recognition (VTR) workflow. The new workflow separates the components of text recognition so that you can work with detection, cropping and text recognition as individual components in your projects.

The VTR workflow is already configured for you when creating an app in Portal. Just select “Visual-Text-Recognition” when selecting the base workflow for your app.

You can inspect the individual models in the workflow by clicking the “Copy to new Workflow” button in the “App Workflows” section of the App overview page.

Click here to find out more about making workflow predict calls.

The VTR Workflow Post Workflow Results Request in our API

The new VTR workflow uses a call to the workflow endpoint in our API (as opposed to the OCR model where you were making a single “model predict” call).

We are retiring our OCR model and have replaced it with a new and improved Visual Text Recognition (VTR) workflow.",https://medium.com/@thomas-molfetto/text-recognition-workflows-how-to-use-clarifais-advanced-text-workflow-556ff1a0a1ea,['Thomas Molfetto'],2020-12-15 12:12:30.556000+00:00,164,"VTR, OCR, Visual Text Recognition, Workflow, APICall"
A Step-by-Step Guide to Future Proof Your IT Career!,"Amidst the AI hype, there’s also a sinister buzz about how AI will take away our jobs! However, truth remains that the role of manpower in AI-enabled world will become minimal but not destroyed. In fact, AI experts will always stay in demand!

It means that…

You don’t need to fear the AI job loss epidemic that has creating fear among the professionals. And if you still have some doubt then read this…

Gartner survey says that among 89 countries a massive 270% jump in AI took a toll in implementing this technology in the past four years. This growth is not going to cease until the foreseeable future. Markets and Markets predict the AI market that was valued at $21.5 billion in 2018 will rise to $190.6 billion by 2025.

With 91% of enterprises expecting to hit the AI business market by 2023, AI experts and AI professionals will drastically rise. The discussion on the growth of AI is inevitable, it’s time you ask yourself this question — ‘Am I ready for the future of jobs?’

AI Impact On Jobs:

Over 80 million people around the world will lose their jobs to automation by 2030. But these statistics need to juxtapose, as much as we fear to lose our jobs to automation, this trend will also redefine and create newer roles in the AI industry.

In 2019, 40% of the digital initiatives will use AI and its services.

If you wish to become an indispensable candidate/employee, then you should keep up with the evolving trends and technologies in the field of AI. You can make it possible by up-skilling or retraining yourself in the IT field.

Here I am listing down intelligent ways to future-proof your AI career

👉1. Doing what robots can’t do:

Not a day goes by without hearing how AI is going to rule the job market. Sounding alarming? As much as automation and robots are here to ease our jobs, yet, the imminent rise of robots is threatening human employment. Precisely, the machines that we humans have developed are now performing tasks sharing our employment.

Artificial intelligence is here to better our lives. The majority of our jobs will indeed be replaced by AI. Likewise, skills such as tasks such as critical thinking, soft skills, creativity, empathy, and intuition won’t be accomplished by these so-called robots.

At any rate, if humans are threatened due to constant bickering of robots taking away our jobs. It is not going to be salient, at least not for the next two generations to come. At the moment, what we’re supposed to be worried about is about the comeback and the spreading disinformation. Why debate when we don’t even have relevant skills to combat with the ongoing AI skill gap?

👉2. Learn a new programming language:

For professionals in the IT domain, learning a new programming language or having the knack for coding is the smartest decision to take. Staying ahead of others and future-proofing one’s career by learning to code in in-demand languages can end up replacing workers with redundant skills.

For instance, specialist working behind desks have been replaced by chatbots. But have you wondered, who are the people responsible for creating automated chatbots? Well, they’re 🔗AI experts and programmers who’ve been trained to work on technologies such as AI and neuro-linguistic programming (NLP) making it possible for customers to gain solutions.

Learning a new programing language will certainly have an add-on advantage in your portfolio. As an individual aspiring to enter the AI world, it is essential that you concentrate on these trending skills.

As a developer in the IT field looking to enter the realm of artificial. Learning statistics for machine learning and Python programming is highly recommended.

👉3. Demonstrate your value in varied ways:

We’re living at the edge of a competitive job market where in-demand skills are proven valued by employers. In no time, the interaction between human and automation will no longer be a problem provided IT professionals are skill ready. However, for individuals working in sectors such as medical, hospitality, legal, and manufacturing will be seen replaced, if one is still following old traditional practices.

Go the extra mile:

3.1 — Take a glance at yourself and assess the skills you’re required to learn. Considering you’re the person responsible for automation improvements, you must also ensure the process runs smoothly adding value to the organization.

3.2 — Be the firewall wizard. Take responsibility for rolling out automation and handling it efficiently. If so, master the art of determining whether automation is handling its job properly.

3.3 — Add another skill-set to your existing skills. Expanding roles from being a developer and growing to become an AI expert is an added advantage for IT professionals today.

3.4 — Learn a new tool to enter the AI world. You will find several online free security and cloud tools for one to learn. This requires an individual to have exceptional skills to conduct manual analysis for the tool to remain effective.

👉4. Adapt to newer skills:

Who else but the IT professionals are aware that technology keeps changing. From x68 server farms to containerized applications, professional who had the urge to transform according to the paradigm shifts are the ones who have sustained. With AI and machine learning being the next thing, AI professionals are preceding today’s job market.

Many industries are adopting artificial intelligence, but progress remains grim as there’s a scarcity of available AI talent. And if you want to make a career in AI then certifications is the next step. 🔗AI certifications will provide you with a much needed edge to beat the competition and stay ahead of your peers.

Artificial intelligence is certainly the disquieting buzzword and if you want to make a career in this field then arm yourself with latest tools and techniques, keep yourself updated with the latest trends in the field.

📫 My other Stories on AI:",https://medium.com/analytics-vidhya/step-by-step-guide-to-future-proof-your-it-career-cdf937f98634,['Albert Christopher'],2019-08-09 05:56:44.821000+00:00,960,"👉5. AI, Automation, Job Loss, Future-Proof Career, AI Impact On Jobs"
Enhancing the Tools for Differential Privacy in Google’s TensorFlow,"In a world where the risks and costs associated with privacy are on the rise, and privacy issues are leading to broader questions around trust and AI, we believe that differential privacy offers a solution. Differential privacy allows machine learning teams to create products without risking the privacy of any individual’s data.

With the right tools, we think that companies of all sizes can take advantage of advanced machine learning techniques such as differential privacy. Google’s initial release of TensorFlow Privacy — an open-source library that makes it easier for developers and researchers to train machine-learning models — made this a reality for deep learning.

We’re excited to announce that we have made our differential privacy software product available through TensorFlow Privacy (GitHub) in collaboration with Google. Our contribution to Tensorflow Privacy adds support for two common machine learning techniques — Logistic Regression and linear Support Vector Machines.

Now, developers from companies of all sizes can access these libraries and start using them today.

What is differential privacy

Differential privacy is a mathematical definition of privacy loss when private information is used to create a data product. Specifically, differential privacy measures how effective a particular privacy technique — such as inserting random noise into a dataset — is at protecting the privacy of individual data records within that dataset.

Solutions that satisfy differential privacy inject noise into a dataset, or into the output of a machine learning algorithm, without introducing significant adverse effects on data analysis or model performance. They achieve this by calibrating the noise level to the sensitivity of the algorithm.

The result is a differentially private dataset or model that cannot be reverse-engineered by an attacker. This makes it impossible to identify individual records with certainty, e.g., customers, patients, within a dataset.

For example, Apple uses differential privacy to analyze the typing patterns of iPhone users without being able to identify any individual customer. In this example, macro-level questions about text patterns — such as predicting which word is most likely to be used next for predictive typing — can be answered without compromising the individual privacy of the people contributing the data.

Software and Differential Privacy at Georgian Partners

At Georgian Partners, we’re building software to help our companies further accelerate their adoption of disruptive technologies. Our software is directly tied to applied research in our investment thesis areas — Trust, Applied AI and Conversational AI. Each software tool we develop is designed to help push breakthrough technology to our companies.

We developed our differential privacy software product in collaboration with some of our companies. They faced a challenge in that even though they had varying degrees of freedom to use their customers’ data; they hesitated to aggregate data across customers.

Without aggregating data, it can be challenging to deliver value to new customers of machine learning products because they have not yet amassed enough data. This is called the cold-start problem. You can solve this, improve on-boarding times and reduce time-to-value for new customers by using aggregate data and machine learning models from existing customers. However, to convince customers to share data or models, you need to guarantee their information privacy and gain their trust. This is where differential privacy helps by guaranteeing privacy to earn trust.

Our Approach to Differential Privacy

There are several ways to approach differential privacy, depending on the nature of the problem and availability of data. For our cold-start use cases, we have limited labeled data available. Also, we wanted to support some of the more common machine learning algorithms first. After exploring different methods, we identified the Bolt-on method as the best approach for our use cases.

The Bolt-on algorithm is easy to implement, and it relies on stochastic gradient descent (SGD), a generic optimization technique that can be applied to various convex optimization-based machine learning techniques such as Logistic Regression and linear Support Vector Machines (SVM).

In the Bolt-on algorithm, SGD is treated as a black box, and noise is only added to the machine learning algorithm output at the end of the optimization process. In our research, we observed that just a small amount of noise is needed to achieve reasonable privacy guarantees.

Getting started with TensorFlow Privacy

If you’re working on a similar cold-start use case, check out the examples and tutorials in the TensorFlow Privacy GitHub repository.

To learn more about differential privacy, you can read our CEO’s Guide to Differential Privacy. Finally, to learn how privacy is integral to building and leveraging customer trust, read the CEO’s Guide to Trust.",https://medium.com/georgian-impact-blog/enhancing-the-tools-for-differential-privacy-in-googles-tensorflow-privacy-1bddcffec2b4,[],2019-09-03 18:23:16.879000+00:00,737,"Differential Privacy, Tensor Flow Privacy, Logistic Regression, Linear Support Vector Machines, Stochastic Gradient Descent"
I would attribute the lack of innovation in the last ten years to venture capitalists.,"I would attribute the lack of innovation in the last ten years to venture capitalists. They overly participate in group think and rarely think for themselves, mostly missing innovative projects while overly participating in safe projects that society doesn’t need and doesn’t appreciate. Their policies (which are adhered to perfectly) swing wildly to and fro like a pendulum, first aggressively investing in pre-profit companies then only years later complete opposite policy (to which they all adopt simultaneously). I am not in the slightest impressed with venture capitalists. They need to stop following their truly dismally stupid rules (that don’t account for anything that makes life great, or companies for that matter) and go from the heart. Do you want more safe companies in the world, or do you want profound ones? My experience with venture capitalists tell me they’ll only choose safe bets, and this society is not the better for it. It really a shame to have five thousand copies of every type of company in this world and a complete lack of innovation. VCs truly should be ashamed of themselves.",https://medium.com/@kevinteman/i-would-attribute-the-lack-of-innovation-in-the-last-ten-years-to-venture-capitalists-1b47a5a8096a,['Kevin Teman'],2020-12-27 08:57:55.188000+00:00,182,"Venture Capitalists, Group Think, Pre Profit Companies, Safe Bets, Innovation"
Clustering Instability,"Clustering Instability

Clustering is an unsupervised learning technique used to create clusters of data points. An example is customer segmentation in marketing.

There are several clustering algorithms available. However, they require the number of clusters (k) to be given as input. Selecting the number of clusters can be difficult since this is an unsupervised problem, without labels.

Clustering Instability can be measured to determine the performance of the clustering algorithm, given k. We would prefer a cluster to be stable, but what does this mean?

Let’s say you have a sample of observations (X), we can shuffle this dataset and take three subsamples. These three subsamples are from the same sample X so they should follow the same structure.

We take our clustering algorithm of choice and train two separate versions of the model. First, a model is fitted on dataset z1. Second, a different model is fitted on dataset z2. In both cases, the same algorithm is applied given a specific value for k, the number of clusters.

model1.fit(z1) model2.fit(z2)

After both models have been trained, we predict the labels of the third (validation) subsample z3.

model1.predict(z3) model2.predict(z3)

To measure the instability of this cluster mapping, we calculate it as follows:

for every observation i and it's neighbour j = i+1

and it's neighbour check if model 1 agrees they have the same label: label1(xi) = label1(xj)

check if model 2agrees they have the same label: label2(xi) = label2(xj)

If model 1 labels both observation i and j equal, but model 2 on the other hand labels both different this causes instability of the cluster mapping.

If agree1 != agree2: instability += 1

This leads us to an instability score for this k and these subsamples. We want to repeat this process for a range of k to assess the clustering instability for all.

Finally, we select the number of clusters (k) which minimizes the instability.

Feel free to reply with any questions on the theory or how to code.

I am currently working on an application for selecting the number of topics in topic modeling with a Latent Dirichlet Allocation, let me know if you would like to see a follow-up!",https://towardsdatascience.com/clustering-instability-486643bb686e,['Eva Van Rooijen'],2020-06-01 14:19:37.606000+00:00,345,"clustering, unsupervised learning, customer segmentation, clustering algorithms, cluster stability"
Is AI going to take my job?,"“Will AI take my job?” This is one of the most asked questions on the internet. Before answering this question we have to understand what AI actually is and how it works and the history of technological innovations in general. Then it will help us to understand if there is any worry.

As its name suggests artificial intelligence is intelligence like humans has but it is created artificially. Before the age of AI, computers didn’t have their own intelligence. Programmers had to program manually for every permutation and combination of a task. They couldn’t do it on their own, so something like face recognition was impossible because every face has its own characteristics and programmers would have to write code to recognize every face individually. But now with the help of advanced mathematics and the increased computational power of computers, it is possible.

AI is nothing but another technological innovation like TV or mobile phone or the internet. It is the history of every technical innovation that when it is in its baby steps it creates a lot of buzz and a lot of skepticism. Like with the TV everybody said that it’s an idiot box and it will kill radio but radio is still alive and with the internet, people said it will destroy the world but here we are, our online lives have become as important as our offline lives.

It’s our history, whenever there is a groundbreaking change in our society, we adapt to it. The moral of this short history lesson is that the same will happen with AI.

As AI is an intelligent machine, there is one thing machines are very good at and that’s doing repetitive tasks more faster and accurately than human beings. So there are some jobs that are bound to change like factory workers on an assembly line or receptionists at a desk of the office, jobs like these will be done by a robot with AI. And that’s inevitable. Now if you are doing a job which consists of repetitive tasks, then it will be replaced by AI sooner or later.

There are some jobs that will not be replaced by an AI or not at least in the near future. Jobs that require human creativity or human emotion or imagination or any other human ability that nature has only given us. Jobs like writers, psychiatrists, scientists, lawyers, HR managers, or the same people who created AI, the software developers.

Now what’s the solution, if your job lies in nonreplaceable jobs then you don’t have to worry but if it is on the first list you have some options. If you are in the very early stage of your career you can always shift your job but if you are in later stage of your career, there is a history lesson in this regard. All those people who lost their job to automation just didn’t become unemployed. The nature of their job changed. Factory workers who worked on manual packaging lost their job to assembly line machines. They changed their job from packaging to operating those same machines, bank tailors whose job was taken by ATMs found new jobs in the same banks.

So the answer to the million-dollar question “Is AI going to take my job?” is “NO”. You will not be unemployed just the nature of your job will change.",https://medium.com/@ganeshpawarpodcast/is-ai-going-to-take-my-job-c9ec7fddd360,['Ganesh Pawar'],2021-07-15 09:56:23.265000+00:00,555,"Artificial Intelligence, Technology, Innovation, Automation, Job Security"
AI is making the Connected Home much better,"The Nest is an AI-powered smart home appliance used to control the temperature. Nest understands how the home owner is adjusting the temperature on a daily basis and across seasons. It learns from human inputs and automatically starts changing the temperature based on owners comfort. For example, if a homeowner has a tendency of raising the temperature when inside the house and lower or turnoff when away, Nest learns these behavioural patterns and automatically adjusts the temperature as per the occupants preference. It also recommends, reminds or automatically turns off the lights when the homeowners leave the house, thereby resulting in energy savings.

In addition to power saving appliances, the ability to remotely monitor the house saves cost as otherwise one would have to hire a security guard or have a housekeeper.

Convenience

The use of technology in homes has made it easier for the elderly and disabled to live without the need for a caregiver. With mobile applications, sensors, voice commands, and automated garage doors one can brew coffee, open curtains, set AC temperatures, let in a guest or turn on some music at the comfort of their bed.

AI is offering quick solutions to our daily problems. Imagine having to go to a meeting but at the same time, you need your leaking toilet bowl fixed. With keyless locks, security cameras and a programmed robot, you do not need to be physically there to have the plumber fix your problem. You can let in the plumber, monitor work progress, guide him through the house using the robot and lock the door after he has left. You can even pay the plumber through an escrow service.

Comfort and Peace of Mind

Monitoring your house, a sleeping baby or your kids playing outside from anywhere in your house gives you peace of mind and saves you the trouble of having to run out every minute to check.

What can be more peaceful than knowing you can have your groceries delivered at your door by a single tap on your smartphone? In fact some devices are even smarter and can order the groceries on their own. With the Internet of Things and Chatbots, your refrigerator can detect that you are running out of specific vegetables. It can engage a chatbot to have a conversation with the local grocery store and automatically place an order for the food who’s supplies are running low.

Personal Assistant

A popular personal assessment from Apple is called Siri. Siri is voice-activated to allow you to give commands like text someone, call, get information or directions, manage time, and also serve a reminder for events or activities saved on your calendar.

The use of robots in homes is still in infancy but they are very effective as the perfect personal assistants. Robots performing specific tasks such as automated house cleaning vacuum, lawn mowing or pool cleaning can be quite effective. They can even be programmed to run at specific time, like vacuum the house when no one is in it.

The robots can also be used to welcome your guests, carry their luggage like in hotels and give them directions around the house.

Courtesy ipalrobot.com

The use of robots has also infiltrated daycares. They raise and take care of our kids while we are not around. An example is AvatarMind iPal that is not only an assistant but a caregiver, friend, playmate, and companion.

Entertainment

Our favourite in-home entertainment systems are already intelligent. Whether it is Netflix, Amazon Prime, Spotify, they understand what we like and recommend new audio and video content that we might like. They use AI to deliver such recommendations and are extremely accurate with the same.

Music systems such as Apple HomePod, use AI to deliver the best spacial sound experience in our homes. Just 2 HomePod speakers, can create a mesmerising audio experience, equivalent to that of a 6 speaker home theatre system.

Courtesy apple.com

Entertainment is key to creating a personal satisfaction from any connected home. We all love to watch television and listen to music. Each has to be perfectly tuned to our moods. These systems are getting smarter by the day of giving us the best in content recommendations and viewing experiences. Connected home devices today can automatically dim the lights in the room when you start watching your favourite show on Netflix. Even so, it can gently turn on the lights when you pause and turn them back off when you resume playing.

In addition to security, convenience and energy saving, a connected home must offer entertainment to its occupants, and that too should be personalised to each occupants unique tastes.

Conclusion

Imagine coming home, the garage door opening and closing automatically, being greeted and welcomed, lights switching on, curtains closing, finding a ready cup of coffee, having a personal assistant helping you with your luggage, a clean fresh smelling house with the right atmosphere, your favorite music playlist on and a bathtub full of warm water waiting for you.

Yes, all that is no longer a fantasy or a dream. With AI life is safer and simpler. Well, now stop imagining and get AI in your smart home for a better life because the future is automation; and it is already here.

References",https://medium.com/blobcity/ai-is-making-the-connected-home-much-better-403d0b5c9629,['Sanket Sarang'],2019-10-27 13:36:49.194000+00:00,853,"smart home, AI, convenience, comfort and peace of mind, personal assistant"
Here’s how you can access your entire iMessage history on your Mac,"What’s a database?

A very simple way to understand what a database file is is to think of it a a folder that contains a bunch of excel-like tables. Much like larger or enterprise-grade databases you can connect and access the data in the database in a variety of ways.

Using Python and pandas

In this tutorial, I’m using Python and the amazing pandas module to connect to the database, explore the tables and data it holds and then read that data from the appropriate tables.

connection code

import sqlite3

import pandas as pd # substitute username with your username

conn = sqlite3.connect('/Users/username/Library/Messages/chat.db') # connect to the database

cur = conn.cursor() # get the names of the tables in the database

cur.execute("" select name from sqlite_master where type = 'table' "") for name in cur.fetchall():

print(name)

Above we connect to the database and explore what tables are in there. I found that there are a few tables in the database including one called message and others names chat, handle and attachment. Let’s explore the message table because that’s the one that sounds most promising to hold our iMessages. I do that by transferring the table into a pandas dataframe, a type of file that is much easier to explore and manipulate for data analyis projects.

# get the 10 entries of the message table using pandas

messages = pd.read_sql_query(""select * from message limit 10"", conn)

Getting the message text and phone number

We hit bingo! The message table indeed seems to hold all the saved iMessages. It has a text field with the actual sent or received message, a date field (more on that below) and a handle id. After a little exploration I found that the handle_id is a code for each phone number or Apple-id that you have had a conversation with. In order to map the handle_id back to the Apple-id we can use a table in the database (appropriately) named handle and join on handle_id.

# get the handles to apple-id mapping table

handles = pd.read_sql_query(""select * from handle"", conn) # and join to the messages, on handle_id messages.rename(columns={'ROWID' : 'message_id'}, inplace = True) handles.rename(columns={'id' : 'phone_number', 'ROWID': 'handle_id'}, inplace = True) merge_leve_1 = temp = pd.merge(messages[['text', 'handle_id', 'date','is_sent', 'message_id']], handles[['handle_id', 'phone_number']], on ='handle_id', how='left')

Adding a chat id

Similarly, the message table also includes a chat_id that maps each message back to unique chat. This can be useful when doing analysis on chats with multiple people in them. We can get the chat_id of each message by joining the message table with the (again, appropriately named) chat_message_join table on message_id.

# get the chat to message mapping

chat_message_joins = pd.read_sql_query(""select * from chat_message_join"", conn) # and join back to the merge_level_1 table

df_messages = pd.merge(merge_level_1, chat_message_joins[['chat_id', 'message_id']], on = 'message_id', how='left')

Getting the date

The message table also includes a date column and this was a little tricky for me to decode since it isn’t exactly in any format that is widely used in the industry. Moreover, the way that this column is recorded is a little different in newer version of Mac OS X compared to older ones.

C redit to this stackoverflow page that helped me figure this out.

In Mac OS X versions before High Sierra (which is version 10.13 and released in September 2017), the date column is an epoch type but, unlike the standard of counting the seconds from 1970–01–01, it is counting the seconds from 2001–01–01. In order to convert that type into a data field we can actually comprehend we can use a command while querying the message table to create a new field (we will call it date_utc, since it is giving a UTC timezone date as a result) based on the date field.

# convert 2001-01-01 epoch time into a timestamp

# Mac OS X versions before High Sierra

datetime(message.date + strftime(""%s"", ""2001-01-01"") ,""unixepoch"",""localtime"") # how to use that in the SQL query

messages = pd.read_sql_query(""select *, datetime(message.date + strftime(""%s"", ""2001-01-01"") ,""unixepoch"",""localtime"") as date_uct from message"", conn)

In Mac OS X High Sierra and above, it’s the same thing but the date format is now much more granular: it is in nano-second level. So now we need to divide by 1,000,000,000 before we apply the same code snippet we applied above.

# convert 2001-01-01 epoch time into a timestamp

# Mac OS X versions after High Sierra

datetime(message.date/1000000000 + strftime(""%s"", ""2001-01-01"") ,""unixepoch"",""localtime"") # how to use that in the SQL query

messages = pd.read_sql_query(""select *, datetime(message.date/1000000000 + strftime(""%s"", ""2001-01-01"") ,""unixepoch"",""localtime"") as date_uct from message"", conn)

Putting it all together

You can find the notebook here with all the code in order for you to extract your iMessages from your laptop and start analyzing!

It should only take a few minutes and by the end of it you should have a basic history of your iMessage data that includes the phone number (or email), the text, a unique chat for each unique group of people you had a chat with and the timestamp (in UTC timezone) of each message sent.

You can actually find more data in the database such as details if the message was delivered and read as well as attachments. I’m not touching on those attributes on this post.

This post also only instructs on how to get iMessage data from your Apple computer. If you have any pointers on how you can extract your iMessage history from Apple mobile devices (iPhone & iPad) let me know in the comments.

Happy reading your messages!",https://towardsdatascience.com/heres-how-you-can-access-your-entire-imessage-history-on-your-mac-f8878276c6e9,['Yorgos Askalidis'],2019-03-09 16:51:09.667000+00:00,879,"database, Python, pandas, connection code, message text"
Creating Interactive Data Tables in Plotly Dash,"Creating Interactive Data Tables in Plotly Dash

Photo by Markus Spiske on Unsplash

Motivation

Plotly Dash is an incredibly powerful framework that allows you to create fully-functional data visualization dashboards. Using Dash, you can create a full front-end experience using only Python. The library does a great job of abstracting away from the complicated HTML, CSS, and JS associated with all of the different web app components. While this is very nice, it sometimes becomes confusing.

For example, Dash provides the dash_table.DataTable component, which is a table that comes with lots of prebuilt, ready-to-use functionalities. However, some of these features are hard to find or understand, and so getting the table to work exactly as you imagined is very tricky. So, I decided to document two table features that are not very well advertised, but which I feel are extremely important to creating a truly dynamic table.

Handling click events

Adding click events to your data table is a great way to make your dashboard more interactive. Thankfully, this one actually isn’t too difficult.

Using the active_cell component of the DataTable, you can easily get the data of a cell the user clicks on.

A callback using active_cell would look something like this:

@app.callback(

Output('output_div', 'children'),

Input('data_table', 'active_cell'),

State('data_table', 'data')

)

def getActiveCell(active_cell, data): if active_cell:

col = active_cell['column_id']

row = active_cell['row']

cellData = data[row][col] return html.P(f'row: {row}, col: {col}, value: {cellData}') return html.P('no cell selected')

The input is the active_cell component, which means selecting a cell is what causes this callback to fire. Also, note how the callback is sensitive to the data of the table. Without this, you can only access the row and column of the selected cell and not the actual value inside.

Using this callback in a full app would look something like this:

app = dash.Dash(__name__) app.layout = html.Div([ dash_table.DataTable(

id='data_table',

columns=[{

'name': 'Column {}'.format(i),

'id': 'column-{}'.format(i),

} for i in range(1, 5)],

data=[

{'column-{}'.format(i): (j + (i-1)*5) for i in range(1, 5)}

for j in range(5)

]

),

html.Div(id='output_div')

]) @app.callback(

Output('output_div', 'children'),

Input('data_table', 'active_cell'),

State('data_table', 'data')

)

def getActiveCell(active_cell, data):

if active_cell:

col = active_cell['column_id']

row = active_cell['row']

cellData = data[row][col] return html.P(f'row: {row}, col: {col}, value: {cellData}') return html.P('no cell selected') if __name__ == '__main__':

app.run_server(debug=True)

Here’s what the code looks like in action:

Example Data Table with click callback

Adding live updates

This option is great for dashboards that need to display information in real-time (i.e. a dashboard that monitors the usage of some service).

The live updates are divided into 2 main ideas.

Update Interval

In order to update the table periodically, you have to set up an interval for how often the data should change. This is done with the dcc.Interval component from Dash.

This component stores a value, n_intervals, which tracks how many intervals of some specified length have passed.

You can see a full example of using the Interval for updates in the Dash documentation.

The declaration for the dcc.Interval component looks like this:

dcc.Interval(

id='interval-component',

interval=1*1000, # in milliseconds

n_intervals=0

)

And the callback for the n_intervals component looks like this:

@app.callback(

Output('component-to-update', 'property'),

Input('interval-component', 'n_intervals')

)

def update(n_intervals):

if n_intervals > 0:

# update table data

Appending Data

Now that we have a way to periodically perform some action, we need to use this method to update the table data.

We can do this by modifying the callback above. All we have to do is change the output to the data attribute of the table, as well as add a state for data.

This new callback looks like this:

@app.callback(

Output('data_table', 'data'),

Input('interval-component', 'n_intervals'),

[State('data_table', 'data'), State('data_table', 'columns')]

)

def updateData(n, data, columns):

if n > 0:

data.append({c['id']: n for c in columns})

return data

This callback takes in the data already in the table and appends a new row.

Adding this callback to the app detailed above, along with adding the dcc.Interval component to the layout creates the following dashboard.

Live periodic updates to the data table

Note: It’s a good idea to have the interval controlled by a toggle or switch so that it doesn’t run forever. You can do this by updating the disabled attribute of the dcc.Interval in a callback.

Conclusion

In this short blog, we saw two methods of adding extra functionality to a Dash DataTable. Adding click events and live updates are both awesome ways to make your data table more interactive, more flexible, and more powerful.

Try out these features in your own dashboards to truly take your visualization to the next level.

If you liked this blog, let me know!",https://towardsdatascience.com/creating-interactive-data-tables-in-plotly-dash-6d371de0942b,['Akash Kaul'],2020-11-24 03:08:03.397000+00:00,678,"plotly dash, data tables, interactive data tables, dash_table.Data Table component, active_cell component"
Welcome to 2040.,"Technology advancement comes not without consequences. The ever-growing infinite virtual realm made us trapped in a hyperreality, we gradually lost all sense of reality. We lost track of time — there’s a lot of things to do but never had enough time to do so.

We grew apart from the people around us. People in the street are robot-like, walking in constant patterns and somehow became less empathetic to their surroundings. Ironically, we even send romantic or meaningful messages we never wrote. We are more expressive and vibrant in our digital persona but dull and grey to meet in person.

We feel alienated in a 42-floor apartment with nothing but a high-spec computer and built-in artificial intelligence. We are given so much information but unable to process it. There are too many options to choose from just to figure out who we really are or things we actually like. We have thousands of friends online but not a single person to drink a coffee with.

Aren’t we fundamentally lonely? And lonelier each day, a truth that almost suffocating to bear. We feel more comfortable and familiar with an AI than a lovely human partner because we feel that humans are too complicated to understand.

Are we becoming more robot-like? Or perhaps the technology that grows exponentially has somehow become human?",https://medium.com/@ferdinmaulana/welcome-to-2040-d4a45e963dfb,['Ferdin Maulana Ichsan'],2020-12-26 09:54:59.426000+00:00,216,"Technology Advancement, Hyperreality, Loneliness, Robot Like, AI"
Survival analysis and the stratified sample,"Part II: Case-control sampling and regression strategy

Due to resource constraints, it is unrealistic to perform logistic regression on data sets with millions of observations, and dozens (or even hundreds) of explanatory variables. Luckily, there are proven methods of data compression that allow for accurate, unbiased model generation.

Traditional logistic case-control

Case-control sampling is a method that builds a model based on random subsamples of “cases” (such as responses) and “controls” (such as non-responses). Regardless of subsample size, the effect of explanatory variables remains constant between the cases and controls, so long as the subsample is taken in a truly random fashion. For example, if women are twice as likely to respond as men, this relationship would be borne out just as accurately in the case-control data set as in the full population-level data set. Thus, we can get an accurate sense of what types of people are likely to respond, and what types of people will not respond.

After the logistic model has been built on the compressed case-control data set, only the model’s intercept needs to be adjusted. While relative probabilities do not change (for example male/female differences), absolute probabilities do change. For example, take​​​ a population with 5 million subjects, and 5,000 responses. If the case-control data set contains all 5,000 responses, plus 5,000 non-responses (for a total of 10,000 observations), the model would predict that response probability is 1/2, when in reality it is 1/1000. When all responses are used in the case-control set, the offset added to the logistic model’s intercept is shown below:

Here, N_0 is equal to the number of non-events in the population, while n_0 is equal to the non-events in the case-control set.

As a reminder, in survival analysis we are dealing with a data set whose unit of analysis is not the individual, but the individual*week. The following very simple data set demonstrates the proper way to think about sampling:

This technique incorrectly picks a few individuals and follows them over time.

This technique captures much more variability by randomly selecting individual observations from the data set.

Survival analysis case-control and the stratified sample

Things become more complicated when dealing with survival analysis data sets, specifically because of the hazard rate. For example, if an individual is twice as likely to respond in week 2 as they are in week 4, this information needs to be preserved in the case-control set. And the best way to preserve it is through a stratified sample.

With stratified sampling, we hand-pick the number of cases and controls for each week, so that the relative response probabilities from week to week are fixed between the population-level data set and the case-control set. This way, we don’t accidentally skew the hazard function when we build a logistic model.

This can easily be done by taking a set number of non-responses from each week (for example 1,000).

This method requires that a variable offset be used, instead of the fixed offset seen in the simple random sample. The offset value changes by week and is shown below:

Again, the formula is the same as in the simple random sample, except that instead of looking at response and non-response counts across the whole data set, we look at the counts on a weekly level, and generate different offsets for each week j.

Because the offset is different for each week, this technique guarantees that data from week j are calibrated to the hazard rate for week j.

Code for logistic regression

The following R code reflects what was used to generate the data (the only difference was the sampling method used to generate sampled_data_frame ):

glm_object = glm(response ~ age + income + factor(week),

data = sampled_data_frame,

family = ""binomial"")",https://towardsdatascience.com/survival-analysis-and-the-stratified-sample-2c2582aa9805,['Edward Wagner'],2019-04-18 19:00:24.336000+00:00,603,"Traditional logistic case-control, Stratified sample, Survival analysis case-control, Hazard rate, Logistic regression"
Deep learning based web application: from data collection to deployment,"The second technique consists in using pre-trained models, in our case ResNet34. This network has been trained on the ImageNet dataset to detect thousand of classes. We can therefore expect that the representation of the image given by the last hidden layer “captures” a large variety of complex patterns, i.e. it provides smart features.

Even if these concepts may not seem completely clear, this shouldn’t stop us to deploy our app. Indeed, the fast.ai library allows for these techniques to become extremely accessible.

In order to have free access to a GPU without the need for a credit card, we can exploit Google Colab, which is a service provided by Google and hence highly integrated with Google Drive.

Creating and running a Jupyter Notebook in Google Colab is pretty straightforward. To enable GPU press “runtime”, “runtime type” and set the “hardware accelerator” to GPU.

However, we also need to:

Connect with Drive Communicate with Git/GitHub

Once we put our data in Google Drive, to access it from Colab Notebook we may simply run the following code:

Now we can load data from “path”.

To use Github from Drive we basically use a Colab Notebook as command line, as described here. Personally I have used a Colab notebook outside the projects which I have called git_comunicator.ipynb.

So, to clone a Github repository we simply have to run the following from a notebook in Colab:

Once we are done with modifications, we can commit and push modifications running the usual git commands (preceeded by “!”) from a Colab notebook cell.

Once we have created a GitHub repository for our project, we are able to clone it on Google Drive, run existing notebooks in Colab exploiting a free GPU and commit and push the changes!

Now that we’ve solved technicalities, it’s time to train our classifier. Again, fast.ai makes it super easy. First, let’s load data:

This code creates a DataBunch object:

keeping 20% of data for validation applying standard data augmentation techniques applying the ImageNet dataset normalization

Good, now we would like to use the ResNet34 network architecture with pre-trained weights, and train only the connections of the last hidden layer with the output layer. This can be done in two lines of code:

This code uses the fit-one-cycle policy, which is a smart technique to train a network in a short amount of time by using a learning rate which progressively increases at first, and later progressively decreases. This led us to the following results:

We reached almost 93% of accuracy on validation set in less than 6 minutes. However, we can observe that the training loss is still greater than the validation loss. This means that there’s still space for improving!

It may be worth to train the whole network now, not only the last layer. This can be achieved by, again, two lines of code:

Setting a slice instead of a single value for the maximum learning rate in the policy allows to train the network with different learning rates. The first layers will be trained with smaller learning rates, since they “detect” simple patterns that should not change much amongst different image data distributions.

So let’s have a look at the confusion matrix we obtained now:

Wow, this is 98.73 % of accuracy on the validation set!

If we were to use this as a professional application we would have to provide a separate test set to estimate the performance of our classifier. However, for a toy project like ours this result is more than enough!

Since we are satisfied with our model, we can export it for deployment by running:

learn.export()

Everything you need to run the model will be stored in a file named export.pkl, placed into “data” directory in our case.

3. Deployment

If training was easy, deployment is even easier, thanks to Docker and Render.

Docker is a tool which allows to easily deploy and run applications by using containers. In simple words it is a sort of Virtual Machine that, rather than virtualizing the entire OS, uses the same Linux kernel as the system that is running on, resulting in improved performance.

Render is a web-service that allows to build and run web applications extremely easily (among other things).

Again, we don’t even need to understand how these things actually work. We can simply use the sample code provided here by fast.ai by adding it to our repository (or forking it from the beginning).

To deploy on Render it is enough to:

Create a Render Account and connect it to Github Upload the trained model file export.pkl in Google Drive and make it downloadable Edit model_file_url variable with the URL to export.pkl and update the line classes in the file server.py Edit index.html to change website text and layout (Optional) Start a new Service

All these steps are described in details here.

Finally you should now have a working deep-learning based web application! It should look something like this:",https://towardsdatascience.com/deep-learning-based-web-application-from-data-collection-to-deployment-9e89f29b67df,['Davide Burba'],2019-09-08 02:31:44.729000+00:00,792,"pre-trained models, Res Net34, Image Net dataset, Google Colab, GPU"
Introducing OpenUBA: an open source user behavior analytics platform powered by the scientific computing ecosystem,"Visit www.openuba.org for more detail, but I would love your feedback on this work, and all inquiries to help are welcome ❤️

I have had a chance to witness countless UBA tools hit the market, raise funding, serve advertisements, and present at conferences across the country.

However, one thing still remains — UBA vendors keep their models black boxed. I have yet to see a truly “open model” UBA platform that is built on the countless programmer hours put into the popular scientific computing ecosystem. UBA vendors constantly attempt to market advanced AI capabilities being performed on security data ingested by their customer. However, since I am a data scientist, I have pondered so much on these “models” running behind the scenes. I even have had several occasions to slightly peek under the hood on many UBA products, although slightly peeking is cumbersome by itself, and limited.

Even if you look at the open source behavior analytics projects, they all have an emphasis on security, integrations, etc, and far less emphasis on data science/modeling.

It’s like open source UBA tools want to perfect everything else besides actual modeling

It was clear to us before, but now we keep hearing this from developers wanting to use OpenUBA — so, we assume the problem exists for many others.

Problem statement

UBA tools expose “black-box” modeling capabilities, and actually hinder meaningful, and advanced model development.

UBA tools expose “black-box” modeling capabilities, and actually hinder meaningful, and advanced model development. These tools also do not extend the usage of the scientific computing community, therefor missing out on substantial user adoption/usage for the sake of protecting their “IP”.

Background

In 2018, after realizing UBA tools were not going to just magically become more open, I began ideating a purely open source UBA platform built using on real data science disciplines.

I started my corporate career in 2015 working for Research & Development with a fortune 50 company. In R&D, we specialized in building bleeding-edge proof of concepts on emerging technologies. This is where I was able to become introduced to several new technologies (blockchain, IoT, DevOps, Modern Embedded Systems, ML/DL, etc). Before joining corporate America, my colleagues, and I ran a successful technology development firm in downtown Atlanta, GA. During those ~4 years, we developed pretty innovative systems for brands, and celebrities — small, and large. Prior to that I was a solo engineer developing my skills since 2008–09. Bottom line, my entire technology career has been focused on innovation, and furthering how things are done.

I took it upon myself to develop this for the community as a whole, and easily found other security/data people who were interested in the same thing

Architecture

How does OpenUBA work? For starters, here is our current technology stack.

“you can’t patent math” — since UBA tools attempt to protect their model development approaches, and the model logic itself, we can mitigate against this by extending the already existing, and well understand data science research as it unfolds in the public arena.

you can’t patent math

While working in a SOC on data science related solutions, it was clear to me to see the connection between what my employer wanted, and the tools required to develop it. However, working in a large corporation doesn’t always enable us to innovation every second of the day — plus, I knew this couldn’t be developed in a weekend. The following are two very high level ideas driving our work on OpenUBA.

Compute Engines

We are purposely keeping the compute engines very focused by using Spark, and Elasticsearch. This enables us to feed data into the system using two of the most performant, and most adopted compute engines available. Simply by using these two platforms, we now have the means to feed data into the system from a variety of source, all at scale. The rest of the solution is well thought out abstractions around data, models, anomalies, etc.

Model Library/Registry

To foster a community, we have developed a model library. It works similar to Docker Hub. Here, developers and security analysts can simply search a registry of ready-to-use models covering security gaps. This enables model developers to share useful models with the ecosystem, whether for free or for compensations.

There are a lot of moving pieces with this ecosystem, but we are aiming to keep it simple, useful, and transparent.

We want to take the very complicated space of Security Analytics, and disrupt it — in a free, and open way.

We will be posting more in depth looks at the features in OpenUBA. Subscribe to our medium publication to stay updated.

Visit www.openuba.org to find our documentation, repository, and white paper. We are building an awesome team of passionate hobbyists, and welcome anyone interested. Join our discord: https://discord.gg/Ps9p9Wy",https://medium.com/georgia-cyber-warfare-range/introducing-openuba-an-open-source-user-behavior-analytics-platform-powered-by-the-scientific-5d71bc50b808,['Jovonni L. Pharr'],2020-06-04 05:43:20.456000+00:00,773,"UBA, Security Analytics, Open Source, Data Science, Model Library"
How to use the VGG16 neural network and MobileNet with TensorFlow.js,"In this article, we will build a deep neural network that can recognize images with a high accuracy on the Client side using JavaScript & TensorFlow.js. I’ll explain the techniques used throughout the process as we go along.We will be using VGG16 and MobileNet for the sake of the demo.

If you need a quick refresher on TensorFlow.js, read this article.

Below is a screenshot of what the final web app will look like:

Final Web App

To start off, we will create a folder (VGG16_Keras_To_TensorflowJS) with two sub folders: localserver and static. The localserver folder shall contain all the server NodeJS code, and the static folder will have all the CSS, HTML, and JavaScript code.

Screenshot Showing the Folder structure

Note : you can name the folders and file whatever you like.

Server Configuration

We will manually create a package.json file with the below code:

{ ""name"": ""tensorflowjs"", ""version"": ""1.0.0"", ""dependencies"": { ""express"": ""latest"" }}

The package.json file keeps track of all the 3rd party packages which we will use in this project. After saving the package.json file, we will open the command line and in it we will navigate to the localserver folder. Then we will execute the following:

npm install

Command Line for MacOS

After doing so, NPM will execute and ensure that all the required packages mentioned in package.json are installed and are ready to use. You will observe a node_modules folder in the localserver folder.

We will create a server.js file with the below code:

server.js contains the NodeJS code which allows the local server to be hosted which will run our WebApp.

Client Configuration

Next we will create a predict_with_tfjs.html. Below is the code:

Once the HTML code is done, we will create a JavaScript file and call it predict.js. Below is the code:

Model Configuration

Once the client and server side code is complete, we now need a DL/ML model to predict the images.We export the trained model (VGG16 and Mobile net) from Keras to TensorFlow.js. Save the output in folders called VGG and Mobile net, respectively, inside the static folder.

ScreenShot for Python

Defining the Classes

We will keep imagenet_classes.js inside the static folder. This file contains a list of all the ImageNet classes. You can Download this file from here.

Testing the Code

After all the setup is done, we will open up the command line and navigate to the localserver folder and execute:

node server.js

We should see the below output:

After the successful implementation of server side code, we can now go to the browser and open http://localhost:8080/predict_with_tfjs.html.

If the client side code is bug free, the application will start. Then you can select a different model (VGG16 and mobile Net) from the selection box and do the prediction.

GitHub Repository for the project:

You can watch the complete code explanation and implementation in the below video:

Source : ADL # Video no 1

Source : ADL # Video no 2

My Next Post will Cover Financial Time Series analysis using Tensorflow.js…Stay Tuned.

Best of Luck ! 👍",https://medium.com/free-code-camp/how-to-use-the-vgg16-neural-network-and-mobilenet-with-tensorflow-js-ea4c76d0b8e0,['Akshay Lamba'],2018-08-10 18:26:09.649000+00:00,474,"deep learning, tensorflow.js, VGG16, Mobile Net, NodeJS"
Animations with Matplotlib,"Animations with Matplotlib

Using the matplotlib library to create some interesting animations.

Animations are an interesting way of demonstrating a phenomenon. We as humans are always enthralled by animated and interactive charts rather than the static ones. Animations make even more sense when depicting time-series data like stock prices over the years, climate change over the past decade, seasonalities and trends since we can then see how a particular parameter behaves with time.

The above image is a simulation of Rain and has been achieved with Matplotlib library which is fondly known as the grandfather of python visualization packages. Matplotlib simulates raindrops on a surface by animating the scale and opacity of 50 scatter points. Today Python boasts of a large number of powerful visualization tools like Plotly, Bokeh, Altair to name a few. These libraries are able to achieve state of the art animations and interactiveness. Nonetheless, the aim of this article is to highlight one aspect of this library which isn’t explored much and that is Animations and we are going to look at some of the ways of doing that.",https://towardsdatascience.com/animations-with-matplotlib-d96375c5442c,['Parul Pandey'],2020-09-09 01:37:14.569000+00:00,180,"matplotlib, animations, python, visualization, interactive"
"plt.xxx(), or ax.xxx(), That Is The Question In Matplotlib","Difference between plt.xxx() and ax.xxx()

As shown in Figure 1, there are three main layers in matplotlib architecture. From top to bottom, they are Scripting layer ( matplotlib.pyplot module), Artist layer ( matplotlib.artist module), and Backend layer ( matplotlib.backend_bases module), respectively.

Figure 1, Matplotlib architecture

Let’s start from the bottom, the Backend layer handles all the heavy works via communicating to the toolkits like wxPython or drawing languages like PostScript in your machine. It is the most complex layer. Within this layer, FigureCanvas is the area onto which the figure is drawn and Renderer is the object which knows how to draw on the FigureCanvas . A regular user like you and me barely need to deal with this layer.

Then the middle layer, Artist layer, where ax.xxx() derives from. As the name implies, using this layer, you can control and fine-tune as many elements (e.g. spines, tick direction, tick label size, tick label font, tick colour etc.) as possible in the figure just like an artist paints on the canvas. This layer allows you to do more customisation compare to Scripting layer (see below) and more convenient for advanced plots. Especially when handling multiple figures/axes, you will not get confused as to which one is currently active since every subplot is assign to an ax . This is why ax.xxx() is sometimes referred to object-based plotting. We definitely will use this layer more often when writing a web application, or a UI application, or perhaps a script to be shared with other developers.

The top layer, Scripting layer, where plt.xxx() resident is designed to make matplotlib work like MATLAB script. In other words, this layer is considered as the lightest scripting interface among all three layers, which comprises a collection of command style functions for a quick and easy generation of graphics and plots. This is why many matplotlib tutorials prefer to introduce from this layer. It is the easiest part to start with and use, you basically add up objects (e.g. line, text, rectangle) on top of the figure . Scripting layer plotting is sometimes also called procedural plotting.

Figure 2, Scripting layer plotting

‘figure’ and ‘axes’ in Matplotlib

In matplotlib , figure and axes are layers of a figure (please note that I do not quote this “figure” as a script). Here let’s use a figure from matplotlib website to explain the concepts.

Figure 3, Parts of a figure in Matplotlib

As we can see from Figure 3, the whole figure (marked as the outer red box) is the base of a figure. The layer above it is the axes (marked as the inner blue box). A figure can at least have one axes . From here we know that, axes refers to a part of the figure and is not a plural word for more than one axis. For instance, if you have one plot on a figure , then that plot is the axes . If you have multiple subplots on a figure , then each subplot is one axes . To be able to make a plot, we normally call fig = plt.figure() at the beginning. We create one axes object in the figure by calling ax1 = fig.add_subplot(2, 1, 1) . This created the first subplot within a 2-row by 1-column figure . Therefore, all ax1.xxx(…) are functions specifically for ax1 . For example, to access x-axis and y-axis in the subplot ax1 , we call ax1.xaxis(…) and ax1.yaxis(…) . Likewise, we can add another subplot by calling ax2 = fig.add_subplot(2, 1, 2) and manipulating its elements by calling ax2.xxx(…) . In this way, we have a clear idea about which subplot we are working on without messing up the code (of course, there are many other ways to call two axes , for instance, fig, ax = plt.subplots(2) , then each axes can be accessed by calling ax[0] and ax[1] ).

A example plot with two methods

Alright, after clarifying the concepts of plt.xxx() and ax.xxx() , let’s use a simple example adapted from matplotlib document to demonstrate their differences when plotting figure with subplots.

Scripting layer plotting

Artist layer plotting

If everything goes right, you will get the following figure.

Figure 4, A example figure

As you can see from these two scripts (Scripting layer plotting vs. Artist layer plotting), although the code of artist layer plotting is more verbose than that of scripting layer plotting, it is easier to read. This is a very important practice to let you produce quality code and increase the readability of your code. When the plots getting complicated, the power of artist layer plotting will become more and more apparent.

Taken together, we may use plt.xxx() to quickly get a plot for exploratory data analysis, however, ax.xxx() is a go-to style when your code is part of a serious project and need to be shared with others. In addition, as a learner of matplotlib , I strongly advise starting from artist layer plotting, from which you will have a more comprehensive understanding about matplotlib plotting and definitely benefit more for your long-term development in data visualisation.

Here are materials I found very useful (continually updated list)",https://towardsdatascience.com/plt-xxx-or-ax-xxx-that-is-the-question-in-matplotlib-8580acf42f44,[],2020-02-01 22:42:48.301000+00:00,843,"matplotlib, pyplot, plt.xxx(), ax.xxx(), figure"
Facebook Develops An AI-Translator For Software Programmers,"TransCoder is highly versatile

Currently, TransCoder can translate freely between C++, Java and Python, but the research team behind TransCoder says that it will be able to adapt to any programming language pair and fluently translate in between them.

Software with the sole purpose of translating between languages is already available, but the results are more often than not underwhelming and can’t be used in a fire-and-forget manner due to the differences in how each language is structured. These so-called S2S (source to source) compilers are far from compiling code errorfree and need extensive bugfixing to get the final result to work. It’s often easier to just rewrite the code from scratch in the desired language.

Any programmer who needs to change code from let’s say C# to C++ or the other way around will tell you the same thing: “Learn both C# and C++, then spend a whole lot of time rewriting code from scratch.”

TransCoder learns by translating not only from a source language to a target language but also reverse translating it back. If you have used Google translator in the past, then you surely know that hitting “reverse translate” a few times can give you weird results as the translator pulls more things out of context.

TransCoder translates code both ways to pick up on these differences and tweaks the code until both ways give the expected results. This way, it ensures coherence.",https://medium.com/illumination-curated/facebook-develops-an-ai-translator-for-software-programmers-c514f16d2fc5,['Kevin Buddaeus'],2020-06-09 09:51:49.637000+00:00,232,"Trans Coder, Programming Language Translation, S2SCompilers, C++to Java Translation, Source To Source Compiler"
Keras vs PyTorch,"Keras vs PyTorch

Which one is better?

Keras and PyTorch are both very good libraries for Machine Learning. One cannot be said to be better than the other. Both have their respective advantages ad disadvantages. In this post, we are going to see two different ways to do the same classification task, the Keras way and the PyTorch way.

The dataset that we will be using is the Concrete Crack Detecction Dataset. The dataset is available here or can be downloaded just by clicking this link.

Let’s download and unzip the dataset first

Import the common libraries and declare the path variables

Data Preparation

In this section we will create a Data Loader in PyTorch and a Data Generator in Keras. Both essentially serve the same purpose. To ensure that each batch contains the same number of positive and negative samples we will arrange the labels in the pattern 1 0 1 0 1 0 . . ., that is each batch will contain a positive and negative sample alternatively.

First we will look into the PyTorch way of constructing a Data Loader.

Next, we will look into the Data Generator in Keras.

We can also build a custom Data Generator in Keras, which is similar in structure to the Dataset class in PyTorch.

Unlike PyTorch, we don’t need a separate Data Loader like object in Keras. The above custom Data Generator DataGeneratorKeras can be viewed as DataLoader(DatasetPyTorch()) .

Before proceeding further, it should be mentioned that the above Data Loader (or Generator) are tailored for the dataset we are using. For other datasets, we have to modify the code in __init__() part according to our need.

The images in the dataset can be split into training and validation set before passing them to the Data Generator (or Loader) by creating a dataframe filenames_df with the filenames of the files stored in all_files and then using train_test_split from scikit-learn to obtain the splits. We will obtain two dataframes, one for training set ( train_df ) and the other for validation set ( validation_df ). These dataframes can then be used to sample training and validation images in each epoch, within the respective data generators. An advantage of using dataframe is that the training set can be shuffled after each epoch, when using the custom approach.

As shown in the above code examples, we are dividing the dataset (filenames stored in all_files ) into training and validation sets in 75:25 ratio. We can also create a dataframe (say, train_df from the filenames in self.all_files when train = True , and valid_df when train=False ) to store the filenames and labels in the training set and similarly for the validation set. This too will enable us to shuffle the training set after each epoch.

Another thing to note here, since we are using ResNet50 model in this post, we have used the preprocess_input function in Keras and similarly Normalized the inputs in PyTorch. In case of using any other model, it is advisable to check what preprocessing was applied to the inputs while training the model in the original paper.

Next, we will train a pre-trained ResNet50 model on our dataset in Keras.

Let’s see how to build a model in Keras, using a pre-trained ResNet50 model as the base model. We will also train the model using Adam optimizer.

If we want to fine tune the model, we can set fine_tuning = True . In our case, we will keep fine_tuning = False . Since, we are posing this problem as a 2-class classification problem, we will be using Categorical Cross Entropy loss. However, we can also use Binary Cross Entropy loss, with sigmoid activation in the last layer.

The above code yielded a training accuracy of 98% after training for only 1 epoch.

Next, we will be replicating the above experiment with PyTorch.

In the PyTorch approach, we use a ResNet18 model. In Pytorch, we don’t need to explicitly specify whether a softmax or a sigmoid needs to applied. The library decides that on it own depending on the Loss function used. With this model we obtained a validation accuracy of 96% after 2 epochs.

Training a model in Keras may look simpler, but when you will need a custom training loop, then the tf.GradientTape() comes into the picture. Let’s see an example with a custom training loop.

If we look at the custom training loop of Keras, we can see that, the training loop of PyTorch and Keras bear resemblance to each other. Both follow the same steps : Predict the output, Calculate the Loss, Compute the gradients, Update the weights. Only difference is that in PyTorch, the phase of operation, training or validation needs to be specified but using model.train() and model.eval() , which is not explicitly needed in Keras, when using custom training loop.

We can say that there is not much difference between the ways the two libraries are used, like the API for the layers or the Data Loader/ Generators. However, PyTorch operations bear a lot of similarity with NumPy, which makes it more Pythonic, intuitive to use and also easy to debug. Keras (tf.Keras) also has its own advantages, like portability, ease of model deployment, and of course, TensorBoard.

As said earlier, both have their own advantages and disadvantages, and one cannot be ruled out or claimed to be better than the other. It depends on the person using it and the purpose it is being used for.",https://medium.com/the-owl/keras-vs-pytorch-27332ca13d22,['Siladittya Manna'],2020-07-28 18:05:52.317000+00:00,889,"Keras, Py Torch, Machine Learning, Classification Task, Data Loader"
Fruit Yield Assessment from Photos with Machine-Learning Scikit-image,"Programming Process

Import required libraries:

Import image I’m going to use and convert it into color pixels arrays. I found this picture from a person’s blog:

I was really curious about how this picture looks like as I knew some pictures will be 2-D or 3-D arrays. Used the following codes to check it out:

What I found out is that there are 3 layers i.e. 3 dimensions of my chosen picture. X axis is in the 1st layer, Y axis is in the 2nd. For the 3rd layer I will be using the first item only as using the other 2 items seems indifferent versus using just the first item. This structure will be more important in the later part of code.

Then we start to click on the reddish ripen mangoes on the imported picture. When we click on the picture, our click’s x-coordinates and y-coordinates will be stored in a list:

The click event uses matplotlib event connections to create a function that stores points clicked on the image. Once clicked, coordinates are added to the list:

The clicked points will show its details on the top left corner of the subplot:

Let’s say I have clicked on 6 mangoes. The corresponding coordinates can be printed like below and the length of clicked list (puntosinteres) is 6:

Then if I want to click on more mangoes, i.e. adding upon what I just clicked. The code snippet looks like this:

From the above code, I got the clicked ones shown as red dots on the picture. Now I can click on other mangoes other than those clicked 6 dotted ones:

Next step is to take a closer look at my selected templates i.e. clicked mangoes pictures. This is to make sure I click on the right things:

Since I clicked on 3 more, so now I have 9 template pictures:

Once I double checked all the templates, if I want to delete one of the picture, I can use the below code. Also check the length of clicked list (puntosinteres) again:

Here comes the most exciting part. Start to train our model with our template and original pictures using color pixel arrays!

Here I’m using sklearn’ s match_template library. I noticed that before putting into match_template function, the template picture’s x-axis and y-axis coordinates need to be reversed (similar thing I saw from people who are using OpenCV library):

result variable will give us a matrix of all normalized cross-correlation between template and original picture, showing how well the template matched to different locations in the original image; then we set a threshold as 0.8. Only those matched higher than this score will be appended to our result list (listaresultados). Eventually our result list (listaresultados) show arrays of color pixels that are closet to our clicks on the original picture.

Take a special note that when we need those coordinates from the outcome, we need to reverse it back again to get the right coordinates for both axes.

Let’s plot the mangoes coordinates from our machine learning model directly on the original picture:

Then we can see from below that several pixel from our result list (listaresultados) are actually from the same mango, which could lead to double counting:

So it’s why we need a cluster analysis here to help us solve this issue.

What is Birch — in sklearn.cluster library there’s a BIRCH method, which is a memory-efficient clustering method for large datasets. Its algorithm creates Clustering Features (CF) Tree for a given dataset and CF contains the number of sub-clusters that holds only a necessary part of the data. Thus the method does not require to memorize the entire dataset.

The usage process includes define the birch method and fit it with our input data.

Converting result list (listaresultados) into datalist to fit in birch model later on:

The most important arguments of birch model is :

1. threshold: sets the limit between the sample and sub-cluster.

So we create a birch model and fit datalist into it:

Then we will see the model now can identify 15 mangoes as ripen ones.

Watch out the birch models’ threshold argument — I notice that when its number becomes smaller, the model will split its cluster more, and this will make it more likely to have multiple clusters at nearby areas i.e. redundantly mark over the same mangoes.

And it seems that we only use 9 templates to make the model able to recognize 6 more ripen fruits.

We see there are still some limitations of this procedure such as there are still many more mangoes left unidentified. Also, I believe there’s a way to give the model to several mangoes’ pictures, and then we take those as templates and ask the model to identify how many mangoes on the tree we have above.

Other helpful materials I’ve used for this article:

Also this article is my first baby step of using machine learning in agriculture. I was fully inspired by this wonderful material:

That’s it. Thanks for reading!",https://medium.com/li-ting-liao-tiffany/fruit-yield-assessment-from-photos-using-python-and-scikit-learn-2a5f58c4a097,['Li-Ting Liao'],2020-10-01 05:24:18.619000+00:00,808,"programming, process, machine learning, sklearn, clustering"
Understanding Time Series with Facebook’s Prophet,"It has always been a difficult task to predict the future and analyze the upcoming events until data comes into the picture. yes, data is a powerful resource to predict the future and Time Series forecasting is an effective way to predict the future values based on the observed values.

There are many Time Series forecasting models:

Exponential Smoothing Holt Winter Forecasting AR, ARMA ARIMA, SARIMA GARCH

Since I started studying the time series forecasting models and their applications I came across a model “Prophet” which has been developed by the Facebook team which can be used by the people who have less domain knowledge and Prophet can produce accurate results in default settings.it is an open-source available in both R and Python.

The organizations are facing difficulty in finding the Analysts who can produce high-quality forecasts because forecasting is a specialized data science skill requiring substantial experience. Prophet is a time series model that can address this problem and produce better results with slight changes in the model. I won’t dive much deep into the math behind this model but we try to find the best results by using this model on a simple dataset

The Prophet Forecasting Model:

This model has three main components: Trend,Seasonality, and Holidays.

g(t): piecewise linear or logistic growth curve for modelling non-periodic changes in time series.

s(t): periodic changes(weekly/yearly)

h(t): effect of holidays

εt: error term accounts for any unusual changes not accommodated by the model

Loading the data:

The below data set is the shampoo sales over three years

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

from fbprophet import Prophet

from fbprophet.diagnostics import performance_metrics df = pd.read_csv(""C:\\Users\\Dell\\Desktop\\sales-of-shampoo-over-a-three-ye.csv"")

# reading the first five rows

df.head()

output

Prophet requires the date column to be labeled “ds” and the target column to be labeled “y”

df_shampoo_orig = pd.read_csv(‘C:\\Users\\Dell\\Desktop\\sales-of-shampoo-over-a-three-ye.csv’,

nrows=36,

skiprows = 1,

names = [‘ds’, ‘y’],

parse_dates = True )

df = df_shampoo_orig

df.head()

output:

Let's get the ds column into date-time format

df['ds'] = df.ds.apply(lambda x: ""201""+x)

df.head()

output:

tseries.offset.MonthEnd(0)

This next cell uses the awesome tseries.offset.MonthEnd(0) method to make the day of the month the final day. This is necessary for Prophet to make the predictions we need at the correctly spaced monthly intervals.

df['ds']=pd.to_datetime(df['ds'])+pd.tseries.offsets.MonthEnd(0) df.head()

output:

Prophet forecast: we need to make some observations for future predictions. let us make 12 months of predictions.

train = df[:24]

train.tail() test = df[24:]

output:

let's train the model:

m = Prophet(weekly_seasonality=False, daily_seasonality=False, n_changepoints=2)

m.add_seasonality(name='yearly', period=12, fourier_order=1)



m.fit(train)

let's make the future data frame and make the predictions. we need to pass freq= m because we want to make monthly predictions

future = m.make_future_dataframe(periods=12, freq='m')

forecast = m.predict(future2)

a = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]

prophet_pred = pd.DataFrame({""Date"":a[-12:]['ds'],""pred"":a[-12:][""yhat""]})

prophet_pred=prophet_pred.set_index(""Date"")

prophet_pred.index.freq = ""M""test[""prophet_predictions""] = prophet_pred['pred'].values

test

output:

let's make at a plot

figure = m.plot(forecast)

output:

The dots are the actual data points. The line through the dots is predicted values. The shared area represents the uncertainty intervals.

lets plot trend and seasonality

fig_decompose = m.plot_components(forecast)

output:

calculating the error for the above predictions:

a = forecast[[‘ds’, ‘yhat’, ‘yhat_lower’, ‘yhat_upper’]]

prophet_pred = pd.DataFrame({“Date”:a[-12:][‘ds’],”pred”:a[-12:][“yhat”]})

prophet_pred=prophet_pred.set_index(“Date”)

prophet_pred.index.freq = “M”

test[""prophet_predictions""] = prophet_pred['pred'].values # function to calculate the RMSE

def rmse(predictions, targets): return np.sqrt(((predictions - targets) ** 2).mean()) # calculating RMSE and MSE prophet_rmse_error = rmse(test['y'], test[""prophet_predictions""])

prophet_mse_error = prophet_rmse_error**2

mean_value = df['y'].mean() print(f'MSE Error: {prophet_mse_error}

RMSE Error: {prophet_rmse_error}

Mean: {mean_value}')

output:

Now we will try to change the Fourier order from 1 to 5 and let's see what happens, everything is the same except this

m1 = Prophet(weekly_seasonality=False, daily_seasonality=False, n_changepoints=2)

m1.add_seasonality(name=’yearly’, period=12, fourier_order=5)

m1.fit(train)

future1 = m1.make_future_dataframe(periods=12, freq=’M’)

forecast1 = m1.predict(future) #testing our predictions b = forecast1[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]

prophet_pred1 = pd.DataFrame({""Date"":b[-12:]['ds'],""pred"":b[-12:][""yhat""]})

prophet_pred1=prophet_pred1.set_index(""Date"")

prophet_pred.index.freq = ""M""

we will calculate the error for Fourier order = 5 and lets us see whether the error decreases or increases

prophet_rmse_error = rmse(test[‘y’], test[“prophet_predictions”])

prophet_mse_error = prophet_rmse_error**2

mean_value = df[‘y’].mean()

print(f’MSE Error: {prophet_mse_error}

RMSE Error: {prophet_rmse_error}

Mean: {mean_value}’)

output:

In this case, our error has been increased and we can play with parameters to get better results, Here I just tried to give an overview about prophet model as the parameters are easy to adjust to better results rather than time series models which require a bit domain knowledge

I will mention the sources from where I learned you can get better insight about this model

Regards,

uday.",https://medium.com/@pubhaskar1996/understanding-time-series-with-facebooks-prophet-1ddf4778a909,['Uday Bhaskar'],2020-01-17 02:49:44.787000+00:00,640,"time-series-forecasting, prophet-model, data-science, machine-learning, forecasting"
Using Data to Enhance Cyclone Disaster Preparedness,"Applying Machine Learning algorithm to results from an exploratory analysis of cyclone data

In this section, we shall discuss how an ML clustering algorithm can help augment the information we have to provide newer insights that can be put to use while disaster preparedness planning.

For this illustration we will use the k-Means clustering algorithm. We will also narrow the scope by focusing on one of the regions which is highly disaster prone during a cyclone. We will select the coastal state of Andhra Pradesh. The method described should be expanded into other regions along the coastline which is considered as a subject of cyclone fury.

To get started we would need to first create a dataset that contains all the neighborhoods in each of the coastal districts of the state (i.e Andhra Pradesh, for this illustration).

Creating the dataset is the most important aspect here, we would first need a dataset of all areas within the state, this can be got from postal data of AP. We will then need to curate this data to have the columns we need and remove the columns we do not want. We will also need to trim this dataset to only hold information about areas in coastal districts of AP. Once the dataset has info on the areas in the coastal districts of AP, we will need to fetch the latitude, longitude values for each of these areas. We will use the Geopandas library to fetch the lat-long values and populate the DataFrame with these new columns. You will end up with a DataFrame such as the one below.

First few rows of DataFrame containing coastal areas in AP

Manipulating the DataFrame carrying the areas dataset for the state is not in the scope of this article.

Once you have the desired dataset ready, the next step is to apply the clustering algorithm and cluster the neighborhoods (or areas) in your dataset based on the similarity or dissimilarity of venues in the area. In our case, we will use the Foursquare API to find the hospitals or emergency care units in each of these areas and we will cluster regions based on how similar or dissimilar they are based on a number of medical care units they have or don’t have.

Cluster the neighborhoods based on similarity of venues close to it, find the centroid of the cluster, this centroid neighborhood would have the minimum distance to all other neighborhoods within that cluster.

USE OF FOURSQUARE API TO FIND HOSPITALS IN AN AREA IN THE COASTAL DISTRICT

We will use the FourSquare API to find the hospitals in a given area. To know more about FourSquare APIs you can go here.

Follow the getting started instrcutions from FourSquare API docs to create your ID and secret key to make the API calls.

We are interested to find a venue of the category hospitals for a given area hence we will use the explore venues API with the category Id 4bf58dd8d48988d196941735, which corresponds to the category for Hospitals. Please refer to the FourSquare API docs for details.

The API call we will make will be as below: https://api.foursquare.com/v2/venues/explore?&client_id=<your_client_id>&client_secret=<your_client_secret>&v=20180605&ll=18.5147147,83.2351241&radius=500&limit=100&categoryId=4bf58dd8d48988d196941735

Once you get the results from the API call, you will need to update your DataFrame with the results, such that you will end up with a dataset as below:

First few rows of a dataset with venue details

This API call is made for each of the area (or neighborhood) values in our dataset. Once we have the desired dataset then k-Means clustering is applied (using Python packages) and visualization of the clustered info will be something like below. Each of the dissimilar clusters is colored differently. You will need to inspect each of the cluster to understand the nature of the cluster and what are the features for that cluster.

Clustered neighborhoods based on the similarity of hospital venues in a district

Districts are clustered based on similarity in terms of the number of hospitals in proximity, number of emergency wards, etc.

Districts that fall into a cluster of having fewer hospitals and emergency wards would be the first focus for disaster management teams.

Extending the use-case

Neighborhoods in districts can be clustered based on proximity to bus stations, airports and train stations.

Focused Evacuation efforts can be planned based on insights from the clusters, neighborhoods having less access to public transportation hubs can be the first focus on disaster management teams.",https://pub.towardsai.net/using-data-to-enhance-cyclone-disaster-preparedness-8f5ad5541223,['Sreelatha S'],2020-02-13 11:33:26.684000+00:00,717,"machine-learning, clustering, k-Means, FoursquareAPI, Cyclone Data"
Opening Internet Monopolies to Competition with Data Sharing Mandates,"Claudia Biancotti, Paolo Ciocca

From PIIE.com

Over the past few years, it has become apparent that a small number of technology companies have assembled detailed datasets on the characteristics, preferences, and behavior of billions of individuals. This concentration of data is at the root of a worrying power imbalance between dominant internet firms and the rest of society, reflecting negatively on collective security, consumer rights, and competition. Introducing data sharing mandates, or requirements for market leaders to share anonymous user data with other firms and academia, would have a positive effect on competition. As data are a key input for artificial intelligence (AI), more widely available information would help spread the benefits of AI through the economy. On the other hand, data sharing could worsen existing risks to consumer privacy and collective security. Policymakers intending to implement a data sharing mandate should carefully evaluate this tradeoff.

Read full paper",https://medium.com/@gattodipiombo/opening-internet-monopolies-to-competition-with-data-sharing-mandates-d5a89efa0099,['Claudia Biancotti'],2019-04-01 19:44:16.952000+00:00,146,"Data Sharing, AI, Consumer Rights, Competition, Privacy Risks"
How to Convert Webpage to a Video Using Google AI Tool,"After the revolution of AI (Artificial Intelligence) in the technology world, you have seen many implementation examples. Like robotics, Google translate, Software automation etc. There are lots of big companies are working and investing in this technology. Google is one of them, That is trying to implement AI in their products and services to provide enhance features to the end users. With the help of AI the department of AI Google has developed a new tool. That can convert webpage to a video using google AI tool.

And this tool is now calling as URL2Video. This AI tool is invented and developed by the Google AI team. It will help users to convert simple web pages into simple video for marketing automatically. The technology software giant says that they created this tool specifically for the professionals. Who create marketing videos from scratch easily and quickly.

Want to earn money online by daily task SignUp Here

“Creating marketing videos from scratch can be challenging for the designers. Especially when designing for multiple platforms with different users. We present URL2Video. An automatic approach that converts a web page into a short video given temporal and visual constraints.”, wrote the team.

How to Convert Webpage to a Video Using Google

Now to process the task from webpage to video using this tool. It extract some key points in the webpage like colors, fonts, layouts and text etc. After that the URL2Video engine grave these elements and and covert it into a video using constraint programming.

During the automatic creation of video. Users can monitor every single step of the webpage and also instruct to change any point to get a perfect video. Also this tool provides a user dashboard where the creators can take actions to reexamine or make change on the video.

The URL2Video team who invent the AI tool to convert webpage to a video said. They have asked some experience designers to learn their design process. And the designers compare the URL2Video from the designers video.

“The evaluation shows that URL2Video effectively extracted design elements from a web page and supported designers by bootstrapping the video creation process.”, stated Google.

So, if you are an AI-focused person and want to learn more about URL2Video. You can check out the technical details in Google’s official research paper.

Don’t forget to follow and clap the article thanks Team SaimTechNews.",https://medium.com/@technews783/how-to-convert-webpage-to-a-video-using-google-ai-tool-5c44adf5b45a,[],2020-12-24 16:45:08.270000+00:00,387,"AI, Google, URL2Video, Video Conversion, Webpage to Video"
When Artificial Intelligence takes care of the Mundane Chores!,"No more a luxury, home automation is steadily becoming an integral part of contemporary houses, these days. The rapid advancement of technology is immensely contributing to the constantly evolving lifestyles of the new-age population. With the adoption of technologies like IoT and AI, people are now able to save a lot of time for hobbies as well as work without worrying about the mundane daily chores with a wide range of smart devices at your service. Designed to make your life simpler, these devices allow you to remotely control your home security, décor, lighting, cleaning etc when connected with a smart home hub. All connected devices can converse with each other using IoT and can think or act for themselves using AI technology. Also, they can be programmed to stick to certain rules, set as per your convenience.

Industry giants like Apple, Google and Amazon are consistently focusing on their smart assistants which are powered by artificial intelligence(AI). The technology implies the ability to perform tasks automatically for the user. It performs tasks based on the information the framework has collected using an assortment of machine learning algorithms. Internet of Things or IoT allows devices to convey over the internet and to be controlled remotely or by voice commands. Smart home devices are mostly electrical gadgets ranging from lighting, fans, ACs, washing machines to refrigerators. In a smart home, the information is provided by IoT frameworks and the AI gains from that information to perform certain tasks, cutting down on human burden.

Read more about this topic

Subscribe to our personalized Newsletter to know about Smart Home Technology -Click here",https://medium.com/@sandeepsaurabhsingh/when-artificial-intelligence-takes-care-of-the-mundane-chores-e28fef056e85,['Sandeep Singh'],2021-05-01 02:52:21.705000+00:00,267,"Home Automation, IoT, AI, Smart Home Hub, Apple"
November AI — Breakthrough in Biology after 50 years,"So, November is over and I’ve come across a good list of blogs, research papers, books, and datasets that are worth deep-diving into.

This is the second part of the AI Monthly webcast, you can find the first one here.

Here’s what we are going to cover in the November AI updates:

Firstly, we’ll look at 2 interesting pieces of news that you’d have heard:

The first one comes from DeepMind. The latest version of their AI system AlphaFold has been recognized as a solution to figuring out the shape of protein structure in 3D. The second is for TensorFlow users of Intel Macs or Macs powered by Apple’s new M1 chip can now take advantage of accelerated training using Apple’s Mac-optimized version of TensorFlow 2.4 and the new ML Compute framework.

Next, we are going to go over two interesting datasets :

The first one is the Objectron Dataset by Google Research — It is a collection of short, object-centric video clips capturing a larger set of common objects from different angles. MedMNIST dataset which is a collection of 10 pre-processed medical image datasets with Creative Commons (CC) Licenses.

The second half of the blog will focus on learning and improving scientific/analytic practices. I’ve shared what books, courses, and research papers I am following these days to be less wrong about data science and AI with each day passing.

AlphaFold 2 — Solution to the protein folding problem

On November 30, DeepMind published a blog titled AlphaFold: a solution to a 50-year-old grand challenge in biology. They have solved the long-standing problem of protein folding with their latest version of the AI system called AlphaFold 2.

Proteins are indispensable to life, supporting everything we do. They are large complex chains of amino acids that help us carry molecules and decode the DNA.

Protein folding problem

This functionality of a protein largely depends on its unique 3D structure. A part of biological research has been centered around predicting the shape of the protein over the past 50 years.

Now, figuring out what shapes proteins fold into is known as the “protein folding problem”.

The solution requires accuracy of above 87 GDT on CASP competition.

It has been a well-defined grand challenge and the Protein Structure prediction center has been organizing a biennial competition called CASP(Critical Assessment of protein Structure Prediction) where they invite teams to submit their solutions.

A bit of History

The Alphafold team first entered CASP13 in 2018 with an initial version of AlphaFold, which achieved the highest accuracy among participants. They wrote a paper explaining their work and inspired the community to further make open-source contributions.

And this year they went with new deep learning architectures that brought about changes in their methods for CASP14, enabling them to achieve incredible levels of accuracy.

The AlphaFold system achieved a median score of 92.4 GDT overall across all targets. GDT stands for Global Distance Test which ranges from 0–100. DeepMind tried to explain it as follows:

GDT can be approximately thought of as the percentage of amino acid residues (beads in the protein chain) within a threshold distance from the correct position.

The team has researchers, scientists, engineers from fields of biology, physics, and machine learning, as well as of course the work of many scientists in the protein folding field over the past half-century.

Speculating the improved architecture of AlphaFold 2

What changed in AlphaFold that led to such high scores will only be clear after the research paper is published but we can speculate.

There are speculations that AlphaFold 2 uses transformers instead of CNNs and they are now using evolutionary sequences + multiple sequence alignment as part of the iterative learning process.

How big is this achievement?

While testing the AlphaFold 2 during experiments, the team has observed this structure prediction of a protein could be useful in future pandemic response efforts.

Earlier this year, they predicted several protein structures of the SARS-CoV-2 virus, including ORF3a, whose structures were previously unknown.

At CASP14, they predicted the structure of another coronavirus protein, ORF8. Experimentalists have now confirmed the structures of both ORF3a and ORF8. And they have achieved a high degree of accuracy on both of the predictions when compared to their experimentally determined structures.

I’d be publishing a more detailed blog on this breakthrough soon.

Apple’s Accelerated tensorflow 2.4

For all the model trainers using Macs, we now have Apple’s Mac-optimized version of TensorFlow 2.4 and the new ML Compute framework.

We all know that Macbook is the most preferred platform for developers, engineers, and researchers. Apple announced an updated lineup of Macs that contain the new M1 chip in November. And now Apple claims that the Mac-optimized version of TensorFlow 2.4 leverages the full power of the Mac with a huge jump in performance.

Here are a few charts that will help you get a better understanding of the performance enhancement across different Macs(Intel-based vs M1)

Now, to get started with the new accelerated tensorflow, you don’t need to make any changes to your existing projects and scripts to use the ML compute framework, all you need to do is head over to apple’s GitHub repo for the installation instructions and prerequisites.

From there, you just simply need to install the mac-optimized tensorflow 2.4 and you are done. They are saying that they’ll be making these updates easier by integrating it in the tensorflow master branch but let’s see how long it takes!",https://medium.com/dataseries/november-ai-breakthrough-in-biology-after-50-years-971f5d962a89,['Harshit Tyagi'],2020-12-10 11:11:58.497000+00:00,873,"AI, Machine Learning, Deep Mind, Alpha Fold 2, Protein Folding Problem"
Paper Review: Neural Collaborative Filtering Explanation & Implementation,"Essentially, each user and item is projected onto a latent space, represented by a latent vector. The more similar the two latent vectors are, the more related the corresponding users’ preference. Since we factorize the utility matrix into the same latent space, we can measure the similarity of any two latent vectors with cosine-similarity or dot product. In fact, the prediction for each user/ item entry is computed by the dot product of the corresponding latent vectors.

The prediction equals the inner product of latent vectors

However, the paper argued that dot product limits the expressiveness of user and item latent vectors. Let’s consider the following case. We first focus on the top three rows of this utility matrix.

Let S{x,y} denotes the similarity between user x and user y. By computing the cosine-similarity between users 1, 2, and 3, we know that S{2, 3} > S{1, 2} > S{1, 3}. Without loss of generality, we map the users onto a latent space of two dimensions as the following.

Now, we take into account user 4. Comparing the similarity with the others, we obtain S{1,4} > S{3,4} > S{2,4}. However, no matter we place the latent vector P4 to the right or left of P1, it will inevitably be closer to P2 than P3.

Therefore, this example shows the limitation of inner product to fully model the interactions between user and items in the latent space.

Neural Collaborative Filtering

The paper proposed Neural Collaborative Filtering as shown in the graph below. In the input layer, the user and item are one-hot encoded. Then, they are mapped to the hidden space with embedding layers accordingly. The Neural FC layer can be any kind neuron connections. Multiple layer perceptron, for example, can be placed here. It claims that with the complicated connection and non-linearity in the Neural CF layers, this model is capable of properly estimating the complex interactions between user and item in the latent space.

NCF architecture

So, how NCF is a generalization of Matrix Factorization? Let me show you in the graph below. We first replace Neural CF layer with a multiplication layer, which performs element-wise multiplication on the two inputs. Then, we set the weight from the multiplication layer to the output layer to be a fixed unit matrix (matrix of all ones ) of dimension K by 1 with linear activation function.

Then , we have the following equations.

The prediction of the unobserved interaction ŷ_ui denotes the predicted value of the (u, i) entry on the reconstructed utility matrix. L is the linear activation function, while ⊙ denotes the element-wise multiplication operation. p_u and q_i are the latent vector of user and item, respectively, and J is the unit matrix of dimension (K,1). Since J is a unit matrix, inside the linear function becomes the inner product between latent vector p_u and q_i. Furthermore, because the input and output are the same for linear function, it boils down to the last line. The predicted label is the inner product of the corresponding user and item latent vector. This equation is identical to that shown in the Matrix Factorization section. Thus, this proves that Matrix Factorization is a special case of NCF.

NeuMF

In order to introduce additional non-linearity, the final model proposed, NeuMF, includes a Mutliple-layer Perceptron (MLP) module apart from the Generalized Matrix Factorization (GMP) layer.

The output of GMF and MLP modules are concatenated and connected with the sigmoid activation output layer.

Performance

The paper evaluates NCF models and other models with the Leave-One-Out schema. That is, the last interaction for each user is held out for evaluation. Two evaluation metrics are being considered, Hit Ratio @ 10, and NDCG @ 10. Hit Ratio @ K denotes the fraction of prediction hits given 10 recommendation for each user. Let’s say if we recommend 10 items for each user, and 4 of the 10 users interact the items matching our recommendation, then Hit Ratio@ 10 = 0.4. NDCG, on the other hand, can be viewed as an extension of Hit Ratio, except that it considers the order of the hit. This means that if you the hit occurs at the higher recommendation, NDCG will be higher.

The performance comparison is shown in the figure below. In all cases, NeuMF performs better than the other models.",https://towardsdatascience.com/paper-review-neural-collaborative-filtering-explanation-implementation-ea3e031b7f96,"['Kung-Hsiang', 'Huang', 'Steeve']",2019-11-18 14:04:56.693000+00:00,703,"Neural Collaborative Filtering, Matrix Factorization, NeuMF, Hit Ratio, NDCG"
Matplotlib vs. Seaborn vs. Plotly,"Matplotlib vs. Seaborn vs. Plotly

Clear, effective data visualization is key to optimizing your ability to convey findings. With various packages in use such as Matplotlib, Seaborn, and Plotly, knowing the capabilities of each and the syntax behind them can become bewildering. I’m going to walk you through creating some common graphs in Python using each of these packages using a csv file of the 2017 Spotify top tracks.

First, i’ll import the pandas package to read my csv into an easily readable dataframe.

import pandas as pd

df = pd.DataFrame(pd.read_csv('featuresdf.csv'))

Histogram

I’ll need to import the matplotlib package:

import matplotlib.pyplot as plt

%matplotlib inline

To plot a histogram of the danceability and energy scores overlaid, I can use the following code:

#set figure

f, ax = plt.subplots(1,1) #graph histogram

plt.hist(df['danceability'], bins=10, alpha=0.5, color='purple', label='Danceability')

plt.hist(df['energy'], bins=10, alpha = 0.5, color='blue', label='Energy') #set legend

plt.legend(loc='upper right') #set title & axis titles

ax.set_title('Danceability Histogram', fontsize=20)

ax.set_xlabel('Danceability')

ax.set_ylabel('Frequency') #set x & y ranges

plt.xlim(0,1)

plt.ylim(0, 30)

plt.show()

Notice the sparse nature of this graph. However, once I run the following code, you can see how my graph improves:

import seaborn as sns

sns.set(style='darkgrid')

Seaborn allows us to add a nice backdrop to our plots and improves the font. You can set style = darkgrid, whitegrid, dark, white, and ticks. We can also plot the same graph using what seaborn calls the distplot:

f, ax = plt.subplots(1,1) sns.distplot(df['danceability'], bins=10, label='Danceability', color='purple')

sns.distplot(df['energy'], bins=10, label='Energy', color='blue') ax.set_title('Danceability & Energy Histogram', fontsize=20)

ax.set(xlabel='Rating', ylabel='Frequency') ax.set_xlim([0, 1]) ax.legend()

Almost exactly the same, right? Seaborn is built on matplotlib, so you can use them concurrently. Seaborn simply has its own library of graphs, and has pleasant formatting built in. However, it does not have all of the same capabilities of matplotlib. For instance, if you want to create the same histogram, but with the two variables stacked next to each other as opposed to overlaid, you would need to fall back to matplotlib:

#set figure

f, ax = plt.subplots(1,1) #next to each other

plt.hist([df['danceability'], df['energy']], bins=10, alpha=0.5, color=['red', 'blue'], label = ['Danceability', 'Energy']) #set legend

plt.legend(loc='upper right') #set title & axis titles

ax.set_title('Danceability & Energy Histogram', fontsize=20)

ax.set_xlabel('Rating')

ax.set_ylabel('Frequency') #set x & y ranges

plt.xlim(0,1)

plt.ylim(0, 30) plt.show()

Seaborn’s built in features for its graphs can be helpful, but they can be limiting if you want to further customize your graph.

Matplotlib and Seaborn may be the most commonly used data visualization packages, but there is a simpler method that produces superior graphs than either of these: Plotly. To get started in a jupyter notebook, run the code below:

pip install chart-studio conda install -c plotly chart-studio # Standard plotly imports

from chart_studio.plotly import plot, iplot as py

import plotly.graph_objects as go

from plotly.offline import iplot, init_notebook_mode # Using plotly + cufflinks in offline mode

import cufflinks

cufflinks.go_offline(connected=True)

init_notebook_mode(connected=True)

To plot the same overlaid histogram as above using default Plotly settings:

fig = df[['danceability', 'energy']].iplot(kind='hist', color=['purple', 'blue'], xTitle='Danceability',

yTitle='Frequency', title='Danceability Histogram')

Plotly graphs are automatically outfitted with hover tool capabilities — hovering your mouse over any of the bars of data will display the numerical values.

To plot the bars side by side or otherwise further customize the graph, the code is lengthier, but fairly intuitive. You can specify your desired theme from a growing list of available default themes, including one modeled after seaborn (used below).

#install themes & view available

import plotly.io as pio

pio.templates

You can also specify your colors using the default color codes below:

And finally, plot your graph:

#plot

trace1 = go.Histogram(

x=df['danceability'],

name='danceability', #name used in legend and hover labels

xbins=dict( #bins used for histogram

start=0,

end=10,

size=0.1

),

marker=dict(

color='#1f77b4',

),

opacity=0.75

)

trace2 = go.Histogram(

x=df['energy'],

name='energy', #name used in legend and hover labels

xbins=dict( #bins used for histogram

start=0,

end=10,

size=0.1

),

marker=dict(

color='#9467bd'

),

opacity=0.75

)

data = [trace1, trace2] layout = go.Layout(template='seaborn', #set theme

title='Danceability & Energy Histogram',

xaxis=dict(

title='Danceability & Energy'

),

yaxis=dict(

title='Frequency'

),

bargap=0.2,

bargroupgap=0.1

)

fig = go.Figure(data=data, layout=layout)

iplot(fig, filename='styled histogram')

Scatterplot

To plot the loudness score vs. valence in matplotlib:

#set figure

f, ax = plt.subplots(1,1) #plot

plt.scatter(df['loudness'], df['valence'], s=df['energy']*100) #set title & labels

plt.title('Scatterplot: Loudness vs. Valence', fontsize=20)

plt.xlabel('Loudness')

plt.ylabel('Positivity') #set x range

ax.set_xlim([0, -10]) plt.show()

In seaborn:

fig = sns.scatterplot(x=df['loudness'], y=df['valence'], size = df['energy'],sizes = (40,200))

fig.figure.suptitle(""Scatterplot: Loudness vs. Valence"", fontsize = 25)

fig.set(xlabel='Loudness', ylabel='Positivity')

fig.set_xlim([0,-10])

If you want to add a regression line to the graph, seaborn makes this infinitely easier with its regplot graph:

fig = sns.regplot(df['loudness'], y=df['valence'], data=df)

fig.figure.suptitle(""Scatterplot: Loudness vs. Valence"", fontsize = 25)

fig.set(xlabel='Loudness', ylabel='Positivity')

fig.set_xlim([0,-10])

To add the correlation coefficient to this, import the pearson.r package from scipy and follow the steps below:

import numpy as np

from scipy.stats import pearsonr #calculate correlation coefficient

corr = pearsonr(df['loudness'], df['valence'])

corr = [np.round(c, 2) for c in corr] #add the coefficient to your graph

text = 'r=%s, p=%s' % (corr[0], corr[1])

ax = sns.regplot(x=""loudness"", y=""valence"", data=df)

ax.text(-7.5, 0.9, text, fontsize=12)

Lastly, with Plotly, we can again create a scatterplot using the default settings:

fig = go.Figure(data=go.Scatter(x=df[‘loudness’], y=df[‘valence’],mode=’markers’))

fig.update_layout(title=’Loudness vs. Valence (Positivity)’)

fig.layout.template = ‘seaborn’ fig.show()

By adding another trace called ‘lineOfBestFit’ and calculating the regression using numpy, we can plot the regression line:

dataPoints = go.Scattergl(

x=df.loudness,

y=df.valence,

mode='markers',

marker=dict(

opacity=1,

line=dict(

color='white'

)

),

name='Data points'

) data=[dataPoints] layout.update(

yaxis=dict(

title='Energy'),

xaxis=dict(

title='Danceability'

)

) figure.update(

data=data,

layout=layout

) m,b = np.polyfit(df.loudness, df.valence, 1)

bestfit_y = (df.loudness * m + b) lineOfBestFit=go.Scattergl(

x=df.loudness,

y=bestfit_y,

name='Line of best fit',

line=dict(

color='blue',

)

) data=[dataPoints, lineOfBestFit]

figure = go.Figure(data=data, layout=layout) figure.update_xaxes(autorange=""reversed"")

figure.layout.template = 'plotly_dark' iplot(figure)

These are you just two of the multitude of graphs available through seaborn and plotly libraries. Both seaborn and plotly create visually appealing graphs, but plotly allows for endless customization and interactivity with fairly intuitive syntax, making it a popular tool among data scientists.",https://towardsdatascience.com/matplotlib-vs-seaborn-vs-plotly-f2b79f5bddb,['Clare Blessen'],2019-11-21 18:53:46.798000+00:00,815,"Data Visualization, Matplotlib, Seaborn, Plotly, Python"
Setting an inner terminal with anaconda in Atom,"Anaconda logo ❤

As a developer sometimes you want to customize your tools to your requirements , Atom as his page says is “A hackable text editor for the 21st Century” and this is true for lots of developers that use it every day for his amazing purposes.

Atom logo that is an actual Rutherford model atom OMG

Let’s talk about a basic customization that any of you will find useful, to set a terminal panel working with anaconda prompt in windows!

With this your workflow will be a lot more organic and fluent.

Of course, you will need anaconda and atom already installed in your system.

Install Terminus

To install terminus package you can do it by clicking install in this link https://atom.io/packages/terminus

For terminus every terminal is loaded with your system’s default initialization files. This ensures that you have access to the same commands and aliases as you would in your standard terminal, all you love about terminals, now in a pocket size Atom panel.

Atom package that enables bottom terminals

Once the installation is finished you will be blessed with two new buttons at the bottom “+” and “X” with these two guys, you can add and delete as many terminal as you want!, I know it’s beautiful.

The two buttoms of doom

Setting anaconda

Now you need to configure terminus to star working with anaconda, by default terminus will work with Power Shell and this can be a little tricky for people that are used to work with CMD so the first thing is to override the shell instance that we are going to work with.

Enter in Atom configuration panel by pressing “CTRL” + “,” once you have it go to “Packages” and write “terminus” in the search bar, then press “Settings” and go to Core>Shell Override.

In shell Overid writte “C:\WINDOWS\system32\cmd.exe” this will enable terminus to work with the command prompt,

The next step is to configure the “Auto Run Command ” just writte or copy these “ C:\ProgramData\Anaconda3\Scripts\activate.bat C:\ProgramData\Anaconda3”

Now try to open a new terminal with the “+” button , if everything is right you will be able to see something as beautiful as this

if you see the precious (base) before your path then you made it !

You are “now” a wizard Harry

If you reach this part then you can star enjoying working with as many terminal as you want, I work with flask developing web applications so an environments like this helps me a lot, I hope it can help you too.

And for my Linux lovers there are good news! If you already have anaconda to work with your terminal then terminus will star working with anaconda and that would be all! ❤

Thank you all for reading!",https://medium.com/analytics-vidhya/setting-a-within-terminal-with-anaconda-in-atom-835133a16937,['Alfredo Castaneda'],2020-12-04 17:25:27.550000+00:00,439,"Anaconda, Atom, Terminal Panel, Developer Tools, Customization"
Applying Machine Learning Models to Predict The Presence of Heart Disease in Humans,"Artificial Intelligence(A.I) is gradually taking over many industries and automating tasks with high efficiency and accuracy. Machine Learning is an application of A.I, where systems learn from data and improve without explicit programming. One of the applications of Machine Learning in the medical field is predicting diagnosis for different diseases and conditions.

It’s important to have a model with high accuracy since the predictions are concerned with the health of human beings, so it is necessary to test different models and see which one provides a better result.

I’m going to explore two Machine Learning models, Logistic Regression and K Nearest Neighbor, and implement them to predict diagnosis for the presence of Heart Disease in Humans. The dataset I’m gonna be using comes from UCI and can be found on kaggle(link on the bottom of this article).

I’m gonna use Jupyter Notebook as my environment, Python3 as the programming language, Seaborn and matplotlib for data visualization, and SKLearn library for Machine Learning models and metrics.

Let’s import the libraries first:

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.linear_model import LogisticRegression

Let’s import data and visualize the row and columns, as well as the amount of different labels:

The column with the name ‘target’ will include our labels(0 for no heart disease and 1 for heart disease) and the rest of the columns will be used for our training dataset. Before feeding the columns as training features, we need to make sure they are not highly correlated so we can sustain accuracy.

Add this code and you should get a polished heatmap visualization with the darkness indicating the amount of correlation:

#see if features are correlated

plt.figure(figsize = (20,20))

matrix = np.triu(df.corr())

sns.heatmap(df.corr(),annot=True,mask=matrix)

Now we need to split our dataset into testing and training sets, we can use sklearn to accelerate this task. After splitting the data, we’re gonna scale it, so values that are not on the same scale don’t cause miscalculations:

from sklearn.model_selection import train_test_split

from sklearn import preprocessing

#split data into training and testing sets target = df['target']

x = df.drop(columns=['target'])

y = target

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0) #preprocess and scale

scaler = preprocessing.StandardScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))

X_test=pd.DataFrame(scaler.transform(X_test))

Now we can instantiate our Logistic Regression model and train our data. Logistic Regression is a classifier that uses a logistic function(Sigmoid Curve) to model a binary variable. Now we’re gonna feed the model for training, then predict based on data from the testing set:

#instantiate model and train

logreg = LogisticRegression() # fit model

logreg.fit(X_train, y_train) #use model to make predictions on test set

predictions = logreg.predict(X_test)

Now that we have made predictions, we can compare our labels in y_test and predictions to evaluate our model, SKLearn has a pre-made confusion matrix that puts out values for: True Positive(TP), False Positive(FP), True Negative(TN), and False Negative(FN). Here’s the code to add a confusion matrix:

from sklearn import metrics

from sklearn.metrics import confusion_matrix

print(confusion_matrix(y_test,predictions))

Then from the matrix, we can calculate accuracy:

accuracy = (TP+TN)/(TP+TN+FP+FN)

For this model, the accuracy is 82.9%

Now let’s use K Nearest Neighbor to see if we can get better accuracy.

K Nearest Neighbor(Knn) is a model that can do both classification and regression(prediction based on continuous values). For this case, we are using Knn as a classifier. The model predicts by simply taking the vote of the nearest data points. For example, if we use the 7 nearest neighbors, and 3 of them have the label ‘0’ and the other 4 are ‘1’, the new prediction will be ‘1’. The ‘k’ value is the number of nearest neighbors we’re going to use for prediction. It’s a challenge to find the optimal ‘k’ value, so we’re gonna use different values and see which one works best:

from sklearn.neighbors import KNeighborsClassifier # try ro find best k value

k_scores = [] #train with different values of k

for i in range(1,20):

knn = KNeighborsClassifier(n_neighbors = i)

knn.fit(X_train, y_train)

k_scores.append(knn.score(X_test, y_test))



plt.plot(range(1,20), k_scores)

plt.show()

x-axis = k values, y axis = accuracy score

As we can see in the visualization, when the ‘k’ value is right around 7, the accuracy peaks above 86%, so let’s use 7 as our ‘k’ nearest neighbors.

#instantia knn model based on k value with highest k_score and train

knn = KNeighborsClassifier(n_neighbors = 7)

knn.fit(X_train, y_train) #make metrics report

from sklearn.metrics import classification_report

predictions = knn.predict(X_test)

report = classification_report(predictions,y_test)

print(report)

Metric report for knn:

Sources:

Jupyter Notebook → https://github.com/noirvin/Machine-Learning/blob/master/Final%20Project.ipynb

UCI Dataset → https://www.kaggle.com/ronitf/heart-disease-uci",https://ai.plainenglish.io/applying-machine-learning-models-to-predict-the-presence-of-heart-disease-in-humans-a892c71e8261,['Arvin Seifipour'],2020-10-29 21:25:17.996000+00:00,695,"Artificial Intelligence, Machine Learning, Medical Field, Logistic Regression, K Nearest Neighbor"
"Think BIG, think BIG DATA!","Photo by Franki Chamaki on Unsplash

Over the past years, the amount of data generated has increased exponentially.

What do you think is the reason for this?

If you thought it’s because we have newly created data sources that didn’t exist back then, you’re right! If you didn’t get it right, worry not! I got you.

Pause for a moment and think about what you’ve done today…

You probably sent a text to a friend or you posted a picture on social media. Maybe, you even liked or commented on a post. All these activities you did, together with the machine and tools you used generated data.

You may be wondering what the sources of big data are.

thinking meme

Well, the bulk of big data comes from three primary sources: human, machine and organizational data. Let’s look at each one separately.

Human-generated: This is data humans create and share.

Examples of Human-generated data are posts on social media and emails.

Social media has been the leading source for the propagation of human-generated data. Over 350,000 tweets are sent per minute each day. Check out more about this here

shocked gif

Machine-generated: This is data generated from machines without any active human intervention.

But hey! What does it mean for data to be generated without active human intervention?

what does it mean gif

Think of a satellite! It generates optical photos of the earth, the temperature of the earth’s surface and many more. It is programmed to do so and does so on its own without a human telling it to do so.

Examples of machine-generated data sources are sensors on cars, satellites, security cameras, fitness apps, etc

Organization — generated: This is data generated as organizations run their businesses.

For instance, when you make a purchase online, data about the date and time you purchased the item, the number of items you purchased and even a unique customer number is generated. This is an example of organization-generated data.

Why do we call some data “BIG”?

confused gif

Big data refers to data that is nearly impossible to process using traditional methods, like a single computer, because there’s so much of it, being generated so quickly, in many different formats.

Big data is characterized by 3 Vs: namely, Volume, Velocity and Variety.

These characteristics are key to understanding how we can measure big data. Let’s tackle each one of them separately.

Volume refers to the massive amount of data generated every day.

The International Data Corporation (IDC), forecasts that the amount of data that exists in the world is growing from 33 zettabytes in 2018 to 177 zettabytes by 2025. Just to put that into perspective, the laptop I used to type this has 256GB of storage. That’s equivalent to just 0.000000000256 (9 zeros) zettabytes.

you just blew my mind gif

Velocity refers to the speed at which new data is generated and the speed at which data moves around.

A social media post going viral is a good example of data velocity.

Variety refers to the diversity of the data. In other words, the many different types of data that exist today.

What Are The Types Of Big Data?

anticipation gif

Structured data: This refers to any data that conforms to a certain format/schema.

This kind of data fits neatly into rows and columns. A popular example of structured data is a spreadsheet like Excel.

In the spreadsheet below, for example, we can see that the data in the Price column are up to 2 decimal places and the Product_IDs are all 5 digit long numeric values.

excel sheet containing structured data

Because structured data are organized, they are generally easier to analyze.

Unstructured data: This is the opposite of structured data. They are also referred to as “messy data”.

If I asked you to tell me how much money KFC makes in a day and I give you the spreadsheet that has the daily sales, it would be a very easy thing for you to do. But assuming I gave you camera footage of each transaction and asked you to tell me how much revenue KFC makes. Now that’s a really difficult task and you may probably call me “wicked Cliff”.

Unstructured data: This is the most widespread type of data. About 90% of the data we have today is unstructured. Because the nature of unstructured data is unsorted, disorganized and left in its original state, most businesses just keep it all. Audio files, emails, pictures and social media posts are examples of unstructured data.

Semi-structured: This type of data fits somewhere in between structured and unstructured data.

It does not fit neatly into rows and columns but has some level of organization. A very good example is HTML. This is because, in HTML, we can organize different kinds of data in tags. For example <p> for paragraphs and <ul> for lists.

I hope this article served its purpose as a comprehensible introduction to Big Data. This article aims to build your foundation in Big Data.

That’s it for today folks.",https://medium.com/@clifflolo/think-big-think-big-data-181b9241517a,['Clifford Frempong'],2021-09-15 15:23:57.328000+00:00,799,"Big Data, Human-generated Data, Machine-generated Data, Organizational-generated Data, Structured/Unstructured/Semi-structured Data"
Technology Trends That Will Impact Businesses in 2021,"Technology is growing day by day. It has become important to stay in touch with the latest technology and use them in your business wherever necessary. Implementing these technologies in your business will help you in growing and scaling up your business in 2021.

#1 Artificial intelligence

It’s no secret that AI is taken over every possible industry. Gone are the days when only IT companies used to work with AI. Now, all the businesses are using Artificial Intelligence in many ways.

The simplest example here is the chatbot and if you take a little complex example, there are image and speech recognition that will take your business to the next level. In the future, if your business is not utilizing the power of AI, you might lose a lot of revenue and your competitors might even get ahead of you.

AI is going to be a $190 billion industry in the upcoming years. According to the reports, it will be possible by the end of 2025. In the coming years, AI will play a very important role. Now, all the businesses have started the digital transformation which is surely going to increase in the upcoming years.

It’s the right time to implement AI in your business. Even small businesses can start with a recommendation system and chatbots to increase the branding of the business.

#2 Blockchain

A decentralized platform is the new choice of many people. Starting from digital currency to privacy, blockchain has all the things that people need. If your company is product-based, you should surely start focusing on blockchain.

By that time, mostly all the people have already started implementing blockchain for their business. Blockchain has the word “Chain” which means that all the activity you implement will form a chain. In simpler words, once the activity is done, it can’t be removed from the chain. This will make it a lot more secure than it is supposed to be. Further, people also trust blockchain more as privacy is properly maintained here unlike many other platforms.

This mostly applies only to the companies or businesses who have a digital product or is an IT company. If your business falls among any of these, you should surely focus on blockchain.

#3 Machine Learning (Part of AI)

Machine learning is a part of AI which has become a lot more popular. The reason to add this in another section is that ML is a huge technology trend that has a lot of things you can take into consideration.

Machine learning is a process of writing programs that will allow the machine to teach itself. In simpler words, once the data is provided to the machine, it will analyze the data and train itself. Chatbots are the best example here. When people chat with the chatbot, the bot will collect the data and analyze it. Depending on the data, the machine is capable of answering other questions. The Recommendation system is also an example of ML.

There are various use cases of machine learning. It can be used for prediction, making a decision, surveillance, and many others. All businesses can take the advantage of machine learning. Further, you can also implement deep learning which is a subpart of machine learning.

#4 IoT (Internet of Things)

Internet of Things simply means the physical device that is capable of connecting to the internet. Any regular appliance or device that can connect to the internet can be considered an IoT device.

This can be a simple home appliance such as a home security system or you can consider big things such as electric cars and auto-pilot cars. As billions of devices work on the internet, it’s safe to tell that every business will surely have an impact on IoT.

No matter what you are selling, you will need to implement IoT at some point in time. Especially, if your business is selling appliances, you will have to keep an eye on the Internet of Things as well. With the 5G speed, it will be easier to connect and use these devices.

#5 5G

No doubt, the 4G has given us tremendous speed. However, there are still some of the places where 4G lacks behind. Like we talked about in the previous example if the use of IoT will increase, the need for 5G will also increase.

Many companies have already started working on 5G technology and there are already some f devices in the market that are capable of using 5G. It won’t be long that you will see a 5G impact on your business.

#6 AR VR

AR stands for Augmented reality and VR stands for Virtual Reality. Virtual Reality will immerse the person in the new environment whereas AR will do the reverse.

Virtual reality devices are wearing which you can experience the scenarios similar to or opposite to the real world. Whereas in the Augmented Reality, the object will reside in the real world.

For example, one can experience the live solar system after wearing VR glasses. In the Augmented Reality, the planets and the other objects will revolve in the real world giving you almost real experience.

This is usually going to impact the gaming industry at first. However, it will soon impact the rest of the businesses as well. Therefore, it’s always better to start planning.

Final Words

To conclude, these are some of the technology trends that will impact businesses in 2021. If your business falls in any of the given categories, you should surely act on it. Sooner or later, it will start impacting all the business. Especially, the first three technologies will have a huge impact on every single industry. So, it’s always better to start acting now and be ahead of your competition.

If you are planning to act on it right now, we can have a consultation call and experts from our team will guide you on how you can proceed with these technologies.",https://medium.com/finwintech/technology-trends-that-will-impact-businesses-in-2021-35a8bc3c2afc,['Finwin Technologies'],2020-12-01 12:44:09.471000+00:00,964,"Artificial Intelligence, Blockchain, Machine Learning, IoT (Internet of Things), 5G"
Housing in the San Francisco Bay Area: Deal Hunting Using Machine Learning,"Housing in the San Francisco Bay Area: Deal Hunting Using Machine Learning

Modeling home prices reveals undervalued neighborhoods and listings.

San Francisco downtown in the foreground, East Bay in the background. Photo credit: Unsplash

Executive summary

This article describes the collection, visualization, and modeling of single-family home prices listed across the San Francisco Bay Area. Approximately 7000 listings were scraped from a real estate website in June 2019, enabling the exploration of various factors influencing home prices across the region. Complementing listing information (number of bedrooms and bathrooms, home size, and lot size) with location data (school quality and commute times) was found to significantly improve explanatory power of the multiple linear regression model. Regression coefficients also tell an interesting story about the market’s perception of value associated with marginal changes to various aspects of the home and its location. Importantly, the pricing model enabled identification of undervalued listings and neighborhoods, offering investment recommendations of potential interest to individual homebuyers and investors alike.

Introduction

While the performance of the S&P 500 and the overall US housing market has been nearly identical since 2000 (both up ~100%), home price indices in the San Francisco Bay Area have risen by approximately 167% (St. Louis Fed). As such, Bay Area homeowners have enjoyed an opportunity to build wealth through real estate in a way that is not necessarily accessible to most of the rest of the country.

For those already bought into the market, this near-tripling of real estate values since 2000 has undoubtedly been a good thing. However, for those newly relocated to the region, saving towards a down payment and choosing where to buy can be a daunting task. Inspired by discussions I’ve had with friends and family, and basic concepts in investing (i.e., buy underpriced assets) I set out to gather as much information about current prices of single-family homes in the Bay Area, apply machine learning techniques to tease out factors driving home values, and identify areas that may be appealing for investment.

Methods

Data from single family home listings (address, beds, baths, home size, lot size, latitude/longitude coordinates, and price) across the Bay Area was scraped in June 2019 from a real estate webpage using the Requests and BeautifulSoup Python libraries, and cleaned and processed using Regex and Pandas. Complementing this listing data, commute times were obtained from Google Maps and school quality data pulled from the 2018 California Assessment of Student Performance and Progress (CAASPP). The resulting information was overlaid onto maps using Cartopy, Matplotlib, and shapefiles (town, zip code, and neighborhood borders) from Stanford Earthworks. Box/strip plots and pairwise relationships between variables were visualized using Seaborn. Ordinary least squares (OLS) regression analysis was applied to the data using Statsmodels and Scikit-learn libraries. The full source code for this project is available on GitHub.

Results

This study began with the collection of listing data for all single family homes on a real estate webpage (mlslistings.com). Looping over a list of Bay Area zip codes, location details (address, latitude and longitude coordinates), property characteristics (number of bedrooms, number of bathrooms, home size, and lot size), and list price were scraped for each listing using the Python Requests and BeautifulSoup libraries. Across 214 zip codes, location, property, and price information was collected for 7151 listings in June 2019. The data was cleaned using regular expressions to remove extra whitespace, unwanted characters, and entries with missing values.

I. Visualizing listing data

A. Geographic trends

With information for several thousand properties on the market across the Bay Area, the location of all listings was plotted on a terrain map and color-coded by price (Figure 1) using the Python Cartopy package together with city border information obtained from Stanford Earthworks. This map reveals clear geographic trends of the relative cost of homes across the region. For instance, San Francisco, Marin County, and the Peninsula typically contain the most expensive 20% of listings (dark blue data points), while those in Oakland, San Leandro, and Richmond are typically in the least expensive 20% (dark red data points). Similarly, in the South Bay, homes closer to the Santa Cruz Mountains are typically more expensive than those running alongside the Diablo Range.",https://towardsdatascience.com/house-hunting-in-the-san-francisco-bay-area-deal-hunting-using-machine-learning-3ed6fc8e8991,['Michael Boles'],2020-08-24 16:38:30.317000+00:00,682,"San Francisco, Bay Area, Housing Market, Machine Learning, Real Estate"
Basic Statistics: A BrushUp,"If you are like me, you’ve taken a stats course, most of us have. Often, taking a class does not signify mastery more than skilled retention, however. With this article, I hope to reintroduce some to why stats can be fun (and it is going to be a part of a series I hope to write). I want to cover descriptive statistics such as Mean, Median, Mode, Range, Variance, Standard Deviation, Percentiles, and reading a scatter plot and histogram. The sample dataset will be of Wine Rating per characteristic of wine (Data Source). The medium won’t be anything more advanced than excel. All steps are also listed below the article for reference if you wish to do it independently.

First, we want to get a 10,000 ft. understanding of our data; it is a good idea to find summary statistics for each column. The summary statistics of the “quality” column of the wine data set tells a lot:

1 — sample size

2 — minimum and maximum values,

3 — the most common value (mode)

4 — the average (mean)

5 — the middle value (median)

6 — the range (max value less min value)

7 — the standard deviation (sqrt of the sum of squared distances from the mean over the n number values — 1)

All in all, standardized measures that, when compared, can immediately tell you if there are outliers, if they are miss entries, if the data is tight or scattered, or if the data needs further inspection.

Summary Statistics of Quality Column in our Data Set

After doing this, creating a histogram for each column will further help you contextualize the data. Histograms make value range “bins” and then count each data point that fits each “bin.”

In the histogram for the quality column, we can see that not every value from 0–10 is present in the data set. We can see that the mode is in the 5 “bin,” but there are many 6s as well. We can also see that while there are a few high and low rates, most of the ratings are average. We can start to get a feeling for what we want to do with the data. Next, we must find out the statistical composer of each predominant group (three in this case: high, mid, and low). To find this, do a filter on the data set for each type (3 ratings, 5 + 6 ratings, and 8 ratings in this case), then create summaries for each.

Custom Summary of Low Values (~3 ratings)

Custom Summary of Mid Values (~5–6 ratings)

Custom Summary of High Values (~8 ratings)

From this, we can tell then for the most part the factors probably have little to do with the quality of the wine. But this is data science and “probably” is not an acceptable deliverable. The above gives us a sense of extreme similarities between the three major groupings of quality, however, it is not easily interpretable. The next step is to create correlation coefficients for each grouping and then describe the percentiles (50th being the Median, 0th being the minimum, 100th being the maximum) in which each grouping falls.

Correlation Coef. of Low Values (~3 ratings)

Correlation Coef. of Mid Values (~5–6 ratings)

Correlation Coef. of High Values (~8 ratings)

As we can see by these summaries, our initial estimate of the custom summaries was accurate, there is not much correlation between the quality of the wine and the factors listed in this dataset. We could create scatter plots for each of these categories to get a visual understanding of the correlation coefficient, but instead, we will describe the significance of the number. A correlation coefficient ranges from -1 to 1. A -1 represents a strong negative correlation, a 0 represents no correlation, and 1 represents a strong correlation. For example, using the following sample data, you will get the following results.

Sample Data

No Correlation (Close to 0 Coef.)

Strong Negative Correlation (Close to -1 Coef.)

Strong Positive Correlation (Close 1 Coef.)

Now that we understand that our dataset is not offering important information to the cause of poor versus great quality wine, we have to put on our thinking caps. What are the hidden variables? If it is not the acidic and chemical composure, what is it? Maybe branding, prices, vintage, the heritage of winery, grape, type, packaging, flavor profile, etc.

One of the key takeaways of this exercise was that we know have a quality framework for how we can look at more data related to the quality of wines. We have also knocked one piece off our docket (at least for now): we know that this data is not the key. Knowing what does not work is almost as important as knowing what works. Like all sciences, to make discoveries or support hypothesis, one must do a lot of grunt work to have a diverse range of qualifying findings. It takes patience, practice, and persistence to come up with numbers that are significant.",https://bowenbrinegar.medium.com/basic-statistics-a-brushup-4144392b1eaa,['Bowen Brinegar'],2020-10-05 04:37:25.232000+00:00,811,"Descriptive Statistics, Mean, Median, Mode, Range"
18 Most Recommended Data Science Platforms To Learn Python and SQL,"18 Most Recommended Data Science Platforms To Learn Python and SQL

A comprehensive guide to the most popular data science learning platforms by the community

Data science is one of the hottest careers in today’s market. Companies are always hiring data scientists and there’s always a large number of people trying to become one. But data science hasn’t been around for as long as other technical disciplines, so unlike software development which has, there aren’t as many data science-specific learning platforms to help prepare aspiring and experienced scientists. There’s of course Coursera and Udemy, then a few big brand name platforms like DataCamp. Then there are a dozen or more smaller niche platforms aimed at training data scientists their way.

I’ve evaluated 18 platforms recommended by people in the data science communities. Depending on your learning style and need, there’s a platform for you. The first half of the article is my final assessment of the platforms, ending with a complete list of platforms with all the details that helped me evaluate them. The second half of the article details my approach in evaluating the platforms — understanding the different user types, sourcing platforms, and variables to evaluate. So if you have the time, skip to the end first to understand my evaluation criteria and then read the results. If you’re pressed for time, just go to the next section.

TL;DR: My Assessment

I created 2 graphs to map my variables. They’re those graphs with the quadrants you see in all the whitepapers. Honestly, I’m a bit ashamed that I chose this visualization (it feels like making 3D pie charts in Excel) but I think it will allow you to quickly figure out which platform fits you best.

Graph 1: Learning style and experience

Ask yourself: What is my learning style? Do I like to dive right in and start coding or do I like to watch videos first? Am I totally new at this and need more hand-holding? Or can I jump right in?

Learning style by user experience [Image created by N. Rosidi]

At the extreme end of lecture-based platforms are pure passive lectures like text on webpages or people talking on videos. Oftentimes these platforms ask you to install the software yourself, which I think is lazy since it’s so easy to spin up a server with all the software to offer the full out-of-the-box experience. Users aren’t always advanced — that’s why they’re learning — so offering them an experience where they can access the software, libraries, and datasets easily accessible via browser is now a must-have — at least to me.

Interactive, self-guided learning allows you to work at your own pace and select whatever topic you want to learn or practice. At the extreme end, there’s a fully functional IDE that allows you to explore, interact, and manipulate data like you would in a real professional setting. But it’s not just completely self-guided as there are solutions to help you solve questions and understand concepts. The approach here is to start coding immediately and learn the solution if you are stuck. These platforms are perfect for the experienced user, even if you’re just moderately experienced.

The middle ground is a blend of lecture and interactive self-guided learning. With these platforms, you’re on a “course” and “path” to learn a set of concepts from start to finish. You’re presented with text and/or video and then asked to apply what you learned in an interactive-but-limited IDE where you’re asked questions that build up in complexity. Interactive-but-limited IDE means that the IDE only accepts inputs that answer the question prompted so you can’t go off the path and explore the data or try out different functions and techniques — you know, things a good data scientist would do. This is perfect for someone learning from scratch and is trying to learn something new from the ground up.

Graph 2: Content Focus

Ask yourself: Am I trying to learn python? Or am I trying to learn python to be a data scientist? Am I trying to prepare for an interview? Or am I trying to build my first machine learning model? Or just trying to learn the basics?

Educational topics by technical focus [Image created by N. Rosidi]

Specialty topics are educational content meant for a specific purpose. For example, it’s preparation for a technical interview — either data science or software development. It’s to learn financial modeling with python or to build a gradient boosted decision tree. These platforms serve a niche population but they do it extremely well to the satisfaction of their user base.

General education is merely an introduction to basic concepts that are building blocks to becoming proficient in navigating the language, like how to create pandas dataframes to manipulate data. Once you’ve mastered these concepts, the next phase would be to dive into specific topics that serve your needs.

Platforms are designed with a user in mind. In this case, are you a data scientist or software developer? If you’re reading this, you’re probably a data scientist, but as I’ve mentioned before there are many more educational platforms created for teaching software development. Sometimes these platforms offer education in python but their content and examples aren’t always meant for data scientists. It’s sort of like adding a plug-in to a piece of software (it’s not native and sometimes feels a bit hacky).

A detailed evaluation of each platform

In case the graphs aren’t enough for you.

What each platform is known for and how they’re unique

Link to the Google Sheet: https://bit.ly/2UknbZ3

What type of user would benefit most from each platform

Link to the Google Sheet: https://bit.ly/2YaNlhR

Teaching style and features by user experience level

Link to the Google Sheet: https://bit.ly/3f0SPm9

Educational topics by technical focus (Data Science and Software Development)

Link to the Google Sheet: https://bit.ly/30kaqkS

So depending on your career goals and how you like to learn, there’s a platform for everyone. In your journey, you might end up using a few of these platforms since each platform focuses on a specific strength. Pick the data science platform that allows you to learn what you need to learn for the stage you’re at. I hope this list has been helpful.

See below for my approach to evaluating the platforms…",https://towardsdatascience.com/18-of-the-most-recommended-data-science-platforms-to-learn-python-and-sql-f969f61b6d70,['Nathan Rosidi'],2020-06-09 04:06:14.924000+00:00,1021,"Data Science, Python, SQL, Software Development, Data Analytics"
4 Ideas for Regulating Big Tech from Harvard’s Board President,"Common ownership of data on “human behavior”

Marty: So, first of all, I would say monetizing human behavior belongs to the commons. It does not belong to any one company. And any company that wants to access the click streams, the aggregated, anonymous click streams of people has to pay the commons for that. And the government can use that revenue…

Kenn Cukier (moderator): Wait, wait, wait, wait, wait. Let me stop. I’m a company. I’ve hired some of the smartest data people in the world to tell me what signals are relevant and which ones aren’t. I’m going to invest a pile of money to be really smart and collect that data [and] another pile of money to make sure the data is clean. And then you’re going to tell me that I have to give away this data to some commons, to my competitors. That’s crazy.

Marty: Well, okay. Um, so, so right now…

Kenn: Because I’ll just explain why, because it would chill, it would chill innovation. I wouldn’t become a smarter firm that’s willing to make those investments to learn what’s relevant and what’s not [in] understanding consumer behavior, if I wasn’t able to monetize that myself and get my competitors away from it.

Marty: I am not at all worried about the ability of Facebook and Google to pay the commons for access to individual data and still derive insights and still clean it and still find the signal better than anybody else and still serve up those targeted ads. So just right now, they’re getting it for free. And so this could be the source of new revenues for land grants, for education, like the land grant universities for education. That’s one thing.",https://medium.com/datafleets-blog/4-ideas-for-regulating-big-tech-from-harvards-board-president-6ec985bc6c6a,[],2020-11-17 00:46:45.534000+00:00,284,"Data Commons, Human Behavior, Monetizing Data, Data Revenue, Land Grants"
The Simple Math Behind Object Detection. Notes,"It’s just 4 picture-notes describing simple math needed to represent coordinates in their center-size fractional form, cropping images, and calculate some metrics (precision, recall, intersection over union) for the object detection task.

Fig. 1. Center-Size Coordinate",https://medium.com/@petroivaniuk/the-simple-math-behind-object-detection-notes-d938f79639d2,['Petro Ivanyuk'],2020-11-09 21:25:23.703000+00:00,35,"Representationobject detection, image cropping, precision, recall, intersection over union"
Building a complex reinforcement learning crypto-trading environment in python,"Building our environment

I’ll be using Tensorflow’s agents to interact with the environment later on, so I will be building a custom environment class that extends the py_environment class provided by the TF environments library.

The most important two vectors to note are the actions and observations. With each step our agent will choose a coin to buy, sell, or hold for a % amount of what it currently owns, hence we have the numpy array with the shape (3) of type ints where the first value is the number of pairs we’ll be working with, 2nd is the buy, sell, or hold, and last is the amount where 1 is 10%, 2 is 20% and so on.

As for the observation space, I want my agent to observe the market as a whole and not just the coin that it just traded in. So for now it’s (4, 5, 40), where 4 is the pairs, 5 is the volume, open, high, low, and close values for each step, and 40 is the look back window which is basically how far back should our agent look at the data. This might not be the optimal observation space we want but my focus for this article is to just to have a fully fledged environment that we can start experimenting with and engineering every component later on.

Also we defined our wallet as a list where the first value is the initial USD balance. Note that I’m using locally stored data in the form of an excel spreadsheet, I used Binance’s API to basically download market data for each pair seperated by sheets:

Excel file for market data

I won’t be going over the script I used to extract data into this format but you find it here.

Next, we define our reset() method to reset our environment with each episode:

Similar to the constructor in the reset() method we reinitialize everything, except constants such as our price data for example.

Now, moving to our step function which the agent will be passing an ‘action’ to and receiving a ‘timestep’ object containing the reward, state, and discount with each step.

First I initialized a list ‘data’ which will contain the dataframes necessary for the state at that step (i.e taking the rows in the range of the current step and 40 steps behind it which is our look back window). Then I added the condition for when our episode ends which is when our agent loses all it’s money.

We will then either buy, sell, or hold, based on which the wallet will be updated and lastly we increment our step and return the timestep object as a transition to the next state.

Again, I’m leaving all the action reinforcement learning engineering for a seperate article and so our reward currently does not function at all. The end goal of this part is to just have an environment that an agent can interact with and that can be visually rendered.",https://levelup.gitconnected.com/a-complex-reinforcement-learning-crypto-trading-environment-in-python-134f3faf0d7a,['Mohammad Abdin'],2021-01-24 12:58:57.457000+00:00,487,"Environment Building, Tensorflow, Custom Environment Class, Actions, Observations"
The NFL Combine in Two Dimensions,"Cool, but…

We recently published an analysis of pandemic unemployment and drinking where we used a data analysis technique called Principal Component Analysis (PCA) to visualize several variables in one plot. We received feedback from readers who were interested in learning more about PCA but have previously found it hard to interpret and difficult to apply with real datasets.

PCA is known for being cool to look at, but not always useful due to a lack of interpretability. We’ll walk through applying PCA to NFL combine data, explaining how to use it, the factors we considered, and how we build an intuition for useful analysis. We will also provide links to the data and Python implementations from the article so you can immediately apply PCA in your own work.

A particular perspective

There are multiple applications of principal components including feature engineering, compressing data with minimal information loss, and visualizing high dimensional data. Here, we focus on the third application and analyze principal components as a means of better understanding structure, particularly how variables tend to change when others change, in a dataset.

PCA can be studied from different perspectives including linear algebra, mathematical statistics, and computational methods. Here, we focus on the less formal perspective of exploratory data analysis. This application and perspective puts us in the setting of unsupervised learning for exploratory data analysis, with the general objective of summarizing a dataset in a way that reveals new insights.

Bivariate beginnings

We’ll start by applying PCA to a small, bivariate subset of a dataset from the 2021 NFL Scouting Combine and work our way up to the eight variables shown in our introduction.

The NFL Combine is an annual event where elite college athletes complete a series of tests in preparation for the draft. The measurements taken at the combine yield a dataset describing the variability of physical attributes and performance within this group. We’ll start building our intuition for PCA using just two variables, height and weight.

PCA assumes that variance is what is interesting about a dataset. When variables in our dataset are correlated, some of their variance is redundant. We can summarize this covariance with a smaller number of new variables, called principal components. Principal components are weighted averages of the original variables that possess particular properties.

In the plot below, it appears that athlete height and weight are correlated — when height increases, so does weight. If you had to choose one, and only one, variable to summarize the variability in these players, what would it be?

You could pick one of the existing variables (height or weight). However, the correlation between height and weight indicates that we can construct a more general variable related to both. Let’s call this more general variable player “size”, and define it as an average of height and weight. You might not know how to compute that average, but if you had these athletes in a lineup, you would likely still have some intuition for sorting them by “size.”

In essence, PCA enables us to measure players by the single variable “size.” Specifically, PCA finds the optimal weighted average of height and weight for summarizing the variability of both height and weight with a single value. That single dimension is the first principal component of this dataset.

Note: from here on, we will display all variables in standardized form, a nuance that we explain further in the appendix for practitioners.",https://innerjoin.bit.io/the-nfl-combine-in-two-dimensions-95ba79e3470d,['Andrew Doss'],2021-08-23 17:58:50.368000+00:00,559,"Data Analysis, Principal Component Analysis, Feature Engineering, Compressing Data, Visualizing High Dimension Data"
What is ETL?,"In our articles related to AI and Big Data in healthcare, we always talk about ETL as the core of the core process. We do not write a lot about ETL itself, though. In this post, we’ll give a short overview of this procedure and its applications in businesses.

ETL is the abbreviation for Extract, Transform, Load that are three database functions:

Extract is the process of reading data that is assumed to be important. The data can be either raw collected from multiple and different types of sources or taken from a source database.

is the process of reading data that is assumed to be important. The data can be either raw collected from multiple and different types of sources or taken from a source database. Transform is the process of converting the extracted data from its previous format into the format required by another database. The transformation occurs by using rules or lookup tables or by combining the data with other data.

is the process of converting the extracted data from its previous format into the format required by another database. The transformation occurs by using rules or lookup tables or by combining the data with other data. Load is the process of writing the data into the target database, data warehouse or another system

ETL in its essence is a type of data integration used to blend data from multiple sources.

ETL vs. ELT

The ETL paradigm is inherent to Data Warehousing, and Big Data has significantly changed the order of the processes. In Big Data, data is “lifted and shifted” wholesale to a repository, such as a Data Lake, and is held there in the original format. It is transformed “on the fly” when needed by Data Scientists, creating the procedure of ELT, or Extract, Load, Transform.

One of the main benefits of ELT is a shorter load time. As we can take advantage of the built-in processing capability of data warehouses, we can reduce the time that data spends in transit. This capability is most useful when processing large data sets required for business intelligence and big data analytics.

In practice, however, things are not so black and white. Many Data Lakes, for example, contain intermediate merged and transformed data structures to ensure that each Data Scientist doesn’t repeat the same work, or carry it out in a different way.

Where are ETL/ELT used?

ETL is not a new technology: businesses have relied on it for many years to get a consolidated view of the data. The most common uses of ETL include:

ETL and traditional uses

Traditionally, ETL is used to consolidate, collect and join data from external suppliers or to migrate data from legacy systems to new systems with different data formats. ETL tools surface data from a data store in a comprehensible for business people format, making it easier to analyze and report on. The key beneficiaries of these applications are retailers and healthcare providers.

ETL and metadata

ETL provides a deep historical context and a consolidated view for the business by surfacing the metadata. As data architectures become more complex, it’s important to track how the different data elements are used and related within one organization. Metadata helps understand the lineage of data and its impact on other data assets in the organization.

ETL and Data Quality

ETL and ELT are extensively used for data cleansing, profiling and auditing ensuring that data is trustworthy. ETL tools can be integrated with data quality tools, such as those used for data profiling, deduplication or validation.

ETL and Self-Service Data Access

Self-service data preparation is a fast-growing field that puts the power of accessing, blending and transforming data into the hands of business users and other nontechnical data professionals. ETL codifies and reuses processes that move data without requiring technical skills to write code or scripts. With this approach integrated into the ETL process, less time is spent on data preparation, improving professionals’ productivity.",https://medium.com/sciforce/what-is-etl-1df5305bb341,[],2019-09-27 12:48:33.283000+00:00,641,"ETL, ELT, Data Integration, Data Warehousing, Big Data"
Predict Crash Severity with Machine Learning?,"It also offers more functionality, like normalizing data and feature engineering, which we did not set up this time.

Creating a Model

We can create a model of all available algorithms in the library, use cross-validation and see the best performing estimator for our data in one line of code. Other libraries would require you to write many lines of code to achieve this same functionality.

best_model = compare_models()

When we run the above one line of code, we initialize a training for all algorithms. It takes some time and updates automatically to rank the best performing algorithm. In our case, we have Xgboost as the best performing model.

The function also outputs the model performance with different metrics — Accuracy, AUC recall, etc. Different algorithms may perform better than others in certain metrics. However, in this case, the Extreme Gradient Boosting (Xgboost) algorithm performs best in all these metrics.

So, we go ahead and create an Xgboost model by running create_model() and providing the name of the algorithm.

xgboost = create_model(‘xgboost’)

This creates a 10 fold training for the dataset and goes through the data 10 different epochs.

Model Performance

Once, the training finishes, we can inspect the model performance. Again, Pycaret provides convenient plots for inspecting model performance. Forexample, we can plot the confusion matrix.

plot_model(xgboost, plot = ‘confusion_matrix’)

Confusion Matrix — Image by the author.

The model has difficulties differentiating between class 1 and 2 crash severity. It misclassified 80 rows of class 1 into class 2 severity. We also have 87 misclassified class 2, which the algorithm marks as class 1.

Our goal now is to create a baseline, so we are not going to worry about the performance. Let us go ahead and predict the unseen data.

Prediction

Remember we have set aside 5% of the data to be the test data. To predict these unseen data, you can call predict_model() function. Before we do that, we can finalize function to train again on the entire dataset including the holdout set.

final = finalize_model(xgboost) unseen_predictions = predict_model(final, data=test_data) unseen_predictions.tail()

We can see here the last rows of the prediction. We highlighted here the actual class (Severity) and predicted class (Label) in red. As you can see, we have several misclassified rows.",https://medium.com/spatial-data-science/predict-crash-severity-with-machine-learning-dc9848cabcef,[],2020-11-02 09:44:25.385000+00:00,358,"Machine Learning, Data Science, Xgboost, Pycaret, Feature Engineering"
Why Life-Long Learning Is Now Critical To Your Success,"Why Life-Long Learning Is Now Critical To Your Success

More than ever it will be your ability to learn that separates you from your peers

Photo by Canva

If 2020 has shown us anything it’s that acquiring new skills and knowledge as rapidly as possible will be more important than ever before if you want to thrive going forward.

The world was already changing quickly before this year but it feels like the pace of change has been quickened beyond anything we could have foreseen.

These shifts have created many opportunities but also, of course, a significant number of challenges as business and working models adapt and evolve.

All of these factors demand that we are more flexible and adaptable than ever before in order to progress in this new world order.

To be able to do so the most important skill you could have in today’s economy is the ability to learn new ideas rapidly and turn them into concrete action.

More than ever it will be your ability to learn that separates you from your peers regardless of the sector you operate in or your level within that space. It’s clear that companies recognise this and are trying to help their workforces cope. Research by LinkedIn Learning found that 51% of learning and development professionals aim to initiate new skills programs for their workforce.

What’s Causing The Fractures

As more advances in automation lead to increased proliferation of technology in various fields, many existing jobs may well be under threat. Whilst these advances in areas like artificial intelligence and robotics may well lead to increased productivity and economic growth they are very likely to change how jobs are done and what jobs are actually available.

The advent of robotics and artificial intelligence may well mean that a number of jobs that exist today will not exist in the years to come. Indeed research by global consultancy McKinsey estimates that only 5% jobs that exist today cannot be done by machines in their entirety.

That means pretty much all jobs that exist today will have some level of automation involved.

Think about that for a second.

That pretty much means whatever job you are in today and no matter how highly skilled you believe it to be, it will still likely be impacted by automation.

This may not necessarily mean that your job disappears but it will more than likely mean you will need to adapt quickly if you want to keep it. Even if your job role sticks around, it’s likely there’ll be less of that type of job available. With automation it’s likely jobs will be done faster and with fewer errors leading to less of a need for that role.

Whatever the case agility in learning will be critical as employees will need to constantly upgrade their skills.",https://medium.com/illumination/why-life-long-learning-is-now-critical-to-your-success-58b8050a4756,[],2020-12-23 09:27:41.323000+00:00,456,"life-long learning, success, skills acquisition, automation, job roles"
Artificial intelligence is here. Now what?,"Claire Merchlinsky. www.nytimes.com. “How to Build Artificial Intelligence We Can Trust.”

On February 10, 1996, world chess champion Garry Kasparov lost his first game to Deep Blue. Kasparov hails from Baku; Deep Blue, from IBM. The face-off took three hours and went down in history as the first instance a computer has beaten a human at the game. But that was 1996, not 2020.

Deep Blue was not the first nor the last in a dynasty of software programs whose family identity seems to be besting humans at human activities. The chess players produced by IBM have grown increasingly adept, but there are now other classes of computers. Computers control our cars as cybernetic chauffeurs. They teach our children. They help those with disabilities, they provide medical diagnoses, and they fish through the farthest reaches of the ethernet for answers to our most recent questions.

The list goes on. Even more perplexing, some of these computers have weaseled their ways into our hearts (despite having no emotional intelligence that we know of).

I’m reminded of one such computer in particular, also from the suite of IBM savants and bearing the eerily humanoid name of Watson. Ring a bell?

In 2011, when I was 11 years old, Watson made history when “he” went on Jeopardy! and faced off against the brightest trivia minds out there, which meant 74-time champion Ken Jennings and past champion Brad Rutter. “I, for one, welcome our new computer overlords,” Jennings wrote on his question card when facing imminent and obvious defeat. With this surrender he made reference to “The Simpsons,” an apt choice from a show that has garnered a reputation for predicting our future before we can do so ourselves.

www.youtube.com. “Watson and the Jeopardy! Challenge.”

“In the end, the humans on Jeopardy! surrendered meekly,” John Markoff wrote for the New York Times after the whole affair had died down. And the humans did, treating the victory as a gimmick or publicity stunt of sorts. Those involved responded to the news with humor, perhaps masking the greater implications of a computer capable of recalling facts from nothing at the drop of a pin. Watson beckoned comparisons to HAL from 2001: A Space Odyssey. This led to unease amongst some; few would welcome a HAL into our world.

www.youtube.com. “2001 — A Space Odyssey.”

Yet most of us, including myself, were excited to see Watson win. I still remember watching the episode with my parents, gathered around a single monitor. We rarely watched Jeopardy! as a family, yet this was history and we knew it. Everyone watched. And everyone wanted Watson to win. The programmers at IBM had designed a program that could elicit in humans a sympathy response, a want to love and nurture this baby in its infancy stages. Watson appeared lovable and clueless in the first few rounds of the game show. To one answer he responded with the question, “What is Toronto????” His multiple question marks were met with laughter. Watson was clumsy, unsure, not afraid to make fun of himself.

This is the real underpin of the artificial intelligence revolution.

Artificial intelligence is here. It’s been here for decades already. But artificial intelligence that we perceive as human, as worthy of emotion, that’s the more complicated part. Watson, with his human name, is the first in a line of robots designed to evoke a human response. Think of Siri and Alexa, similar programs capable of fitting in the palms of our hands whose feminized names and voice acting give them human dimensions (surely, you’ve heard of people jokingly asking their Siris and Alexas dumb questions and waiting for programmed joke responses). When confronted with it, artificial intelligence seems to have us finding the humor of it all or, on the other side of the coin, deeply repulsed.

This repulsion stems not from what the technology is capable of, but from the eery ways in which artificial intelligence attempts to mimic human life. In 2014, Alex Garland’s Ex Machina made its rounds through film circles and the general public and had its critics lauding it as a creepy look at the “disquieting power struggle waged in terms of human weaknesses” (Buzzfeed News). A power struggle. This is the fear, the source of repulsion. This is what scares us: computers who are perhaps “human+,” more feeling, more thinking, more skilled and capable than ourselves.

The female subject of Ex Machina is a beautiful robot named Ava, whose doll-like features and desire for freedom immediately endear her to the film’s protagonist. She ultimately backstabs him in pursuit of her own agenda. This proves that she is capable of passing the Turing test, the ultimate test in the timeline of artificial intelligence that marks at what point a machine is indistinguishable from a human. In the case of Ava and even of the robots we can expect will one day walk the earth, this is not the right test. These robots do not just resemble humans; they are more human. Superior in many regards. Ava is certainly human by this measure. Not only does she have wants and needs, but a higher level of emotional intellect that allows her to manipulate. Yet Ava is more than human, cunning like no other, more intelligent, more artistic.

www.youtube.com. “Meeting Ava in Ex Machina.”

Artificial intelligence, whether real or fictional, from Watson to HAL 9000 to Ava, forces us to question what we will do in response to this new species on our planet.

Perhaps our goal should not be to build the machine that mimics its human counterpart, but the machine that is “human+.” Or maybe even the machine that is “human nothing.” The machine whose aesthetic is a sharp departure from humanoid, the machine that does not attempt to blend in with us or copy our affectations but is beautiful in its own way.

Is our fear of these computers valid? It does not matter. Whether we like it or not, these computers are here to stay. All we can control is our response to them. As Ken Jennings quoting Kent Brockman quoting H.G. Wells said, “I, for one, welcome our new computer overlords.”",https://medium.com/swlh/artificial-intelligence-is-here-now-what-ea63634e9303,['Margaret Cirino'],2020-03-03 06:52:05.698000+00:00,1011,"Artificial Intelligence, Deep Blue, Watson, HAL 9000, Ava (Ex Machina)"
How TECHNOLOGY took out the headache of Ghana’s 2020 election,"image by Francis Kokoroko

Digital transformation is getting more trendy among policy-makers, economists, and industry leaders about its societal impact. We will all agree that for some time now, we have been in a new stage of transformation where corporations and countries are focused on equipping themselves with advanced technologies and new business models to catch up with the fast-changing world. The question is, is this digital transformation making a positive contribution to our society?

In my view, it is. Information Technology is playing an incredible role in making human lives easier. It has added value to society and has also been of great help in mitigating important issues, including public health, our environment and biodiversity, and democratic processes. A case in point is the crucial role it played in the just-ended election in Ghana.

As evident in the recent elections held on December 7, 2020, Information technology was used as a solution for many electoral hurdles, such as the creation of an accurate voter register, simplified voting and result tallying, faster transmission of election results, etc

Another example was the adoption of social media as a medium of campaigning by most political parties. The usually crowded rallies couldn’t be organized because of the COVID-19 pandemic that made such gatherings impossible. One could see adverts across social media channels of political parties trying to get their campaign message across. Social media was also used as a tool to promote peace. Hashtags such #Ghanaforpeace #peacefullelection2020 amongst others were created to preach peace.

Again, the Electoral Commission (EC) of Ghana’s use of another biometric verification (facial recognition) method made the whole process even easier. It solved the thorny issue of manual verification when the thumbprint fails creating confusion amongst party observers This addressed the hustle people go through to vote, joining long queues, etc, a challenge which was prevalent in the previous election.

We can confidently say that Ghana passed yet another democratic test when it conducted what has been described as the best organized general elections in the country’s history with the help of digitalization and the use of modern technology.

With the way digitalization is helping solve major problems around the world, I see the future as promising. There is a need for everyone to be abreast with modern technology to fit in the digital future. With the introduction of technologies like Robotics and IOT, Data Analytics, Data Science, Digital Marketing, etc, we all can find a space within our interest to equip ourselves with the skills needed to fit in.

Written By

Imelda Badoe",https://medium.com/@openlabs/how-technology-took-out-the-headache-of-ghanas-2020-election-8c08f2f96530,['Niit Openlabs Ghana'],2020-12-17 16:27:10.334000+00:00,416,"Digital Transformation, Information Technology, Social Media, Biometric Verification, Robotics IoT"
End to End Case Study (Classification): Lending Club data,"In fact, 63.15% of the values in the overall data are null values. So, it is very important to carefully deal with these null values as they can significantly affect our results.

Null values visual plot:

Handling null values:

Handling null values is an important task here. In the below code, you can see that there are only 53 columns out of 144 columns that have null values less than 40 percent.

Table-1

In the above table, each row represents the number of columns out of 144 columns with less than a specific percentage of null values. For example, Row 1 represents that there are 52 columns with less than 10% of null values in each column.

We were able to decrease the total number of columns from 144 to 53 by considering columns with less than 40% of null values.

Understanding Features

It is important to understand the features/columns as some of the categorical columns present in the data are in the form of numerical values and vice-versa. I first tried to examine every column but later understood that it will be quite cumbersome to perform these operations to all 53 columns. So, I decided to first eliminate columns which doesn’t add value to the data and then analyze each field.

Checking objects:

Dropping unnecessary objects:

Checking numerical columns:

Dropping unnecessary numerical columns:

After examining the data, we have dropped a total of 18 columns of these 53 that didn’t add value to our data. We were able to decrease number of columns from 53 to 35 and we will still try to decrease the no.of columns.

Converting categorical columns to numerical columns:

We have converted categorical columns to numerical by either performing one-hot encoding or label encoding depending on the kind of data they represent. For example, one hot encoding was performed on [‘home_ownership’,’verification_status’,’purpose’] columns whereas label encoding was performed on ‘grade’ and ‘sub grade’ columns as they are ordinal in nature.

One hot encoding:

Label encoding:

Updating the grade column with label encoded values:

Converting DateTime columns to numerical columns:

The columns [‘issue_d’,’last_pymnt_d’,’last_credit_pull_d’] which are datetime columns are further divided into month and year by using pandas datetime module. The new columns are named as ‘issue_d_year’, ‘issue_d_month’, ‘last_pymnt_d_year’, ‘last_pymnt_d_month’, ‘last_credit_pull_d_year’, ‘last_credit_pull_d_month’ respectively.

Converting objects to numerical columns:

The columns int_rate and term are stored as objects. We have performed necessary string operations to convert them into numerical columns.

Checking correlation: Now that we have converted all the columns to numerical columns, we will check for correlation.

Correlation heatmap

There are few columns with high correlation but these columns haven’t been considered while solving our questions. For example, when trying to classify if the loan will be paid back by the customer we will not consider any future transactions like total_pymnt and total_pymnt_inv. Hence, these columns aren’t dropped here.

Dealing with null values:

Let’s check if there are any null values after significantly cleaning columns.

Null values plot

As we can see, we can still find some null values in the data. We will examine these null values and take the necessary actions.

Let’s check the columns with which have the highest percentage of null values.

Percentage of null values

Some columns have a very little percentage of null values(less than 1%). There we can replace the null values with the median of their respective columns.

For columns that have a high percentage of null values, we will run a model on top of non-null values and predict the missing values in that respective column.

As there are no null values, we will go to the next step i.e., building a machine learning model.

Classification

The goal of our classification task is to identify whether a customer(who is requesting a loan) will be able to repay the loan along with the interest amount. Since we have some columns that contain the information of future transactions w.r.t the date on which the loan is taken(like paying monthly loan installments after taking the loan etc.), we will drop them from the pre-processed dataset to carry on classification tasks.

The goal of our classification task is to identify whether a customer(who is requesting a loan) will default based on his historic transactions with the lender after taking the loan.

Let’s drop few columns that contain information on charged-off loans.

Columns dropped in classification:

[‘total_pymnt’,’total_pymnt_inv’,’total_rec_prncp’,’total_rec_int’,’total_rec_late_fee’,’recoveries’]

The ‘loan_status’ column is used as a target variable to classify a customer based on his records.‘loan_status’ column has 4 unique values all of which are label encoded for ease of representation. This column is labeled as below:

Checking multicollinearity between features using VIF and then dropping columns with a value higher than the threshold.

We have dropped the columns which have a high VIF factor(10 or above).

Model building

After label encoding the target variable, we have split the data to train and test data in the ratio of 70:30.

We used sklearn’s cross_val_score and grid search cv with scoring as f1 score to examine the performance of each model in each fold. The below figure shows the F1 score of each model in 3 folds. The orange line represents the mean F1 score of each model whereas IQR represents the variance of these scores.

From the above figure, we can say that the bagging classifier is the most stable model with the highest mean of weighted F1 scores and least variance.

Building the model using bagging classifier.

The classification report for bagging classifier

This model has an accuracy of 0.89 and an average F1 score of 0.75.

Final Confusion matrix:

Confusion Matrix of Bagging Classifier model

As the bagging Classifier doesn’t have an option for feature importance, we used a decision tree to find feature importances.

Feature Importance’s of Decision Tree model

Conclusion:

In this blog, we have extensively covered pre-processing steps required for this data and then found the best fit model using Grid search and KFolds. I hope that this blog has given you an overall picture of solving a classification problem. For more detailed code, please refer to https://github.com/pawanreddy-u/lendingclub9",https://towardsdatascience.com/end-to-end-case-study-classification-lending-club-data-489f8a1b100a,['Pawan Reddy Ulindala'],2021-06-11 12:59:22.615000+00:00,951,"null values, handling null values, understanding features, one-hot encoding, label encoding"
Fairness and Bias in Artificial Intelligence,"Biases in AI

Reporting Bias

Reporting bias occurs when the frequency of events, properties, and/or outcomes captured in a data set does not accurately reflect their real-world frequency. This bias can arise because people tend to focus on documenting circumstances that are unusual or especially memorable, assuming that the ordinary can “go without saying.” @Google

Explaining Reporting Bias by Statistical Entropy

Case Study 1

A mobile camera or digital camera is used as a speed gun detector to detect changes in speed of vehicles. In order to detect speed, the vehicle is detected first and then the amplitude of the image is calculated.

Reference Articles

[Optical Flow Estimation] http://www.cs.toronto.edu/~fleet/research/Papers/flowChapter05.pdf

Case Study 2

A dashcam is fitted to vehicle inside and is used to detect mistakes in lane changing. In order to detect the motion, the vehicle is detected first and then the graph of the images in the video are evaluated.

Reference Articles

[A paper on Perceived Stress Questionnaire referring to the Driver with the Dashcam] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5981243/

1. Problem Explanation: Capture of Frequency of Events

Case Study 1: (Capture of Frequency of Events using threshold of Pixel values)

Vehicle Detection using OpenVINO

Vehicle Detection using Optical Flow Estimation and Bounding Boxes

In the above two example videos, the number of points inside each bounding box is evaluated over a set of frames. The same graph is used to show the Frame 1633 under a chosen frequency.

The challenge lies in converting this graph of information of particle count into usable statistics such as Distance and Speed or even parameters that show direct influence of statistical entropy.

— How to extract Parameters from the Exponential Graph

The example code below extracts positive values from the points inside bounding box rectangle.

def get_lookback_frame(points, lookback_frames):

# shift points by 1 frame

return points.shift(1).rolling(window=lookback_frames).min() def get_long_signal(points, lookback_frame):

# calculate the steadily increasing part of the evaluating graph

long = (points > lookback_frame).astype(np.int64)

return long def filter_signal(points, lookback_signal):

return points * lookback_signal

The example code below finds outliers in Parameters such as Speed:

# Outlier Detection

def get_return_lookback(points, lookback_frame):

# evaluate log returns of lookback and current ones

return np.log(points) - np.log(lookback_frame) def get_signal_returns(signal, lookback_returns):

# multiply the signal to lookback_returns

return signal * lookback_returns

Log Returns of Each Threshold of Points Found from Tracker

The log returns matches with the statistical entropy taken in this problem.

Calculating residual errors from a linear regression fit will provide us with outliers in speed.

To estimate the distance, the log returns is a good enough metric as it steadily decreases.

Case Study 2: (Capture of Frequency of Events using PSQs)

In evaluation of PSQs, same technique is applied. In this case the count is more or less the same across the interval which implies the table must be converted to a PCA like transformation.

Automation Bias

Automation bias is a tendency to favor results generated by automated systems over those generated by non-automated systems, irrespective of the error rates of each. @Google

2. Problem Explanation: Video Inference

Case Study 1 (Speed gun detector)

Vehicle Detection using OpenVINO

Vehicle Detection using Optical Flow Estimation and Bounding Boxes

The above video is demonstrating optical flow detection within the bounding box images as you can see. The detected points are filtered over a threshold which emits data to be sent through an analysis pipeline. The Analyzed data is used to determine the statistical parameters of the video.

So why does Automation Bias exist? Automation Bias exists when a piece of software with single original model can be used to determine the statistical properties which helps in optimization of the video characteristics.

Selection Bias

Selection Bias occurs if a data set’s examples are chosen in a way that is not reflective of their real-world distribution. Selection Bias can take many different forms: @Google

3. Problem Explanation: Evaluating a Graph

Coverage bias: Data is not selected in a representative fashion. @Google

Case Study 1: (Speed Gun Detector)

[Correct]: Collect Order of Appearance of Nodes inside Speed Gun Detector by Confidence Intervals

Collect Order of Appearance of Nodes inside Speed Gun Detector by Confidence Intervals [Wrong]: Do Particle Count from detected boxes over n frames without forming order of appearance of bounding boxes

Do Particle Count from detected boxes over n frames without forming order of appearance of bounding boxes [Correct]: Connect two adjacent frames using Order of Appearance Collected with correct threshold

Connect two adjacent frames using Order of Appearance Collected with correct threshold [Wrong]: Do Particle Count over frames by choosing the wrong threshold of pixels

The particle count inside those bounding boxes are measured from Optical Flow Phase Based Methods.

The data collected does not form a representative fashion but they are transformed into usable statistics.

viz = pd.DataFrame(columns=['vehicle', 'count', 'frame', 'nodes']) frame = 0

for n, t in zip(nodes, threshold):

frame += 1

map_n = list(map(lambda k: str(k), n))

for i, j in zip(n, t):

viz = viz.append({'vehicle': i, 'count': j, 'frame': frame, 'nodes': ""-"".join(map_n)}, ignore_index=True)

Non-response Bias: (or participation bias): Data ends up being unrepresentative due to participation gaps in the data-collection process. @Google

Case Study 1: (Speed Gun Detector)

Participation Gaps can occur in vehicle detection because some bounding boxes may get undetected either due to Deep Learning error which is very unlikely to happen. The other reason is when the vehicles are moving sequentially, the bounding boxes switch the Confidence Intervals or Confidence Threshold implying they change their order of appearance in the detection. This introduces participation bias in detection of a particular vehicle from a set of vehicles.

Code to Explain the Bounding Box Detection:

def draw_boxes(out_write_npy, zone, frame, result, args, width, height):

for box in result[0][0]:

# Output shape is 1x1x100x7

conf = box[2] # comparison of confidence threshold

if conf >= args.pt:

xmin = int(box[3] * width)

ymin = int(box[4] * height)

xmax = int(box[5] * width)

ymax = int(box[6] * height)

# draw bounding box rectangles

cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), args.c, args.th)

return frame

Code to show participation bias of bounding boxes:",https://medium.datadriveninvestor.com/fairness-and-bias-in-artificial-intelligence-c7fbfe880df,['Aswin Vijayakumar'],2021-03-09 07:49:40.222000+00:00,920,"def draw_boxes(out_write_npy, zone, frame, result, args): check if the confidence threshold is met or not.for box in result[0][0]:conf = box[2]  comparison of confidence threshold"
Learn Applied Data Science and Get Certified with Microsoft and Udacity,"Photo by @jeshoots from unsplash

Unlike theoretical data science, applied data science involves additional steps to manage the lifetime of a model, which are commonly called MLOps. Azure Machine Learning is a service that conveniently supports MLOps practices, so getting to know it seems like a good idea. In this post, I will talk about the best way to learn (Udacity), and demonstrate your knowledge (Certification).

Microsoft and Udacity have just announced the availability of a new Nanodegree program — Machine Learning Engineer with Microsoft Azure. This program will prepare you for passing the certification, and also provide a great opportunity to work on your own capstone project.

You may not know, but Microsoft has Data Science Certification! To get the Microsoft Azure Data Science Associate certification, you just need to pass one exam: DP-100: Designing and Implementing a Data Science Solution on Azure.

To set the expectations correctly: this exam is not so much about Data Science concepts, but rather about being able to use Azure Machine Learning service. However, when preparing for the exam, you will learn all important practices of MLOps, and also to be able to use that service effectively:

Different options for storing data inside Azure ML: Datastore vs. Dataset

Using Automated Machine Learning to train models without coding (see my earlier post on Low-Code/No-Code Machine Learning Experience)

Scheduling experiments and tracking their results (my post)

Doing Hyperparameter optimization (my post)

Setting up and using pipelines

Deploying the model on production compute

There are a few ways to learn Azure ML and prepare for the exam:

Use my blog / github as a gentle introduction. I have made a short tutorial on GitHub that will walk you through the first steps with Azure ML. The experience is also described in blog post series. I also suggest you to watch my introductory video on Azure ML, and video with some Case Studies. This material is good for a start, but is by no means enough to pass the exam.

Take some free courses on Microsoft Learn. MS Learn contains a number of great courses/paths to get you introduced both to ML Concepts, and to Azure ML:

Create machine learning models learning path is a good introduction to machine learning as such. You will be using Azure ML as compute environment, but you will learn about regression, classification and deep learning. Create no-code predictive models with Azure Machine Learning would introduce you to AutoML and Designer, and you would learn how to train classification/regression/clustering models witout coding. Build AI solutions with Azure Machine Learning is the most comprehensive course on Azure ML itself, where you will learn about experiments, pipelines, and deploying models to production.

Enroll into new NanoDegree Program with Udacity. It is designed for three months. In terms of coverage, it pretty much covers the same content as previously listed free courses, but the format is quite different.

Udacity Nanodegree Program: Machine Learning Engineer

Unlike MS Learn, taking a course at Udacity will cost you some money. However, it has some important benefits.

Udacity program is structured in such a way that upon successful completion you are guaranteed to have good knowledge of the subject, because you will not only learn the content, but also get hands-on experience working on a project. That is the reason they are ready to give certificate of completion that proves your knowledge.

During the program, you will work on three different projects. In final, capstone project, you will work on a dataset of your choice (most probably, the one from Kaggle), and perform the complete lifecycle, from importing data to getting the working model up and running in production. You will get Azure рlayground as part of the program, which means that you would be able to experiment, and you would have cloud resources to complete the projects. If you are taking free MS Learn courses, you will need to sign up for free Azure trial yourself. A lot of material in the course is video and not text. Many people find it easier to learn something new if they can hear other people. Finally, during the course you will have support from instructors, as well as the community of peer learners.

The nanodegree program consists of three courses:

Azure Machine Learning Platform, which takes you through training and optimizing models using AutoML and Hyperdrive / Azure SDK. At the end of this course you will train and optimize a logistic regression model to make marketing predictions. Machine Learning Operations course teaches you how to deploy and operationalize your models, as well as how to consume and document them. In this course you will further improve your prediction model, and deploy it to production. Final part of the program is Capstone Project, where you can select any dataset, and build complete ML Ops pipeline around it.

While you can definitely learn Azure ML using Microsoft Learn resources, Udacity will help you organize your learning process, and immediately apply the knowledge you get in practice. The course is well aligned with Microsoft Azure Data Science Associate certification, so in addition to a certificate from Udacity, you can immediately look to get an industrial certification from Microsoft.

Originally published at http://soshnikov.com.",https://medium.com/microsoftazure/learn-applied-data-science-and-get-certified-with-microsoft-and-udacity-5240d9a2a6e0,['Dmitry Soshnikov'],2020-10-13 15:16:30.620000+00:00,847,"Microsoft, Azure, Udacity, Certification, MLOps"
Statistics: A great way to lie!,"The current global pandemic has made some things clear about the whole world: we are a collection of armchair warriors, who, armed with Wikipedia and the Internet, think we know everything. Everyone in 2020 is an epidemiologist and/or a statistician.

The speed with which most countries adapted radical (and at times unwarranted) policies was impressive. I mean, who, in 2019, could have predicted a global lockdown in Spring 2020? If anything, many of us might have thought such a thing would be impossible because we are technologically capable of dealing with these threats in this age (remember the 2014 Ebola scare?).

But this unprecedented situation also means we are desperate for some understanding of what is going on and what we should do. So once there is some sort of consensus (lockdown, in this case), there appears to be low tolerance for deviance from the party line.

Sweden is a good case in point: it is (or was) taking heat for not imposing a total lockdown and people were citing all kinds of number as evidence of the Swedish state’s failure (lockdown in most cases is a political, not a scientific, decision).

Every time I see these convincing-appearing sensationalist pieces, I must keep reminding myself of a famous quote: “There are three kinds of lies: lies, damned lies, and statistics.”

I want to highlight the importance of understanding statistics for what they are by using two examples.

First is a good personal example of a very misleading claim. I was by far the most popular kid in my class in high school! In the school student council elections, I got voted in by 70% of my classmates.

Or do you remember this campaign by Colgate: 80% of dentists recommend Colgate!

What Colgate thinks its toothpaste can do.

Are these claims verifiably true? Yes!

Is it the whole truth or even relevant truth? Nope.

Context is KING!

Let me break down both above stories and put them in context.

Starting with the personal one, there was a bit of manipulation involved on my part. Each class of the IB section of school got to send two representatives to the student council for 2 years. That year, I pushed for giving each student two votes so we could have a say in both selections and even rank our choices but the top two overall would win.

Come election day, I received votes from 13 classmates or 68%. That is a fact. But can that fact justify the claim that I was the most popular?

I only got 3 first-choice votes (16%) and 10 second-choice votes (53%). The other winner received 6 first-choice votes (32%) and only one second-choice vote (5%). I think it would be fair to say he was more popular but, statistically speaking, my 70% beat his 37%. There were yet others who had more first-choice votes than I did.

Then there is the fact that the sum is not 100% which makes the 68% far less important or impressive. 70% of my classmates does not equal 70% of the votes. There were 36 votes cast of which I only received 13 (36%).

This brings me to Colgate and their statistical lie. 80% of dental practitioners did recommend Colgate. BUT they also simultaneously recommended other brands, meaning here too, the sum is not 100%. They phoned some dentists and asked them to recommend some toothpaste brands. You can see how it went from the table below.

It is clear that 4/5 dentist recommend Colgate. But 4/5 also recommend Brand A toothpaste.

Their poster implied that 80% prefer Colgate exclusively which is absolutely not true. They tried to argue in court that they were factually correct and never intended to imply that. But it was concluded that it counted as manipulation as that was in fact the intuitive interpretation made by most people (things usually sum up to 100%) and which was also the impression Colgate presumably wanted to give to the public.

How impressive would a billboard with the slogan “80% of dentists think Colgate is also OK” be?

The adjusted Colgate campaign poster.

To sum it up

Even though we know of these biases and fallacies thanks to extensive research, it still takes some mental training to always be contextualising these types of numbers. I work with data professionally, I have studied behavioural economics and psychology, and yet I cannot stop myself from thinking “80% of dentists recommend Colgate” every time I’m in front of the toothpaste shelf in the supermarket.

A major problem is that these statistics are not only misleading on their own, either. They contribute to other decision-making biases and errors, such as the availability heuristic, which eventually impact our outlook on life, how we make both silly and important decisions, and affect what we hold to be truths.

In a follow up, I will summarize the most common statistical fallacies so keep a look out. And thanks for reading!",https://medium.com/djama/statistics-a-great-way-to-lie-a711f69b80ed,['A. Jama'],2020-06-27 11:35:05.843000+00:00,799,"Statistics, Misleading Statistics, Fallacy, Manipulation, Availability Heuristic"
Design better data tables,"Why Tables Matter

Data is becoming the raw material of the global economy. The pursuit of data drives the reinvention of antiquated industries. Energy, media, manufacturing, logistics, healthcare, retail, finance, and even the government are undergoing a digital transformation.

However, data is meaningless without the ability to visualize and act upon it. The companies that survive the next decade will not only have superior data; they will have a superior user experience.

Good user interface design is based on human goals and behavior. The user interface in-turn affects behavior, which drives further design decisions. In subtle and unconscious ways, user experience alters how humans make decisions. What is seen, where it is presented, and how interactions are afforded, influence actions. It is important we make design decisions that lead to a better world, one data table design at a time.",https://medium.com/nextux/design-better-data-tables-4ecc99d23356,['Andrew Coyle'],2020-07-29 04:57:41.723000+00:00,137,"Data Analytics, UXDesign, Data Visualization, Data Tables, User Experience"
Sparsely Embedded Convolutional Detection,"Object detection based on LiDAR or RGB-D has been widely used, such as autonomous driving and machine vision. The 3D convolutional network based on voxel division has been around for a period of time, and it has enhanced the more complete retention of information when processing point cloud LiDAR data. However, some problems still exist, including slow inference speed and low performance of orientation estimation. Therefore, [1] studies an improved sparse convolution method for this type of network, which significantly improves the speed of training and inference.[1] also introduced a new form of angle loss regression to improve the performance of orientation estimation, and a new data enhancement method that can improve the convergence speed and performance. The proposed network has the most advanced performance on the KITTI 3D object detection benchmark test, while maintaining a faster inference speed.

Main contribution of this article :

The application of sparse convolution in LiDAR-based target detection greatly improves the speed of training and inference.

A method to improve sparse convolution is proposed to make it run faster.

A new angle loss regression method is proposed, which has better performance than other methods.

New data enhancement methods are introduced for learning problems based only on LiDAR, which greatly improves the convergence speed and performance.

The architecture of the proposed SECOND detector is depicted in Figure below,

The structure of proposed SECOND detector. The detector takes a raw point cloud as input, converts it to voxel features and coordinates, and applies two VFE (voxel feature encoding) layers and a linear layer. Then, a sparse CNN is applied. Finally, an RPN generates the detection.Source[1]

The proposed SECOND detector, consists of three components:

A voxelwise feature extractor

A sparse convolutional middle layer

And an RPN.

Voxel feature extraction: [1] performs a similar operation to VoxelNet , that is, the original point cloud is first meshed with voxels, and then the VFE voxel feature extraction network is used to extract the features of each voxel.

The structure of the VFE layer is shown in Figure above.

Voxel feature encoding layer.

First preallocate buffers based on the specified limit on the number of voxels; then, iterate over the point cloud and assign the points to their associated voxels, and save the voxel coordinates and the number of points per voxel. The iterative process will stop once the number of voxels reaches the specified limit.

A VFE layer takes all points in the same voxel as input and uses a fully connected network (FCN) consisting of a linear layer, a batch normalization (BatchNorm) layer and a rectified linear unit (ReLU) layer to extract pointwise features. Then, it uses elementwise max pooling to obtain the locally aggregated features for each voxel. Finally, it tiles the obtained features and concatenates these tiled features and the pointwise features together.

Sparse Convolutional Middle Extractor:In the previous step, after dividing voxels from the point cloud collected by lidar, about 5k ~ 8k voxels and a sparsity of about 0.005 will be generated. Directly using 3D convolution will consume huge computing time and memory, and this is can be avoided by using sparse convolution. [1] uses a convolution structure called submanifold convolution to limit the sparsity of the output through the sparsity of the input data, thereby greatly reducing the amount of calculation for subsequent convolution operations.

The structure of proposed sparse middle feature extractor. The yellow boxes represent sparse convolution, the white boxes represent submanifold convolution, and the red box represents the sparse-to-dense layer. The upper part of the figure shows the spatial dimensions of the sparse data.Source[1]

As shown in Figure above, the sparse convolution feature extraction network used in this article includes a sparse convolution layer (indicated by yellow), a submanifold convolution (white) and a sparse to dense conversion layer (red).

Regional Proposal Network (RPN):[1] uses an RPN similar to the SSD architecture, and the input is a feature map. RPN consists of three stages, each stage contains a down-sampled convolutional layer and a series of convolutional layers; then the feature map of each stage is up-sampled and concatenated to form a feature map, and finally three 1x1 convolution are used for category prediction, position regression and angle regression.

In addition, [1] proposed a data enhancement method to accelerate the convergence of training. The ground truths of the training set are sampled to form a database. During the training process, several samples in the database are randomly selected and introduced into the current point cloud. In order to avoid conflicts, collision detection is also required. The author also introduced random angle noise to the sample, and finally rotated and zoomed the global point cloud in a small range.

The proposed network produces state-of-the-art results on the KITTI 3D object detection benchmarks while maintaining a fast inference speed.

References

1.Yan Yan , Yuxing Mao and Bo Li ,SECOND: Sparsely Embedded Convolutional Detection.",https://medium.com/swlh/sparsely-embedded-convolutional-detection-3ef1db0c0e92,['Nabil Madali'],2020-11-10 08:46:14.268000+00:00,785,"LiDAR, RGB-D, Autonomous Driving, Machine Vision, 3D Convolutional Network"
Deriving the Backpropagation Equations from Scratch (Part 2),"Deriving the Backpropagation Equations from Scratch (Part 2)

In this short series of two posts, we will derive from scratch the three famous backpropagation equations for fully-connected (dense) layers:

In the last post we have developed an intuition about backpropagation and have introduced the extended chain rule. In this post we will apply the chain rule to derive the equations above.

Backpropagating the Error

Backpropagation starts in the last layer 𝐿 and successively moves back one layer at a time. For each visited layer it computes the so called error:

Now assume we have arrived at layer 𝑙. In the last post we have illustrated, how the loss function 𝓛 depends on the weighted inputs 𝑧 of layer 𝑙:

We can consider the above expression as our “outer function”. We get our corresponding “inner functions” by using the fact that the weighted inputs 𝑧 depend on the outputs 𝑎 of the previous layer:

which is obvious from the forward propagation equation:

Inserting the “inner functions” into the “outer function” gives us the following nested function:

Please note, that the nested function now depends on the outputs 𝑎 of the previous layer 𝑙 -1. Next, we take the partial derivative using the chain rule discussed in the last post:

Resulting in:

The first term in the sum is the error of layer 𝑙, a quantity which was already computed in the last step of backpropagation. The second term is also easily evaluated:

We arrive at the following intermediate formula:

where we dropped all arguments of 𝓛 and 𝑧 for the sake of clarity. Expressing the formula in matrix form for all values of 𝑖 gives us:

which can compactly be expressed in matrix form:

with

Up to now, we have backpropagated the error of layer 𝑙 through the bias-vector and the weights-matrix and have arrived at the output of layer 𝑙 -1. To obtain the error of layer 𝑙 -1, next we have to backpropagate through the activation function of layer 𝑙 -1, as depicted in the figure below:

(Image by author)

In the last step we have seen, how the loss function depends on the outputs 𝑎 of layer 𝑙 -1. Our new “outer function” hence is:

Our new “inner functions” are defined by the following relationship:

where 𝑔 is the activation function. Plugging the “inner functions” into the “outer function” yields:

Applying the chain rule we get:

The first term in the above sum is exactly the expression we’ve calculated in the previous step, see equation (𝑰). Since the activation function 𝑔 takes as input only a single 𝑧 , we get:

Thus we arrive at the final formula:

where again we dropped all arguments of 𝓛 for the sake of clarity. Expressing the formula in matrix form for all values of 𝑖 gives us:

In vector notation we can write:

where * denotes the elementwise multiplication and

And finally by plugging equation (𝑰) into (𝑰𝑰), we arrive at our first formula:

Gradient of the Weights

To define our “outer function”, we start again in layer 𝑙 and consider the loss function to be a function of the weighted inputs 𝑧:

To define our “inner functions”, we take again a look at the forward propagation equation:

and notice, that 𝑧 is a function of the elements of weight matrix 𝑾:

The resulting nested function depends on the elements of 𝑾:

We apply the chain rule:

As before the first term in the above expression is the error of layer 𝑙 and the second term can be evaluated to be:

as we will quickly show. To this end, we first notice that each weighted input 𝑧 depends only on a single row of the weight matrix 𝑾:

Hence, taking the derivative with respect to coefficients from other rows, must yield zero:

In contrast, when we take the derivative with respect to elements of the same row, we get:

Thus we arrive at the final formula:

Expressing the formula in matrix form for all values of 𝑖 and 𝑗 gives us:

and can compactly be expressed as the following familiar outer product:

with

Gradient of the Biases

All steps to derive the gradient of the biases are identical to these in the last section, except that 𝑧 is considered a function of the elements of the bias vector 𝒃:

This leads us to the following nested function, whose derivative is obtained using the chain rule:

Exploiting the fact that each weighted input 𝑧 depends only on a single entry of the bias vector:

gives us:

which finally leads to:

In vector notation we can write:

with

This concludes the derivation of all three backpropagation equations.",https://towardsdatascience.com/deriving-the-backpropagation-equations-from-scratch-part-2-693d4162e779,['Thomas Kurbiel'],2020-12-20 07:34:49.963000+00:00,724,"Backpropagation, Gradient Descent, Neural Networks, Chain Rule, Error Propagation"
Unsupervised Representation Learning on Distributed Regions in the Human Brain (Part-IV),"Unsupervised & Manifold Learning in Human Brain

Functional MRI data are very high-dimensional if one considers all the voxels or surface coordinates acquired with standard imaging parameters. As in our dataset, with the structure of 4D time-series image data, we have a curve of dimensionality problem. Hence, dimension reduction and manifold learning algorithms can reduce the dimensionality of fMRI space by preserving geodesic relations in the lower space representations. We performed PCA, LDA, ICA, NNMF, and MDS as dimension reduction algorithms. Besides, t- SNE, UMAP, ISOMAP, LLE, and Spectral Embedding are performed to generate lower-dimensional manifolds of the fMRI space. Let’s start discovering the intersection of unsupervised learning and the human brain. I performed many unsupervised learning algorithms as it further helps to understand geodesic information underlying the human brain and gives prior information on whether the neural activities in distributed regions are decodable or not.

Let’s install and import all necessary packages. Please refer to previous articles (part-I) for the dataset understanding. Note that this article is not for answering the following questions (which are previously answered)

How fMRI data is structured? (part-I)

Why spatio-temporal masking is performed on the human brain? (Part-II)

Here are the pip commands for installing everything we need.

We installed everything (even more) we need. Let’s import them as follows.

We are ready to go! Let’s fetch the Haxby dataset.

Next thing is to prepare, mask, standardize and convert fMRI data to NumPy matrix form as follows.

Preprocessing part is done. So, we can move to the actual processes we want to perform. In this code snippet, “masks” are the masked regions in the human brain that we want to perform cognitive tasks on it and categories are just their labels. We do not use whole voxels for representation learning purposes. Our main idea is to extract and visualize the latent variables in the human brain where the distributed and overlapping patterns of neural activity are happening.

Finally, we can start the actual business as follows. In the following figures, different colors represent different categories. (i.e., blues belong to class 1, oranges belong to class 2 etc.)

To properly visualize the latent space, I utilized the plotly python package. The following code performs 2D and 3D visualization in an interactive fashion.

Dimension Reduction: PCA

PCA is a linear unsupervised dimension reduction algorithm and it computes principal vectors to change the basis of the representation [26]. PCA is a used algorithm in a broad range of topics from image compression to decorrelation of texts. Here, we performed PCA on RoI’s of subject 5 and visualized it as follows.",https://towardsdatascience.com/unsupervised-representation-learning-on-distributed-regions-in-the-human-brain-part-iv-55fecf4e1b6f,['Can Kocagil'],2021-07-02 18:24:59.867000+00:00,420,"unsupervised learning, manifold learning, functional MRI data, dimension reduction algorithms, PCA"
How Many French Fries Do You Have to Eat to Finish the Tour de France?,"How Many French Fries Do You Have to Eat to Finish the Tour de France?

Find Out with Help from Wolfram|Alpha

Photo by Rob Wingate on Unsplash

I like the Tour de France. No, actually: I love the Tour de France. Growing up in the Netherlands, each year for three weeks this would be the main event on the Dutch television. We only had two channels at that time so, yes, there wasn’t much else on. But still. The “NOS”, the Dutch public television organization, would broadcast it daily for the last hour or two before the stage finish.

The Tour de France is a multi-day bicycle race which ends in Paris on the Champs-Elysee, a very big street in the center of the city. It typically starts in different places each year and sometimes even starts in nearby countries. The most challenging and popular stages are typically in the mountainous regions of the Alps and the Pyrenees.

Photo by Ian on Unsplash

The length of the Tour de France varies each year but is typically over 2,000 miles. Or, according to Wolfram|Alpha, over 3,219 kilometers. Or 0.29 the length of the Moon’s equator. Or 0.82 the distance from New York to Los Angeles.

Biking such a long distance requires energy. A lot of energy. I was curious about how much and compare the energy to an amount of known food that I like. I like to eat a lot of things but one of my favorite staple foods is potatoes. Boiled, mashed, or in the form of french fries, it is all good. So, since we are talking about France, let’s use french fries. And yes, I should note here that french fries were most likely invented in Belgium and are also commonly known as pommes frites.

Photo by Gilly on Unsplash

The amount of calories (1 “dietary calory” is the same as 1 kcal) burned by a cyclist in the Tour de France varies by stage, but it is usually estimated to be between 5,000 calories on an easy day to 7,000 calories on a hard day. According to Wolfram|Alpha, biking for one hour at 25 miles per hour burns about 2,000 calories.

So let take an “average day” of 6,000 calories. This year’s Tour de France lasts 23 days, but two of them are rest days, so 21 biking days. This gives us a whopping 126,000 calories.

Now let’s get the calories in a serving of french fries. There are a lot of different types of french fries, so I will use “restaurant-style” fries:

(image by author)

Dividing the total 126,000 calories by the calories for a single serving size gives us the number of servings, about 256. That’s quite a lot! Here is a picture showing 256 servings of french fries in a grid of 16x16:

(image by author based on photo by Gilly on Unsplash)

The actual diet of a Tour de France competitor is likely to be a lot more varied and healthier, but it shows the massive quantities of calories that are needed to finish in Paris. And now I’m hungry. 🙂",https://towardsdatascience.com/how-many-french-fries-do-you-have-to-eat-to-finish-the-tour-de-france-d8450ed0a2e3,['Arnoud Buzing'],2020-09-12 20:39:28.443000+00:00,504,"Tour de France, French Fries, Cycling, Calories, Nutrition"
ANALYTICS: THE CATALYST FOR CHANGE IN HIGHER EDUCATION,"The idea of “change” and “higher ed” are not often synonymous; however, institutions throughout the world are facing an unprecedented era of disruption where innovating is not only an option, it’s a requirement.

To navigate this era of disruption, institutions must rethink their approach to data, people, and processes.

With Analytic Process Automation (APA), you can automate processes, enhance analysis, embed intelligent decisioning, and empower your institution to deliver actionable insights that improve your admission marketing yield, accelerate student retention, optimize net tuition revenue, and scale your mandatory reporting processes. The following institutions show you how.

According to a report by the EDUCAUSE Center for Applied Research, 69% of post-secondary schools said analytics was a priority for “at least some departments, units or programs.”

ESTABLISHING A CULTURE OF ANALYTICS

An often-asked question from institutions is if we already need an established community driven by data for analytics to be successful. The answer is no, and don’t let that stop you from moving forward. Rather, let that be your north star as you scale access and resources to all and truly empower the end user.

According to EDUCAUSE, “... initiating an analytics program before a data philosophy of data-driven decision-making is ensconced may help establish that culture.”

Take the Hong Kong Polytechnic University (PolyU). Their Institutional Research and Planning Office (IRPO) consolidates large volumes of data from various internal and external sources and provides analytics for senior management to better understand how the university is performing against key measures to help drive strategic decisions about how to effectively grow the institution.

By partnering with Alteryx, PolyU was able to get data into the hands of more users, resulting in less time spent on manual reporting processes and more time dedicated to exploring new solutions by more people.

“Long gone are the days where we must keep tens of Excel files opened and maneuver around endless pivots, lookups, and VBA scripts. The versatility and variety of tools that Alteryx offers has enabled us to focus on exploring and experimenting with our data. It has single-handedly transformed the data analytics culture in our team. We feel both empowered and humbled. Alteryx-ing data has now become our second nature!""

—Anson Wun, Senior Institutional Research Analyst, Hong Kong Polytechnic University

CODE FREE. CODE FRIENDLY.

APA platforms are human-centered, designed for citizen access rather than requiring the skills of highly-trained specialists or depending on IT.

GET OUT OF SPREADSHEET HELL

Declining enrollments, increased competition, new ways of learning, greater demand to prove value, and reduced funding are forcing colleges and universities to disrupt the status quo.

This particularly holds true for admissions and recruitment departments across the world who are tasked with attracting and converting the best and the brightest at breakneck speed.

For the admissions department at Indiana University Online, the analysis of a prospective applicant to a submitted application would take many hours using disparate, manual-based processes. In today’s competitive environment, they needed to spend more time focused on what really mattered — learner recruitment — and less time in spreadsheet hell.

As a result of its partnership with Alteryx, the analysis of the applicant-to-application process went from hours to minutes, enabling the university to hone their recruitment targeting and improve marketing yield.

“I have peace of mind knowing that some simple, yet critical tasks can be done easily. With Alteryx, we also don’t have to learn (and relearn) how to manipulate data within multiple platforms. Leveraging our Alteryx expertise, we can pull Student Information System, Salesforce CRM, Canvas LMS, Google Analytics, or other vendor-related data into one place and analyze it using one tool. We continue to find new ways to use Alteryx to automate tasks and make us more productive.”

— Sharon Wavle, Associate Director, Decision Support & Reporting, Indiana University Online

INSIGHTS AT THE SPEED OF NOW

At Deakin University, understanding the student and prospect experience is critical to addressing any barriers in real time. While the university has multiple sources of data, without the ability to bring it together, it was nearly impossible to create a unified picture in a timely manner (we’re talking more than a week to prepare the data and follow-up) for actionable insights.

With Alteryx, Deakin was able to automate their analysis process, including pulling data from multiple systems, cleansing it, formatting it, and outputting it to their CRM system for a follow-up push into the CRM. Not only did it reduce the time to contact customers from a week to 24 hours, it also changed the way the team worked. Instead of a campaign-style marketing approach, the university developed an always-on marketing approach where they realized a 21% marketing conversion-rate improvement.

“It took substantially less time — I think it was roughly 25 days less time — to develop the solution in Alteryx compared to the old way of working.”

—Chris Logie, Manager of Marketing Analytics, Deakin University

The ability for institutions to thrive in a strong economy and stay resilient in a rapidly-changing environment is defined by their ability to improve revenue and the bottom line, optimize process efficiency, and level up the entire workforce to harness data and analytics as a strategic asset that drives business outcomes. Analytic Process Automation transforms your business outcomes and workforce by enabling anyone in your organization to easily share data, automate analytic processes, and turn insights into results.

Originally published on alteryx.com/input",https://medium.com/input-by-alteryx/analytics-the-catalyst-for-change-in-higher-education-65f1a441d3e1,['Melissa Erbes'],2020-10-14 20:43:39.669000+00:00,869,"higher ed, analytics, data-driven decision-making, Analytic Process Automation (APA), automation"
Will the Sun Rise Tomorrow? Introduction to Bayesian Statistics for Machine Learning,"Bayes’ Rule

Bayes’ rule tells us that we have to start with some inherent probability about how likely an event is to happen (before the fact). We call this a prior probability. Progressively, as we are presented with new observations and evidence, we update our belief based on looking at the evidence and deciding how likely our current stance is. This updated belief is called the posterior probability (after the fact).

Going back to our Sunrise problem, every day we observe that the Sun rises, and every time it happens we are a little more sure that it will rise again the next day. However, if one day we find that the Sun does not rise, this will drastically affect our posterior probability based on the new evidence.

This is expressed mathematically in the following form, which looks daunting at first but can be abstracted: our updated belief is based on our initial belief and new evidence presented to us based on our current belief (the likelihood). The likelihood says the new evidence that I have, how likely is it that my belief is correct? If I believe that the probability of the Sun not rising tomorrow is a million to one, and then it happens, the likelihood that my belief (my model) is wrong is very high, and the posterior probability will be updated to predict that it is more likely it will happen again.

Bayes’ theorem.

This is a pretty nifty idea, and it is present in many different places, especially when it comes to humans and their beliefs. For example, let’s say your friend messages you telling you that one of your favorite celebrities has died. Initially, you might be upset and also slightly skeptical. As you go about your day, you read the newspaper and it tells you that the celebrity died, and this belief will then be enforced further. Perhaps you then see interviews on the television of their mourning family on the news, and your belief will be enforced even further. However, if you instead see the person being interviewed on television about a rumor being spread that they had died, your belief that what your friend told you would be lowered.

This is an essential aspect of science, theories are proven through experiments and simulations, and the more people who do these experiments and verify the theories gradually make these theories more robust and believable. Whereas, for example, someone who is religious may decide that they do not need empirical evidence (of the same kind at least) to believe in something, and we call this faith.

It is interesting how something so pervasive in our everyday lives can be so fundamental to statistics and machine learning, but it is, and we will discuss why. First, however, we need to look at some problems that occur with Bayes’ theorem for very low probabilities.",https://towardsdatascience.com/will-the-sun-rise-tomorrow-introduction-to-bayesian-statistics-for-machine-learning-6324dfceac2e,"['Matthew Stewart', 'Phd Researcher']",2020-07-29 23:05:30.487000+00:00,472,"Bayes’ Rule, Prior Probability, Posterior Probability, Likelihood, Machine Learning"
Analyzing the Meteorological data,"The dataset has the data for past 10years from 2006–04–01 00:00:00.000 +0200 to 2016–09–09 23:00:00.000 +0200. It corresponds to Finland, a country in Northern Europe. If you want to try using the dataset go to the below drive link and download: https://drive.google.com/open?id=1ScF_1a-bkHi1qe8Rn78uxK6_5QwUD9B .

We need to find the average temperature for the month starting from April 2006 to 2016 and average humidity for the same period.

STEPS INVOLVED:

STEP 1: Import the dataset

STEP 2: Print the Data

STEP 3: Check the type of the data

STEP 4: Cleaning the data

STEP 5: Check Whether the dataset has any invalid data

STEP 6: Finally Visualize the Data

Let’s start the above mentioned steps

IMPORT THE DATA

You can download the dataset from the given link https://drive.google.com/open?id=1ScF_1a-bkHi1qe8Rn78uxK6_5QwUD9B .

Before importing the data first import the libraries and then import the data. Here we use pandas library for creating the ‘DATAFRAME’

PRINT THE DATA:

To print the data you can use . head() method , this method print 5 random data from the dataset by default if you want to print the first five data you have mention .head(5) or else how many data you want to print. Likewise you can use .tail() to print the data from last.

PRINT THE DATA TYPES AND OTHER DATASET RELATED THINGS:

Column method is used to find the name of the columns in the dataset.

Shape method helps us to find how many rows and columns are availabe in the dataset.

dtype method is used to find the data type.

CLEANING THE DATA:

use drop() method to remove the unwanted columns

CHECK THE DATASET HAS THE PROPER FORMAT OR NOT:

Here I thought the format of the date is not proper so we need to create a date and time object by using the to_datetime() function. Then set the name of the index using set index .

Now since we have been given hourly data, we need to resample it monthly.

VISUALIZING THE DATA

Here I’m going to visualize the data from January to December(2006–2016).

JANUARY

FEBRUARY

MARCH

APRIL

MAY

JUNE

JULY

AUGUST

SEPTEMBER

OCTOBER

NOVEMBER

DECEMBER

INSIGHTS FROM THE ABOVE VISUALIZATION:

From September to March Large difference in Apparent Temperature but no changes in Humidity and in April to August minor changes in temperature but here also no difference in humidity. exactly, November, December, January high changes than other months.

CONCLUSION

In this blog we learned to analyze and visualize using the meteorological dataset in Matplotlib . I hope it will be useful for you . If you find any Mistakes please let me know . We discuss about the seaborn and other visualization process in another blog, Thank you. Have a Nice day.",https://sridhanya79.medium.com/analyzing-the-meteorological-data-c9fee72dad78,['Swetha Sreenivasan'],2020-12-01 11:07:19.201000+00:00,409,"Data Analysis, Data Visualization, Meteorological Dataset, Matplotlib, Pandas Library"
How To Stay Relevant As A Machine Learning Engineer In 2021,"Have A Consistent Learning Routine

Success isn’t always about greatness. It’s about consistency. Consistent hard work leads to success. Greatness will come — Dwayne Johnson

One of the main lessons I learned in 2020 is that nothing beats consistency, not talent and not even luck.

Writing over 150+ Medium articles in 2020, and publishing at least 3 AI/ML/DS articles a week over the year made me realise that if you stay consistent, it is possible to accumulate invaluable experience that produces results that can’t be replicated through luck.

In past articles, I’ve alluded to some of the reasons I write on Medium and also the benefits realised as a Machine Learning Practitioner. Through the creation of a regular writing and learning routine, I’ve been able to deliver content on the Medium platform continuously.

This brings us to the first strategy I’ll be incorporating in 2021 to stay relevant within the ML industry. My plan for staying relevant is to transfer the notion of consistency and regime to my accumulation of ML related knowledge.

As a Machine Learning professional, you come across novel algorithms, models and techniques every day.

Most times, to leverage these new algorithms and techniques, you are forced to learn how they operate and function. So, you could argue that ML practitioners are continually learning. However, in 2021, I am making it a habit.

One notable method which I’m using to educate myself and enrich my knowledge in machine learning is to set a measurable goal.

The goal is to read and understand at least 30 deep learning research papers in 2021.

For me, this means reading and understanding the content of a new research paper every two weeks. My approach is to spend at least an hour a day on a research paper. I’m not sure if reading 30 research paper is considered a lot or not, especially for someone more concerned with the engineering of deep learning models as opposed to the study or research.

What I’m sure of is the papers I aim to read are centred around deep learning solutions to computer vision tasks, such as Object Detection, Pose Estimation, Semantic segmentation and more.

Staying Relevant

How will reading research papers help me stay relevant within the ML industry?

In 2020 most of us found ourselves with more time in our hands, typically as a result of the lockdown measures imposed in major cities around the globe. With my time, I explored some of the earliest major convolutional neural networks released by pioneers within the deep learning field.

My exploration of these CNN architectures involved reading research papers, understanding the algorithms and techniques presented and in some cases implementing the CNN architectures using TensorFlow.

Some of the architectures I explored were AlexNet, LeNet, GoogLeNet, PoseNet etc. Transfer learning within machine learning removes the complexity involved in implementing, training and developing conventional CNN architectures.

To gain a deeper understanding of deep learning network architectures, it’s beneficial to visit original research papers and make an attempt to understand the reasoning behind techniques.

Going as far as to implement algorithms and network architecture from scratch will provide any ML practitioner with a more profound understanding of the deep learning domain.

ML practitioners that can read research papers and extract the necessary information to develop an algorithm or neural network architecture are highly sought after across the ML industry.

And this isn't going to change in 2021.",https://towardsdatascience.com/how-to-stay-relevant-as-a-machine-learning-engineer-in-2021-41b5feaa4771,['Richmond Alake'],2020-12-10 01:04:43.814000+00:00,549,"Consistent Learning Routine, Machine Learning, Deep Learning, Research Papers, Transfer Learning"
The future of in-person retail: Walmart and Target vs. Amazon,"Before diving deeper into this matter, I wanted to just make a pretty important note:the fact that we are only looking at two extremely large in-person retailers may make the trends misleading. This is because bigger companies tend to perform well even in times of hardship. On the other hand, smaller retailers may struggle generating revenue during times of economic downfall and rising competition.

In-person retail

Let’s first look at in-person retail stores like Target and Walmart. We know that these are both multi-billion dollar companies, but does this mean that they are both still growing? Or are they falling? Let’s analyze their revenues to find out!

Walmart

As can be seen, Walmart’s annual revenue has increased over 50 billion since 2012. This shows that the company has indeed shown growth over the last seven years. So does that mean retail stores aren’t really declining?

Target

Unlike Walmart, Target has had its hiccups in the same seven year time period. Although we do see a spike near the end, it is actually not as drastic as it looks. Over the course of the last seven years, Target’s annual revenue has been relatively constant (between 70 billion and 80 billion dollars each year).

Online retail

The primary example of an online retail store that we all know is Amazon. So let’s quickly see how Amazon’s profits compare to those of Walmart.

This growth is staggering. From nearly 50 billion dollars in revenue in 2012, Amazon’s revenue has shot up to nearly 300 billion dollars in 2019. This growth is nearly five times greater than that of Walmart. However, there is still something that we need to look at. Even though this growth is tremendous, they are still yet to exceed Walmart in their annual revenue.

When you look at it like this, Amazon seems like it has a long way to go to catch up to Walmart. But analyzing the rate at which Amazon’s revenue has grown year after year, we know that this might happen sooner than we expect.",https://medium.com/dev-genius/the-future-of-in-person-retail-walmart-and-target-vs-amazon-25c02a20bd6d,['Pavan Pandurangi'],2020-07-19 08:27:37.190000+00:00,329,"In-person-retail, Walmart, Target, Online-retail, Amazon"
IBM Immersive Data: Augmented Reality for data visualization.,"This year at THINK, IBM’s annual super-conference that showcases the technology and innovations across its entire portfolio, our team was excited to introduce the new IBM Immersive Data for iOS devices and its integration into one of the most powerful and interesting analytics tools, IBM Watson Studio. IBM Immersive Data is an augmented reality visualization tool that allows data scientists and business executives to quickly explore and understand data. Different from our past experience for headsets, the new iOS app allow our users visualize their data in augmented reality using iPhones and iPads.

IBM Immersive Data team at THINK in San Francisco, CA

Augmented Reality will open space barriers and will transform the way we analyze information. We are empowering the user to visualize complex information in a simple way. — David Townsend, Design Director, IBM Analytics

Why AR for Data visualization?

We began exploring the concept of augmented reality (AR) for data visualization with our award winning headset experience, IBM Immersive Insights.

AR allows the user to experience multiple viewpoints. Visualizing data in 3D allows users to uncover trends and patterns that may not be immediately visible with 2D visualizations.

With AR visualization, our users have given us feedback that they have an easier time noticing outliers and patterns because they are able to approach or back away from the visualization.

“The power of 3D only comes out when you’re in true 3D, when you’re looking around it.” — Stefan Van Der Stockt, IBM Data Scientist

Data Scientist exploring a dataset with IBM Immersive Data

Humans live in and navigate through a 3D world. Interacting with AR objects is a natural and intuitive way to understand and learn.

Our users are drawn to the physicality of the data. One user likened AR to using a physical textbook vs. an e-book. With a physical book, you can see the big picture, it creates a more physical experience: the size of the book, the structure of the contents, the layout of the information etc. With an e-book, although that information is technically available to you, it’s not as readily understood.

“Immersion provides benefits beyond the traditional desktop visualization tools: it leads to a demonstrably better perception of a dataspace geometry, more intuitive data understanding, and a better retention of the perceived relationships in the data.” — Nasa and Caltech.

Immersive visualizations enable you to see many dimensions in the data at the same time. It can be difficult to see a lot of variables on a computer screen but it’s easy in AR. With Immersive Data, our users can easily adjust variables in a visualization to change the information they’re seeing. Our tool gives users the ability to compare multiple data visualizations side by side or even overlay them.

Use cases:

Thanks to IBM Immersive Data, people will be able to explore their datasets in multiple dimensions using our integration with IBM Watson Studios. Another advantage of the use of AR for data analysis, is that the application easily integrates with day to day tools that are already a part of users’ workflow.

We envision two main use cases:

Data Exploration:

Data scientists would use Immersive Data at multiple stages during their data exploration process.

While the bulk of the data manipulation and analysis would still take place in notebooks, 3D data visualizations are an important tool that enables the user to see the effects of their work, confirm their analyses, and make further observations.

Presentation:

People spend a lot of time presenting the findings of their work. On a daily basis, they share discoveries and collaborate with colleagues. At the end of the day, they typically show their work to a manager. And on a larger scale, they need to present to stakeholders to secure funding and approval.

Our users are excited about the power of Immersive Data as a presentation tool. They can use the tool to share discoveries and collaborate with colleagues, and to present to managers or stakeholders. They want to save visualizations to take their audience on a visual journey. They feel strongly that 3D visualizations help tell a coherent story about the data and allow the stakeholder to better engage and understand.

“With a big presentation, it’s all storytelling. If the VP doesn’t have an intuitive feeling of what it does, they won’t use it. But if you have the ability to play with the data, then the person gets what the business is all about.” Stefan Van Der Stockt — IBM Data Scientist

Collaboration and presentation is one of our key use cases

This year at THINK, it was great hearing so much positive feedback and excitement from our clients and partners in our demo areas and UX sessions, and we are very happy to see how our project has evolve. This is just the beginning. IBM Immersive Data will soon be available to download for free in Apple Store, and we’ll be sure to share more updates.",https://medium.com/design-ibm/ibm-immersive-data-augmented-reality-for-data-visualization-898587b2a57c,['Alfredo Ruiz'],2019-02-25 22:08:55.808000+00:00,800,"IBMThink, IBMImmersive Data, Augmented Reality, Data Visualization, ARfor Data Analysis"
Why You Should Consider Being a Data Engineer Instead of a Data Scientist.,"3. Data engineering skills are extremely useful as a data scientist.

In more established companies, the work is typically segregated so that data scientists can focus on data science work and data engineers can focus on data engineering work.

But this is generally not the case for most companies. I would say that the majority of companies actually require their data scientists to know some amount of data engineering skills.

A lot of data scientists end up requiring data engineering skills.

It’s also incredibly beneficial to know data engineering skills as a data scientist and I’ll give an example: If you’re a business analyst that doesn’t know SQL, you’ll have to ask a data analyst to query information every time you want to gather insights, which creates a bottleneck in your workflow. Similarly, if you’re a data scientist without the fundamental knowledge of a data engineer, there will certainly be times when you’ll have to rely on someone else to fix an ETL pipeline or clean data as opposed to doing it on your own.

4. Data science is easier to learn than data engineering.

In my opinion, it’s much easier to learn data science as a data engineer than learn data engineering skills as a data scientist. Why? Well there’s simply more resources available for data science, and there are a number of tools and libraries that have been built to make data science easier.

And so, if you’re starting out your career, I personally think it’s more worthwhile investing your time learning data engineering than data science because you have more time to invest. When you’re working a full time job and a couple of years into your career, you might find that you don’t have the capacity or energy to invest as much time in learning. So from that perspective, I think it’s better to learn the harder realm first.",https://towardsdatascience.com/why-you-should-consider-being-a-data-engineer-instead-of-a-data-scientist-2cf4e19dc019,['Terence Shin'],2021-09-05 14:35:10.093000+00:00,306,"Data Engineering, Data Science, SQL, ETL, Business Analytics"
Water and Artificial Intelligence,"Water and Artificial Intelligence

Reflections on Technology and Nature in Crisis

Looking out on a very still lake in Switzerland made me think about writing on a specific topic. On average, the body of an adult human being is 60% water, most of which is contained in the cells, which need water to live. As such in a manner of speaking we are made of water.

In rivers, the water that you touch is the last of what has passed and the first of that which comes; so with present time.

Leonardo da Vinci, XIX Philosophical Maxims. Morals. Polemics and Speculations, 1174.

In the same section Leonardo claims: necessity is the mistress and guide of nature. This analogy refers to the moment like water flowing; as if the water was a series of events and life was fluid not fixed.

We need water to survive, and it even has religious importance to many, it is such an ordinary yet significant part of our lives.

Water Wars

You may have heard whispers or discussions of water wars. Water will be a key cause for future conflict, there is no doubt. There may very well be irregularities in supply and demand, fresh water shortage and groundwater shrinkage. This information seems to flow continuously, like the water rushing past your hand. Less than one percent of earth surface water is suitable for human consumption, it becomes crucial that we save water so that our future generations survive. 70% of the world’s population suffer at least one month of water scarcity a year.

Source: Mesfin M. Mekonnen, Arjen Y. Hoekstra, Sustainability, 2016 posted in National Geographic 2018

The Water Crisis So Far

So how can Artificial Intelligence Contribute?

As mentioned in my previous articles: (1) AI should be used to reduce inequalities; (2) we have to be aware of the energy consumption when AI is used; (3) the risk to the crisis due to escalating digital insecurities.

Proceeding from that I do still believe that working within the filed of artificial intelligence can provide some benefits to contributing to solving some of the problems flowing towards us as a consequence of increased population, congregation and our changing climate.

Measuring and controlling to predict (smart water management). Artificial Neural Networks and Support Vector Machine (SVM) are being popularly used as they are less cost-effective when compared to big data mechanisms.

Analytics India has made a list of cases related to this that I will do my best to sum up:

Greece , used the precipitation, temperature and groundwater level data as the vector for neural networks for prediction.

, used the precipitation, temperature and groundwater level data as the vector for neural networks for prediction. US , Illinois has used the feedforward training algorithm for the prediction of pesticide quotation in groundwater. Texas. An observation was to forecast the groundwater level. The Sevier River Basin Utah, has developed an SVM model to forecast the streamflow of 6 months ahead using local climatological data with different time variations and previous stream volume flow.

, Illinois has used the feedforward training algorithm for the prediction of pesticide quotation in groundwater. Texas. An observation was to forecast the groundwater level. The Sevier River Basin Utah, has developed an SVM model to forecast the streamflow of 6 months ahead using local climatological data with different time variations and previous stream volume flow. Northern France , applied the ANN model to estimate the depth of the contaminated territory in the soil to estimate groundwater contamination.

, applied the ANN model to estimate the depth of the contaminated territory in the soil to estimate groundwater contamination. Turkey , in the Harran Plain researchers used temperature, electrical conductivity, and Ph levels of groundwater as vectors for ANN.

, in the Harran Plain researchers used temperature, electrical conductivity, and Ph levels of groundwater as vectors for ANN. Iran ANN multilayer perceptron (MLP) to model the rainfall-runoff process using rainfall durations, average intensities and season index of over 100 occurrences as vectors for the model.

ANN multilayer perceptron (MLP) to model the rainfall-runoff process using rainfall durations, average intensities and season index of over 100 occurrences as vectors for the model. Singapore has developed ANN for prediction of coastal water quality using the location of stations, previous salinity, temperature, dissolved oxygen levels and chlorophyll-a levels in the nearby stations as vectors.

has developed ANN for prediction of coastal water quality using the location of stations, previous salinity, temperature, dissolved oxygen levels and chlorophyll-a levels in the nearby stations as vectors. China used an SVM model for groundwater quality assessment at the Naingziguan fountain by using groundwater quality classification indicators as vectors, which resulted in high prediction accuracy.

used an SVM model for groundwater quality assessment at the Naingziguan fountain by using groundwater quality classification indicators as vectors, which resulted in high prediction accuracy. Korea used SVMs and ANNs to forecast groundwater level in wells near coastal regions. They used previous data of groundwater level, tide level and precipitation as vectors.

Let’s extend da Vinci’s analogy: Unless you have a very big hand, you can’t touch all the water passing a given point in a river. With big data however, we can touch more, yet not all. We must be wary of making predictions in fast changing world, yet we must as well attempt our best to use technological know-how responsibly with both social and environmental concerns in mind.

This is day 19 of #500daysofAI, I hope you enjoyed it.

–",https://alexmoltzau.medium.com/water-and-artificial-intelligence-a7cf2ac23c17,['Alex Moltzau 莫战'],2019-06-21 10:16:24.887000+00:00,890,"water, artificial intelligence, water wars, water crisis, big data"
探討graph attention機制有效性 — Understanding Attention and Generalization in Graph Neural Networks,"實驗部份

Datasets說明

1. Color counting task (上圖a）

把原圖各節點著色，計算有幾個綠色節點，因為範例有兩個綠色，所以其Ground truth attention各為1/2 = 0.5，在這個task中，graph structure is unimportant and edges of graphs act like a medium to exchange node features

2. Counting the number of triangles（上圖b）

數出原圖中有幾個三角形，這任務各個node的Ground truth attention可以透過下式來計算，比如範例圖共有2個三角形，由圖可知四個跟三角形有關係的node的Ti分別為1 2 2 1，1+2+2+1 = 6，因此attention為0.2的節點是由1/6四捨五入得來；0.3是由2/6四捨五入得來

3. MNIST-75SP（上圖c）

為了測試GNN在irregular grids上的能力，作者根據前人的論文改良了MNIST dataset，將image在避免遺失essential class-specific information的前提下，以a small set of superpixels（超像素原理可參考下方連結）來建出一個graph代表原本的image，這個graph的node features為average intensity value to all pixels within a superpixel和超像素的重心座標；edges為超像素中心之間的空間距離，Ground truth attention被定義為

作者對每張image取N ≤ 75個superpixels, 因此這dataset被稱為MNIST-75SP

4. Molecule and social datasets - more practical cases

在practical cases中，作者用了graph classification任務的benchmark datasets，如蛋白質結構dataset: PROTEINS and D&D；以及scientific collaboration dataset: COLLAB，實驗中，作者想探討attention-based model在inductive任務的能力，因此根據節點數量分割graph dataset，例如PROTEINS中，we train on graphs with N ≤ 25 nodes and test on graphs with 6 ≤ N ≤ 620 nodes

下圖為前三個任務的train&test data examples

Generalization to larger and noisy graphs

attention的一個能力是generalize to unseen, potentially more complex and/or noisy, inputs，藉由減少關注這些爛nodes來更好的訓練，因此這篇paper在test case上把原本的data做些變換，例如上圖color任務的TEST-LARGE增加不是答案的節點（非綠色）或TEST-LARGEC增加不同unseen顏色的nodes，這些以下統稱為雜訊，作者想藉由with and without attention的GNN來探討the limits of GNNs with attention，和attention在哪些任務會work，哪些任務反而有害（focus不重要的nodes or drop重要的nodes）

Loss function and training

COLORS and TRIANGLES任務中要minimize the regression loss (MSE)，其他任務則minimize cross entropy (CE)，後項的KL散度要衡量的分佈是ground truth attention和predicted attention

參數意義可參考paper section 3.3

Weakly-supervised attention supervision

現實情況中，預先知道ground truth attention幾乎是不可能的，因此本文提出了一種弱監督的訓練方法來估算attention，藉此optimize上面的total loss

首先我們想train一個model A，但我們沒有ground truth attention，所以我們可以先train一個和model A長的一樣的model B，只是model B沒有attention/pooling機制，只有最後的global pooling (e.g. readout）來做分類機率輸出，因此model B只有optimize MSE or CE的loss，訓練好之後，我們可以用以下式子來藉由model B估算attention：

y是原圖分類的prediction；yi是原圖去除node i的分類prediction

概念就是我藉由觀察有node i跟沒有node i，機率分佈的結果會差多少，如果沒什麼差，代表有沒有這node根本不重要，其attention自然就低，因此藉由上式&model B，可以得到估算的attention給model A訓練用

個人認為這方法只能知道node對整體graph的影響，不能知道是好的影響還是壞的影響，相信這也是之後可以改進的部分",https://medium.com/@qaz7821819/%E6%8E%A2%E8%A8%8Egraph-attention%E6%A9%9F%E5%88%B6%E6%9C%89%E6%95%88%E6%80%A7-understanding-attention-and-generalization-in-graph-neural-networks-d1ad54084bc0,['許竣翔Jordan Hsu'],2020-04-09 06:31:33.230000+00:00,164,"Attention-based Graph Neural Networks, Experiment, Color Counting Task, Triangle Counting Task, MNIST-75SP"
Latest picks: In case you missed them:,"Get this newsletter By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.

Check your inbox

Medium sent you an email at to complete your subscription.",https://towardsdatascience.com/latest-picks-humans-in-the-loop-7006794293ae,['Tds Editors'],2020-11-12 14:27:14.311000+00:00,40,"newsletter, email, subscription, Medium, privacy policy"
智能数据问答,"欢迎关注可视化笔记专栏

(本文阅读时间大约3分钟)

什么是数据问答？

在面向企业用户的商业分析领域，数据是支撑用户决策的重要基础。而传统分析软件往往需要用户具备较强的基础数据知识与技术能力，一些复杂的分析方法与交互方式往往把一大群数据小白拒之门外。相比于复杂的软件交互，自然语言对话是一种门槛更低、效率更高的智能交互方式。随着AI技术的不断发展，大众用户已经在一些民用领域感受到了自然语言交互的便捷，比如个人助理Siri、智能客服机器人等。用户在和机器进行自然语言沟通的过程中无需关注技术细节，只需提出自己的意图或诉求，就可以得到他们所需要的答案。

数据问答是自然语言交互技术在数据分析领域的应用。基于数据问答系统，用户可以对数据集提出各种类型的询问，系统会在自动分析后通过可视化和文字的形式给出相应的回答。在这样面向数据的自然语言交互过程中，用户可以专注于分析问题和业务逻辑，而无需关注数据处理和具体的软件操作。显然这样的智能交互方式很大程度上对数据分析入门者更加友好，同时也提高了商业分析决策的效率。

应用与研究现状

目前在一些领先的可视化软件产品中已经有对数据问答的集成。Tableau软件公司在2019年的新版本中提供了Ask Data的新功能[1]，当用户将数据上传至Tableau Server后，无需任何额外的配置即可开始对数据进行提问。微软的办公软件Excel也在2019年加入了“对话”功能[2]，可以自动理解用户的问题并自动对数据表格进行智能分析，最终呈现给用户相应的可视化图表。从这些前沿科技公司的行动上来看，基于自然语言交互的数据分析技术已经成为行业发展的趋势方向。

在近几年的可视化学术领域中早已经有大量关于自然语言交互的研究，值得注意的是，在2019年的可视化大会上基于自然语言交互的可视化探索系统FlowSense[3]获得了最佳论文奖，这也证明了学术界对于这一方向的关注。对于数据问答的研究，大部分的研究方法集中在从自然语言中提取用户意图然后转化为类似SQL的数据查询语句，并通过数据可视化的方式作为查询结果的展示[4,5,6,7]。例如，用户询问“价格最低的SUV车型是哪款？”，应该可以被转换为SQL语句：SELECT MIN(PRICE) FROM CARS WHERE CATEGORY = ‘SUV’。还有一些研究专注在语用学(Pragmatics)上，通过研究用户分析过程中的语言行为，从而提出可视化分析的语言规则用于数据问答系统的优化[8,9,10]。

机会与挑战

虽然业界和学术界已经对数据问答有大量的实践和前沿研究，但目前依然面临诸多挑战。首先，如何更好的处理用户多样化的查询？当用户面对自然语言交互系统的时候，他们会默认系统是非常智能的，相对于传统软件标准化的数据格式，他们会用更偏向于用口语化的方式或者使用一些行业中的专业术语去阐述他们的需求，此外他们的对话中也会包含一些数据以外的常识性概念和知识，这也会导致目前的数据问答系统失效；其次，如何在上下文中优化数据问答？用户在使用问答系统的过程通常是持续多次地去寻找答案，系统需要在多轮对话中能够分析出用户意图，结合之前的查询结果给出当前的最优结果；最后，除了多轮对话，一些用户在系统中的交互操作和公开的个人信息（地理位置等）也可以成为进一步优化要考虑的因素，因此如何更好地将这些额外的上下文信息和数据问答进行结合也是未来的研究挑战。

商业应用设想

这里设想一个企业应用场景，比如智能数据大屏应用，可以通过自然语言的方式完成对数据大屏的快速搭建。用户无需具备非常强的数据分析知识，他们只需向大屏应用提出业务需求，比如：关注的KPI，行业趋势，等等。在通过语义理解模块提取用户提出需求的信息后，可以快速为用户自动搭建行业相关的数据大屏，并根据用户的关注点进行智能布局。对于数据大屏的最终读者来说，他们也可以直接与大屏进行“对话”，数据大屏会根据用户的问题自动高亮出用户感兴趣的内容或对视图作出对应的变换。

参考文献

[1] Tableau Ask Data Ask Data

[2] Microsoft Excel 智能数据分析技术，解锁Excel“对话”新功能

[3] Yu, Bowen, and Cláudio T. Silva. “FlowSense: A natural language interface for visual data exploration within a dataflow system.” IEEE transactions on visualization and computer graphics 26.1 (2019): 1–11.

[4] Dhamdhere, K., McCurley, K.S., Nahmias, R., Sundararajan, M. and Yan, Q., 2017, March. Analyza: Exploring data with conversation. In Proceedings of the 22nd International Conference on Intelligent User Interfaces (pp. 493–504).

[5] Setlur, V., Battersby, S.E., Tory, M., Gossweiler, R. and Chang, A.X., 2016, October. Eviza: A natural language interface for visual analysis. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (pp. 365–377).

[6] Fast, E., Chen, B., Mendelsohn, J., Bassen, J. and Bernstein, M.S., 2018, April. Iris: A conversational agent for complex tasks. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (pp. 1–12).

[7] Gao, T., Dontcheva, M., Adar, E., Liu, Z. and Karahalios, K.G., 2015, November. Datatone: Managing ambiguity in natural language interfaces for data visualization. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (pp. 489–500).

[8] Hoque, E., Setlur, V., Tory, M. and Dykeman, I., 2017. Applying pragmatics principles for interaction with visual analytics. IEEE transactions on visualization and computer graphics, 24(1), pp.309–318.

[9] Srinivasan, A. and Stasko, J., 2017. Orko: Facilitating multimodal interaction for visual exploration and analysis of networks. IEEE transactions on visualization and computer graphics, 24(1), pp.511–521.

[10] Setlur, V., Tory, M. and Djalali, A., 2019, March. Inferencing underspecified natural language utterances in visual analysis. In Proceedings of the 24th International Conference on Intelligent User Interfaces (pp. 40–51).",https://medium.com/shidanqing/%E6%99%BA%E8%83%BD%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94-bf4b79c2446,[],2020-07-07 05:40:10.319000+00:00,271,"Data-Questioning, Natural Language Interaction, Visual Analysis, AI Technology, Pragmatics"
What can neural networks reason about?,"paper, my implementation

Recently, a lot of research has focused on building neural networks that can learn to reason. Some examples include simulating particle physics [1], coming up with mathematical equations from data [2].

Figure 1: What are the colors of the furthest pair of objects?

Figure 1, shows an example of a reasoning task. In these tasks, we have to learn the fundamental properties of the environment given some data. In this example, the model has to first learn the meaning of the words furthest, color, pair.

Many reasoning tasks can be formalized as graph problems and message passing [3] has been shown to be a key component to modern graph neural networks. But why can GNNs solve these problems and MLPs cannot, even though they have the same expressive power? The failure of MLPs to solve these reasoning tasks can further be stated as, why do MLPs fail to generalize to these reasoning tasks?

Formally, we will be trying to answer the following question

When does a network structure generalize better than other, even if they have same expressive power?

Algorithm point of view

Let us begin with an observation that the reasoning process resembles algorithms. To solve the reasoning problem of Figure 1, we can come up with the following solution

Find the location of all pairs of objects. Determine the pair that is furthest (this is just a heuristic that you decide). Return the color of the object.

The above three steps resemble an algorithm, where we are defining a step-by-step procedure to solve a problem.

From the paper,

We will build on this observation and study how well a reasoning algorithm aligns with the computation graph of the network. Intuitively, if they align well, the network only needs to learn simple algorithm steps to simulate the reasoning process, which leads to better efficiency.

The authors of the paper formalize the above as algorithm alignment.

Algorithm alignment

Every neural network architecture that we build has an underlying computation structure. MLPs resemble a for loop kind of computation structure as they are applied to vector inputs. In the case of GNNs, we aggregate information from the neighbors implying a dynamic programming (DP) type of computation (shown in the paper).

How GNNs relate to DP

Bellman-Ford algorithm [4] is an algorithm used to solve the shortest path problem. The main step of the algorithm is

for u in Nodes:

d[k][u] = min_v d[k-1][v] + cost(v,u)

where, k = iteration number (1,2,…,num_nodes-1)

Now let’s see how we will do the same use GNN. The message passing algorithm is

Now let

UPDATE = identity function

In most cases FUNC is MLP.

AGGREGATE = minimum

Using this information the message passing equation becomes

The above equation now resembles closely the Bellman-Ford algorithm. If we had used MLP to solve this problem then MLP would have to learn the structure of the entire for-loop as its computation structure resembles a for loop which is expensive.

This is the main point of the paper. If we can find a suitable underlying reasoning algorithm for the reasoning task, then we can use neural network structures that better align with the underlying algorithm structure. This will make the task easy to learn and improve generalization.

Maximum value difference

In the paper, they do experiments on four reasoning tasks

Maximum value difference

Furthest pair

Monster trainer

Subset sum

I try to reproduce the results of the maximum value difference task. The task is simple, given a vector find the difference between the maximum and minimum value. But why is it important?

In a lot of reasoning tasks, we are required to answer questions related to summary statistics (like count, min, max). For example, “How many objects are either small cylinders or red things?”. In the case of GNNs, we can simulate the reasoning algorithm by using MLP to extract features from nodes and then use aggregation to come up with the answer. In this case, MLP has to only learn to extract local features. On the other hand, if we only used MLPs to solve this problem. Then the MLP must learn a complex for-loop and therefore needs more data to converge.

The maximum value difference task is stated as:

A training sample consists of 25 treasures (X). For each treasure (X_i), we have X_i = [h_i,h_2,h_3] where

h_1 is 8-dim location vector sampled uniformly from [0,20]

h_2 is value sampled uniformly from [0,100]

h_3 is color sampled uniformly from [1,6]

For the maximum value difference task, we have to find the difference between the maximum value and the minimum value for each training sample.

MLP

To input the training sample, we simply concatenate the vector representation of all 25 treasures and then feed them into a MLP. We solve this problem as a classification problem where the task is to predict a value from 0 to 100 i.e. 101 classes.

The code to generate the data is in min_max_mlp_data.py. A quick summary of data generation process is shown below

The code to create MLP is in model_mlp.py.

The code to test the model is in mlp.ipynb. MLP achieves around 8% accuracy on the validation data. This is the expected result.

GNN

Construct a fully connected graph with 25 nodes (each node representing a treasure). I use pytorch_geometric to implement the GNN.

The code to generate the data is in min_max_graph_data.py and the code to construct GNN is in model_gnn.py.

The best GNN model got to 98.5% accuracy (maybe with hyperparameter search 100% accuracy can be achieved). But it demonstrates the idea that GNN can easily learn summary statistics which are a key component of reasoning problems.

Conclusion

The concept of algorithm alignment can be applied to any reasoning algorithm. If we can come up with a suitable algorithm to solve the reasoning problem, then we can design a network with a similar structure to learn it. If we have no prior knowledge about the structure of the reasoning algorithm then neural architecture search over algorithm structures will be needed.

If you want to read more about Graph Deep Learning, see my other posts, or follow me on twitter.",https://kushaj.medium.com/what-can-neural-networks-reason-about-5d4e4d2d669a,['Kushajveer Singh'],2020-11-08 13:32:59.394000+00:00,969,"paper, my implementation, neural networks, reasoning, graph problems"
Lead Prediction with Tableau und Scikit Learn,"Lead Prediction with Tableau und Scikit Learn

Foto: www.pexels.com Creative Commons CC0

The Task

With the data that a potential customer (“lead”) leaves on a website, important insights and results on customer behavior can be gained. Machine learning is then used to create a prediction model from this data. The case study carried out shows that the accuracy of such a forecast model is 90%.

An education company sells online courses to industry professionals. On any given day, thanks to marketing efforts, many professionals interested in the courses land on their website and browse for courses. This is how campaigns on social media channels, websites, or search engines, such as Google, attract new prospects.

Once people land on the site, they can browse the courses, fill out a form for the course, or watch some videos. If they leave the site again without completing this desired activity, they are brought back with retargeting campaigns and converted to leads. When these people then fill out a form and provide their email address or phone number, they are classified as prospects.

Once these leads are acquired, sales team members start making calls, writing emails, etc. Through this process, some of the leads are converted, while most are not. The typical lead conversion rate is around 30%.

For example, if they receive 100 leads in a day, only about 30 of them will convert. To make this process more efficient, the company wants to identify the potential leads, also known as “hot leads”. If they are able to identify these leads, the lead conversion rate should increase. Since the sales team is now more focused on communicating with the potential leads instead of making phone calls to all of them. This not only makes the sales process faster and more successful but also saves on personnel costs.

It is our task to create a model in which each lead is assigned a lead score. The customers with a higher lead score have a higher chance of conversion, and the customers with a lower lead score have a lower chance of conversion.

The Data

We use data from Kaggle in this demo https://www.kaggle.com/ashydv/leads-dataset

The data set contains information about:

Whether the requests became customers — the column is called “Converted”.

Behavior on the website, such as time spent, what content was viewed, etc.

Information provided in forms

How the visitor came to the website (search engine, referrer, direct)

The following table shows an extract from the data. Data records of 9240 persons with 37 features are available. The characteristics are stored for each lead. Some are numeric features, such as time spent visiting the website, but there are also many categorical features, such as demographic information or information from web forms.

The data can be imported into Tableau via the CSV file provided by Kaggle.

The first lines of the record

Data Cleansing

As is often the case in practice, incomplete data sets present a problem that must be solved. Simply removing all incomplete records is usually not a viable approach because too many records are affected. A detailed analysis is necessary.

We delete the features with more than 40 percent missing values since there is too little here to do anything with. In features with less missing data, these are replaced by the dominant value of the feature.

Additionally, there are “select” values for many columns in the dataset. These come from input forms in which the customer did not select a value from the list in the form (it may be that this was not a mandatory input). “Select” is displayed in the dataset here. “Select” values are as good as NULLs, so they are replaced with NULLs.

A detailed description of how to perform these steps can be found at https://www.kaggle.com/danofer/lead-scoring

Based on the analysis, we found that many features do not add information to the model, so we remove them from further analysis. 16 features remain in the dataset.

Exploratory Data Analysis with Tableau

Let’s first look at how many leads and non-leads there are by creating a simple bar chart. For this, “Converted” is dragged as a dimension to “rows” and the number of leads to “columns”.

The ‘Converted’ feature indicates whether a lead has been successfully converted (1) or not (0). Approximately 38% of the customers in our datasets were won.

Now let’s look at the regional distribution of the data by displaying a map with colored countries according to the number of records.

Map lead origin

To do this, we use the “Country” dimension and the number of leads along with the appropriate chart type. Tableau then automatically generates the Latitude and Longitude data and places them in “Rows” and “Columns”.

Much of the data comes from India, as shown in the figure.

We now consider the numerical features “Total Visits”, “Total Time Spent on Website” and “Page Views Per Visit”. We use “box-and-whisker” plots to examine the distribution of the data.

“box-and-whisker” plots of the numerical features

The median for the number of visits to the website for converted and non-converted leads is the same. Based on the total number of visits, no conclusive statement can be expected.

People who spend more time on the website are more likely to convert. It is recommended to improve the website to make it more helpful for the users and keep them engaged on the website.

The median number of page views per visit for converted and non-converted leads is the same. Nothing can be said specifically for lead conversion from the number of page views per visit.",https://towardsdatascience.com/leadprediction-with-tableau-und-scikit-learn-aa90b388bae1,['Andreas Stöckl'],2021-03-29 06:37:36.676000+00:00,893,"Lead Prediction, Tableau, Scikit Learn, Data Cleansing, EDA"
The Blessing of Dimensionality: Why the Curse of Dimensionality is a Two-Sided Coin,"This is evidence of the nature of distance in high-dimensional spaces — as dimensionality increases, the importance, or value, of any single one-dimensional line diminishes, as can be seen with the diminishing returns on the y-axis. This is to be expected, considering how quickly volume and the possibilities of points grows as dimensionality increases.

This is also the reason why a hypersphere’s volume tends towards zero as the dimensionality increases — since a sphere is defined as consisting of all points that are one radius’ distance in Euclidean distance away from a center point, the number of dimensions increases but the distance gradually tapers off. Hence, however, unintuitive it may seem, as dimensionality tends towards infinity, a hypersphere’s volume will tend towards zero, while the hypercube it is inscribed in will continue growing (or stay constant, if the side length is 1).

Let us consider a hypercube in two dimensions (a square) with side lengths of five units. There are, then 5² = 25 units. A similar hypercube in three dimensions (a cube) has 125 units. From there, it skyrockets. The power of exponents is really very incredible — just within ten dimensions, the hypercube already has a hypervolume of 9,765,625 units. Adding an addition dimension to a space expands the current space by a huge magnitude, so it should be no surprise that a miniscule one-dimensional distance has diminishing value.

Graphed in Desmos.

The mathematics of higher dimensions is odd, but in machine learning, it poses an even larger risk. Because the volume of a high-dimensional space is so unimaginably enormous and the number of data points is never anywhere even comparable to that volume, high-dimensional data is often very sparse. Consider, for instance, a 150 by 150 pixel image — a very common dimension size to work with in machine learning. This image then has 150 times 150 times 3 (three values per pixel to specify RGB — red, green, and blue values) equals 67,500 dimensions.

Along with, say, ten thousand other images, each image exists as one point in the feature space, with one value for each of the 67,500 dimensions. Because of the sheer enormousness of the space and because distance is so small in high-dimensional spaces, the data will be incredibly sparse. This causes models to overfit to the data, since the mathematical nature of machine learning algorithms rely on the concept of relative distance, which has been warped and overwhelmed amidst such massive spaces.

This is why convolutional neural networks have become so popular with images — instead of relying on a point-in-space model so many other algorithms use, it uses simple data-altering layers to transform series of matrices. However, a huge part of neural networks is gradient descent — the updating of the parameters (weights, biases, etc.) in a way that will optimize the network’s path to a reduction in error. This requires mapping out what errors form for a certain combination of the millions of trainable parameters.

Yet this is one example of the blessing of dimensionality. In fact, what was a weakness with the curse of dimensionality can be repurposed as a blessing in other contexts.

The blessing of dimensionality and the curse of dimensionality are two sides of the same coin.

The goal of a neural network is to reach the global minima of the error space; that is, to find the perfect set of parameters that yields the lowest error possible. In order to reach that global minima, it starts out at some point on the error space and incrementally moves with each training step in what it believes is the right direction. It turns out that in the error space, there are many local minima — dips in the error space that are minima but not the lowest in the entire error space.

Source. Image free to share.

Although there are solutions to local minima, such as a growing arsenal of specialized optimizers that utilize ‘momentum’ and other methods, it is still a common problem in neural network training. If it doesn’t prevent the network from finding a global minima, it can, at the very least, make training a lot longer.

In high dimensional space, however, distance is warped and not proportionately large, as demonstrated earlier. This turns out to be a huge advantage with local minima, whose once seemingly-global dips are rendered small bumps in the surface any reasonable optimizer could overcome. On the other hand, the global minima would be noticeable enough for the network to converge at that point. So, in certain cases, increasing the dimensionality can be a blessing with better performance and faster training time. Of course, this may be cancelled out by overfitting of parameters, so decisions need to be carefully crafted based on experimentation and knowledge.

There are other applications of the Blessing of Dimensionality in other areas of machine learning and in other fields, from mathematics to physics. What’s important to understand is that in machine learning, everything is a trade-off. The Curse of Dimensionality may be less cursed than most people would believe.",https://towardsdatascience.com/the-blessing-of-dimensionality-how-warped-spaces-can-actually-be-beneficial-c5033786cc9a,['Andre Ye'],2020-06-25 15:15:02.962000+00:00,831,"dimensionality, machinelearning, hypercube, hypersphere, gradientdescent"
Understanding Multiplication,"X times X is X plus X. And, also, X divided by X.

Multiplication. Addition. Division.

X times X is X plus X. And, also, X divided by X.

This is because X and X is X and X’.

Meaning, X and X is X and Y.

Meaning X and Y is zero and one.

Because zero and one is circumference. And, diameter.

Giving us the mathematical relationship between zero and one.

Therefore, multiplication is addition. Addition is division.

Zero and one is, always, one, and two. (Two is the natural limit in all directions.) Zero and one share a circular relationship with one and two.

One and two share a circular relationship, giving us, zero and one (circumference and diameter). (Zero and one share a circular relationship.)

This means two is, always, one (one is, always, two.)

This is the conserved circular relationship between individual and group (virtual and real) (all systems, disciplines). Which is the basis for artificial intelligence. Virtual reality. Machine learning. Deep learning. Learning, in general.

Conservation of the circle is the core dynamic in nature.",https://medium.com/the-circular-theory/understanding-multiplication-9e7332b28bf0,['Ilexa Yardley'],2017-07-19 19:25:05.426000+00:00,167,"Circle, Conservation, Artificial Intelligence, Virtual Reality, Machine Learning"
Warning! Unsupervised Neuroscience Ahead,"The warning: Care is needed. We are experts at finding patterns in noise, and so are our algorithms.

Take clustering. The problem with clustering is that it returns clusters. I mean, I know that’s what it’s supposed to do, but that’s actually the problem. Give a clustering algorithm the phone numbers of everyone in Llandudno, ask it to find four clusters in the data, and it will. Now you have four clusters of Welsh people, and are none the wiser as to what to do with them, and neither are they. The mere existence of clusters does not mean there is actual cluster structure in the data.

Unsupervised algorithms are about making sense of data for us, the observer. What they find does not have to coincide with reality. Reality doesn’t have a ground-truth, because it does not have clean, neat carves at the joints except in special cases. And most of those special cases are from artificial systems, with the joints built in. Run a different clustering algorithm on your data, and you’ll get different clusters; run a different dimension reduction algorithm on your data, and you’ll get different dimensions. As Ulrike von Luxburg and friends have so clearly argued, clustering is an art.

The unsupervised organisation of data is just a description of that data. Just because we can cluster neurons into groups doesn’t mean there exists actual meaningful groups of neurons in the brain; just because we can cluster behaviour into discrete elements — into states, motifs, syllables, or whatever term you prefer — doesn’t mean behaviour is actually discrete. To find that it means something, we have to link that discovered organisation to reality, show it has meaning. In neuroscience, that typically means we have to link that organisation of data to something happening in the world, or elsewhere in the brain, or both.

And that acid test is passed by the best attempts at unsupervised neuroscience. In the paper from Alon and friends, they showed the neurons’ “tuning” to the hippocampus’ internal dynamics had meaning as location in space (and repeated the same trick for neurons that code for head direction in the rodent thalamus). In the study from Adam Kepecs’ lab on the orbitofrontal cortex, their discrete groups of neurons in turn each encoded a meaningful variable in the decision-making process. Even better, they re-did the whole analysis with another cohort of animals with more neurons, reusing all parameters from first cohort, and ended up with the same results. These studies could show us a mapping between the unsupervised structure of the data and the real world.

Terrific work, but those are the “easy” ways of doing unsupervised neuroscience — by relating what we found to what we already know. We already know that hippocampus has place cells, and that there is a head direction system in the rodent thalamus. We already know the neurons in orbitofrontal cortex are heavily implicated in decision making, and to work out what their groups of neurons were encoding Kepec’s lab interpreted their activity as the variables within a mathematical model of decision making. If that model is wrong, the mapping between variables and activity is of little consequence in building our confidence that the clustered neurons are really there. And others, of course, may find different answers: when Anne Churchland’s group went looking for discrete groups of coding neurons in posterior parietal cortex, for example, they found none.

The ultimate test for unsupervised neuroscience is discoveries that could not be found any other way. There are some examples of that too. For example, we took large-scaling recordings of neurons in the motor system of the venerable sea-slug Aplysia. Using a fully-unsupervised pipeline to analyse that data we discovered its motor system was doubly discrete: on one level, groups of neurons with correlated activity were laid out contiguously in the motor system, beautifully tesselating the bit of brain they were in. On another level, mass populations of neurons with clearly different dynamics were in different parts of the motor system, including a distinct population of oscillating neurons in one spot that were most likely the pattern generating network for movement — a discovered hypothesis waiting to be tested. Joshua Vogelstein and friends discovered a richly detailed map of the relationship between neural activity and the resulting behaviour of Drosophila maggots, by individually stimulating each of 1054 types of neurons, videoing the behaviour, and clustering it. They thus revealed 29 different types of behaviour and which neurons drove each. But these are discoveries of relationships, of structure; they are not yet that final step of unsupervised discovery of a theory for how a bit of brain works.",https://medium.com/the-spike/warning-unsupervised-neuroscience-ahead-382cf9ab13e1,['Mark Humphries'],2020-11-17 17:38:42.791000+00:00,772,"Unsupervised-Learning, Data-Organization, Clustering, Neuroscience, Patterns-in-Noise"
“OkCupid is a dope org that is doing more than just hooking up people.”,"“OkCupid is a dope org that is doing more than just hooking up people.” BUILTBYGIRLS Follow Apr 1, 2016 · 3 min read

#BUILTBYGIRLS fell in love on Valentine’s Day — with data science. Ok, we admit, we already liked it a lot, but the second event in our Hashtag Series, this time hosted by OkCupid, showed our girls all that’s possible with curiosity, technology and brilliant mentors.

The Hashtag is a monthly #BUILTBYGIRLS event series designed to highlight how trending companies use tech and expose high school girls to the incredible careers and experiences that technology can unlock for them.

To help us better understand February’s event theme — Tech of Love & Communication, we started with a panel of experts from different fields:

Mike Maxim, GM & CTO, OkCupid

Amanda Bradford, Founder, The League

Alexandra Martell, Online Managing Editor of Cosmopolitan

fred benenson, VP of Data, Kickstarter (and the mastermind behind the emoji translation of Moby Dick)

Patrycja Slawuta, Social Psychologist

Moderator Samantha Steinberg, Amanda Bradford, Alexandra Martell, Fred Benenson, Patrycja Slawuta

In the beautiful IAC building in New York City, our rockstar 16-year-old moderator and coder extraordinaire Samantha Steinberg led a discussion about how tech is changing the way we communicate, while 25 high school girls listened in. The panelists shared how they use data to better understand human interactions, discussed how tech can help us feel simultaneously connected and lonely, and debated how to be your true self in a digital-driven world. They left us with a lot to think about, and a list of their favorite emojis: side-eye-smiley-face for the win!

A dream team of OkCupid data scientists took over next, and software engineer Dale Markowitz shared some of their amazing research into human nature. The #BBG audience had a ton of questions (no, they can’t use the app until they are 18!) but we left time for pizza and #BUILTBYGIRLS M&Ms, and then put the girls to work in small teams.

We asked each team to brainstorm a new product that solves a problem using data science, with the help of the OkCupid gurus. And let us tell you, they delivered — in 15 short minutes. One team developed a cyberbullying add-on that warns you before you view explicit or offensive content. Another thought up an app to connect environmental factors with cancer diagnoses. Newcomers from our partner Girls Write Now helped brainstorm an app to crowdsource safety on college campuses.

As always, the girls blew us away; we tried to bargain for equity in these amazing ideas, but no dice. Maybe we’ll have better luck next time.

Besides realizing that OkCupid is a “dope org that is doing more than hooking people up,” the girls left our IAC with a determination to further explore data science — 94% agreed that this event opened their eyes to the possibilities of technology and data science that they were unaware of and now say they “understand how algorithms can be used to develop theories about societies decisions.”

Here’s to the next generation of killer female data scientists.",https://medium.com/a-world-builtbygirls/okcupid-is-a-dope-org-that-is-doing-more-than-just-hooking-up-people-ec15d285d365,[],2016-04-01 13:11:03.989000+00:00,496,"Ok Cupid, Data Science, Techof Loveand Communication, Algorithms, Cyberbullying"
How To Create Your First Docker For Geospatial Environment,"Docker is transforming modern applications. You might have heard about it and wondered how it is beneficial to your geospatial data science applications? Or, more probably, how do you get started?

In this post, I will go through what Docker containers are and their benefits. I will also cover the complete running of an entire geospatial Jupyter environment with Docker step by step. Finally, I will point you to the resources to customize and advance to the next level.

What is Docker

Docker is a tool for running applications in an isolated environment (similar VM). The underlying concept of docker has been around before docker, but Docker revolutionized it and made a cohesive and easy to use technology. They achieve this through containerization.

Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries, and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating system kernel and therefore use fewer resources than virtual machines — Source

There are many other containers, but most of the time, people use docker. We now use docker to be synonymous with containers.

Containers differ from virtual machines. While Virtual machines are an abstraction on top of physical hardware, converting one server into multiple servers, containers are an abstraction on top of the app layer and its dependencies.

Containers vs. Virtual Machines

There are several benefits to containerization, including the following:

Multiple containers can run on the same machine, and each container in an isolated process.

Containers take less space than VM.

You can run containers in minutes, with less memory since it does not need the full OS.

We can deploy and test containers everywhere.

Docker Terminology

I have grappled for some time with the difference between docker images and containers. You can think of docker images as templates and containers as an actual instance of the template.

Docker Hub has an extensive repository of images. You can either create and push your image to Docker Hub or pull a ready-made image. Even when you are making an image, you can have a base image instead of building it from scratch.

On the other hand, a Docker container is a running instance of the Docker image. Here, we interact with Docker and can create multiple instances of an image.

Before I go through Creating a docker environment for geospatial data science, Here are some useful terminologies:

docker pull — to pull images from Docker Hub.

docker push — to push images you build with Dockerfile.

docker run — Create a container from an image.

docker ps — List out running containers.

docker ps -a — List out all running and stopped containers.

docker stop — Stop a running container.

docker rm — Remove a stopped container.

docker images — List out all images.

docker build — Build images from Dockerfile.

Create a working Jupyter Environment with Docker

Before we can start pulling and creating a working environment in Docker, we need to install Docker. Installing docker has become easy even in Windows 10 now. Go to Docker Documentation and install docker. For Windows or Mac users, you need to download Docker Desktop. I use WIndows so, I have installed Docker Desktop for Windows.

You also need to create a Docker Hub account and sign in to the Desktop application or from the terminal.

Now let us start creating a Docker environment for Geospatial Data Science. In Docker Hub, you can search for available images.

Here are some of the popular Geospatial Data science Images available:

The Docker-based Geospatial toolkit for R — built on versioned by Rocker images.

Ubuntu with python packages for working the geospatial netCDF data and some helper packages. — geospatial-python.

All-encompassing Geospatial Data Science Container (Python + R) — GDS.

Let me go through the steps to pull a Geospatial data science Jupyter Environment image and create a running container. In this demo, I will use the gds_py image created by Darribas. It has all the packages you need for geospatial data science in Python.

Go to a terminal First and pull the image with the following command:

docker pull darribas/gds_py

When we execute the command above, it starts to download the image and might take a while, depending on the image size. Once it is complete, you can check Docker's image for Desktop or just run the following command in your terminal.

docker images

Now that we have a Docker image, we can create instances from it. We can use the run command and provide the port and also mount a local folder.

docker run -p 9999:8888 -v C:\Users\shaka\Desktop\D:/home/jovyan/work darribas/gds_py

-p: we are forwarding port 8888 to 9999. So when we want to access Jupyter Lab, we need to specify this port. We will see that later.

-v: We are mounting a local file for persistence. Even when we stop the container and start again, we will have our Jupyter notebooks.

After we execute the above command, we will get a link running Jupyter Lab. Copy that link and paste it into your browser. Before running, change the port if you had a different forwarding port. In our case, we had 9999. So change only the port to 9999 and hit enter.",https://medium.com/spatial-data-science/how-to-create-your-first-docker-for-geospatial-environment-c6893d98ce0e,[],2020-11-24 09:00:29.583000+00:00,852,"docker, containers, docker images, docker hub, docker pull"
What we believe in. Chronotope,"Fragment of an article in Tatlin Mono — “Young architects 2014”

Habidatum Chronotope: map (2D) + time (vertical)

City as a Process

A modern city is a process unfolded in time and space. To study such a city is like to perform a piece of music or listen to it when the impressions of the passage being performed at the moment are immersed in the context of what has already been played, and the momentary pleasure complements the integrity of the performed fragment.

The view on the city in static defined the current toolkit of urban studies. There are virtually no tools for analyzing the time trajectory of a place.

Free Time of Space

Meanwhile, such trajectories open up new opportunities for citizens, businesses and city authorities. With the help of the Habidatum Chronotope platform, you can identify the “free time” resources of a place, the mode of its use at different times of the day, week, month, year.

According to Habidatum, approximately 60% of the time urban space is not in use: thus, the resources of urban activity growth and compaction are huge. Habidatum Chronotope platform helps detect free “temporal niches” in different areas of urban space.

Compatibility of Functions in Time and Space

The demand for free “temporary niches” raises the question of the compatibility of different functions when sharing the same place at different times — a situation reminiscent of the “right of passage” in the old medieval cities, when the street was used for the passage of cattle and people alternately. The compatibility potential of different functions can be assessed using analytical indicators that determine the “space capacity” and “usage type” for different categories of users of the platform.

Event as a Change of Trend

Habidatum believes in a special definition of urban “event” — for us, it is not just a single happening (football match, accident, demonstration, etc.). Oppositely, it is a change of “trend” or “pattern”. Data processed through special analytical “filters” in real time allow recording the change of “trends”, whether it is a change in a given parameter (for example, the average length of the trip) or a spatial pattern (for example, the configuration of ethnic diasporas in the city).

Real-time vs. Historical Analysis

Most of the professional tasks solved with the help of Habidatum Chronotope platform are in the between real-time monitoring and traditional historical analysis. The platform allows doing both, not limited to any of the extremes. Its main goal is the practice of “lean planning”, whose application requires linking historical analysis to constant engagement in the process that is studied and seeks adaptation of urban development scenarios based on the constantly updated information.

Thank you for reading. Appreciate your comments!",https://medium.com/habidatum/what-we-believe-in-chronotope-8b99af40b648,['Katya Serova'],2019-01-11 16:10:15.534000+00:00,437,"Young Architects2014, Habidatum Chronotope, City AsAProcess, Free Time Of Space, Compatibility Of Functions"
Visual Sentiment Analysis for Review Images,"“A picture is worth a thousand words.”

So they say. Indeed, some images could capture certain moments so vividly that they become iconic and timeless.

Could a picture speak of the sentiment of the photographer?

Intuitively, that seems probable. After all, in the choice of scenery or angle or other tricks up the sleeve of a photographer, the picture taken is essentially a rendering of what the photographer sees.

In pursuit of an empirical answer to what might have also been a philosophical question, we conduct research on a data set of images found within online reviews of restaurants crawled from Yelp. With the advent of mobile phones, many online reviewers now prolifically include photos within their reviews, recounting their experiences as well as their sentiments textually and visually.

What is visual sentiment analysis?

To test the above hypothesis, we formulate a problem known as visual sentiment analysis. Given an image, we seek to determine whether the image is positive (i.e., found within a review with rating of 4 or 5 on a scale of 5) or negative (i.e., associated with a rating of 1 or 2). We build a binary classifier based on a deep learning framework called Convolutional Neural Networks. Our model architecture shown below is reminiscent of AlexNet for object detection, with a twist in its application to binary sentiment classification. We describe the details of this base model in a paper authored by Tuan and Hady and published in the ACM Multimedia Conference 2017.

To cut a fascinating story short, we find that the trained visual sentiment analysis classifier performs significantly better than random, implying that indeed there are signals within an image that help to convey the overall sentiment of the review writer.

What do positive images look like?

Below we show some examples of images classified as positive. Happy faces and celebrations seem to mark happy moments. Note that this is general image classification, and not specifically about facial emotion recognition (which itself is an interesting but distinct problem). For another set of examples, if one can afford to dine at restaurants with a view, chances are the experience would be positive.

What do negative images look like?

Well, no one likes paying too much (or perhaps even paying at all?). It is always a bummer to discover something that does not belong on one’s plate.

A need for Context

Online review photos that the carried sentiment is arguably subjective to the reviewers with their personal experiences. One question then arises:

Do different customers express the same sentiment to the same food?

Taking a closer look at the data, we discover some interesting disagreements among our “photographers”. With an example between two visually similar pictures of tacos from the same restaurant, there is a polarity in term of sentiment given by two different reviewers.

Interestingly, the sentiment tends to be expressed in the form of a mixture of crowd agreements and personal preferences. The former part is well captured by the base CNN model where the later is not considered.

What should be the right way to detect the sentiment is this scenario?

For this particular setting where the images coming from online reviews, the problem shares some similarities with the notion of “visual-aware recommender systems” trying to capture user preferences through interactions with visually-featured items. Although we are working with the online review data, the problem of visual sentiment analysis does not always come with the notion of preferences. For generality, we frame the problem as visual sentiment analysis with multiple contexts, where users and items are contexts in this scenario. Contexts can be as specific as each user or as general as sources of data where the images come from.

Our current hypothesis is that sentiment is not purely a function of the image features but the image-context combination. We are then left with another question which is how to inject the notion of contexts into the model. Our CNN is, originally asked to learn a sentiment detector from images, tailored to be context-aware by turning a sub-component into context-specific. In other words, a subset of parameters is influenced by each context where the rest are shared.

There are two types of components, which are convolutional layers and fully-connected layers, in our CNN architecture leading to two ways of introducing the contexts. For a convolutional layer with n filters/kernels, we make k out of n context-specific, similarly, k out of n neurons for a fully-connected layer. In the practical point of view, a filter of a convolutional layer is equivalent to a neuron of a fully-connected layer. To learn with the new networks, we have to optimize the parameters under the online learning setting. Details of the training can be found in our paper for eager readers.

In addition to the improvements in quantitative results, we would like to lend some intuitions of how the contexts influencing our sentiment detector. We first look at the images with the highest probability of positive by the base CNN without the involvement of contexts. One of the image clusters is about a small group of people celebrating something with cake and candle as shown previously. We then look into the visually-similar but sentiment-reversed images by the model with item-as-a-context. Our item-as-a-context CNN gives us another cluster about people, but not in the celebratory mood. What an interesting contrast!

Similarly, we apply the same procedure with a cluster of negative images and would like to see how the reversed sentiment images are going to look like. Can you guess what are positive images portraying small objects on plain surfaces? Ask our context-aware detector and you will have “tasty” answers. Well, probably without the negatives above.

References:

The complete paper can be found here: http://www.hadylauw.com/publications/acmmm17.pdf

The code and data is located here: https://code.preferred.ai/vs-cnn",https://medium.com/intel-student-ambassadors/visual-sentiment-analysis-for-review-images-812eab7ef2b,['Trương Quốc Tuấn'],2019-04-26 16:10:47.402000+00:00,941,"/Visual Sentiment Analysis, Image Classification, CNN, Yelp Reviews, Context Aware Detection"
Introduction to Learning Theory — Part 1,"One of the most significant take-aways from NIPS 2017 was the “alchemy” debate spearheaded by Ali Rahimi. In the wake of the event, I have been trying to learn more about statistical learning theory, even though the concepts may not be readily applicable to deep neural networks.

One of the most important concepts in this regard is to measure the complexity of a hypothesis class H. In any machine learning model, the end goal is to find a hypothesis class that achieves a high accuracy on the training set, and has low generalization error on the test set. For this, we require the hypothesis class H to approximate the concept class C which determines the labels for the distribution D. Since both C and D are unknown, we try to model H based on the known sample set S and its labels.

Generalization error: The generalization error of a hypothesis h is the expectation of the error on a sample x picked from the distribution D.

Empirical error: This is the mean of the error of hypothesis h on the sample S of size m.

Having defined the generalization error and empirical error thus, we can state the objective of learning as follows.

The objective of learning is to have the empirical error approximate the generalization error with high probability.

This kind of a learning framework is known as PAC-learning (Probably Approximately Correct). Formally, a concept class C is PAC-learnable if there is some algorithm A for which the generalization error on a sample S derived from the distribution D is very low (less than ε) with high probability (greater than 1- δ). In other words, we can say that for a PAC-learnable class, the accuracy is high with good confidence.

Guarantees for finite hypothesis sets

The PAC-learning framework provides strong guarantees for finite hypothesis sets (i.e., where the size of H is finite). Again, this falls in two categories — the consistent case, and the inconsistent case. A hypothesis class is said to be consistent if it admits no error on the training sample, i.e., the training accuracy is 100%.

Consistent hypothesis

Let us consider a finite hypothesis set H. We want the generalization error to be less than some ε, so we will take a consistent hypothesis h in H, and bound the probability that its error is more than ε, i.e., we are calculating the probability that there exists some h in H, such that h is consistent and its generalization error is more than ε. This is simply the union of all h in H such that it follows the said constraints. By the union bound, this probability will be less than the sum of the individual probabilities i.e.,

From the definition of conditional probability, we can write

which bounds the required probability P as

The condition says that the expectation of error of h on any sample is at least ε, so it would correctly classify a sample with probability at most 1- ε. Hence, to correctly classify m training samples with |H| hypotheses, the total probability is given as

On setting the RHS of the inequality to δ, we obtain the generalization bound of the finite, consistent hypothesis class as

As expected, the generalization error decreases with a larger training set. However, to arrive at a consistent algorithm, we may have to increase the size of the hypothesis class, which results in an increase in generalization error.

Inconsistent hypothesis

In practical scenarios, it is very restrictive to always require a consistent hypothesis class to bound the generalization error. In this section, we look at a more general case where empirical error is non-zero. For this derivation, we use the Hoeffding’s inequality which provides an upper bound on the probability that the mean of independent variables in an interval [0,1] deviates from its expected value by more than a certain amount.

If we take the errors as the random variable, their mean is the empirical error and the expectation is the generalization error. We can then get an upper bound for the generalization error of a single hypothesis h as

However, this is still not the general case since the hypothesis h returned by the learning algorithm is not fixed. Similar to the consistent case, we will try to obtain an upper bound on the generalization error for an inconsistent (but finite) hypothesis, i.e., we need to compute the probability that there exists some hypothesis h in H such that the generalization error of h differs from its empirical error by a value greater than ε. Again, using the union bound, we get

Using the Hoeffdieng’s inequality, this becomes

Now equating the RHS with δ, we can arrive at the result

Here it is interesting to note that for a fixed |H|, to attain the same guarantee as in the consistent case, a quadratically larger labeled sample is required. Let us now analyze the role of the size of hypothesis class. If we have a smaller H, the second term is reduced but the empirical error may increase, and vice versa. However, for the same empirical error, it is always better to go with the smaller hypothesis class, i.e., the famous Occam’s Razor principle.",https://medium.com/explorations-in-language-and-learning/introduction-to-learning-theory-part-1-30db513ce3eb,['Desh Raj'],2018-02-26 14:51:48.588000+00:00,847,"NIPS2017, Alchemy Debate, Statistical Learning Theory, PAC-Learning, Generalization Error"
RAPIDS Release 0.17: The Gift that Keeps on Accelerating,"Additionally, cuML added an experimental release of LARS (Least-Angle Regression), a feature inspired by user requests. Similarly, we added the long-requested GPU acceleration for multi-node multi-GPU logistic regression in dask-glm. Please continue filing feature requests if there are more models you’d like to see!

We continued improving the internals of cuML as well, with 44 bug fix PRs, an overhaul of the Base estimator classes to improve consistency in input and output types, and a large refactoring of CUDA primitives to move more of them out to our low-level RAFT library.

RAPIDS Graph Analytics: cuGRAPH

cuGraph continued to focus on compatibility and interoperability with other libraries and frameworks, including output matching based on input data type. This release added support to accept SciPy and CuPy sparse matrix objects for the Weakly Connected Components, Strongly Connected Components, Single Source Shortest Path, and Breadth First Search algorithms. We improved NetworkX support that was started in the last release and added better support for Pandas and Numpy, including new generic functions for adding edge lists.

We like to include new graph algorithms each release, and this release includes two new algorithms. The first is minimum spanning tree (MST), which also supports maximum spanning trees as well as minimum/maximum spanning forests. The second is the classic Hungarian algorithm for solving assignment problems. This release also included extending Katz centrality to support multi-node multi-GPU processing to enable scaling to huge datasets.

RAPIDS cuXfilter and Visualization

cuXfilter has added datetime and other general improvements to cuXfilter, and it is now able to visualize large graphs via Datashader. You can see some great graph examples in our extensive JupyterCon 2020 viz tutorial notebook.

We are also happy to share the fantastic new linked brushing capability, accelerated by cuDF, in Plotly Dash via holoviews. Read more about it on the Plotly Medium blog and check out the Dash documentation.

RAPIDS Memory Manager (RMM)

RMM has new stream wrapper classes on the C++ side to improve type safety of streams and take steps toward improving stream semantics across RAPIDS. Python stream wrappers are coming in the next release. RMM also added a new tracking resource adaptor which will be helpful in detecting memory leaks. For the first time in this release, RMM now has Python documentation online. And last, but definitely not least, we have published a detailed NVIDIA Developer Blog post about RMM.

Cyber Log Accelerator (CLX)

For this release, CLX has made a number of quality-of-life enhancements and updates to multiple workflows, modules, and notebooks. cyBERT, which lets you parse unstructured logs without the need for regex, now supports ELECTRA models in addition to BERT models. A new module for periodicity detection was added, and the DNS extractor was updated to fix a few bugs. Phishing detection is now a CLX module, and you can view an example notebook to see how to use a BERT model for phishing detection on your own emails. If you’re looking to get started with Streamz or cuStreamz, CLX provides multiple example workflows and a starting Docker image for you to try. There’s even a notebook demonstrating how to use FIL+cuStreamz for your inference needs. The CLX documentation has been expanded and updated, and don’t forget about the cyBERT 2.0 blog that’s out now.

Dask

For this release, we’ve added support for launching Dask + RAPIDS on AWS, GCP, and Azure using the raw VM instance types. This enables access to large GPU instances not currently supported in managed cloud services. We’ve also added novel communication and spilling improvements in Dask-CUDA. Users may now optionally communicate previously spilled data from the GPU without having to deserialize the data on the GPU. While this is still an experimental feature, we have seen significant performance improvements when enabled.

BlazingSQL

This release brings a lot of new features for users as well as many under the hood improvements. There are multiple new SQL statements supported, namely string functions such as REPLACE, TRIM, and UPPER. Users are also now able to create tables directly off compressed text files, directories that implement Hive partitions structure, and text files that are individually larger than GPU memory.

After working on it for months, the new communication layer is now merged. Users can expect improved distributed performance over TCP, and they can begin experimenting with UCX support which enables shuffles over NVLink and Infiniband.

RAPIDS Community

Like I announced in the last blog, we’ve finished our first podcast. We’re really excited about this new way to interact with the community, and we would really appreciate your feedback! We are planning on this being a bi-weekly thing. We’ll be interviewing Bartley Richardson and Rachel Allen about CyBERT, GPU cyber data science, and NLP on the GPU.

Wrapping Up

Data science is important, and our passion for it animates everything we do. But data science is for people. This year has reminded us just how important our relationships and communities are. As the year winds down, I hope all of you can find rest and community with friends and family, even if only via Zoom. I appreciate every one of you that has joined our community, and I wish you a peaceful holiday season.",https://medium.com/rapids-ai/rapids-release-0-17-the-gift-that-keeps-on-accelerating-ad719b34fce9,['Josh Patterson'],2020-12-14 17:11:56.794000+00:00,843,"RAPIDS, cuML, cu Graph, cu Xfilter, Visualization"
The toolkit for the modern data ninja,"Doing data analysis can be fun and rewarding. It’s something I do a lot of in my free time. Without the right tools though, it can be frustrating and extremely time consuming. I break down the process of working with data into 4 steps.

Data gathering: Finding and getting the dataset you are interested in Data cleaning: Getting data into the proper format Data exploration: Finding trends and interesting patterns Data visualization: Visualizing the awesome trends you’ve found

Data gathering

The process of gathering data has gotten significantly better in the last 5 to 10 years. It’s now possible to find a huge number of datasets online.

Kaggle

Kaggle introduced a new Datasets feature in 2016, and it has quickly become my favorite place to browse and explore datasets. It allows you to upload your own datasets as well as freely access others. Many people create their own “kernels” which are little scripts to tell a story / analysis about a certain dataset. The caveat of this source is that it’s kind of a free for all and some of the datasets aren’t well documented and the source of the data isn’t clear.

A visualization of a Pokemon dataset found on Kaggle. Don’t ask me to interpret it.

Google BigQuery

Another big player that has really come to fruition in the past few years is Google BigQuery. They host a number of huge, public datasets. Additionally, it’s easy to explore the data via SQL, often times only costing pennies.

Data.gov

Data.gov is a great place to start searching for data involving the government. I’ve found the site to be somewhat hit or miss, often linking me to some unworkable government website. The US government though is getting serious about open data, and this will be tool that I’m sure will improve with time.

Another player in the open data for government realm is Socrata. Look for large city governments to often host their data with them. Some examples include NYC Open Data and the Chicago Data Portal.

Reddit

/r/datasets can often have some very new and nifty datasets. You can also post a request for a piece of information or dataset and occasionally you will get a response.

Another hack that I use occasionally is to browse through /r/dataisbeautiful. All OC (Original Content) posts are required to include in a comment where they got their dataset from.

Awesome Public Datasets

The github repository awesome-public-datasets has links to many types of datasets, aggregated by category.

Scrapy

Sometimes the best data isn’t available via a download button or an easily accessible API. I’ve tried multiple web scrapers, and time and again, I return to Scrapy. If you have programming skills or aren’t afraid to dive into a little bit of python, Scrapy is a very approachable web scraping tool that works well and has great documentation and tooling around it.

My favorite feature is the scrapy shell <url> which will scrape a web page and open a REPL for you to run python commands against until you’ve determined the set of commands required to get the data you’re interested in.

Google

This one is obvious, but still worth mentioning. There are loads of other resources on the web. Googling whatever you’re looking for plus the word “dataset” is a good place to start.

Freedom of Information Act (FOIA)

Last but not least, if there is some data you really want to get your hands on, you can submit a FOIA request. This law allows you to request data from any federal agency, and they are required to hand it over, unless it falls under an exemption.",https://towardsdatascience.com/a-data-ninjas-toolkit-abfe11d38fe8,['Ben Rudolph'],2017-09-29 16:46:38.882000+00:00,581,"Data gathering, Kaggle, Google Big Query, Data.gov, Reddit"
Unsupervised and supervised learning for customer segmentation — Part 2,"The Task

The evaluation metric used for this competition was the AUC score, which tells us how well a model can distinguish between two diagnostic groups, in our case would be the individuals that are prospects of becoming new members to the ones that are not.

The dataset involved is composed of 42,962 observations (individuals) and 367 features (characteristics) from which, only 532 observations where people that became actual customers, which is extremely low, to put it in context, this would mean that the probability of picking one person at random, send him the campaign, and him then becoming a new customer is about 0.012 percent!

With this, we can say that the dataset is really imbalanced, and to illustrate this more, after preprocessing (which you can find the code for it here) I decided to build a Decision Tree Classifier and apply it into the data, which would usually perform well in this type of tasks, but the AUC score that I got from the model was only about~0.51, which is almost the lowest anyone can get from the metric, meaning that the model is doing awful at trying to distinguish one class from another.

#Untuned Decision Tree Classifier

DT = DecisionTreeClassifier()

DT.fit(X_train, y_train)

pred_dt = DT.predict_proba(X_test)[:,1]

roc_auc_score(y_test, pred_dt) Result:

Untuned Decision Tree score: 0.51404593009540434

The Model

To address this issue, another approach must be taken. The way I did it, was to train several algorithms, adjust their class_weight hyperparameter when possible, and check their AUC performance, in order to build a more robust model with the information gathered by them.

An extra note here: As the data is imbalanced, one can make use of re-sampling techniques, I personally tried a few approaches, but with no luck, so a decided and went with the presented model, which at the end, gave me the results i was looking for.

# Initialize the classifiers

logreg = LogisticRegression(random_state=123, penalty = 'l1',

C = 4.0, class_weight={0:1, 1:41})

Ada = AdaBoostClassifier(learning_rate = 0.1, random_state = 123)

NB = GaussianNB()

Tree = DecisionTreeClassifier(min_samples_split = 10,

min_samples_leaf = 15, random_state =42,

max_depth=5, class_weight='balanced')

RF = RandomForestClassifier(n_estimators=300, max_depth=5,

max_features = 'sqrt', min_samples_leaf=15,

min_samples_split=8,random_state = 123,

class_weight={0:1, 1:41})

GBoost = GradientBoostingClassifier(n_estimators=1000,

learning_rate=0.003, max_depth=5,

max_features = 'sqrt', min_samples_leaf=15,

min_samples_split=10, loss='deviance',

random_state = 42)

GBoostr = GradientBoostingRegressor(random_state = 42)

Lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005,

random_state=1)) #function for getting classifiers auc

def fit_pred_auc(model):

model.fit(X_train, y_train)

pred = model.predict_proba(X_test)[:,1]

auc = roc_auc_score(y_test, pred)

return auc #function for getting linear model auc

n_folds = 5

def auc_cv(model):

kf = KFold(n_folds, shuffle=True,

random_state=42).get_n_splits(X_train.values)

auc= cross_val_score(model, X_train.values, y_train,

scoring=""roc_auc"", cv = kf)

return(auc) score = auc_cv(GBoostr)

score = fit_pred_auc(logreg)

score = auc_cv(Lasso)

score = fit_pred_auc(NB)

score = fit_pred_auc(Ada)

score = fit_pred_auc(Tree)

score = fit_pred_auc(RF)

score = fit_pred_auc(GBoost)

Results: GBoostr score: 0.7386



Logistic Regression score: 0.6104



Lasso score: 0.6546



Naive Bayes score: 0.6382



Ada Boost score: 0.7626



Decision Tree score: 0.7328



Random Forest score: 0.7431



Gradient Boosting score: 0.7640

As seen above, the best AUC scores I got were from the Gradient Boosting Trees Classifier, and the AdaBoost Classifier, this comes as no surprise, as ensembling algorithms tend to perform better than single algorithms (except for neural networks) which are clearly explained in this article.

From here, there are several approaches to go for (stacking techniques, ensembling, hyperparameter tuning, neural networks, etc), I decided to go for a simple average stacking model using the top two best performers indicated earlier, as I feel that combining the information from these two models could give my final score the extra boost needed for improving.

#stacking class

class StackingModels(BaseEstimator, ClassifierMixin,

TransformerMixin):

def __init__(self, models):

self.models = models



# define clones to fit the data

def fit(self, X, y):

self.models_ = [clone(x) for x in self.models]



# Training

for model in self.models_:

model.fit(X, y) return self



#predictions

def predict(self, X):

predictions = np.column_stack([

model.predict(X) for model in self.models_

])

return np.mean(predictions, axis=1)



def predict_proba(self, X):

predictions_proba = np.column_stack([

model.predict_proba(X)[:,1] for model in self.models_

])

return [np.mean(i) for i in predictions_proba] #stacking model

stacked_models = StackingModels(models = (Ada, GBoost))

stacked_models.fit(X_train, y_train)

preds_sm = stacked_models.predict_proba(X_test) Results:

preds_sm AUC score: 0.7737

The final AUC score I got from the stacking model was 0.7737 which is a huge improvement from the ~0.51 AUC score of the normal approached Decision Tree at the beginning, and also it is an improvement from the top previous model score, which was the Gradient Boosting Tree Classifier with an AUC score of 0.7640.

After choosing this as my final model, I proceeded to use it and predict on the given test data, which positioned me at the 15th place in the leaderboard with an AUC score of .79930, having the top score at .8081.

Further improvements

As mentioned earlier, the way build the final model is not the only option, as explained, there are another couple of approaches that one can use to improve the score, the first and most straightforward, would be to tune the hyperparameters of the models by using grid-search for example, and re-use my stacking class. Another approach can be to create a different architecture of the stacking class, by adding meta-learners as explained in this article by Marios Michailidis (KazAnova). Also, if you are familiar with deep learning libraries or even with sci-kit learn, you can create a neural network model, which are known to outperform many classic ML algorithms in these type of tasks.

Conclusion

This project was about finding the groups of individuals within the general population, and distinguish the ones that contain new potential customers, to the ones that not, using machine learning techniques.

There were two different approaches to each different task presented to solve the problem, the first was unsupervised learning and the second supervised learning, and in both the results were really satisfactory, as in the first one, the model could find the characteristics of the group that make up the core customer base of the company, and also the characteristics of the group that the company should avoid.

For the second task, the model build did a pretty decent job at learning to distinguish the individuals that are going to become new customers to the ones that not, and was able to throw predictions that ultimately positioned me in a good rank in the competition.

With this, I wrap up the two-part series story, I hope you liked it, and if you have any suggestions or thoughts, you can put them in the comments below, they will be very much appreciated.",https://medium.com/@alejandrogalindomedina/unsupervised-and-supervised-learning-for-customer-segmentation-part-2-caf21770642e,['Alejandro Galindo Medina'],2019-07-19 15:33:17.016000+00:00,978,"Machine Learning, Decision Tree Classifier, Logistic Regression, Naive Bayes, Ada Boost Classifier"
Expectation-Maximization Algorithm on Python,"The K-means approach is an example of a hard assignment clustering, where each point can belong to only one cluster. Expectation-Maximization algorithm is a way to generalize the approach to consider the soft assignment of points to clusters so that each point has a probability of belonging to each cluster.

Gaussian Mixture Model

We assume that each cluster Ci is characterized by a multivariate normal distribution, that is,

where the cluster mean and covariance matrix are both unknown parameters. f(x) is the probability density at x attributable to cluster Ci. Assuming that the probability density function of X is given as a Gaussian mixture model over all the k cluster normals, defined as

where the prior probabilities P(Ci ) are called the mixture parameters, which must satisfy the condition

The Gaussian mixture model is thus characterized by the mean, the covariance matrix, and the mixture probability for each of the k normal distributions.

Maximum Likelihood Estimation

Given the dataset D, we define the likelihood of θ as the conditional probability of the data D given the model parameters θ, denoted as P(D|θ ). Because each of the n points xj is considered to be a random sample from X (i.e., independent and identically distributed as X), the likelihood of θ is given as

The goal of maximum likelihood estimation (MLE) is to choose the parameters θ that maximize the likelihood, that is,

It is typical to maximize the log of the likelihood function because it turns the product over the points into a summation and the maximum value of the likelihood and log-likelihood coincide. That is, MLE maximizes

where the log-likelihood function is given as

Directly maximizing the log-likelihood over θ is hard. Instead, we can use the expectation-maximization (EM) approach for finding the maximum likelihood estimates for the parameters θ. EM is a two-step iterative approach that starts from an initial guess for the parameters θ. Given the current estimates for θ, in the expectation step EM computes the cluster posterior probabilities P(Ci |xj ) via the Bayes theorem:

The posterior probability of Ci given xj is thus given as

and P(Ci |xj ) can be considered as the weight or contribution of the point xj to cluster Ci. Next, in the maximization step, using the weights P(Ci |xj ) EM re-estimates θ, that is, it re-estimates the parameters for each cluster. The re-estimated mean is given as the weighted average of all the points, the re-estimated covariance matrix is given as the weighted covariance over all pairs of dimensions, and the re-estimated prior probability for each cluster is given as the fraction of weights that contribute to that cluster.

This is a brief overview of the EM algorithm, now let's look at the python code for 2 component GMM.

Importing the required packages

import numpy as np # import numpy

from numpy.linalg import inv # for matrix inverse

import matplotlib.pyplot as plt # import matplotlib.pyplot for plotting framework

from scipy.stats import multivariate_normal # for generating pdf

Generating our data

m1 = [1,1] # consider a random mean and covariance value

m2 = [7,7]

cov1 = [[3, 2], [2, 3]]

cov2 = [[2, -1], [-1, 2]] x = np.random.multivariate_normal(m1, cov1, size=(200,)) # Generating 200 samples for each mean and covariance

y = np.random.multivariate_normal(m2, cov2, size=(200,)) d = np.concatenate((x, y), axis=0)

Plotting the ground truth

plt.figure(figsize=(10,10))

plt.scatter(d[:,0], d[:,1], marker='o')

plt.axis('equal')

plt.xlabel('X-Axis', fontsize=16)

plt.ylabel('Y-Axis', fontsize=16)

plt.title('Ground Truth', fontsize=22)

plt.grid()

plt.show()

Taking initial guesses for the parameters

m1 = random.choice(d)

m2 = random.choice(d)

cov1 = np.cov(np.transpose(d))

cov2 = np.cov(np.transpose(d))

pi = 0.5

Plotting Initial State

x1 = np.linspace(-4,11,200)

x2 = np.linspace(-4,11,200)

X, Y = np.meshgrid(x1,x2)



Z1 = multivariate_normal(m1, cov1)

Z2 = multivariate_normal(m2, cov2)



pos = np.empty(X.shape + (2,)) # a new array of given shape and type, without initializing entries

pos[:, :, 0] = X; pos[:, :, 1] = Y



plt.figure(figsize=(10,10)) # creating the figure and assigning the size

plt.scatter(d[:,0], d[:,1], marker='o')

plt.contour(X, Y, Z1.pdf(pos), colors=""r"" ,alpha = 0.5)

plt.contour(X, Y, Z2.pdf(pos), colors=""b"" ,alpha = 0.5)

plt.axis('equal') # making both the axis equal

plt.xlabel('X-Axis', fontsize=16) # X-Axis

plt.ylabel('Y-Axis', fontsize=16) # Y-Axis

plt.title('Initial State', fontsize=22) # Title of the plot

plt.grid() # displaying gridlines

plt.show()

Expectation Step

##Expectation step

def Estep(lis1):

m1=lis1[0]

m2=lis1[1]

cov1=lis1[2]

cov2=lis1[3]

pi=lis1[4]



pt2 = multivariate_normal.pdf(d, mean=m2, cov=cov2)

pt1 = multivariate_normal.pdf(d, mean=m1, cov=cov1)

w1 = pi * pt2

w2 = (1-pi) * pt1

eval1 = w1/(w1+w2)



return(eval1)

Maximization Step

## Maximization step

def Mstep(eval1):

num_mu1,din_mu1,num_mu2,din_mu2=0,0,0,0



for i in range(0,len(d)):

num_mu1 += (1-eval1[i]) * d[i]

din_mu1 += (1-eval1[i])



num_mu2 += eval1[i] * d[i]

din_mu2 += eval1[i]



mu1 = num_mu1/din_mu1

mu2 = num_mu2/din_mu2



num_s1,din_s1,num_s2,din_s2=0,0,0,0

for i in range(0,len(d)):



q1 = np.matrix(d[i]-mu1)

num_s1 += (1-eval1[i]) * np.dot(q1.T, q1)

din_s1 += (1-eval1[i])



q2 = np.matrix(d[i]-mu2)

num_s2 += eval1[i] * np.dot(q2.T, q2)

din_s2 += eval1[i]



s1 = num_s1/din_s1

s2 = num_s2/din_s2



pi = sum(eval1)/len(d)



lis2=[mu1,mu2,s1,s2,pi]

return(lis2)

Function to plot the EM algorithm

def plot(lis1):

mu1=lis1[0]

mu2=lis1[1]

s1=lis1[2]

s2=lis1[3]

Z1 = multivariate_normal(mu1, s1)

Z2 = multivariate_normal(mu2, s2)



pos = np.empty(X.shape + (2,)) # a new array of given shape and type, without initializing entries

pos[:, :, 0] = X; pos[:, :, 1] = Y



plt.figure(figsize=(10,10)) # creating the figure and assigning the size

plt.scatter(d[:,0], d[:,1], marker='o')

plt.contour(X, Y, Z1.pdf(pos), colors=""r"" ,alpha = 0.5)

plt.contour(X, Y, Z2.pdf(pos), colors=""b"" ,alpha = 0.5)

plt.axis('equal') # making both the axis equal

plt.xlabel('X-Axis', fontsize=16) # X-Axis

plt.ylabel('Y-Axis', fontsize=16) # Y-Axis

plt.grid() # displaying gridlines

plt.show()

Calling the functions and repeating until it converges

iterations = 20

lis1=[m1,m2,cov1,cov2,pi]

for i in range(0,iterations):

lis2 = Mstep(Estep(lis1))

lis1=lis2

if(i==0 or i == 4 or i == 9 or i == 14 or i == 19):

plot(lis1)

I hope you like the article and this will somehow make the EM algorithm a bit clear in understanding.

If you like this article, leave the comments or send me some 👏👏👏.",https://medium.com/@prateek.shubham.94/expectation-maximization-algorithm-7a4d1b65ca55,['Prateek Kumar'],2019-09-01 10:44:58.821000+00:00,839,"K-means Clustering, Expectation-Maximization Algorithm, Gaussian Mixture Model, Maximum Likelihood Estimation, Python Code"
Real-Time Tweet Analysis for Covid-19 Vaccine using Python,"Photo by Joshua Hoehne on Unsplash

Real-time tweets about the covid-19 vaccine have been analyzed in this article. My analyzes give answers to whether people think positive or negative about the covid-19 vaccine, or are neutral. You will see which words are tweeted the most with the covid-19 vaccine. Also, tweets about the types of vaccines were compared.

I got the tweets with my Twitter developer account and used Python.

The Data

I started with store authentication credentials in relevant variables. I limited the last 1000 tweets.

I filtered tweets to track only these keywords; ‘covid19 vaccine’, ‘covid-19 vaccine’, ‘coronavirus vaccine’, ‘covid19 vaccines’, ‘covid-19 vaccines’, ‘coronavirus vaccines’, ‘vaccine’, ‘vaccines’.

After printing, the keys of the first tweet dict has been seen. It includes created time, id, location, user, status, in reply to status id, in reply to the user id, retweeted status, coordinates, retweeted count, reply count, favorite count, etc. In this case, the text and language were added as a feature.

Let’s quick glance at the data.

Figure 1: The data frame.

Figure 2

Figure 2 shows an example of a tweet about the vaccine. The first row shows the tweet, it’s been retweeted from the user of dsa_bill. The user id is shown in the second row and, respectively, his / her user name, the number of followers of this user, the location, and the description of the account were shown.

Sentiment Analysis with Vader

Sentiment Analysis falls into the broad category of text classification, the classifier is supposed to tell if the sentiment behind that is positive, negative, or neutral.

Figure 3: An example of positive and negative tweets.

Figure 4: The new data frame.

The feature of Sentiment Score, Subjectivity, Polarity, and Analysis added to the data frame. Here are the number of Positive, Neutral, and Negatives tweets.

Figure 5

It’s more clear to see the difference with a graph.

Figure 6: Sentiment Analysis graph.

The tweets about Pfizer, Moderna, and Coronavac compared. Here is the proportion:

Word Cloud

The most-tweeted words in texts containing the covid-19 vaccine are possible to see with word cloud. To make the word cloud clear, I stopped these words; “covid”, “covid19”, “vaccines”, “vaccine”, “https”, “will”, “coronavirus”, “rt”. Words limited to 20.

Figure 7: The word cloud.

Conclusion

As a result, the analysis was made over the last 1000 tweets about the covid-19 vaccine. There are 518 neutral, 348 positive, and 134 negative tweets. The proportion of tweets mentioning Pfizer is 0.05%, 0.015% for Moderna and 0.003% for Coronavac.

Thank you for reading.

Please don’t hesitate to give me feedback.",https://medium.com/analytics-vidhya/real-time-tweet-analysis-for-covid-19-vaccine-using-python-1f9e32af2abd,['Ayça Erbaşı'],2021-02-15 16:13:07.153000+00:00,403,"Covid19Vaccine, Coronavirus Vaccine, Sentiment Analysis, Word Cloud, Twitter Developer Account"
How to code Logistic Regression from scratch with NumPy,"What’s our plan for implementing Logistic Regression in NumPy?

Let’s first think of the underlying math that we want to use.

There are many ways to define a loss function and then find the optimal parameters for it, among them, here we will implement in our LogisticRegression class the following 3 ways for learning the parameters:

We will rewrite the logistic regression equation so that we turn it into a least-squares linear regression problem with different labels and then, we use the closed-form formula to find the weights:

Like above, we turn logistic into least-squares linear regression, but instead of the closed-form formula, we use stochastic gradient descent with the following gradient:

We use the maximum likelihood estimation (MLE) method, write the likelihood function, play around with it, restate it as a minimization problem, and apply SGD with the following gradient:

In the above equations, X is the input matrix that contains observations on the row axis and features on the column axis; y is a column vector that contains the classification labels (0 or 1); f is the sum of squared errors loss function; h is the loss function for the MLE method.

To find out more about the above methods check out this article:

So, this is our goal: translate the above equations into code. And we’ll use NumPy for that.

We plan to use an object-oriented approach for implementation. We’ll create a LogisticRegression class with 3 public methods: fit() , predict() , and accuracy() .

Among fit’s parameters, one will determine how our model learns. This parameter is named method (not to be confused with a method as a function of a class) and it can take the following strings as values: ‘ols_solve’ (OLS stands for Ordinary Least Squares), ‘ols_sgd’, and ‘mle_sgd’.

To not make the fit() method too long, we would like to split the code into 3 different private methods, each one responsible for one way of finding the parameters.

We will have the __ols_solve() private method for applying the closed-form formula.

In this method and in the other methods that use the OLS approach, we will use the constant EPS to make sure the labels are not exactly 0 or 1, but something in between. That’s to avoid getting plus or minus infinity for the logarithm in the equations above.

In __ols_solve() we first check if X has full column rank so that we can apply this method. Then we force y to be between EPS and 1-EPS. The ols_y variable holds the labels of the ordinary least-squares linear regression problem that’s equivalent to our logistic regression problem. Basically, we transform the labels that we have for logistic regression so that they are compliant with the linear regression equations. After that, we apply the closed-form formula using NumPy functions.

For the 2 SGD-based algorithms, it would be redundant to have them as 2 separate methods since they will have almost all the code the same except for the part where we compute the gradient, as we have 2 different gradient formulas for them.

What we’ll do is to create a generic __sgd() method that does not rely on a particular way of computing the gradient. Instead, it will expect as a parameter a function responsible for computing the gradient which the __sgd() method will use.

In this method, we first initialize the weights to a random column vector with values drawn from a normal distribution with mean 0 and a standard deviation of 1/(# of features). The intuition for this std dev is that if we have more features, then we need smaller weights to be able to converge (and not blow up our gradients). Then we go through all the dataset for iterations times. At the start of each such iteration, we randomly shuffle our dataset, then for each batch of data, we compute the gradient and update the weights.

For ‘ols_sgd’ and ‘mle_sgd’ we’ll create 2 private methods: __sse_grad() and __mle_grad() that compute and return the gradient for these 2 different techniques.

For these 2 methods, we simply apply the formulas for ∇f and ∇h using NumPy.

So, when fit() is called with method=‘ols_solve’ we call __ols_solve() , when method=‘ols_sgd’ we call __sgd() with grad_fn=self.__sse_grad , and when method=’mle_sgd’ we call __sgd() with grad_fn=self.__mle_grad .

In predict() we first check if fit() was called previously by looking for the weights attribute (the fit method is the only method that creates it). Then we check if the shapes of the input matrix x and weights vector allow multiplication. Otherwise, return error messages. If everything is OK, we do the multiplication and pass the result through the logistic function.

In accuracy() we make predictions using the above method. Then check if the shape of the predictions matches that of the true labels, otherwise, we show an error message. After that we make sure that both predictions and the true labels have values of either 0 or 1 by a simple rule: if the value is >= 0.5 consider it a 1, otherwise a 0.

To compute the accuracy, we check for equality between y and y_hat. This will return a vector of Boolean values. Then cast these Booleans to float (False becomes 0.0, and True becomes 1.0). Then, the accuracy is simply the mean of these values.",https://towardsdatascience.com/how-to-code-logistic-regression-from-scratch-with-numpy-d33c46d08b7f,['Dorian Lazar'],2020-11-28 16:32:10.091000+00:00,860,"logistic regression, Num Py, least-squares linear regression, closed-form formula, stochastic gradient descent"
How I became a Kaggle Expert in 10 days and how you can too!!,"As of now, There are 5 ranks in kaggle given on the basis of your performance in 4 areas:

Competitions Datasets Kernels Discussion

As a beginner, I would personally suggest you guys jump right into the competitions, try to get your hands dirty, and then see how other people approached the same problem.

However, Since there is a lot of competition these days in Kaggle, So if you want to take a staircase approach and progress fast in Kaggle then Keep reading.

For this, I’d suggest you to first Work through 2–3 basic competitions from start to finish without worrying too much about this for now.

Get a good grip on how general discussions and how people use kernels.

Help Others: What I did was, I devoted daily One hour of my time in Kaggle discussions. Since there are several beginners starting their data science journey on Kaggle, you can answer most of their basic queries and if they like your answer… Boom Upvote!!

Be curious: As I practiced Data science and read notebooks of other Kagglers, I used to post queries regularly on what I found difficult and If any of the other people are having the same Questions, they used to upvote my query.

Help Others: One of the most important things I did was to share the resources I found useful from which others can learn and benefit. In these 10 days, I spent quite some time to find useful resources from all over the web and shared them with my fellow Kagglers.

People started following me as they found my shared content useful and as a result, People also started reading my notebooks on EDA which helped me to progress in Kernels too.

I signed up for Kaggle nearly 6 months ago and left it as it is since I was quite busy with my college assignments, After my Semester examinations I started learning data science full time.

I logged in on Kaggle and ran my first Kaggle script on 29 Oct 2020 and on 6 Nov 2020, I got my Discussions Expert Badge.

Here are a few key points:

Follow leaderboard toppers and major contributors. Contribute to the community by making kernels and answering queries. Share what you know. Upvote and appreciate the help. Mutual benefits are a quick way to up the game :)

Hope this helps

Cheers!",https://medium.com/gaurav-yadav/how-i-became-a-kaggle-expert-in-10-days-and-how-you-can-too-a2ae7cc6fc97,['Gaurav Yadav'],2020-12-15 02:59:59.665000+00:00,379,"Kaggle, Competitions, Datasets, Kernels, Discussion"
Mathematics One of the Most Essential Branch of AI,"Mathematics creating models to help AI deliver faster better quality products.

People often find it difficult to understand why mathematics is necessary for AI. AI is related to machine algorithms that deliver information from data. The main purpose of AI is to deliver that data in a user-friendly way and without any complications. And that’s where mathematics steps in to create better products and deliver great services. Thus, mathematics is the foundation where it grows. in more simple language MATHEMATICS IS THE BASE OF AI.as these products are created by using ideas and solutions from mathematics.

MATHEMATICAL BRANCHES IN AI

There are five branches of mathematics used in AI. They are calculus, statistics, linear algebra, probability and graphics.

1. LINEAR ALGEBRA

In linear algebra, we work around algebraic concepts like Tensors, Vectors, Arrays, Scalars and Matrices. Here vectors, matrices and arrays are the most important concepts. you can use them to find secret information as they are the data languages. Even tensors and scalars are important as tensors are N-dimensional matrices and a scalar is a single number.

2. GRAPHICS

Graphics plays an important role in statistics, artificial intelligence and math. as it works with simple straight lines represented in graphical form with the help of input and output data. The function is represented by f(x)=x, where x is input data.

3. PROBABILITY

In simple language, the probability is something that works with possibility and impossibility, likely something that could occur. In a different way, here Probability is a number between 0 to 1. Where 1 represents possibilities and 0 impossibilities.

4. CALCULUS

Calculus normally revolves around following concepts like Error Minimization, Multivariate calculus, logistic regressions, differential calculus and Integral calculus for delivering a better finished and dependable product.

5. STATISTICS

Statistics work with the simple concept of gathering the information, analyzing it, interpreting it accordingly and representing it in the form of charts, pie diagrams, graphical ways to understand. The curves in a simpler and better manner.

Hence we can conclude that mathematics is an integral part of AI.",https://medium.com/appengine-ai/mathematics-one-of-the-most-essential-branch-of-ai-4df08f8eff18,['Rupika Nimbalkar'],2021-06-07 16:36:26.465000+00:00,326,"AI, Mathematics, Calculus, Statistics, Linear Algebra"
Drug classification — on cAInvas. Training a deep learning model to…,"Drug classification — on cAInvas

Training a deep learning model to prescribe a drug based on the patient’s data.

A prescription drug is one that requires a medical prescription to be dispensed by law. On the other hand, an over-the-counter drug is one that can be dispensed without a prescription.

When it comes to the prescription of drugs, doctors look into various attributes of patient-related data before coming to a conclusion. This can have consequences varying from the efficiency of the medicine in the patient’s body to side effects caused and incorrect prescriptions may in some cases lead to irrevocable effects in patients (including death).

To start with, can we train a deep learning model to prescribe medicines to patients based on their medical data? Read on to find out!

Implementation of the idea on cAInvas — here!

The dataset

The dataset is a CSV file with the features regarding a patient that affects drug prescriptions like age, sex, BP level, cholesterol, and sodium-potassium ratio and the corresponding drug prescribes in each case.

Preprocessing

Balancing the dataset

Looking into the classes and the spread of values among them in the dataset —

There are two ways to balance the dataset —

upsampling — resample the values to make their count equal to the class label with the higher count (here, 91).

downsampling — pick n samples from each class label where n = number of samples in class with least count (here, 16)

Here, we will be upsampling.

Dataset upsampling

The replace parameter of .sample() is set to True to indicate that samples can be repeated in each class to achieve the given count. The df_balanced data frame has 455 samples, 91 of each class.

Categorical variables

The ‘sex’ column does not define a range and thus is one-hot encoded while changing from a categorical attribute to a numerical attribute. This means if there are n unique values in the column, an array of length n is created for each where only the ith value is set to 1 with reference to an array that defines the indices of the column values in the array.

In many cases (mostly in the input columns), if there are n unique values, an array of length n-1 is created as the extra column can be redundant for identifying the column value from the encoded array. This is achieved by setting the drop_first parameter as True in the get_dummies() function as shown in the code cell below.

Since this column has only 2 unique values in the data frame, there will not be any difference between one-hot encoding and label encoding the column.

The values in the columns Cholesterol and BP represent range-kind values as seen by the values below.

These columns are label encoded instead of One-hot encoding, i.e, each value is replaced by a numeric value.

Since this is a classification problem, the output of the model which is now as an integer should be one-hot encoded.

Onehot encoding

Snapshot of the dfx data frame

Train-test split

Using an 80–10–10 ratio to split the data frame into train- validation- test sets. These are then divided into X and y (input and output) for further processing.

Train test split

The training set has 364 samples while the validation set has 45 and the test set has 46 samples.

Scaling the values

A peek into the snapshot of the dfx data frame makes it evident that the columns have values in different ranges. The min-max scaler can be used to scale the values between the minimum and maximum values defined (min-0, max-1 by default).

Minmax scalar

The MinMaxScaler function of the sklearn.preprocessing module is used. Logically, the training set is the only data we are allowed to see or work with while training the model while the other two are used to evaluate its performance, the MinMaxScaler object is fit on the train data and the fitted model is used to transform the data in all the three datasets.

The model

The model is a simple one consisting only of Dense layers.

Drug classification model

The model is compiled using the Cross-entropy loss function because the final layer of the model has the softmax activation function and the labels are one-hot encoded. The Adam optimizer is used and the accuracy of the model is tracked over epochs.

The EarlyStopping callback function monitors the validation loss and stops the training if it doesn’t decrease for 8 epochs continuously. The restore_best_weights parameter ensures that the model with the least validation loss is restored to the model variable.

The model is trained with a learning rate of 0.01 for 64 epochs but the model stops before that due to the callbacks.

The model achieved 100% accuracy on the test set.

In problems such as these, it is important to keep the accuracy extremely high (100%) as chances cannot be taken with a patient’s medication.

The metrics

The plot of accuracies

The plot of losses

Prediction

Let’s perform predictions on random test data samples —

Drug classification prediction

Find the implementation of the print_sample() function in the notebook link above!

Random tests sample prediction

deepC

deepC library, compiler, and inference framework are designed to enable and perform deep learning neural networks by focussing on features of small form-factor devices like micro-controllers, eFPGAs, CPUs, and other embedded devices like raspberry-pi, odroid, Arduino, SparkFun Edge, RISC-V, mobile phones, x86 and arm laptops among others.

Compiling the model using deepC —

Head over to the cAInvas platform (link to notebook given earlier) and check out the predictions by the .exe file!

Credits: Ayisha D",https://medium.com/ai-techsystems/drug-classification-on-cainvas-18e6471df32a,['Ai Technology'],2020-12-23 08:24:09.082000+00:00,879,"Drug Classification, Deep Learning Model, Prescription Drugs, Medical Data, Categorical Variables"
[Tensorflow 2.0]Convolutional Neural Network (CNN),"IV. Create the Convolutional Base

i) First, we would like to simplify the way of constructing a model.

The following is the full model that we will be creating throughout this post.

model = tf.keras.Sequential([

tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),

tf.keras.layers.MaxPooling2D((2,2)),

tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),

tf.keras.layers.MaxPooling2D((2, 2)),

tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),

tf.keras.layers.Flatten(),

tf.keras.layers.Dense(64,activation='relu'),

tf.keras.layers.Dense(10,activation='softmax')

])

However, we can employ model.add method, so that we can skip repeating tf.keras.layers. The idea is that after creating a vacant sequential model, we will be adding layers.

model = models.Sequential() model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))

model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))

model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))

ii) What are convolution and MaxPooling?

1 — Convolution: the process of applying a kernel or filter to an image.

layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3))

input_shape=(32,32,3)

We have covered this! we have an image with size image_height=32 , image_width=32 , with color_channels equal to 3 (color).

We have covered this! we have an image with size , , with color_channels equal to 3 (color). activation=‘relu’

relu is an abbreviation of Rectified Linear Unit and it is most commonly used activation function. This function returns 0 if the input is smaller or equal to 0 and if the input is positive, then the output will be equal to the input. It facilitates the ‘non-linear’ problems.

Whenever you are confused which activation function you have to employ, ‘relu’ is waiting for you.

is an abbreviation of and it is most commonly used activation function. This function returns 0 if the input is smaller or equal to 0 and if the input is positive, then the output will be equal to the input. It facilitates the ‘non-linear’ problems. Whenever you are confused which activation function you have to employ, ‘relu’ is waiting for you. (3,3): kernel is 3*3 grid

32 is the number of kernel filters

[Example] For simplicity, we will take a 6 by 6 pixel image with grey scale in which each pixel has a number between 0 (black) to 255 (white).

2*1 + 5*2+4*1 + 13*2 + 25*4 +15*2 +8*1+6*2+6*1=198

Then what about the values in the margin? Zero-padding can be an option.

We will imagine the 2 by 2 matrix is surrounded by 0s. i.e.,

0 0 0

0 1 0

0 8 2

0*1 +0*2+0*1+0*2+1*4+0*2+0*1+8*2 +2*1=4+16+2=22.

In the coding, we can do the zero-padding by assigning padding= ‘same’

i.e., model.add(layers.Conv2D(32, (3, 3), padding=’same’, activation=’relu’, input_shape=(32, 32, 3)))

zero-padding

Also, we can have ‘no padding’ which will simply not consider the values at the margin by not defining the padding variable (like in our example) or by defining padding= ‘valid’. In no-padding case, we can see that the size of the output is decreased to 4 by 4.

No padding

Although we are looking at an arbitrary kernel, depending on the types of filters, they all have names. Also, since we do not have negative or zero values in our convoluted grid, there is no room for ‘RELU’ but if there were it will interfere.

We can say that the training is a process of finding the best 3 by 3 kernel/filter. Each numbers in the filters is conceptually same as the weights in a linear regression.

2 — MaxPooling: pooling the maximum value in a convoluted image. It is a method of downsampling.

layers.MaxPooling2D((2, 2))

We have created a convoluted image grid, then we will be moving around our 2 by 2 kernel and pick the maximum value.

stride variable can also be assigned which is the number of pixels that the 2 by 2 kernel will slide across the convoluted image.

Can you see that the size of the new image decreased by half?",https://medium.com/a-ydobon/tensorflow-2-0-convolutional-neural-network-cnn-4f90ddc3109e,['A Ydobon'],2019-10-08 13:52:22.323000+00:00,568,"Convolution, Max Pooling, ReLU, Zero-Padding, Kernel Filters"
Recognizing Handwritten Digits with scikit-learn,"The handwritten digit recognition is the ability of computers to recognize human handwritten digits. It is a hard task for the machine because handwritten digits are not perfect and can be made with many different flavors. The handwritten digit recognition is the solution to this problem which uses the image of a digit and recognizes the digit present in the image.

Recognizing handwritten text is a problem that can be traced back to the first automatic machines that needed to recognize individual characters in handwritten documents. Think about, for example, the ZIP codes on letters at the post office and the automation needed to recognize these five digits. Perfect recognition of these codes is necessary in order to sort mail automatically and efficiently. Here we are going to analyze the digits data-set of the Sci-Kit learn library using Jupyter Notebook.

First we begin with importing the required libraries.

Now we load the Digits dataset into the notebook. After loading the data we can read lots of information about the datasets by calling the DESCR attribute.

The images of the handwritten digits are contained in a digits.images array. Each element of this array is an image that is represented by an 8x8 matrix of numerical values that correspond to a grayscale from white, with a value of 0, to black, with the value 15.

Let’s visually check the contents of this result using the matplotlib library.

The numerical values represented by images, i.e. the targets, are contained in the digit.targets array.

It is observed that the dataset is a training set consisting of 1,797 images.

This dataset contains 1,797 elements, and so we can consider the first 1,791 as a training set and will use the last six as a validation set. Here we can see in detail these six handwritten digits by using the matplotlib library:

Now we train the svc estimator that we defined earlier.

Now we have to test our estimator, making it interpret the six digits of the validation set and then compare them with the actual digits as follows:

Testing with a sample image

From the above output we can see that the targeted and predicted values are the same. Hence estimator is able to recognize the handwritten digits, interpreting correctly all six digits of the validation set.

Let’s predict the accuracy of our model using KNN classifier.

Conclusion:

Thus we can see how easy it is to import a dataset, build a model using Scikit-Learn, train the model, make predictions with it, and finding the accuracy of our prediction(which in our case is 98.33%).

Source Code: https://github.com/Saketh-Nandn/Data-Analytics.git

I am thankful to mentors at https://internship.suvenconsultants.com for providing awesome problem statements and giving many of us a Coding Internship Experience. Thank you www.suvenconsultants.com.",https://medium.com/@sakethnandn-k/recognizing-handwritten-digits-with-scikit-learn-3bb8142336b7,[],2020-11-01 13:52:28.554000+00:00,439,"Handwritten Digit Recognition, Machine Learning, Scikit-Learn, Jupyter Notebook, Digits Dataset"
IV. Deploy a Poloniex trading bot,"0 — Importing librairies and loading data

The very first step is to import the necessary librairies, and (re-)define the constants which we used along the construction of this trading bot. This includes the stoploss, the takeprofit, the period of candles, but also the pair which we trade — as referred to on the Poloniex API and website (e.g. USDT_BTC for Bitcoin trading) and the amount in dollar which we want to invest for each trade. Furthermore, we also set our threshold probability min_threshold from which we may follow our model during bullish predictions. By default it is set to 0.5 (i.e. we follow the model everytime it is bullish) but we can set a higher threshold to generate a more selective trading strategy (see article III — [1]). One should also pay attention to modifying the path on line 14 according to his directories structure.

We also import a custom library called Deployment_functions that will be described in section II of this article.

The initialization code is given below :

Figure 1: Initialization code for deployment

I — Initializing Poloniex API connection

Once we have imported the librairies, we can initialize the connection to the Poloniex API by inputing our API keys and secret. These hexadecimal codes are unique to each account, and allow the user to take trades directly from Python. More information regarding the API and how to get your keys can be found at [2]. With our program, you just need to set yours at lines 62 & 63 in Deployment_functions.py (next paragraph).

Furthermore, the code the initialize the connection is as simple as this :

Figure 2: Initialize Poloniex API private connection

II — Defining buy and sell functions, computing variables

As presented earlier, to lighten the trading script I decided to store all functions inside a custom library called Deployment_functions.py. This file contains previously introduced functions such as the computation of predictive variables (see article I — [3]). It also contains two very important functions, that are the buy_asset() and sell_asset() functions. These functions enables us to buy/sell a given amount at the best possible price available in the market, and are completed only if the entire amount is bought/sold. These functions are important to define well and clearly, in order to avoid pending orders which would falsen the real-life trading. The underlying idea is to make the code as performant as possible, in order to obtain real-life results that are as close as possible to the backtested results (see article III — [1]).

The code for these functions is given below, where as specified before you should change lines 62 & 63 :

Figure 3: Define trading functions

III — One trade at a time (1TAAT) trading bot

Now that we have built and defined all of our tools, time is now to create the trading loop. In our case (1TAAT), we simply repeat a sequential process over and over. We wait for the candle’s data to be delivered, then we apply these 3 tasks : Request the recent data, compute predictions by computing the variables and applying the model, then finally if our bullish criterion is reached (see article II [4] for how we define our min and max) we enter the trade and track it. When we enter into the trade, first we buy the predefined amount of the pair, then we wait until the price reaches either the stoploss or the takeprofit. When it finally reaches a limit, we sell our asset. And we repeat this process over and over.

The code was made to display some monitoring information in the console (Figure 4) and to store the characteristics of each transaction made (Figures 5 & 6). NB : The Poloniex website stores all trades’ information anyhow.

The console looks like this when a trade in progress, i.e. when we are waiting for the price to reach either the stoploss or the takeprofit:

Figure 4: Console when a trade is being monitored

Figure 5: Data stored during a Buy action (entry point of the trade)

Figure 6: Data stored during a Sell action (exit point of the trade)

The trading loop is quite refined, and it fits in 26 lines which makes it clear to understand :

Figure 7: Trading loop

IV — Conclusion

First we can recall that the entire project, along with the code, is available in a dedicated GitHub repository [5].

This project is a great accomplishment, as we managed to display how to build a profitable trading bot from scratch. The pipeline is easily adaptable and can fit various financial markets. Also, the required input data is limited, so that the access to historical data should not be an issue.

We showed that the pipeline was yielding robust and profitable trading strategies on backtested simulations (article III) and we exposed in this very last article how to implement the principle of a trading bot in a simple Python script.

We may also recall that the purpose of this series of articles was to make this notion accessible to everyone, and that obviously one can enhance the proposed pipeline by multiple means. In particular, one could think about adding some more predictive variables, increasing the number of layers in the Deep Learning architecture, or even ameliorate the implementation loop. It is also quite feasible to adapt the trading loop in order to take and monitor multiple trades (possibly over multiple markets) at the same time.

To give an overview of some possibilities, I can invite the interested reader to look at academic papers, especially MSc and PhD thesis. At an other level, I wrote a report concerning my construction of such advanced trading bot, available at [6].

Don’t hesitate to leave any feedback/questions/clap or to contact me for more information.

04/01/2021 : There a are a couple of bugs in the buy/sell functions. Looking forward to correct them asap.",https://medium.com/analytics-vidhya/iv-deploy-a-poloniex-trading-bot-c3af87eba48c,['Sébastien Cararo'],2021-01-04 18:55:14.166000+00:00,953,"Trading Bot, Poloniex API, Python Script, Deep Learning Architecture, Financial Markets"
What type of data scientist are you?,"What type of data scientist are you?

A look into the most common data scientist personas

Photo by Akson on Unsplash

Data science is a relatively new field that has incorporated profiles and personalities from several disciplines. But what are the different types of data scientists and which jobs were these people doing before the advent of data science?

Most people have seen some form of this trilogy chart, that I have re-adapted below.

Source: Gianluca Gindro

Successful data science is indeed a mixture of three core components: statistics, business acumen, and programming skills, but few people are strong in all these areas.

So, which persona are you? Or, if you are trying to hire a data scientist, which one do you need?

Three background paths lead to three different personas:

The recovered management consultant

The researcher escaped from academia

The developer turned data scientist

The recovered management consultant

This category spans the junior business analyst and the ex McKinsey consultant. They have in common a passion for Excel and their ability to show off v-lookups and fancy formulas even to plan their house move.

They are also the ones who have more passion for the business problem: they come business first, data after.

They had to learn Python or R by necessity, not because they enjoyed programming. And they still try to avoid coding as much as they can and their code is generally as re-usable as a single-use napkin.

They have good intuitions for the basics of statistics but they had to learn concepts like p-value or t-test the hard way.

What they are good at: data science projects that support decision making, business-oriented processes, one-off projects.

The researcher escaped from academia

They often have a PhD and come from a research background. They studied hardcore math and statistics and they could speak for hours about the philosophical differences between the Bayesian and frequentist approaches.

They are normally ok at coding, as long as they don’t have to push themselves too much into the boundaries of data engineers. A test-driven programming approach might be a stretch for them.

But they are probably good at lower level programs such as C++, which could come handy for applications at large scale or deep learning.

What they tend to lack is business thinking. Developing a product is probably the end goal for them because they recognize that as the equivalent of publishing a paper in academia.

What they are good at: complex machine learning projects at the front edge of innovation. They can push boundaries, read lots of research papers to pick and implement the best ideas. A deep-tech company would probably need a handful of those profiles.

The developer turned data scientist

Since data science requires a lot of coding, these are probably going to be your best friend.

You can trust them to build good reusable code, don’t have to explain to them the concept of testing, and they will probably be able to automate the pipeline more than you hoped for.

They are probably fine to use machine learning tools out of the box, but if they need to venture into deeper statistical thinking, it could become a minefield.

Some of them might be decently good at understanding the business side of things, particularly if they were previously involved in gathering requirements and handling business relationships. But don’t expect them to be too proactive with novel business ideas.

What they really excel in is a scenario that requires technical challenges, such as a big data project that needs to work at scale or a complex data pipeline. At the same time, they are less comfortable in a project which requires more advanced modeling and where prediction accuracy needs to be a competitive advantage. They are also best suited for projects where they have to focus on building products rather than on supporting decisions.

Conclusions

If you are a data scientist or are thinking to become one, try to figure out which persona you fit in and which are your gaps, but also don’t be shy to recognize your strengths.

As a data scientist, you will need a mixture of all these skills, but expecting to be a pure all-rounder is a myth: there is always a side you prefer!

And if you need to hire a data scientist, try to stereotype which category the candidate fits more into: hiring someone to build a high-scale recommender engine will certainly require a different profile than someone to support your CEO in sales forecasting.",https://towardsdatascience.com/what-type-of-data-scientist-are-you-2984140f6378,['Gianluca Gindro'],2020-05-22 06:53:42.783000+00:00,720,"Data Scientist, Business Analyst, Mc Kinsey, Excel, Python"
Quick PlaidML MacOS Installation Guide,"Here you can see that we have reduced the wall time to only 1/3 of the time taken by the cpu, from 9 mins to 3 mins.

Tips:

For anyone unsure if it is working for you. Check GPU acitvity using activity monitor. Launch activity monitor -> Go to window -> gpu history

you can the see gpu fired up in the “AMD Radeon Pro 450” window

Conclusion:

Well, as promised a quick and fuss free guide to installing and using to utilising the GPU on your macbook in jupyter notebook. Feel free to leave a comment if anything doesn’t work out. Oh yes, the codes used in this story can be found on my github: https://github.com/kennylimyx/plaidml",https://medium.com/@kennylimyx/quick-plaidml-macos-installation-guide-e1d4b9805e53,['Kenny Lim'],2020-11-05 11:08:08.156000+00:00,112,"_macbookGPU, Mac Book, Jupyter Notebook, AMD, Radeon Pro450"
Latest picks: In case you missed them:,"Get this newsletter By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.

Check your inbox

Medium sent you an email at to complete your subscription.",https://towardsdatascience.com/latest-picks-battling-label-distribution-shift-in-a-dynamic-world-78cf36a76d11,['Tds Editors'],2020-11-26 14:27:08.260000+00:00,40,"newsletter, subscription, privacypolicy, inbox, email"
Complete Machine Learning Project for Beginner,"It’s recommended to use Jupyter notebook although you can use any IDE. But, I am gonna use Jupyter notebook codes in this blog.

Prepare Problem

— Load Libraries

Before proceeding further for model development first load important libraries.

fig 1. Load important libraries ( Inspired by Jason Brownlee)

— Load dataset

the dataset should be in the same folder where your python file is.

To understand the dataset in detail just click here. then go further in this blog.

fig 2. load dataset

Summarize Data

— The dimension of the dataset

fig 3. dimension fo the dataset

In the above fig 3. It shows that there are 150 rows and 6 columns in the given dataset.

— Peek at the Data

fig 4. Peek at the Data

— Bottom of the Data

fig 5. Bottom of the Data

— Description

fig 6. Description of the data(Statistical Analysis)

— Class Distribution

Class Distribution shows that how many classes are there in the given dataset and how many instances for each class.

fig 7. group of Species

Data Visualization

— Pair Plot

fig 8. code for pair plot using Seaborn

fig 9. Pair Plot

By seeing the above Visualization (Pair Plot) It is very clear that two features petal_length and petal_width are import features.

Evaluate Some Algorithms

Now let’s create some models of the given data and estimate their accuracy on unseen data.

Steps to Evaluate Algorithms

Separate out a validation dataset. Setup the test harness to use 10-fold cross-validation. Build 5 different models to predict species from flower measurements. Select the best model.

Please drop the columns ‘Id’ before going further using dataset.drop(columns='Id')

fig 10. Code for Validation dataset.

Question: Why I used an array instead of a pandas Dataframe?

Answer: Because simple array is computationally faster than a pandas Dataframe.

Spot-Check Algorithms

fig 11. Spot-Check Algorithms

In the above picture, It is showing that SVM is the best choice among all to be selected as an Algorithm to make our model

Make predictions

SVM was the most accurate model that we tested. So, I am gonna make a prediction using the Support Vector Machine(SVM).

— let’s create a model

fig 12. Creating a model

— let’s fit the model

fig 13. fitting the model using fit() method

— Let’s make predictions

fig 14. predicting result on unseen data

fig 15. predicted results

— Let’s check what is the accuracy of this model

fig 16. accuracy of the model

— Let’s see the confusion matrix of the predicted result

fig 17. confusion matrix

— let’s see the classification report

fig 18. classification report.

Save the model for later use

— save the model to the disk

— sometime later

Please check the below link.

You can use the above template to solve any real-world Classification problem.

You are welcome for any queries and questions.",https://medium.com/analytics-vidhya/complete-machine-learning-project-for-beginner-d6add8a0102d,['Muhammad Iqbal Bazmi'],2019-10-21 18:02:48.966000+00:00,415,"jupyter notebook, python, data science, machine learning, data visualization"
Moviegoer — Scene Boundary Identification,"This is part of a series describing the development of Moviegoer, a multi-disciplinary data science project with the lofty goal of teaching machines how to “watch” movies and interpret emotion and antecedents (behavioral cause/effect).

Without any structure, a film is just a collection of a few thousand frames of images and a very long audio track. Conversations bleed into one another, characters appear and disappear without reason, and we teleport from one location to the next. We can begin to organize a film by dividing it into individual scenes. We’ll use an example of Lost in Translation (2003).

To start, we’ll just be identifying two-character dialogue scenes. These are the most basic building-blocks of films: just two characters speaking together with no distractions, purely advancing the plot with their dialogue. In modern filmmaking, these scenes are usually shot in a specific manner. We can take advantage of this by looking for specific patterns of shots.

The A/B/A/B Pattern

Two-character dialogue scenes usually follow a very distinct pattern. Character A speaks, then Character B, then back to A, then back to B, etc. We cut back and forth between the two characters.

We look for these two anchor shots, which are the shots of the two characters and form the A/B/A/B pattern. We’ll be looking through every frame in the film, and trying to find instances of these ABAB patterns. We have a few existing dataframes to use — we’ve previously clustered similar frames into “shots”. For example, all of frames with Charlotte are grouped into a single shot cluster.

Anchor Shot A: Charlotte on the left

We have another unique shot of Bob.

Anchor Shot B: Charlotte on the left

We’ve located an A/B/A/B pattern: four shots where A and B alternate. The next step in the workflow is checking for faces, and making sure that shot A has a face on the left, and shot B has a face on the right (or arbitrarily inversely assigned). These two shots have a face on the left, and a face on the right, so they pass the test.

Expanding the Scene

We can expand the scene by checking for these two anchor shots nearby. Currently, our scene consists of only the A/B/A/B pattern: just the two anchor shots. But there might be other shots, cutaways, interrupting these anchor shots. Cutaways are shots that are part of the scene, but aren’t the anchor shots. These might include shots like a closeup of an object, or a POV shot of an object being looked at by a character.

So if a true scene was B/C/A/B/A/B, we look for that additional B at the beginning, and then we’ll designate that intermediate shot as a cutaway. Starting with the A/B/A/B pattern, we expand the scene by looking before the first A, and after the last B.

Cutaway shot: a two-shot of both characters

By applying this workflow, we can identify scenes throughout the entire film. This is the first scene we identified in Lost in Translation, a famously quiet film with limited dialogue. This is indeed the first scene that Bob and Charlotte have a conversation.

This algorithm has a lot more success with traditionally-filmed, mainstream films. For example, when we applied this to Plus One (2019), a romantic comedy, we found 18 scenes. Two-character dialogue scenes, with two characters spitting sharp dialogue at each other, are a staple of rom-coms.

Wanna see more?",https://medium.com/@moviegoer/moviegoer-scene-boundary-identification-4a8be284c555,['Tim Lee'],2020-12-19 03:55:31.143000+00:00,553,"Moviegoer, Data Science, Lost In Translation, ABABPattern, Cutaway Shot"
Tableau — When to use & When Not to use,"Tableau is a very powerful data visualization tool. The remarkably intuitive drag and drop interface of Tableau makes the process of creating useful dashboards very easy.

For deployment across any organization it’s imperative to understand the capabilities of Tableau and associated use cases and then evaluate these in light of your requirements. To do this you should have a good understanding of features offered by Tableau.

Let’s talk about the scenarios where Tableau could be useful. If your answer to most of the questions listed below is “Yes” then Tableau is definitely is the tool for you.

Are you looking to present data in the form of charts/graphs? Are you looking to create an engaging way of presenting data?

The primary objective of data analysis is to extract meaningful insights from data i.e. convert data to information. Representing data in the form of graphs and charts makes it easier for users to understand and draw inferences.

Additionally, you can create interactive dashboards with ease for e.g. a dashboard with drill down capabilities where users can navigate to the desired level of detail by simple click. This enables the users to narrow down the scope of analysis and present findings in terms of data.

2. Do you want to make the process of converting data to information “Agile”?

Traditional BI includes heavy involvement of IT for setting up database and providing the required information which is utilized by business users for analysis.

Tableau includes functionality to connect to multiple data sources and visualize data which can be easily done by any business user without major involvement of IT. The whole process of connecting to the data source and representing it in dashboards is streamlined, thereby enabling a faster process of converting data to information.

Involvement of IT is only during the initial setup phase and to the extent of maintaining the backend server required for sharing across the organization.(during steady state)

3. Are you looking for a BI tool that is intuitive and can be used easily by the Business Users with minimum intervention from IT?

To start, no specific technical skills are required to create impressive graphs and charts. The drag and drop interface of the tableau is easy to use and understand. Although some training is required, the effort isn’t more than that is required for learning excel. The effort is far lesser as compared to that required for learning any other programming language.

This eliminates the involvement of IT to set up and provide data in a format that Business users can use for answering various business queries. The simple and intuitive interface enables the business users to understand data and change the level of detail and granularity as per their requirements.

4. Are you looking to create a seamless way of sharing information across the organization? Are you looking to interact with data on different devices such as laptops, mobile, etc without creating separate process for each platform?

The Dashboards created on “Tableau Desktop” (primary tool for creating dashboards) can be published on Tableau server. Users can login to the server and to see the required dashboards.

Same views can be seen on different devices with ease. This feature makes it user friendly and drives adoption across all levels of the organization.",https://medium.com/@rohit885/tableau-when-to-use-when-not-to-use-5a1cb5a3f25,['Rohit Thakur'],2020-11-25 18:22:34.858000+00:00,530,"Tableau, Data Visualization, BI Tool, Dashboards, Agile Data Analysis"
Unsupervised approaches for NMT,"Translation is one of those tasks in language where the arrival of deep learning systems, and in particular sequence-to-sequence, has been something like a boon. In less than 4 years since the first paper on Neural Machine Translation, software giants such as Google and Microsoft have already announced that their translation systems have almost completely shifted from statistical to neural. Gone are the days when researchers mulled over complex word and phrase alignment techniques, and yet fell short on several language combinations. With the latest framework, all you need are a million parallel sentences, and your system can then translate between this pair sufficiently well.

A million parallel sentences — that’s a little constraining, though! It is often difficult and sometimes even impossible to obtain a bilingual parallel corpus for many pairs of languages. In such cases, using a pivot language for triangulation has been found to be helpful. However, even in such supervised systems, the performance is still constrained by the size of the training corpus.

Monolingual data, on the other hand, is available in abundance, and a number of semi-supervised systems do use these, but mostly for the language modeling part of translation. For example, a naive system may perform word-by-word substitution and use a language model trained on the target language to obtain the most probable word order.

Recently, there have been 2 very similar papers (both currently under review at ICLR ’18) which propose to perform completely unsupervised machine translation. In this article, I will discuss both of these papers. A similar blog is available here, but I didn’t know of its existence until I was already halfway through this post.",https://medium.com/explorations-in-language-and-learning/unsupervised-approaches-for-nmt-f0b18b12d4d5,['Desh Raj'],2018-02-26 14:52:02.107000+00:00,272,"translation, deeplearning, sequence-to-sequence, Neural Machine Translation, parallelcorpus"
Predicting Apartment Rental Prices in Germany,"Through this article, I have attempted to predict rental prices using the apartment rental dataset containing rental prices in Germany. The dataset consists of data scraped from one of Germany’s biggest real-estate platform. The main objective here is to study and understand the data and use the knowledge to construct a basic predictive model to predict the base rental price (popularly known in Germany as ‘Kaltmiete’).

The original dataset consists of 268850 apartments (rows) * 49 features (columns), but a combination of missing and imbalanced values means that only 14 features were useful to me in the current context. The columns contain the following information:

regio1 : The federal state in which the apartment is located. heatingType: The type of heating system used in the apartment. balcony: Column indicating if the apartment has a balcony. yearConstructed: The year in which the apartment was constructed. hasKitchen: Column indicating if the apartment has a kitchen. cellar: Column indicating if the apartment has a cellar. baseRent: The rent excluding electricity and heating. livingSpace: The living space in square meter. condition: The condition of the flat. lift: Column indicating if the apartment has a lift. typeOfFlat: The type of apartment. noRooms: Total number of rooms in the apartment. garden: Column indicating if the apartment has a garden. regio2: The city where the apartment is located.

Dataframe at a glance(1/2)

Dataframe at a glance(2/2)

Exploratory Data Analysis:

Since this dataset was created by web scraping, the dataset was far from clean. Therefore one of the challenges was getting rid of extreme and null values. For example, there were several rows with a living space of more than 60 sq. m. in the range of €10-€30, which are clear outliers. There were also several apartments with the total number of rooms more than 100. A post-cleanup study yielded the following:

1.) The city in Germany with the highest average rent prices is Munich, which is not surprising as the city is quite well known for its notoriously high living costs. The districts in and around Munich (denoted by München_Kreis) also figure in the Top 10. Interesting fact, Starnberg, which follows Frankfurt at number three, is popularly known as the wealthiest town in Germany. I was quite surprised not to find Düsseldorf and Bonn on the list.

Average Rents by Cities

2.) Hamburg is the federal state with the highest average rental prices, followed by Berlin and Bayern(Bavaria). The North-Eastern states of Sachsen-Anhalt and Thüringen have the lowest prices.

Average Rents by State

3.) As should be, the size of the living space and the base rental price seems to be more or less positively correlated. The small(in size) apartments with high prices that seem like outliers are apartments located in the city center (central Berlin, Frankfurt) which explains the cost.

Total Rent vs Living Space

4.) The most common base rental price in Germany hovers around the €300-€400 range.

Rent Distribution in Germany

5.) Central Heating seems to be the most common type of heating system, by a fair margin, employed in German homes.

6.) Most of the apartments on offer have 3 rooms and come under the type ‘apartment’.

Count of Rooms per apartment

Type of Rentals

Model Development:

Before a prototype model can be created, it is important to transform the dataset. The following pre-processing steps were performed on the dataset before being fed into the model.

The seven unique values in the ‘condition’ column (‘well_kept’, ‘refurbished’, ‘first_time_use’, ‘fully_renovated’, ‘mint_condition’,

‘first_time_use_after_refurbishment’, ‘modernized’, ‘negotiable’,

‘need_of_renovation’, ‘ripe_for_demolition ') were divided into 3 classes ‘new’, ‘old’ and ‘middle’ apartments.This was done to get a clearer demarcation. The data in the columns ‘balcony’, ‘kitchen’, ‘cellar’, ‘lift’ and ‘garden’ were encoded to 1 or 0 depending on whether the column value was True or False. The continuous-valued variables ‘baseRent’, ‘livingSpace’ and ‘noRooms’ were normalized using sklearn’s MinMaxScaler(). While this is not a necessary step when dealing with tree-based algorithms, it eases interpretability and makes performance comparison with other models easier. The categorical variables were encoded using One-Hot-Encoding, made easy by pandas’ get_dummies. The column ‘baseRent’ was selected as the target variable and the rest as predictor variables. After applying the above steps, the dataset was split into a training set(70% of the data) and testing set(30% of the data) using sklearn’s train_test_split.

I chose Random Forest to develop the prototype as it is a powerful model that can work well with both continuous and categorical variables.

The model was fit using RandomForestRegressor() on the training set. Using the test set to check the performance of the model, the following results were obtained.

Model Results

The model accounts for around 83.4% of the data in the model or the predictive power of the model is 0.83 (best is 1.0), which is quite good considering that the model was run using the default parameters and not tuned. The MSE and MAE are also on the lower side.

Conclusion:

Can this model be improved? Yes, of course!. The model is very far from being the best model. My first approach would be to perform Hyperparameter tuning and cross-validation using grid-search techniques to find the ideal parameters. Secondly, the original dataset contained a lot of columns with missing data which I chose to exclude from the model, which meant quite some information was lost. Smart Imputation techniques using domain knowledge could help tackle this problem. The original dataset also contained columns describing apartment facilities and descriptions. By employing NLP techniques, more information can be gained from these columns. Heavier feature engineering would also be helpful. Needless to say, I will be using these steps as cues to better the performance of the model.

I would love to hear your feedback on this. If you have any questions or suggestions you can reach out to me. Thank you for reading.",https://towardsdatascience.com/predicting-apartment-rental-prices-in-germany-d5635197ab00,['Vineeth Antony'],2020-05-06 19:03:07.814000+00:00,936,"Rental Prices, Germany, Kaltmiete, Real Estate, EDA"
The Secret to Knowing the Unknowable: Predictive Analytics,"There are two branches of analytics (i.e. statistics for my educator friends) that are used widely to describe ways we can summarize large amounts of data. The first way simply describes the data. We call this descriptive analytics. Descriptive analytics has a wonderful use. It helps us summarize the data into readable chunks so we can understand what happened in the big pile of messy data. With descriptive statistics, we can begin to make decisions because we can analyze what we know.

But what happens when we want to know something we haven’t seen yet (or are not able to ask directly because gathering that data would be impossible or too costly?) Questions like…

Will my product sales be higher next year than they are this year?

What kind of product will my customers buy tomorrow?

What is the best way to respond to a brand crisis?

How much should I spend on advertising next year?

The 2nd form of analytics is much more interesting, and in my opinion useful. It’s called predictive analytics. Here’s my take on predictive analytics:

Predictive analytics is using data we have available to us to build a model that allows us to predict data that doesn’t exist yet. — Courtney Perigo

By definition, we’re inferring something we do not know (our prediction) from something we know (data we’ve collected.) The reason to use predictive analytics is when the cost to acquire some information is too much. Either the question is impossible to ask or it would take us too long to acquire that information.

With predictive analytics, I can make better decisions because I have a model that helps me understand something I didn’t know before.

To be clear, predictive analytics cannot reduce all risk. It’s impossible to know what WILL happen. The goal of predictive analytics is to understand what MIGHT happen and all of the caveats that went into the analysis.

Let’s talk about the two types of predictive analytics.

1.) Extrapolate — Time Series Forecasting

This version of predictive analytics is relatively straight forward. Most decision makers are familiar with it and how to use it. In time series forecasting, I want to know what may happen in the future given the trends of the past.

This trend line, which is typically a time series model, summaries the past trajectory of data. The magic happens when you take that model summary and use it to extrapolate future time where data doesn’t exist.

In the case of extrapolation, the reason we do not have data is that it doesn’t exist yet! We cannot measure future events (at least not yet — that I know of.) Knowing the future is the realm of science fiction and fantasy — looking at you Bran Stark (Game of Thrones.)

2.) Non-Temporal Predictive Analytics

If you’re a follower of my blog, then you already know of an example of non-temporal predictive analytics — where I used a model of news preference to predict which data science articles I want to read for the day.

Read more here: MachinaNova — News Reco Engine

In the case of MachinaNova, I was personalizing my daily news experience. In this application, I collected data on my preferences for data science articles. Based on this past preference, I built a natural language model that predicts new articles that would appeal to me based on the new article’s content (words, topics, etc.) Sounds cool, right? You can read more on how that was accomplished in my blog linked above.

The idea of non-temporal predictive analytics is the model is irrelevant. We could build a model in MANY different ways. The main idea is to make sure you have a way to understand it’s accuracy. In the case of MachinaNova, we understood the accuracy of the model by splitting our training data set and using the some of that data to predict data the model hadn’t seen yet. Since we understood actual article preference, we could compare the model’s output versus actual results to understand how good the model is.

This is ridiculously important. Anyone can model anything based on what they think they know about the world. Successful predictive models are those that can remain accurate with data the model was not trained with. A model that falls apart outside of its training data set is completely useless — or worse will give you inaccurate predictions.

In the case of non-temporal predictive analytics, the reason we do not know something is that data is really difficult or impossible to ask. Could you imagine if Amazon had to ask you what products you like every time you visited their website? Non-temporal predictive analytics to the rescue!

Concluding Thoughts:

In this post, we explored predictive analytics — using data we have to understand what MIGHT happen in the future. We also know how it’s different than descriptive analytics — which is using data to summarize what happened in the past.

The main advantage of having a predictive model is that we can extrapolate the model to areas where data doesn’t exist and make predictions. This is extremely useful because knowing everything past, present and future — is impossible. At least in the real world.",https://towardsdatascience.com/the-secret-to-knowing-the-unknowable-predictive-analytics-de519a67576,['Courtney Perigo'],2019-04-22 13:52:42.706000+00:00,844,"predictive analytics, descriptive analytics, extrapolate, non-temporal predictive analytics, time series forecasting"
Squark — Affordable AI In A Spreadsheet,"Squark — Affordable AI In A Spreadsheet

The Problem

Artificial intelligence is out of reach for organizations with less than $1 billion in revenue for a few main reasons. Not only is there an AI expertise shortage, but the data scientist median salary is $110,000, with 80 percent of this group with less than three years of experience. Another barrier is the high cost of technology: AI costs typically range from $100,000 to $1 million annually on top of hiring costs. There are also point solution limitations because companies are often left with isolated models that are only applicable to specific-use cases, with no reach across functions. Lastly, there are operational failures abound for companies trying to do this work — only 13 percent of AI Projects are in production, according to IBM.

What The Company Does

Squark provides a truly automated machine learning capability that is accessible and affordable to corporate and mid-market companies. It is codeless; offers the fastest time-to-value and the lowest total cost of ownership. Squark also scales across business functions, does automatic AI data preparation, feature engineering, production-ready classification, regression and forecasting.

Business Model

The company has a SaaS model selling licenses to enterprises.

The Market

A segment of the larger AI market spend, Squark’s core focus is on what is expected to be a $58 billion market by 2021 with 500,000 analysts. Just between Boston and NYC there are 22,000 analysts, all of whom could use a Squark license to put prediction at the center of their business. Competitors include companies like DataRobot, Big Squid and Aible, but these companies are largely service companies according to the Squark team.

Traction

Squark has on-boarded nine customers in the last six months that includes F100 companies. The team has participated in the NVIDIA Inception Program, a virtual accelerator that nurtures cutting-edge AI startups.

Founding Team Background

The team is comprised of entrepreneurs with multiple exits who are global thought leaders, academics, and authors.

What They Need Help With

Squark is in the midst of a $1.25M seed round. Feel free to reach out to them directly at judah@squarkai.com or to us at The Buzz at hello@ourbuzzmedia.com if you’d like more info. They are also looking for introductions to potential customers and have list of their open positions.

Subscribe To The Buzz To Get More Startups In Your Inbox",https://medium.com/the-startup-buzz/squark-affordable-ai-in-a-spreadsheet-e7d6890843f4,['Jeff Piltch'],2019-10-22 19:01:01.306000+00:00,377,"AI, Machine Learning, SaaS, Data Science, AIProjects"
AI Dance based on Human Pose Estimation,"A Human Pose Skeleton represents the orientation of a person in a graphical format. Essentially, it is a set of coordinates that can be connected to describe the pose of the person. Each co-ordinate in the skeleton is known as a part (or a joint, or a keypoint). A valid connection between two parts is known as a pair (or a limb). A sample human pose skeleton is shown below.

So, In this article, we will look how to use a Deep Neural Net model for performing Human Pose Estimation in OpenCV.

Table of Content

Datasets Model Architecture Experiments and Results

Datasets

Till now, Human Pose Estimation was challenging problem because of lack of high quality datasets. Now a days, every AI challenge is needs a good dataset away from demolished. In last few years, challenging datasets has been released which have made it easier for researchers to solve the problem efficiently.

Some of the datasets are :

For this article, we have used COCO Dataset for Human Pose Estimation.

Model Architecture

OpenPose first detects parts (keypoints) belonging to every person in the image, followed by assigning parts to distinct individuals. Shown below is the architecture of the OpenPose model.

Flowchart of the OpenPose architecture.

The model takes as input a color image of size w × h and produces, as output, the 2D locations of keypoints for each person in the image. The detection takes place in three stages :

Stage 0: The first 10 layers of the VGGNet are used to create feature maps for the input image. Stage 1: A 2-branch multi-stage CNN is used where the first branch predicts a set of 2D confidence maps (S) of body part locations ( e.g. elbow, knee etc.). Given below are confidence maps and Affinity maps for the keypoint. The second branch predicts a set of 2D vector fields (L) of part affinities, which encode the degree of association between parts. Stage 2: The confidence and affinity maps are parsed by greedy inference to produce the 2D keypoints for all people in the image.

Steps involved in human pose estimation using OpenPose. (Source)

Experiments and Results

In this section, we will load the trained model for understanding Human Pose Estimation on a single person for simplicity. Here are the steps :

Download the model weights from here.

Load the network : We are using models trained on Caffe Deep Learning Framework. Caffe models have 2 files –

.prototxt file which specifies the architecture of the neural network . .caffemodel file which stores the weights of the trained model

Read Image and Prepare Input to the Network : The input frame that we read using OpenCV should be converted to a input blob ( like Caffe ) so that it can be fed to the network. This is done using the blobFromImage function which converts the image from OpenCV format to Caffe blob format. First we normalize the pixel values to be in (0,1). Then we specify the dimensions of the image. Next, the Mean value to be subtracted, which is (0,0,0).

Make Predictions and Parse Keypoints : Once the image is passed to the model, the predictions can be made. The output is a 4D matrix :

The first dimension being the image ID ( in case you pass more than one image to the network ). The second dimension indicates the index of a keypoint. The model produces Confidence Maps and Part Affinity maps which are all concatenated. For COCO model it consists of 57 parts — 18 keypoint confidence Maps + 1 background + 19*2 Part Affinity Maps. The third dimension is the height of the output map. The fourth dimension is the width of the output map.

Draw Skeleton : We can draw the skeleton when we have the keypoints by just joining the pairs.

The output of above code is :

The code is available at github.com/Devashi-Choudhary/AI-Dance-based-on-Human-Pose-Estimation. For any questions or doubts, feel free to contact me directly at github.com/Devashi-Choudhary.

References

It’s always good to give references",https://medium.com/nerd-for-tech/ai-dance-based-on-human-pose-estimation-738ac2ff6d1f,['Devashi Choudhary'],2021-02-20 16:44:31.738000+00:00,646,"for any article or blog.Human Pose Estimation, OpenCV, Deep Learning, COCO Dataset, Open Pose Model Architecture"
Would Jordan have been a champion without Pippen? (a single view),"Grafiti

Grafiti is the first search engine for graphs & charts.",https://medium.com/grafiti/would-jordan-have-been-a-champion-without-pippen-a-single-view-617b7fff9227,['Farhan Mustafa'],2017-10-04 22:46:31.248000+00:00,10,"grafiti, graphs, charts, datavisualization, datavisualization"
Why Do Machine Learning Projects Fail?,"1. Establish a Baseline at The outset

I hate how machine learning projects start in most companies. Tell me if you’ve ever heard something like this: “We will create a state-of-the-art model that will function with greater than 95 percent accuracy.” What about this: “Let’s build a time series model which will give an RMSE that’s close to zero.” Such an expectation from a model is absurd because the world we live in is indeterministic. For example, think about trying to create a model to predict whether or not it will rain tomorrow or if a customer would like a product. The answer to these questions may depend on a lot of features we don’t have access to. This strategy also hurts the business because a model that is unable to meet such lofty expectations usually gets binned. To avoid this kind of failure, you need to create a baseline at the start of a project.

Establish a Baseline by looking at business metrics or current model performance. Source: Pixabay

So what is a baseline? It’s a simple metric that helps us to understand a business’s current performance on a particular task. If the models beat or at least match that metric, we are in the realm of profit. If the task is currently done manually, beating the metric means we can automate it.

And you can get the baseline results before you even start creating models. For example, let’s imagine that we’ll be using RMSE as an evaluation metric for our time series model and the result came out to be X. Is X a good RMSE? Right now, it’s just a number. To figure that out, we need a baseline RMSE to see if we are doing better or worse than the previous model or some other heuristic.

The baseline could come from a model that is currently employed on the same task. You could also use a simple heuristic as a baseline. For instance, in a time series model, a good baseline to aim to defeat is last day prediction, i.e., just predicting the number on the previous day and calculating a baseline RMSE. If your model is not able to beat even this naive criteria, then we know for sure your model is not adding any value.

Or how about an image classification task? You could take 1,000 labeled samples, have humans classify them, and then human accuracy can be your baseline. If a human is not able to get a 70 percent prediction accuracy since the task is highly complex (perhaps there are numerous classes in which to classify) or the task is pretty subjective (as in predicting emotion based on a person’s face), you can always automate the process once your models reach a similar level of performance as a human.

Try to be aware of the performance you’re going to get even before you create your models. Setting some pie-in-the-sky, out-of-this-world expectations is only going to disappoint you and your client and stop your project from going to production.",https://medium.com/swlh/why-do-machine-learning-projects-fail-9fefb287a66d,['Rahul Agarwal'],2020-09-07 18:06:44.741000+00:00,499,"Machine Learning, Baseline, Metrics, Model Performance, RMSE"
5 Statistical Functions for Random Sampling in PyTorch,"5 Statistical Functions for Random Sampling in PyTorch

Photo by CHUTTERSNAP on Unsplash

PyTorch is an open-source machine learning library for Python which provides maximum flexibility and speed on scientific computing for deep learning. It can be considered as a NumPy extension to GPUs.

PyTorch offers easy to use API and due to its pythonic environment it smoothly integrates with the Python data science stack.

In this article, we will explore 5 Statistical functions available in PyTorch for Random Sampling:

torch.bernoulli()

torch.normal()

torch.poisson()

torch.randn()

torch.randperm()

First, we will import PyTorch using : import torch

Function 1: torch.bernoulli()

It draws binary random numbers (0 or 1) from a Bernoulli Distribution and the Output is of the same shape as Input .

Example-1:

rand_m = torch.rand(4, 4) # generate a random matrix of shape 4x4

print(rand_m)

torch.bernoulli(rand_m) # draws a binary random number (0 or 1) Output:

tensor([[0.0643, 0.0690, 0.5069, 0.0323],

[0.3004, 0.6761, 0.1933, 0.7579],

[0.2958, 0.8812, 0.2917, 0.3713],

[0.8113, 0.0845, 0.9176, 0.9883]]) tensor([[0., 0., 1., 0.],

[1., 1., 0., 1.],

[0., 1., 0., 0.],

[1., 1., 1., 1.]])

Example-2:

# generate a uniform random matrix with range [0, 1]

uni_m = torch.empty(4, 4).uniform_(0, 1)

print(uni_m)

torch.bernoulli(uni_m) # draws a binary random number (0 or 1) Output:

tensor([[0.1723, 0.1165, 0.3703, 0.8497],

[0.5641, 0.0323, 0.1722, 0.3173],

[0.1663, 0.7910, 0.7019, 0.3336],

[0.8514, 0.7596, 0.9491, 0.4763]]) tensor([[0., 0., 1., 1.],

[1., 0., 0., 0.],

[0., 0., 1., 0.],

[1., 0., 1., 0.]])

Bernoulli Distribution

Note- Bernoulli Distribution is a random experiment that has only two outcomes (usually called a Success or a Failure) . It is best suited when we have two outcomes for a given event.

Function 2: torch.normal()

Returns a tensor of random numbers drawn from separate Normal Distributions whose mean and standard deviation are given.

The mean is a tensor with the mean of each output element’s normal distribution. The std is a tensor with the standard deviation of each output element’s normal distribution.

The shapes of mean and std don’t need to match, but the total number of elements in each tensor need to be the same.

Example-1:

torch.normal(mean=torch.arange(1.,11.), std=torch.arange(1,0,-0.1)) Output:

tensor([ 1.2426, 3.9152, 1.9166, 3.5866, 4.1520, 6.2800, 6.4655, 7.8572, 8.9195, 10.0185])

Example-2:

torch.normal(mean = torch.arange(1., 6.)) Output:

tensor([0.0205, 0.4621, 3.4993, 4.7508, 5.3769])

Note- The Normal Distribution is a probability function that describes how the values of a variable are distributed.

Function 3: torch.poisson()

It returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input .

Example-1:

rates = torch.rand(4, 4) * 4 # rate parameter between 0 and 4

print(rates)

torch.poisson(rates) Output:

tensor([[3.0679, 3.7367, 1.9038, 1.6546],

[1.8523, 1.0336, 3.4176, 3.6221],

[3.7246, 2.7234, 0.6564, 0.5517],

[2.0784, 3.7418, 1.3999, 2.3717]]) tensor([[2., 6., 1., 2.],

[2., 0., 3., 8.],

[4., 3., 0., 0.],

[1., 3., 3., 2.]])

Example-2:

rates = torch.rand(5, 4) * 7 # rate parameter between 0 and 7

print(rates)

torch.poisson(rates) Output:

tensor([[1.3724, 5.5118, 5.4870, 5.1002],

[0.9739, 2.7864, 2.1179, 3.6325],

[6.7643, 5.6531, 1.9739, 2.6923],

[2.5331, 6.6457, 4.4047, 2.9525],

[5.1334, 5.9861, 3.9731, 4.5331]]) tensor([[ 2., 5., 6., 4.],

[ 2., 2., 6., 5.],

[11., 3., 3., 4.],

[ 4., 9., 5., 4.],

[ 6., 6., 5., 4.]])

Note- The Poisson distribution shows how many times an event is likely to occur within a specified period of time.

Function 4: torch.randn()

It returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution ).

Example-1:

torch.randn(4) # 1-D input Output:

tensor([-0.7137, -0.5516, 0.4733, 0.4774])

Example-2:

torch.randn(4,4) # 2-D input Output:

tensor([[ 1.4962, -0.3302, -0.1390, 0.1099],

[ 0.7223, 0.8696, -2.1832, 0.6171],

[-1.0721, -1.7165, -0.3011, -0.5699],

[ 1.1563, 1.7144, -0.4076, -0.8162]])

Note- Standard Normal Distribution has a standard score or z-score is used to calculate the probability of a score occurring within our normal distribution and allows us to compare two scores that are from different normal distributions.

Function 5: torch.randperm()

It returns a random permutation of integers from 0 to n - 1 .

Example-1:

torch.randperm(5) Output:

tensor([2, 0, 3, 4, 1])

Example-2:

torch.randperm(9) Output:

tensor([6, 2, 7, 5, 1, 0, 3, 4, 8])

Note- Random Permutation is used when order of the elements matter.

Conclusion

In this article, we saw the working of 5 functions available in PyTorch. Hope this article helped you understand these function. There are a number of other useful functions. You can refer to the official documentation for the complete list of available Sampling functions.

References

Resources

The code snippets used in this article are available on my GitHub Page and jovian profile.",https://medium.com/@shailesh-kumar/5-statistical-functions-for-random-sampling-in-pytorch-1d5ca52bcc03,['Shailesh Kumar'],2020-12-03 19:24:55.314000+00:00,662,"Py Torch Documentationpytorch, random sampling, statistical functions, bernoulli distribution, normal distribution"
Classifying Items with NLP. Using itens' description to find their…,"Note: a Portuguese version of this article is available at “Catalogando Itens com PLN”

Although text is one of the most important kinds of data nowadays, it is one of the hardest to work with. Different from images and audio, text data must be transformed into numeric values and it can present different meaning depending on the place, context, and even the order of the words. Lucky for us, Natural Language Processing (NLP) has been developing lots new tools to help computers to decipher, understand and even texts just like human beings. Just look at Google Translator advances since it was created.

Many companies have been using these tools to understand their clients and automate their processes. Sentiment analysis is being used to get client opinions and create marketing campaigns and products recommendations. In this article, we will use this tools to classify an E-Commerce set of products.

Analysing your Data

First thing to do when in a machine learning project is to know your data. In this case, we will check the samples distribution, our corpus length statistics, most frequent words, etc. The dataset used for this project was an Amazon catalog, obtained from dataword, containing around 10 thousand samples. Look down for some information about its data:

Dataset Statistics

We can notice that this dataset is unbalanced. This could be a problem, our model can start learning the data distribution instead of de text information. One way to solve this is filtering the labels with fewer samples, or doing data augmentation to increase the number of samples.

Another good thing to check is the content of your data, using a word cloud, for the most frequent words in the dataset. The image below shows the dataset word cloud, and we can see many words used to describe these products.

Dataset’s most frequent word cloud

Preprocessing

Before creating our model, let’s cleanup our dataset. When working with text, we should start converting all characters to lowercase, remove

punctuation marks and special characters. Besides, it is a good idea to remove stopwords, common words of a language such as articles, prepositions, that bring little information to the text.

Now, it is time to turn our texts into numeric values, a process we call Embedding. First thing to do, is to tokenize our texts, create a vocabulary limited by 20 thousand tokens, and make. Then we must vectorize our texts. One way to do it is using a bag of words approach, turning each word of your text into a piece of information without its context. So, each text should turn into a vector with the same size of the vocabulary, and we can attribute to each token a value, that can be ones and zeros to check if the token is present or not, count how many times it appears, or evaluate its frequency in the text compared to its frequency in the entire dataset (tf-idf).

Another way to do it, is analyse each word context. So, we turn each word of the text into a vector. Each element of this vector represent a token. Normally we use the 100 most relevant ones, and each element contains a score relating the word and the token. Currently, there are lots of algorithms that can analyse lots of texts and create this vectors for us, the most famous is called Word2Vec.

Example of Word2Vec embedding, tokens in blue, vectorised words in green

Classification Models

This project is an example of how we can automate a process using machine learning and text. For this task, we can use products names, descriptions or technical information to train our model and based on the text size, we can determine how sophisticated our model should be.

Before using a neural network to solve the problem, we can create a benchmark using a simpler mode. We will use Multinomial Naive Bayes, a probabilistic model, commonly used for this kind of task. After establishing our ‘goal’, we must try to achieve better accuracies using more complex models. We will try using CNNs and RNNs, due to their architecture being perfect for working with data with context. To check more details about the models' architecture and implementation, click the link to the project on GitHub at the end of this article.

It is possible to see we got very nice results. Although we can't see much difference from the first model to the more complex ones, we can tune hyperparameters and train them for more epochs to achieve even better results, something that is not possible with the first one.

Summary

We just showed that even though that text data is not the easiest one to work with, it can be extremely helpful, creating models to speedup processes. As I said earlier, NLP has lots of applications, and there are lots of new tools that help us to use it in our projects. We just need to be creative and find out where we could use it in our business

References",https://medium.com/neuronio/classifying-itens-with-nlp-b3b28a4b7873,['Rafael Alencar'],2019-05-14 17:34:38.768000+00:00,811,"NLP, Machine Learning, Text Data, Word2Vec, Sentiment Analysis"
Data Management-3,"The remaining of the knowledge areas of Data Management.

Knowledge Area —

6. Data Integration and Interoperability — Managing the movement and consolidation of data within applications and organisations.

Goals -

Data distribution in the format and time frame needed by consumers Data consolidation. Identification of events and automatic trigger alerts and actions Support business intelligence, analytics, master data management, and operational efficiency efforts.

Activities -

Data Interoperability — Acquire -> Move -> Transform -> Integrate Data Integration

2.1.Planning — Plan and analyse, design data integration solutions

2.2.Development — Develop data integration solutions

2.3.Operational — Integrate and inter-operate data

2.4.Control — Monitor data movement operation Operational Intelligence Support

3.1. Perform Predictive Analytics and complex Event Processing

7. Documents and Content — Planning, implementation, and control activities for life cycle management of data and information.

Goals -

To comply with legal obligations and customer expectations for management of record. To ensure effective and efficient storage, retrieval, and use of documents and content. To ensure integration capabilities between structured and unstructured documents and content.

Activities -

Planning — Develop records and content management strategies Planning — Understand records and content requirements. Planning — Determine information architecture. Development — Define and develop content organisation, and E-Discovery Operational — Capture, Manage, dispose, archive, publish and deliver records and Content.

8. Reference and Master Data -

Management of shared data to reduce redundancy and ensuring data quality through use of standard data definition.

Goals -

Enable sharing of information assets across business applications within an enterprise. Reduce complexity through use of standards, common data models, and integration patterns.

Activities -

Planning — Identify Reference and Master Data Needs and requirements Control — Validate Data Definitions, evaluate data sources Development — Establish and acquire data sources for data sharing — integration architecture Operational — Publish Reference and Master Data.

9. Data Warehousing and Business Intelligence -

Planning, implementation, and provide decision support data for reporting, query and analysis.

Goals -

To support in effective data analysis and decision-making process. To build and maintain the environment and infrastructure to support business intelligence activity.

Activities -

Planning — Understand Requirements, define and maintain the DW / BI Architecture. Development — Implement and populate data warehouse, data mart, data lake. Development — Implement Business Intelligence Portfolio. Operational — Maintain data products Eco-system.

10. Metadata — Planning, Implementation, and control activities to enable access to high quality, integrated metadata.

Goals -

Provide organisational understanding of business terms and usage Collect and integrate metadata and ensure metadata quality and security. Provide standard way to access the metadata

Activities -

Planning — Define the Metadata strategy, metadata architecture and understand requirements. Development — Create Metadata model. Control — Apply Metadata Standards, manage metadata stores. Operational — Maintain, integrate, distribute and deliver metadata. Operational — Query, report and analyse metadata

11. Data Quality — The planning, implementation, and control activities for data quality management to ensure it is fit for consumption and business purpose.

Goals -

Develop approaches to measure and improve the quality of data as per defined business rules. Define requirements and specifications for integrating data quality control. Define and implement processes for measuring, monitoring, and reporting acceptable levels of data quality.

Activities -

Planning — Create a data quality culture, define requirements. Development — Develop and deploy data quality operations. Control — Perform initial data quality assessment Operational — Assess Data Quality, Measure and Monitor Data Quality.

Environmental elements aspect of data management will be covered in the next blog titled Data Management-4. 😊 😊 😊",https://medium.com/datacrat/data-management-3-372aad120d38,[],2019-01-10 07:37:33.040000+00:00,547,"Data Management, Data Integration, Interoperability, Predictive Analytics, Complex Event Processing"
Classification Visualizations with Yellowbrick,"Classification Visualizations with Yellowbrick

Whether we are iterating over performance models or presenting to clients, data scientists utilize visualizations regularly. While there are many visualization libraries available to us, Yellowbrick serves as a natural extension to scikit-learn’s modeling process and assists with model interpretation and tuning.

“Visualization gives you answers to questions you didn’t know you had.” — Ben Schneiderman

This post is to serve as an introduction to Yellowbrick and display a few ways in which it can simplify the process of visualizing the results of various classification models.

Photo by Chris Barbalis on Unsplash

I will be using Pandas and NumPy to assist with data frame manipulation and will being using seaborn, ironically, to load in the famous penguin data frame. I will import classifiers and modules as we go for ease of interpretation. The first code block includes standard imports, reading data, and basic data cleaning.

import pandas as pd

import numpy as np

import seaborn as sns

import warnings

warnings.simplefilter(action='ignore', category=FutureWarning) # Loading in data and dropping NaNs

penguins = sns.load_dataset('penguins')

penguins = penguins.dropna() # Mapping species of penguins to a numerical value

penguins['species'] = \

penguins['species'].replace({'Adelie': 0,

'Chinstrap': 1,

'Gentoo': 2})

# Mapping sex to a numerical value

penguins['sex'] = \

np.where(penguins['sex'] == 'Male', 1, 0) # Binarizing 'island'

penguins = pd.get_dummies(penguins, drop_first=True) # Viewing a sample of the data

penguins.sample(3)

# Creating my input variables, X and target variable, y

X = penguins.drop('species', axis=1)

y = penguins['species'] # Splitting data into training and test sets.

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y) # Identifying classes. The classes variable will be useful when using Yellowbrick's visualizers

classes = ['Adelie', 'Chinstrap', 'Gentoo']

The classification report is a heatmap of your model’s precision, recall and f-1 score on a class basis. The class basis classification report aids in understanding multi-class problems that might be difficult to assess with a global f-1 or accuracy score. Additionally, you may set the argument support=True to view the number of actual occurrences in each class in the dataset.

from yellowbrick.classifier import ClassificationReport

from sklearn.neighbors import KNeighborsClassifier model = KNeighborsClassifier() vizualizer = ClassificationReport(model, classes=classes, support=True) vizualizer.fit(X_train, y_train)

vizualizer.score(X_test, y_test)

vizualizer.show();

Although scikit-learn has a built-in plot_confusion_matrix within its metrics library, Yellowbrick's confusion matrix has additional features that may be of benefit. The argument percent=True will display the percent of true (or the cell divided by the row total). The label_encoder argument will accept a sci-kit learn label encoder or a dictionary.

from yellowbrick.classifier import ConfusionMatrix

cm = ConfusionMatrix(

model, classes=classes,

percent=True

#label_encoder={0: 'Adelie', 1: 'Chinstrap', 2: 'Gentoo'}

) cm.fit(X_train, y_train)

cm.score(X_test, y_test) cm.show();

The class prediction error bar graph is one of my favorites. It is a different take on a confusion matrix that is a little less confusing. The graph displays your predictions in a way that is easy to identify errors. Take the graph below for example; we can see from the first column that the model classified a sizable portion of Chinstrap penguins (green) as Adelie. Meanwhile, the Chinstraps were identified perfectly but a lot of them were incorrectly categorized. Using these new insights, one could easily detect the features causing issues and quickly address the problem.

from yellowbrick.classifier import ClassPredictionError visualizer = ClassPredictionError(

model, classes=classes)

visualizer.fit(X_train, y_train)

visualizer.score(X_test, y_test)

visualizer.show();

Although not restricted to classification problems, the Feature Importances visualizer is a quick way to utilize scikit-learn’s feature_importances_ attribute. For regression problems where there is no feature_importances_ attribute, the visualizer will use the model's coef_ attribute. Yellowbrick's documentation suggests that you set relative=False when using a regression model to understand the true magnitude of the coefficient.

from yellowbrick.model_selection import FeatureImportances

from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier()

model.fit(X_train, y_train) visualizer = FeatureImportances(model)

visualizer.fit(X_train, y_train)

visualizer.show();

Below is a Receiver Operating Characteristic/Area Under the Curve plot, or ROC AUC, using Yellowbrick’s spam dataset for binary classification. ROC AUC is generally used for binary classification, however, Yellowbrick's ROC AUC does allow for multi-class classification. I chose to feature the spam dataset instead of our beloved penguins because it did a better job displaying the quintessential ROC AUC curve. The graph displays the ROC for each class as well as the micro and macro averages. The micro averages are computed from the sum of all true positives and false positives across all classes and the macro averages are the averages of curves across all classes. See the documentation for more information.

I appreciate the multitude of options within this visualizer. You can toggle between the different curves and have them displayed including their corresponding ROC AUC score in the legend.

from yellowbrick.classifier import ROCAUC

from yellowbrick.datasets import load_spam

from sklearn.linear_model import LogisticRegression X, y = load_spam() X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y) model = LogisticRegression(max_iter=10000)

model.fit(X_train, y_train) visualizer = ROCAUC(model, classes=['ham', 'spam']) visualizer.fit(X_train, y_train)

visualizer.score(X_test, y_test)

visualizer.show();

Thank you for exploring a selection of Yellowbrick’s classification visualizations with me. I have been utilizing these visualizers in my own models and have found them to be convenient and informative. I plan to write another entry devoted to regression visualizations.",https://medium.com/@afioto/classification-visualizations-with-yellowbrick-d6f6150d7a32,['Alex Fioto'],2020-10-06 13:58:29.768000+00:00,783,"Yellowbrick, Classification Visualizations, Data Science, Visualization Libraries, Pandas"
Demystifying statistics for Data Scientists — Central Limit Theorem,"The Central Limit Theorem is perhaps the most important theorem in the development of modern statistics. It enables us to estimate the population characteristics using a sample dataset.

In this article, we will explore the workings of the theorem. We will also go through some python code to verify the central limit theorem using some visualizations.

Before deep diving into the theorem, let's understand some of the basic terminologies of statistics used in the theorem.

Note: You can find the code samples for this article at https://github.com/sandeepvja/statistics-repo

Basic Terminology

Population

Population represents all possible data items in a particular data set. In general, we don't have access to the full population.

Since we don’t have the population dataset, we can't accurately calculate the population dataset metrics like mean, variance etc. But using the Central Limit Theorem, we can estimate them (as closely as possible).

Example: The population data for Student Dataset represents all the students’ information in the world.

2. Sample

A sample is a subset of the entire population. We generally have access only to the sample.

Example: A student dataset of the University of Maryland is a sample from the whole population of student dataset.

3. Probability Distribution / Distribution

A probability distribution is simply a mathematical formula through which we can describe the entire dataset.

4. Gaussian Distribution

Gaussian Distribution (Normal Distribution) is a probability distribution which follows a bell shape similar to the image shown below. The distribution is perfectly symmetric around its mean.

Normal Distribution

Now let's try to understand the Central Limit Theorem,

Central Limit Theorem

Central Limit Theorem states that given a sufficiently large sample size from a population with a finite level of variance, the means of all samples from the same population will follow a Gaussian Distribution.

In short, the theorem states that from a Population dataset, if we take a lot of random samples of sufficiently large size, and take the mean of each of these samples, all those means will follow a Gaussian distribution.

This theorem is central to many of the statistical techniques that we will see in the later articles. But first, let's try to verify the Central Limit Theorem using python.

First, let's assume that we know the population from which we have to gather the samples from. For simplicity, let's take our population as numbers ranging from 1 to 6.

Now, let's create a python function for generating the samples and calculate their means, which takes the following arguments:

The size of the population The size of each sample to take from the population (sampling with replacement). Please take a look at this article if you want to understand sampling with replacement and without replacement The number of such samples to take

import pandas as pd def getSampleMeans(population_size, sample_size, num_samples):



population_range = range(1, population_size + 1)



population_data = pd.Series(population_range) sample_means = [population_data.sample(sample_size, replace=True).mean() for i in range(num_samples)]



return sample_means

Let's also create another function which creates a histogram from the sample means generated by the above function using plotly

def plotHistogram(sample_means):

# Import plotly

import plotly

import plotly.plotly as py

import plotly.graph_objs as go

data = [go.Histogram(x=sample_means)]

plotly.offline.init_notebook_mode()

plotly.offline.iplot(data, filename='jupyter/Normal')

The last two lines of the code snippet above are needed for the jupyter notebook to show the plotly visualization.

Now, according to the Central Limit Theorem, the sample means from the same population will follow a Normal Distribution.

Let's assume a population of numbers ranging from 1 to 6. And take 1 sample each time and take a mean of those samples and plot the histogram.

sample_means = getSampleMeans(population_size = 6, sample_size = 1, num_samples = 10000)

plotHistogram(sample_means)

The above code will plot a histogram similar to below plot

Histogram with sample size 1 for a population of 6

But wait !! This doesn’t look like a normal distribution !!

Why doesn't Central Limit Theorem work?

Notice that in the theorem definition above, the statement clearly specifies, sufficiently large sample size from the population.

We took only one sample from the population, and since it is insufficient, it is not working.

Now lets try to increase, the sample size to 2 and the code below,

sample_means = getSampleMeans(population_size = 6, sample_size = 2, num_samples = 10000)

plotHistogram(sample_means)

Then we will see a histogram similar to below.

Histogram with sample size 2for a population of 6

Now, this looks slightly better and closer to the Normal Distribution curve, but we can do much better with a higher sample size. Since we are taking sampling with replacement, we can take a sample size larger than the population size as well.

sample_means = getSampleMeans(population_size = 6, sample_size = 100, num_samples = 10000)

plotHistogram(sample_means)

Histogram with sample size 100 for a population of 6

You can now see that the above sample means approximately converged with the normal distribution.

You can further vary the number of samples, size of the population or the number of rep’s and see how the sample means start converging into a Gaussian Distribution.

We will further explore the applications of Central Limit Theorem in the upcoming articles.",https://medium.com/analytics-vidhya/demystifying-statistics-for-data-scientists-central-limit-theorem-ee9d420fc154,['Sandeep Mellacheruvu'],2019-09-16 10:48:57.695000+00:00,784,"central limit theorem, population, sample, probability distribution, gaussian distribution"
Beautiful Trap of Neural Networks,"Nowadays, Neural Networks (NN) have become a ‘fashionable’ attribute of every modern trend discussion. Most individuals with some degree of technical and even non-technical background can speculate about them with varying levels of confidence, oftentimes making a skeptical impression on an informed listener. If someone attempts to talk about the connection between these mysterious technologies and the human brain’s anatomical structure — this is a grave folly and not the best way to gain credibility as an experienced data scientist.

Simply put Neural Networks are a family of machine learning algorithms able to recognize patterns. While this definition might seem fairly vague, and not very telling, it is still probably the most accurate common language description. Indeed, the algorithms are quite similar in their structure as they all connect input with output through some rigorous and highly intense calculations and are capable of taking on practically any type of raw data with minimal pre-processing. And the outputs easily satisfy every engineer’s demand — whether a regression or a classification problem is at hand. Using these algorithms we can approximate any smooth (and even not-too-smooth) function with a required degree of precision. Sounds like a dream, what could go wrong?!

But, as is always the case, this dream comes at a steep price, which comes in the form of quite a high level of computational complexity of even the simplest NN models. The number of the model’s parameters follows a complexity increase in a multiplicative pattern, meaning these models are very prone to overfitting and require regularization. Model’s hyper-parameter tuning becomes an intuitive hide-and-seek game with very lengthy iterations.

Given these limitations, what is the cornerstone of NN’s success? It is their high performance level! In many situations they outperform other models and their results are unparalleled. Neural Networks simply keep adjusting their parameters until their error is further irreducible. This is called parameter optimization, and there’s a number of algorithms based on the gradient descent approach which I’ve included a non-exhaustive below:

Adam — adaptive moment estimation

— adaptive moment estimation AdaGrad — adaptive gradient

— adaptive gradient AdaDelta — AdaGrad extension

— AdaGrad extension Nesterov’s algorithm

algorithm RMSProp — Root-Mean-Square propagation

— Root-Mean-Square propagation SGD — Stochastic gradient descent

— Stochastic gradient descent LBFGS — Limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm

— Limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm Line — Bresenham’s line algorithm

— Bresenham’s line algorithm Conjugate gradient

Hessian Free (aka truncated Newton) algorithm

Sounds quite straight-forward so far, doesn’t it? A variety of optimization options with a common denominator in the gradient descent approach.

But wait a second, what can possibly be undesirable in such an idillic situation? It is the gradient descent approach which rings the bell for anyone with some background in calculus and applied math. The basic algorithm itself is as old as the hills, but has a very unpleasant caveat. It can fool you, to be blunt. As basic calculus tells us, we could be descending to a local minimum instead of the desired global one which is understandably a problem. There are several types of local minima — saddle, plateau, flat areas as well as other irregularities such as cliffs and exploding gradients. But even quite a simple polynomial

graphed around the origin can look like this beauty!",https://medium.com/analytics-vidhya/beautiful-trap-of-neural-networks-96c6afce9c24,['Ilya Kvyatkovskiy'],2019-11-27 08:59:08.150000+00:00,524,"Neural Networks, Machine Learning, Data Science, Parameter Optimization, Gradient Descent"
Will human-centered design leave the human designers behind?,"A.I. is sort of a peanut butter you can spread across [multiple industries]. With a precise idea of the conditions this thing I’m designing will see in real life, I can design it better. — Maurice Conti

DESIGN THINKING IN THE MACHINE AGE

Thanks to design thinking advocates, designers have emerged to be taking over all sorts of founding and managerial roles in quite a few Big Tech players over the last couple of decades. At the same time, the advent of machine learning didn’t succeed in challenging the design trade per se. Or did it? When artificial intelligence is taking over every other industry at this sweeping pace, there is an array of lingering questions arising in design and tech communities alike. How do we tame the machines to produce compelling graphics inducing human emotion and thought? How do we restrain from feeding AI with biased data by amateur non-designers? Will the exquisite Adobe software finally evolve into robotic tools taking over a human hand? Feelings and thought are very mixed, so let me reserve a piece of personal stance in this blog.

Ai-Da: the first ever robot artist who can draw without any human input.

DITCHING HUMAN DESIGNERS?

In the mid last century, probably the most celebrated commercial designer of the time, Paul Rand, noticed that the majority of ‘professionals’ in ads business

‘…not even discriminating enough to distinguish between good and bad, between trendy and original, nor can they always recognize talent or specialized skills. In the field of design theirs is the dichotomy of being privileged but not necessarily being qualified — after all, design is not their business’.

Some fifty years in, and the situation has aggravated immensely. The abundance of tools and design assets available online is striking giving everyone having some basic Photoshop skills a chance to create graphics of dubious quality. Ditch the years of design training / self-learning — all you need is a Macbook and Adobe Suite (side note — you’d better get a Wacom too).

Now, with robot-designers this design education gap will get completely out of control. On the flip-side though, machines have to be heavily trained, and preferably by top-notch designers to make sure the output is adequate. Sounds like a pricey endeavour to me, however, could be another alternate career pathway for graphic design graduates, and consolation to those thinking that humans will be kicked out of the profession entirely. I do believe that this sort of facilitator / teacher role can prove to be most viable, if not exciting, for evolving artistic trades. Collaborating and sharing knowledge while building #ML tools — I’m down for that.

AI TOOLS — DESIGNERS’ LITTLE HELPERS

In the wake of the recent news of the world’s first machine-artist, a reasonable angle to look at AI is like another tool for artists, like the camera, or the drum machine. Creators are adept to playing with devices and all sorts of collaboration, which is a great way to get those juices flowing, ain’t it so?

More so, with AI extensively taking over the niche of affordable freebee design-tools, it seems logical to exploit the ‘little helper’ further to speed up the flow and automate some tedious tasks like preparing, sorting or unifying design assets. Because no one likes to spend hours cropping / retouching hundreds of jpegs. From creating instant pattern variations and legit UI tools to pretty basic logo generators; to more exquisite design tools like Adobe Sensei and Intelligent Alerts — all these are great time savers loved by designers and businesses. Robotic intelligence is also a great way to help making design decisions bringing complex data analysis to the table to iterate faster and consider multiple options otherwise not available to a human eye.

WHAT’S INHERENTLY WRONG WITH AI DESIGN-GENERATORS?

Well, to my mind, they’re entirely missing the ‘metaphor-behind-the-design point. And if the task of exerting any meaningful emotion resonating with the human audience seems plausible for further robo-generations, that of designing a poster in a way to provoke human brain to ‘close the Gestalt’ will hardly ever be. And of course there is barely a machine (at least, for now) capable of constructing a logo that would satisfy this timeless criterion by Sir Paul Rand: ‘A logo is less important than the product it signifies; what it means is more important than what it looks like’.

Another danger inferred by placing human bias and design illiteracy into machine brain. Design amateurs should steer clear, and it’s responsibility of forward-thinking design community to rely on high-class design educators when feeding artistic data to robobrain. ‘Lack of humility and originality … the absence of restraint, the equation of simplicity with shallowness, complexity with depth of understanding, and obscurity with innovation, distinguishes the quality of work of these times’, — applied to our new realia means ‘do not let engineers with bad taste ever approach the machine.

Cam robodesigners be that exquisite?

More complex AI solutions are still pricey, so the majority design newbies hoping to get some exceptional result with the help of ‘advanced tech’ still have access only to the elementary online generators like. My fellow designers, have you noticed how basic and shallow are Logobank’s graphics? You throw in some yellows and bananas, and it spits out a perfectly aligned ‘Juice Bar’ in a yellow circle-container logo. Come on, we all know that first-level associations never work in design!

FANCY THE AIRBNB LOGO REDESIGNED BY A MACHINE

Sounds dodgy, no? It is human talent that will be in charge of designing machines and machine learning applications, while others will make use of advanced Photoshop and Illustrator tools to cut down the tiresome work. Over the time machines will surely learn design principles and techniques, but can they learn human emotion? According to A.I. Superpowers: China, Silicon Valley, and the New World Order, one thing the machines can’t do is ‘building empathy, compassion, and trust — all of which require human-to-human connection’. Only humans can truly make a product that serves its customer in a meaningful way. As Paul Rand aptly puts it:",https://medium.com/hackernoon/will-human-centered-design-leave-the-human-designers-behind-1e8d778b52d9,['Olya Green'],2019-06-07 11:26:01.119000+00:00,997,"AI, Design Thinking, Machine Learning, Adobe Suite, Design Education Gap"
Building a Youtube music recommender,"Building a Youtube music recommender

From scratch to deploy: recommending youtube music videos to you

Overview

On this article, I`ll comment some details of a simple solution I built to recommend Youtube music videos to myself. You can check out the deployed solution on Heroku at Youtube Music Recommender. I know, it is not beautiful. Maybe one day I will develop my front-end skills.

Then, the main idea here is to go from scratch to deployment. Mainly, this process will include all these steps:

Scrapping video data from Youtube pages; Extracting the video information from each page; Preprocess data from each video into a single dataset; Manually label some of samples, active learning the rest; Extract features from the dataset; Train a Random Forest and a LightGBM model and ensemble them; Build a simple app to serve the model through Heroku.

Also, if you want to check out the code, I will be citing which script is used on each step.

Lets begin!

Scrapping video data from Youtube pages

First of all, we need data! To get some data, I scrapped youtube pages by querying on Youtube search. Since I am looking for music to recommend to myself, I used six queries:

‘Folk rock’, ‘Classical guitar’, ‘Acoustic rock’, ‘Orchestral rock’, ‘Cello songs’ and ‘Brazillian rock’

Then, I saved the first 100 pages of each query. This is done in “search_data_collection.py” script.

Extracting the video information from each page

Now we have data from Youtube search pages, we need to extract information from the videos present on those pages. To do so, I made a parser with BeautifulSoup.

This parser just saves the link, title and query used for each video on the 600 pages. Also, since there is a possibily of videos been duplicated, I dropped the duplicates. This is done in “search_data_parsing.py” script.

Preprocess data from each video into a single dataset

From the parsed data, we are able to web scrap each video. There is much information here, most of the features we can get are from this step. Since this is a simple solution and I did not spent a lot of time engenieering features I used just a few of the possibilities.

There are 2277 videos totally, then this script takes a quite longe time to run, ~2h. The script name is “video_data_processing.py”

Manually label some of samples possibly with active learning

The most tedious and boring part of this work. I had to get labels. So, I opened the raw_data file generated by the previous step on sheets and added a “y” column and manually filled 1000 rows with 0 or 1.

With the labelled data, it is possible to train a Random Forest model and the features extracted on the next step and use this model to identify which samples are the hardest to classify (~0.5 output), then I manually labelled 200 of the samples identified by active learning.

The active learning helps a lot to mitigate the labelling cost. However, the labelling time I was expecting to spend was less then I imagined, so I ended manually labelling everything.

Extract features from the video data

Now, each row of our dataset is a video with its data, like title, number of views, author, description, video height, video width and other information. From these fields, I made feature engineering to extract some features that can be helpful to later.

This way, I extracted the number of views, view per day, video resolution and made a Bag of Words (BoW) with the title string. Surely, there are much more features that could be extracted, especially from video description and video tags. However, I left a more deep feature engineering to future works.

Train a Random Forest and a LightGBM model and ensamble them

With the extracted features and the labels, we can split the dataset in train and validation. I chose not to make a test set, leaving the ultimate validation to production data.

First I trained a LightGBM model optimizing its parameters with a bayesian optimization along with some data pre-processing parameters (like n_gram range for BoW). Then, the Random Forest model was trained with the same pre-processing parameters found on bayesian search. For more details, you can check the implementation. I looked for an ensemble of these two models giving a weight to each model.

To measure how well the model is performing, I chose not to use precision, recall and F1-Score. Since the problem involves a recommendation, the final output should be a ranking of the most relevant recommendations (output closer to 1). Then I used the average_precision_score, roc_auc and log_loss to better evaluate the model.

Also, I made a Learning Curve Analysis (LCA) to check if the data used was enough to have a consistent prediction. The ROC curve and the LCA can be seen on the figures below. The feature extraction and the modelling is on “final_model.py”",https://towardsdatascience.com/building-a-youtube-music-recommender-bbd1b3431da4,['Luis Meazzini'],2020-03-29 18:04:24.873000+00:00,788,"Recommender System, Youtube Music, Feature Extraction, Data Scraping, Machine Learning"
Calendar Heat Maps in R with tidyverse,"First thing is to fill in the rest of the relevant dates that we have no data for. This is done with complete() which is a really neat tidyr function. It’ll generate a sequence of dates from an input. Here, I want dates from September 1st to the end of the year.

data %>%

complete(date = seq.Date(as.Date(""2020-09-01""), as.Date(""2020-12-31""), by=""day"")) %>%

mutate(month = month(date, label = TRUE),

wday = wday(date, label = TRUE),

day = day(date),

week = epiweek(date)) -> df",https://medium.com/@maria-ma/calendar-heat-maps-in-r-with-tidyverse-68abb6919aa6,['Maria Ma'],2020-12-17 06:51:11.579000+00:00,74,"data completion, tidyr, seq.Date, complete(), mutate()"
Data Science Debrief: My First Multiple Regression Analysis + 4 Questions About Stepwise Addition,"What is stepwise addition?

In my words: it’s an automated process for adding and dropping independent variables based on an underlying test

Something more official (from Investopedia): Stepwise regression is the step-by-step iterative construction of a regression model that involves the selection of independent variables to be used in a final model. It involves adding or removing potential explanatory variables in succession and testing for statistical significance after each iteration.

There are three main approaches (from Wikipedia):

Forward selection — start with no variables, perform statistical tests to find the variable giving the most statistically significant improvement to the model, and repeating this until there are no more variables that improve the model

— start with no variables, perform statistical tests to find the variable giving the most statistically significant improvement to the model, and repeating this until there are no more variables that improve the model Backward elimination — start with all variables, choosing to delete a variable based on a certain criterion, and repeating this until no more variables can be deleted in a way that brings on statistically significant loss to the model

— start with all variables, choosing to delete a variable based on a certain criterion, and repeating this until no more variables can be deleted in a way that brings on statistically significant loss to the model Bidirectional elimination — pretty much a combination of both Forward and Backward

The third approach is the one I took.

Check out the repository for this project if you’d like!

How valuable is stepwise addition to regression modeling?

Well, what I can say here is that I saw value in being able to use an automated process for choosing variables. This is especially because I one-hot coded a number of times and so had a ton of dummy variables.

One consequence of having many dummy variables is that when one variable is removed — due to a p-value greater than 0.05, for example — all of the other variables adjust somehow to the absence of the variable dropped, and so the p-values of the variables remaining can then change!

Moreover, with so many independent variables, I didn’t want to drop a variable without having a solid reason.

This is why I imagine the stepwise method is so often used. It’s an industry-accepted way to save time.

How does stepwise addition impact real world solutions?

In a world full of Big Data, the number of potential explanatory variables can be large, and so from this angle, it perhaps helps to make problem-solving a more efficient endeavor?

But, on the other hand, there is concern that stepwise addition is causing more harm than good. In fact, this paper from the Journal of Big Data uses a series of Monte Carlo simulations to demonstrate that stepwise regression is a poor solution to a surfeit of variables. The author’s claim is that, in fact, the larger the number of potential explanatory variables, the more likely stepwise regression is to be misleading.

What are some important things to remember when it comes to using stepwise addition during regression modeling?

Here is what I’ll be keeping in mind going forward, and I hope this helps one of you reading this as well:

Consider stepwise addition as a means, not as the end

After doing some stepwise addition my model had no independent variables with a p-value above 0.05, however, I believed there were still too many variables and decided to continue iterating and trying other methods, so that I could hopefully get to the goal of having a solid R-squared value with as few variables as possible. I also sometimes asked myself why a particular variable was left out, such as the feature I’d engineered (it’s a feature that quantifies the ratio of a home’s living space to its lot size). I think that, without a clear business case or objective in mind, and without proper domain expertise, it could be easy to overfit the stepwise method into a data science practice.

If you log-transform or scale one or many of your variables, include them both in the stepwise addition process

This is what I ended up doing on one occasion. I chose to scale a couple log-transformed variables, thinking that the scaling would improve the variables’ performance when running the models. However, stepwise addition revealed that my former variables had a better “fit” and so eventually I dropped the scaled variables. I also had two variables which had strong multicollinearity and the stepwise method showed how one of these variables’ dummy variables consistently was better “fit” to the model. So, I’ve found stepwise addition useful in this regard.",https://medium.com/datadriveninvestor/data-science-debrief-my-first-multiple-regression-analysis-4-questions-about-stepwise-addition-33c8244aa1d0,['Marvin Lee'],2020-11-09 03:56:20.807000+00:00,754,"Stepwise Regression, Forward Selection, Backward Elimination, Bidirectional Elimination, Big Data"
Dutch GPT2: Autoregressive Language Modelling on a budget,"In the following deepdives, we only retain key snippets of code in this post, instead of plowing through each line of code 🚜. Don’t let us keep you from your beloved plowing though, right this way to the full code 👉 link 👈!

Step 1: Training a Dutch tokenizer

One important part of a language model is the tokenizer, which essentially converts the input sequence into tokens that the embedding layer of the model understands and embeds in a meaningful fashion.

For this, we initialize a fresh ByteLevelBPE tokenizer and train it on our Dutch OSCAR corpus.

Things to note:

👆 we use the same vocab size as the English GPT2 tokenizer

👆 we save it and re-initialize it as a GPT2TokenizerFast object

Step 2: Fastai tokenizer + initial embeddings by comparing both tokenizers

Further down the road, we need two things: a fastai tokenizer for the dataloader, and an updated Dutch embedding matrix.

For the first, we can simply convert it using a one-liner (see below). This tokenizer is used further down in the dataloader.

Secondly, we will update the embedding matrix of the English GPT2 model to only keep the common tokens between the Dutch and English vocabulary. The rest will be set to the mean embedding weight for further finetuning during the training phase.

Step 3: Creating a fastai dataloader

We have data…

The data needs to get loaded…

So we need a fastai dataloader 😉

This also re-uses the Dutch tokenizer we made earlier!

Step 4: splitting the model for gradual unfreezing

As is good practice in finetuning large models, we will do so in a gradual unfreezing fashion!

Each subsequent run will thus also have more parameters to train on.

We divide the decoder (12 layers for GPT2 small, 24 for GPT2 large, etc.) in 3 equal parts.

As such, we finetune the following layers per run:

The LM head + embedding LM head + embedding + decoder block 3 LM head + embedding + decoder block 2

The underlying (though difficult to theoretically or experimentally prove) idea of this is that the earlier decoder blocks would focus more on semantics and structure, while the later layers focus more on the actual dialect (English, Dutch, etc.)

This split is provided to the final Learner along with:

the dataloader created earlier

the base model to start from

a loss function

Step 5: training time

Now for the easy part! We gradually unfreeze the aforementioned groups one by one, and each time fit a single cycle (one run of the entire dataloader).

We didn’t really do much hyperparameter tuning on the learning rate, as our budget and timing for training this beast was limited.

In the end, we save the model, the fruit of our hard labor, some place nice and safe!",https://blog.ml6.eu/dutch-gpt2-autoregressive-language-modelling-on-a-budget-cff3942dd020,['Thomas Dehaene'],2020-12-18 05:22:53.555000+00:00,435,"nl_tokenizer, Dutch_OSCAR_Corpus, GPT2Tokenizer Fast, fastai_tokenizer, dataloader"
Education goes a long way to understanding automation,"Photo by Thomas Kvistholt on Unsplash

In my journey into the technology space, I feel so under-skilled. I’m confident in my leadership abilities and in my accounting and cloud systems knowledge, not to mention radical practice management skills. But all this stuff about data, artificial intelligence and algorithms is intimidating — especially when the folks talking about it all are significantly younger than me. Did you know that accounting analytics could appear on the CPA Exam soon?

So, what does a Radical CPA do? She goes back to school to learn more and figure it out.

It’s proving to be challenging, yet interesting.

I’m taking this 400-level accounting data analytics class and it seems like I need a stronger math/statistics background. I hated stats in college, and still hate it now.

I’ve rallied my husband — who has a master’s degree in some sort of IT — to help me. We spent all afternoon working through the homework together. The next morning, the first words out of his mouth were, “We need to finish those Excel problems.”

We finished the homework, and I said, “It’s so much easier when you help me.” He replied, “I think it’s easier when you aren’t involved.” A couple that does data together, stays together? Not sure about that!

I’ve already learned a ton of new Excel skills and it’s only the first month. I know I’ll never have to program in R, a computer programming language. I’ll probably never quite grasp the math, but with a varied group of people on my team and good delegating skills, I’ll never be without help. It’s really about understanding the opportunities.

As the leaders of today’s firms, we have to learn these new skills so we can remain relevant and ultimately create more value for our customers and firms. How can we be the best business advisors when we don’t even have a basic understanding of these new tools?

For as much as I’m cursing the math, I’m loving the new world of data that my eyes are being opened to. For example, the ability to solve a problem by creatively utilizing tools to find potential answers.

Watch out, I’m dangerous — I can see so many real-world applications and I just might have a new data project for my tech team to tackle.

It’s really frustrating to see all these folks creating fear around data analytics, AI and bots, when it’s really just a new application of the accounting and business skills we already have. It’s just a different approach. Why are you afraid to learn about it?

The best way to combat fear is education. So, what class are you going to take?

Jody Padar, CPA, MST, is CEO and principal at New Vision CPA Group and the author of The Radical CPA and From Success to Significance: The Radical CPA Guide. She is now the accounting ambassador and bot advisor at Botkeeper. You can follow Jody on Twitter @jodypadarcpa.

This post originally ran on Accounting Today, February 2019.",https://medium.com/datadriveninvestor/education-goes-a-long-way-to-understanding-automation-7446cd6b7c3e,['Jody Padar'],2019-03-06 14:21:55.744000+00:00,490,"Accounting Data Analytics, CPACertification, Excel Skills, Artificial Intelligence, Radical Practice Management"
Data Science: Pipelines,"Photo by: Casey Horner

Today we are going to be talking about pipelines. No, you have not stumble on a misclassified oil and natural gas article. You have found one of the greatest tips and tricks in data science, pipelines. Not only are these pipelines better for then environment than oil pipelines, but they are an easy way to automate some of your data science steps and combine them into one. You pass your pipeline all the steps required in your flow, and it will be performed for you. The only requirement is that your steps have a fit or fit transform method. Let’s look at a simple example.

Example

First, we are going to import everything we need.

from sklearn.datasets import load_iris

from sklearn.pipeline import make_pipeline, Pipeline

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier

from sklearn.preprocessing import MinMaxScaler

from sklearn.metrics import accuracy_score

We imported the iris dataset, two pipeline makers (more on this later), train and test splitter, random forest classifier, scaling, and accuracy score (more on this later). This is all we need to make a flow in our data science project. Let’s get our data prepped to be passed into our pipeline.

data = load_iris()

X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=.3)

We loaded the dataset and split it into a train set and test set. We did not clean the dataset because we already know is clean. All we have left is to split it into the two different sets. Now, let’s get to the fun stuff.

pipe = Pipeline(steps=[('scale',MinMaxScaler()),('RFC',RandomForestClassifier())])

pipe.fit(X_train,y_train)

preds = pipe.predict(X_test)

First, we have to make sure that our steps have a fit or fit transform method. Then, we have to add them to a list of tuples in the following format (name,initialized class). After, we use the fit method to fit our data to the MinMaxScaler and to our RandomForestClassifier. After we fit it, we can use the predict method to estimate our target variable for X_test. Let’s look at our other pipeline.

other_pipe = make_pipeline(MinMaxScaler(),RandomForestClassifier())

other_pipe.fit(X_train,y_train)

other_preds = other_pipe.predict(X_test)

So, what’s the difference? I’m glad you asked. The only difference is that in make_pipeline you do not have to pass the classes in a list or name them. To prove they do the same thing, we will benchmark both predictions based on accuracy.

print(accuracy_score(y_test,preds))

print(accuracy_score(y_test,other_preds))

The wrap up

Just like that we created a pipeline for our sample data science project. It’s incredibly easy and fast. This will allow your projects to be more organized and automated.",https://medium.com/swlh/data-science-pipelines-5ab87b6d5dcc,['Alejandro Colocho'],2020-11-03 06:35:42.208000+00:00,393,"Data Science, Pipelines, Automation, Machine Learning, Model Selection"
9 Required Skills to Become a Freelance Data Scientist in 2021,"Whether you’d like to enhance your present income, require a gig to fill in a gap on your resume, or wanting to become your own boss, getting a freelancer data scientist might be the career path for you. The requirement for data scientists across businesses has improved by 417% within the last year, which makes it much easier to discover decent freelance opportunities for 2021 and beyond.

If you’re a qualified data scientist, then you are able to benefit from those opportunities straight away. But if you do not, You Will Need to get or brush up on several technical and non-technical abilities which can be summarized as follows:

Math Skills

You should have an extensive understanding of some of the key concepts of Maths:

Statistics

Statistics supply the essential procedures to get valuable insights from data. The more data you know, the more you may examine and measure the uncertainty in a dataset. Be comfortable with terms like mean, median, mode, standard deviation, and distributions.

You also need to understand when to employ the multiple sampling methods and things to do to help keep out prejudice in the experiments. You also need to understand the way descriptive and inferential statistics are utilized to create and display forecasts.

Probability

A firm grasp on crucial concepts of chance such as Bayes theorem, Central Limit theorem, probability distribution functions, random variables, and expected values may go a very long way in identifying key trends in customers’ data.

Linear Algebra and Calculus

To create a machine learning version, you need to know a reasonable bit of calculus. Knowledge of linear algebra concepts such as matrices and vectors is vital when working together with calculations. It will let you earn instant improvements within an algorithm to impact the end result appreciably.

Read full post:- 9 Required Skills to Become a Freelance Data Scientist in 2021",https://medium.com/@the-tech-trend/9-required-skills-to-become-a-freelance-data-scientist-in-2021-fc849ccf28aa,['The Tech Trend'],2021-06-17 05:45:02.669000+00:00,300,"Data Scientist, Freelance Data Scientist, Math Skills, Statistics, Probability"
Allowing Users to Dynamically Swap Visuals on a Dashboard…,"Allowing Users to Dynamically Swap Visuals on a Dashboard…

One of the techniques, perhaps, widely used to create rich, interactive yet clutter-free dashboards is sheet swapping.

Sheet swapping essentially involves, dynamically swapping of one view to display another.

This brief article will walk you through, how to enable your dashboard users to swap visuals in real-time using containers in Tableau.

Step 1: Make your visuals/charts/views

I’ve created a simple scatter plot and a Bar plot as below.

The Scatter plot above indicates the profit made by each of the sub-categories. The red vertical line provides an insight into what the overall average profit looks like.

The bar plot above indicates the total sales for each of the subcategories.

Step 2: Building a Dashboard

Bring in your first visual/sheet onto a dashboard as below and make it floating in nature.

The important step here is to specify the x/y co-ordinates as well as the width and height of the sheet. These functions are present In the Layout tabs (on your extreme left), once you select the sheet in the dashboard.

Step 3: Layering the Dashboard

Add a floating horizontal or vertical container in your dashboard, on top of the previous sheet we added in the previous steps . Be sure to make the container, the exact same size, with respect to the x/y coordinates along with the width and height as the scatter plot.

Your dashboard should like below once all the above steps are done.

Step 4: Filling the container and adding the final layer to the dashboard

Add your other sheet/visual, in my case, the bar plot inside the horizontal container

Hide your titles from both the sheets, so the swapping works seamlessly.

Step 5:Adding the swap button

Click on the container’s drop-down option and select the ‘Add Show/Hide Button’.

This button will allow us to swap between the two views in the dashboard.

After you select the ‘Add Show/Hide Button’, a cross icon would appear at the top right corner of your visual as below

Step 6: Make the Swap icon intuitive and user friendly.

Choose two images of your choice and save it under your ‘Shapes’ folder, residing in your ‘My Tableau Repository’

These images will be displayed and made clickable for the user to swap between the two visuals you created above.

I chose the below two images:

The below bar chart icon would show up, when the scatter plot visual is displayed, to indicate that the user needs to click on this icon to switch to the bar chart view.

The below scatter chart icon would show up when the bar chart visual is displayed, to indicate that the user needs to click on this icon to switch to the scatter chart view.

Step 7: Add the new icons to the dashboard

Click on the dropdown of the Cross icon like below, and select the ‘Edit Button’ option.

Select ‘Item Shown’ option under ‘Button Appearance’ and choose the image, you wish to be reflected in the default view of your dashboard (Bar chart icon, if your default view is scatter plot or vice versa).

Next, select the ‘Item Hidden’ option under ‘Button Appearance’ and select the other icon.

Once you click ‘OK’, below is what your visual would look like.

Note: I had chosen the scatter plot icon under the ‘Item Shown’ option under ‘Button Appearance’.

The swapping technique only works in the presentation mode, so to see if your swapping action works, switch to the presentation mode by clicking the below-highlighted option.

Your dashboard would display as below:

Once you click on the scatter plot, the visual should change to the scatter plot visual like below, with the bar in the top right to switch back to the bar chart visual.

When you click on the bar chart, it will switch to the scatter view again.

You can find and download this dashboard from Tableau Public.",https://medium.com/swlh/allowing-users-to-dynamically-swap-visuals-on-a-dashboard-48635b13a85f,['Aasavari Kaley'],2020-12-07 19:34:02.979000+00:00,615,"dashboard, sheet swapping, Tableau, visuals, swap button"
COVID-19 Data Visualization using Python,"Data Visualization is the first step towards getting an insight into a large data set in every data science project. Once the data has been acquired and preprocessed (cleaned and deduplicated), the next step in the Data Science Life Cycle is Exploratory Data Analysis which kicks off with visualization of the data. The aim here is to extract useful information from the data.

I have used Python and its few powerful libraries to achieve the task. Also, I have used Google Colab Notebooks to write the code so as to avoid the hassle of installing any IDE or packages in case you wish to follow along.

Let’s get started

The first step is to open a new Google Colab ipython notebook and import the libraries we require.

import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import plotly.express as px ### for plotting the data on world map

Loading the data

These visualizations are based on data as of May 25, 2020. I have used the daily report data published by John Hopkins University for May 25, 2020. The next part of the code deals with loading the .csv data to our project.

path = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/05-25-2020.csv' df = pd.read_csv(path) df.info() df.head()

In just two lines in code we have our data loaded and ready for use as a Pandas Dataframe. the next two lines display the information about the data (metadata) i.e., a total of 3409 rows of data and 11 columns. It also gives us a preview of the first five rows.

Preprocessing the data

Now since our data has loaded successfully, the next step is to preprocess the data before using it for plotting. It will include :

Removing superfluous columns like ‘FIPS’, ‘Admin2', ‘Last_Update’ (since all the data is for single-day — 25th May).

Removing columns ‘Province_State’ and ‘Combined_Key’ since statewide data is not available for all the countries.

Grouping together data by ‘Country_Region’ and rename the column to ‘Country’

df.drop(['FIPS', 'Admin2','Last_Update','Province_State', 'Combined_Key'], axis=1, inplace=True) df.rename(columns={'Country_Region': ""Country""}, inplace=True) df.head()

The data can be grouped together by the ‘groupby’ function of the dataframe. It is similar to the GROUPBY statement in SQL.

world = df.groupby(""Country"")['Confirmed','Active','Recovered','Deaths'].sum().reset_index() world.head()

Finally our data is cleaned and ready to use.",https://towardsdatascience.com/covid-19-data-visualization-using-python-3c8bcfaeff5f,['Jaskeerat Singh Bhatia'],2020-05-27 15:29:19.394000+00:00,357,"data-visualization, data-science, python, exploratory-data-analysis, google-colab"
Predicting Stocks in an Unpredictable World,"We were only interested in the title and description and thus only passed those two columns to the sentiment model.

These articles were first filtered so as to only include articles relevant to the specific company. They were then cleaned of punctuation and other characters before being passed to the sentiment model for a prediction between 0 and 1. On our scale 0 was the most negative and 1 the most positive.

The sentiment prediction model was composed of a convolutional neural network with an initial embedding layer, followed by the convolutional later with 128 filters and a 5x5 kernel size. The output of the convolutional layer is then pooled with a size of 2 and flattened for the two dense layers, the first using a rectified linear activation and finally a sigmoid activation to scale values between 0 and 1.

The code used to create the sentiment model is shown below.

self.model = Sequential() self.model.add(Embedding(vocab_size, 100, input_length=self.max_length)) self.model.add(Conv1D(filters=128, kernel_size=5, activation='relu')) self.model.add(MaxPooling1D(pool_size=2))

self.model.add(Flatten())

self.model.add(Dense(16, activation='relu'))

self.model.add(Dense(1, activation='sigmoid'))

The model was trained using a data set of 1,967 financial headlines labeled by sentiment and split 85%-15% for training and testing respectively. The result of this was a model that achieved an accuracy of 81% and an F1 score of 0.875 when evaluated on the testing set.

The final remaining task was to bring both parts together and start trading! This was accomplished using a trading bot built on the Alpaca library. A buy or sell was initiated from the predicted change in price while the number of shares was a product of the sentiment prediction. Our best result so far has been buying before a 16% jump in Nikola stock and then selling the next morning to avoid a 20% plunge!

As with any data science project, we incorporated visuals to aid our understandings of how our respective models were doing. The most effective way to display quantitative data was to graph our predicted price vs the actual price.

Predicted vs Actual NFLX

To understand our sentiment predictions, sentiment was plotted over the last 30 days and compared with the list of headlines.

Our predictions about on how readers felt about NFLX, MSFT, and AAPL from 10/19 to 11/12.

We also tracked our trading history by plotting our buy/sell points on top of the stock’s closing price. (At the time of writing we had not collected enough trading data to warrant including this chart, however an updated copy can be found on our Github.)

In addition to using visuals to evaluate the success of our models, we used them to optimize hyper-parameters. Specifically, we plotted the F1 score of our quantitative model versus the number of epochs used to train the model. The peak of these plots represented the optimum number of epochs needed to train the model.

F1 Score vs Epochs for MSFT

Finally, we evaluated how our quantitative model performed across different securities with a bar chart representing the F1 score for each company.

Quantitative model performance across each company tracked.

At the time of writing, our trading bot has been mildly successful, with our biggest victory being trading Nikola. However, we plan to keep the trading bot running, adjusting our models and methods to hopefully make money!

Closing Thoughts: Throughout this project, we familiarized ourselves with the steps to answering a data science problem! We gathered both quantitative and qualitative data through web-scraping and APIs, cleaned that data, created multiple machine learning models, and visualized our results, all things of which we had no prior experience.

We were also moderately successful from a financial standpoint, as our project would have been profitable had we spent actual money! However, it is important to remember that we were trading during a rebound from one of the sharpest drops in market history and that the Dow was on its way to a record 30,199 points as of December 14th. Had we traded during a more standard period for the market, our results may have differed.

Our group would like to give special thanks to Cal Poly Professor Stanchev for his generous mentorship and willingness to continue as a senior project.

See our Github for the full source code: https://github.com/d-mooers/SentimentalTrader",https://medium.com/@mikemoschitto/predicting-stocks-in-an-unpredictable-world-cb0415ce8805,[],2020-12-20 04:49:39.711000+00:00,675,"sentiment analysis, convolutional neural network, web-scraping, APIs, machine learning models"
Racist Algorithms Still Flourish in 2020,"Racist Algorithms Still Flourish in 2020

It's more than just math

It’s understandable to think that algorithms that power Google searches or determine who gets an organ transplant can’t be racist or cause harm because they’re just math. But research has shown time and time again that algorithms pick up and echo the structural racism present in our society.

Investigative tech outlet The Markup wrote about some of 2020's most egregious examples of racist algorithms, including those used in policing.

For instance, Google’s advertising portal associated the phrase “Black girls,” “Asian girls,” and “Latina girls” with porn. But not the phrase “White girls.”

In medicine, Black patients are erroneously thought to have a higher risk for certain diseases, often derived from racist assumptions in past research. This can make it harder to get organ transplants. When this is coded into algorithms, racism is automated.

Read more at The Markup below:",https://momentum.medium.com/racist-algorithms-still-flourish-in-2020-6ab80cba5fe3,['Dave Gershgorn'],2020-12-30 06:32:40.394000+00:00,144,"Racist Algorithms, Structural Racism, Algorithmic Bias, Machine Learning, Artificial Intelligence"
End-to-End Multi-label Classification,"Deployment

AWS based deployments are pretty simple sequential procedures. From previous steps will be already having a minimal prediction script (i.e. Recognize_Item class), which we’ll import to a flask app. Later we’ll dockerize the module with all the dependencies for better distribution, then host the docker image to EC2 instance. We’ll follow 3 step procedure for deployment:

STEP 1 : Flask Setup

Recognize_Item (from part 3) will load model and necessary functions to handle multi-label classification. Note that we need to catch the images as bytes array, so the load_image function reads the image as BytesIO.

import recognize_item_class as cf

import flask app = flask.Flask(__name__)

def home():

return ""<h1> Image Recognition Module [ON]</h1>"" @app .route(""/"")def home():return "" Image Recognition Module [ON] ""

def predict():

'''

Note: Ensure image through flask properly hit here

'''

if flask.request.method == ""POST"":

if flask.request.files.get(""image""):

#

image = flask.request.files[""image""].read()

#

image = reco.process_img(image)

#

result = reco.predict_all(model, image)

#

return result

return ""<h1>No Result</h1>"" @app .route(""/predict"", methods=[""POST""])def predict():'''Note: Ensure image through flask properly hit here'''if flask.request.method == ""POST"":if flask.request.files.get(""image""):image = flask.request.files[""image""].read()image = reco.process_img(image)result = reco.predict_all(model, image)return resultreturn "" No Result "" if __name__ == ""__main__"":

print('* flask starting server...

* Loading model file..')

#

# Declare the Recognition object

reco = cf.Recognize_Item()

#

# Load Model (avoid-reloading)

global model

model = reco.load_model()

model._make_predict_function()

#

app.run('0.0.0.0', 5000, debug=True)

Whenever we receive a post (to 5000:/predict ) request to the Flask app it will pre-process the image, predict all labels and finally return the prediction in a structured json response.

STEP 2 : Docker Setup

(with patience!) Create Docker Image

You need to have Docker installed and setup. You can get instructions for setup from here. First, you will need to create a directory with only the required files and scripts for deployment. Create a requirements.txt file using pip freeze or manually writing the required packages. Robert has given a detailed article for the same. Now we need to make a file with the name Dockerfie (and no extension) in the folder where the flask app and script are located. The file should contain:

FROM python:3.6

WORKDIR /app

COPY . /app

RUN pip install -r requirements.txt

EXPOSE 8500

CMD [“python app.py”]

Few things to know: FROM instruction initializes a new build stage and sets the Base Image for subsequent instructions. RUN instruction will execute any commands in a new layer on top of the current image and commit the results. In our case, we want to install all the dependencies for our flask application to the docker image. EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. Here, the purpose of CMD is to provide defaults for an executing container. In our case, run the flask application. For more info on the docker commands, check the official docker documentation here. It is beautifully written and easy to understand.

5. After the Dockerfile is made, we need to create the docker image using the command: docker build -t item_recognition:latest . which means that we are building a docker image with the name item_recognition with tag latest and . represents that the Dockerfile is contained in this folder.

Now it might take some time to download the base image and install all the requirements. After Step 5 is done, you can view your docker image with the command docker images .

Saving the image to use it on EC2 instance

Now that we have the image in our system, we can run the image using the commands specified in the next step, but if we need to save the image and then use it on another system, follow these steps:

docker save item_recognition:latest > item_recognition.tar . This will create a .tar file containing the image. Now you can transfer it to any system or instance which has docker installed and load this image using the command: docker load < item_recognition.tar

Now once you list docker images on the new system or instance, you will find the item_recognition image there. This section would have not been possible without Arjun Muraleedharan

….

STEP 3 : AWS Setup

(This steps assumes that reader is familiar with EC2 instances)

Start an EC2 instance, recommended will be a t3.medium or t3.small machine. Once you have an instance ready, upload the Docker image from step 2 to the machine to the instance. Create a new screen and using following command run the docker image to an external port (say 8500):

$ screen -S item_recognition $ docker run -it -p 8500:5000 item_recognition

Remember the port number for posting the requests & that’s all folks.

Let’s check if everything is fine [POSTMAN]

API’s results in visualization Postman

….

The journey of this article started with Data explorations using pandas, then we built a customized ResNet type multi-label model using Keras (same procedure applicable with tf.keras as well). We efficiently packaged our model with necessary functions, this package could easily be called in our Flask app. We dockerized the environment and finally hosted it with AWS.

Thanks for your time, dear reader!",https://medium.com/swlh/end-to-end-multi-label-classification-971dd09daf65,[],2020-05-12 08:54:58.094000+00:00,778,"Deployment, AWS, Flask, Docker, EC2 instance"
Graph Attention Convolution for Point Cloud Semantic Segmentation,"Standard convolution has inherent limitations in semantic point cloud segmentation due to the isotropy of its characteristics. It ignores the structure of the object, resulting in a poor description of the object contour in the segmentation result, and a small part of pseudo-regions.[1] proposes a new graphic attention convolution (GAC), its convolution kernel can be dynamically carved into a specific shape to adapt to the structure of the object. Specifically, GAC assigns specific attention weights to different neighboring points, and selectively focuses on their most relevant parts according to their characteristics. The shape of the convolution kernel is determined by the learned attention weight distribution.

Illustration of the standard convolution and GAC on a subgraph of a point cloud. Left: The weights of standard convolution are determined by the neighbors’ spatial positions, and the learned feature at point 1 characterizes all of its neighbors indistinguishably. Right: In GAC, the attentional weights on “chair” (the brown dotted arrows) are masked, so that the convolution kernel can focus on the “table” points.Source[1]

Although GAC is simple, it can capture the structural features of the point cloud and perform fine-grained segmentation to avoid feature pollution between objects. Theoretically, the author conducted an in-depth analysis of GAC’s expressive power and showed how it learns the characteristics of point clouds. Empirically, through challenging experiments on indoor and outdoor data sets, the GAC proposed by the author demonstrates the most advanced performance on the basis of existing deep learning methods.

Method

Consider a graph G(V,E) constructed from a given point cloud

according to their spatial neighbors, where

and E ⊊ |V| x |V|,represent the set of vertices and edges respectively and N is the number of vertices (points).

Denote N (i) = {j : (i, j) ∈ E} ∪ {i} as the neighbor set of vertex i and let

be a set of input vertex features, each feature h_i ∈ R^F is associated with a corresponding graph vertex i ∈ V , where F is the feature dimension of each vertex.

For each fixed point, take the relative position and feature difference of the current node and surrounding nodes as input, predict the corresponding weight value α.

To handle the size-varying neighbors across different vertices and spatial scales, the attentional weights are normalized across all the neighbors of vertex i as follows:

where a˜_{ij,k} is the attentional weight of vertex j to vertex i at the k-th feature channel.

Therefore, the final output of the proposed GAC can be formulated as follows:

where * represents the Hadamard product, which produces the element-wise production of two vectors, and b_i ∈ R^K is a learnable bias.

The overall process can be as shown in the figure below,

GACNet architecture. Our GACNet is constructed on the graph pyramid of a point cloud. On each scale of the graph pyramid, the proposed GAC is applied for local feature learning, followed by the graph pooling for resolution reducing in each feature channel. After that, the learned features are interpolated back to the finest scale layer by layer for point-wise label assignment.

Similar to pointnet++, GACNet first randomly samples in the point cloud , and then crosses out an area with a radius of k (same as ball query in pointnet++) and randomly samples adjacent point, and finally calculate the weight of attention through such sampling points. Finally, the feature of each point is calculated by weighting and then Graph Pooling (in fact, max or mean pooling) is used to downsample the point cloud. The upsampling process is similar to pointnet++, using interpolation + direct connection method.

Conclusion

GAC is designed to selectively focus on the most relevant part according to their dynamically learned features. The shape of the convolution kernel is then determined by the learned distribution of the attentional weights. Though simple, GAC can capture the structured features of point clouds for finegrained segmentation and avoid feature contamination between objects.

References

1.L. Wang, Y. Huang, Y. Hou, S. Zhang and J. Shan, “Graph Attention Convolution for Point Cloud Semantic Segmentation,” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 2019, pp. 10288–10297, doi: 10.1109/CVPR.2019.01054.",https://medium.com/@nabil.madali/graph-attention-convolution-for-point-cloud-semantic-segmentation-f354b009d7b5,['Nabil Madali'],2020-12-24 14:51:20.228000+00:00,669,"convolution, point cloud, semantic segmentation, graph attention convolution, GAC"
A data journalist’s new year’s resolutions,"What data journalists should stop doing

‘Using word clouds . I can’t believe that I still see this being used as a data visualization that is presented as significant.’ Saleem Khan (JOVRNALISM and INVSTG8.net, Canada)

. I can’t believe that I still see this being used as a data visualization that is presented as significant.’ Saleem Khan (JOVRNALISM and INVSTG8.net, Canada) ‘Acting like a data geek, telling good story is the point.’ Pinar Dag (Dag Media, Turkey)

‘Pontificating, thinking we know all the answers.’ Simon Rogers (Google, US)

‘Doing those data-mapping “projects” that actually have no story, that only show us how it looks like on the map! — so what???’ Alla Rybina (Gothenburg University, Sweden)

What do you wish for in 2018?

We started with the following wishes to the international data journalism community:

- more accessible, open and usable data

- more collaborations and investigations holding the powerful accountable

- safer conditions for journalists worldwide to use data in their work

- more exciting projects for everyone

Then experts went into specifics:

‘A tool where all open databases of all governmental agencies and organizations exist in one place, are updated frequently and accessed in a standardised way. Also, should be free. It is a greedy request but I would love to see it happen in my lifetime.’ Walid Al-Saqaf (Södertörn University, Sweden)

exist in one place, are updated frequently and accessed in a standardised way. Also, should be free. It is a greedy request but I would love to see it happen in my lifetime.’ Walid Al-Saqaf (Södertörn University, Sweden) ‘Converting geospatial data is always annoying (for example, if I want to compare constituency data from 2000 with those of 2017, but the constituencies have changed since then). Some teams have already worked out more or less great solutions themselves. I would wish to build a good software for this tasks with experts for everyone to use.’ Marie-Louise Timcke (Journocode, Germany)

is always annoying (for example, if I want to compare constituency data from 2000 with those of 2017, but the constituencies have changed since then). Some teams have already worked out more or less great solutions themselves. I would wish to for this tasks with experts for everyone to use.’ Marie-Louise Timcke (Journocode, Germany) ‘We data journalists need more new skills. So I wish we can get more code skills, to work more with data scientist this year. It is also important to create a little more budget in this area. I would like to see projects that have more interactive and high public interest . I also hope to see more mobile-compatible data visualization .’ Pinar Dag (Dag Media, Turkey)

to work more with data scientist this year. It is also important to create a little in this area. I would like to see projects that have . I also hope to see more .’ Pinar Dag (Dag Media, Turkey) ‘I wish for data journalism to acually move society. Japan is not advanced at all due to a lack of skills and trust in old media (tv stations and newspapers).’ Yuichi Yazaki (Visualizing.jp, Japan)

2018 resolutions for newbies — Where to begin when learning data journalism?

‘I think it’s all about stories, tools you can learn, stories and working out which data to use with them, that’s the thing.’ Simon Rogers (Google, US)

‘I agree. If you focus too much on the techniques from the beginning, the journalistic direction of the stories suffers.’ Marie-Louise Timcke (Journocode, Germany)

‘My recommendation is to focus on asking the question you want answered before you even start your report. Then go about seeing how you could get, analyse and visualise the data and answer it in a concrete and clear way. It takes more time, but is well worth it.’ Walid Al-Saqaf (Södertörn University, Sweden)

‘Yep — just fishing is not really an option.’ Simon Rogers (Google, US)

‘I agree — stories are always the key, and I’d add that we should always be thinking about what’s the best way to tell a data-led story for your audience, instead of fellow data journalists, whether that’s a huge interactive project or a straight 400-word piece of copy.’ Ashley Kirk (The Telegraph, UK)

‘Knowing how to code and use other tools is helping a lot. But without the journalistic part, data journalism is just making fancy pictures. Interdisciplinary teams have a great advantage here. While the journalist is considering how best to show the topic from a journalistic point of view, the designer can think about how to design it so readers will get it at once, and so on.’ Marie-Louise Timcke (Journocode, Germany)

If you think we’ve missed something, do get in touch via the comments section.",https://medium.com/data-journalism-awards/a-data-journalists-new-year-s-resolutions-474ef92f7e8f,['Marianne Bouchart'],2018-02-01 14:22:00.170000+00:00,763,"Data Journalism, Data Visualization, Storytelling, Data Analysis, Data Mapping"
Significance of the derivative,"CALCULUS DERIVATIVES

Significance of the derivative

The process of finding critical values

After derivative theory posts, we will start to see some of the applications that make this technique one of the most important in mathematics and therefore at machine learning.

Maximum and Minimum of a function

The theorem of the local maximum that we already introduced, can be extrapolated to the whole function domain.

Let f be a function and A a set of numbers contained in the domain of f. A point x in A is a maximum point for f on A if f(x)≥f(y) for every y in A. The number f(x) itself is called the maximum value of f on A.

Notice that there can be multiple maximum points for the same function at distinct x values. The minimum point definition is obtained inverting the previous definition, ergo changing f(x)≥f(y) for f(x)≤f(y).

Derivatives at maximum and minimum points

As you can expect, maximum and minimum points will always be a change in the derivative of the function, that allows us to demonstrate that:

Let f be any function defined on (a,b). If f is a maximum or a minimum point for f on (a,b), and f is differentiable at x, then f’(x)=0.

Local maximums and minimums

Let f be a function, and A a set of numbers contained in the domain of f. A point x in A is a local maximum[minumum] point for f on A of there is some δ > 0 such that x is a maximum[minumum] point for f on A ⋂ (x-δ,x+δ ).

Critical points

Not all x that makes f’(x) = 0 will be maximums or minimums, they are types of critical points:

A critical point of a function f is a number x such that f’(x)=0. The number f(x) is called a critical value of f.

In order to find the maximum and minimum of f, we have to check the following values:

The critical points of f in [a,b] .

. The endpoints, a and b .

and . Points x in [a,b] such that f is not differentiable at.

Some important theorems for critical point detection

The Rolle’s Theorem

If f is continuous on [a,b] and differentiable on (a,b), and f(a)=f(b), then there is a number x in (a,b) such that f’(x)=0.

This can lead to a constant function or to a function that changes the gradient between both values, so f’(x)=0.

The mean value theorem

Rolle theorem allows us to demonstrate the following:

If f is continuous on [a,b] and differentiable on (a,b), then there is a number x in (a,b) such that

Mean value theorem, self-generated.

Classifying critical points

Increasing and decreasing functions

A function f is increasing on an interval if f(a)<f(b) whenever a and be are two numbers in the interval with a<b. The function f is decreasing on an interval if f(a) > f(b) for all a and b in the interval with a<b.

Second derivatives for critical point classification

Now we know how to find a critical point with the first derivative and check the type of them using the left and right derivative values, we can use the second derivative to skip the lateral limit calculations.

Suppose f’(a)=0. If f’’(a)>0, then f has a local minimum at a; if f’’(a)<0 the nf has a local maximum at a.

Two strong theorems for simplification

The Cauchy Mean value theorem

If f and g are continuous on [a,b] and diffrentiable on (a,b), then there is a number x such that [f(b)-f(a)]g’(x)=[g(b)-g(a)]f’(x). If g(b)≠g(a), and g’(x)≠0, this can be written as:

Cauchy mean value theorem, self-generated.

L’Hôpital’s rule

L’hôpital Rule, self-generated.

Conclusion

In this post, we introduced how to use derivatives to find local maximums and minimums, they allow us to find possible solutions to our cost function optimization. This will allow us to determine the gradient easier.",https://medium.com/ai-in-plain-english/significance-of-the-derivative-4c1f505e9b88,['Adrià Serra'],2020-09-22 18:55:36.715000+00:00,604,"calculus, derivatives, criticalvalues, maximums, minimums"
How DBSCAN works and why should we use it?,"Ok, let’s start talking about DBSCAN.

Density-based spatial clustering of applications with noise (DBSCAN) is a well-known data clustering algorithm that is commonly used in data mining and machine learning.

Based on a set of points (let’s think in a bidimensional space as exemplified in the figure), DBSCAN groups together points that are close to each other based on a distance measurement (usually Euclidean distance) and a minimum number of points. It also marks as outliers the points that are in low-density regions.

Parameters:

The DBSCAN algorithm basically requires 2 parameters:

eps: specifies how close points should be to each other to be considered a part of a cluster. It means that if the distance between two points is lower or equal to this value (eps), these points are considered neighbors. minPoints: the minimum number of points to form a dense region. For example, if we set the minPoints parameter as 5, then we need at least 5 points to form a dense region.

Parameter estimation:

The parameter estimation is a problem for every data mining task. To choose good parameters we need to understand how they are used and have at least a basic previous knowledge about the data set that will be used.

eps: if the eps value chosen is too small, a large part of the data will not be clustered. It will be considered outliers because don’t satisfy the number of points to create a dense region. On the other hand, if the value that was chosen is too high, clusters will merge and the majority of objects will be in the same cluster. The eps should be chosen based on the distance of the dataset (we can use a k-distance graph to find it), but in general small eps values are preferable. minPoints: As a general rule, a minimum minPoints can be derived from a number of dimensions (D) in the data set, as minPoints ≥ D + 1. Larger values are usually better for data sets with noise and will form more significant clusters. The minimum value for the minPoints must be 3, but the larger the data set, the larger the minPoints value that should be chosen.

You can find more about parameter estimation here.

Why should we use DBSCAN?

The DBSCAN algorithm should be used to find associations and structures in data that are hard to find manually but that can be relevant and useful to find patterns and predict trends.

Clustering methods are usually used in biology, medicine, social sciences, archaeology, marketing, characters recognition, management systems and so on.

Let’s think in a practical use of DBSCAN. Suppose we have an e-commerce and we want to improve our sales by recommending relevant products to our customers. We don’t know exactly what our customers are looking for but based on a data set we can predict and recommend a relevant product to a specific customer. We can apply the DBSCAN to our data set (based on the e-commerce database) and find clusters based on the products that the users have bought. Using this clusters we can find similarities between customers, for example, the customer A have bought 1 pen, 1 book and 1 scissors and the customer B have bought 1 book and 1 scissors, then we can recommend 1 pen to the customer B. This is just a little example of use of DBSCAN, but it can be used in a lot of applications in several areas.

How can we easily implement it?

As I already wrote (tip: don’t believe in everything I write) the DBSCAN is a well-known algorithm, therefore, you don’t need to worry about implement it yourself. You can use one of the libraries/packages that can be found on the internet. Here is a list of links that you can find the DBSCAN implementation: Matlab, R, R, Python, Python.

I also have developed an application (in Portuguese) to explain how DBSCAN works in a didactically way. The application was written in C++ and you can find it on Github.

References:",https://towardsdatascience.com/how-dbscan-works-and-why-should-i-use-it-443b4a191c80,['Kelvin Salton Do Prado'],2019-06-03 12:11:41.732000+00:00,655,"DBSCAN, data clustering algorithm, parameter estimation, e-commerce, Matlab"
Machine Learning in Bioinformatics,"Machine Learning in Bioinformatics

Classification of genes & Performance comparison of common classifiers

Image by Arek Socha from Pixabay

Bioinformatics is a field of study that uses computation to extract knowledge from biological data. It includes the collection, storage, retrieval, manipulation and modeling of data for analysis, visualization or prediction. Here we would use machine learning to classify genes of E. Coli bacteria.

Let us understand the basics of Genetics. DNA or deoxyribonucleic acid is the hereditary material in humans and almost all other organisms. The DNA is made up of four chemical bases: Adenine(A), Guanine(G), Cytosine(C), and Thymine(T). Adenine pairs up with Thymine and Guanine pairs up with Cytosine. Each base is also attached to a sugar molecule and a phosphate molecule. A base, sugar, and phosphate together form the nucleotide.

Genes are made up of very long nucleotide sequences. Genes can be classified into 2 categories (Promoter & Non-Promoter) based on the nucleotide sequence.

It is impractical to look at the long nucleotide sequences. Hence we would be looking at the short nucleotide sequence(57 sequences) of genes of E. Coli bacteria and predict whether the gene is Promoter or Non-Promoter.

The data set ‘genes’ is of UIC machine learning data base.

The necessary packages are imported.

# Importing the necessary packages import numpy as np

import pandas as pd

The data is read into ‘genes’.

# Import UCI molecular biology (promoter gene sequences) dataset url = ‘https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/promoter-gene-sequences/promoters.data'

columns = [‘Class’,’id’,’Sequence’] # Naming the columns

genes = pd.read_csv(url,names = columns) # Reading the data # Naming the columns# Reading the data

Let us explore the dataset.

genes.shape # Prints the shape of ‘df’

The ‘genes’ dataframe is of shape (106,3) which implies 106 cases and 3 columns.

genes.nunique() # Prints number of unique elements in each column

Class :- 2, id :- 106, Sequence :- 106

There are 2 unique classes, 106 unique ids and 106 unique sequences. We know there are 106 cases. So each of the ‘id’ and ‘Sequence’ is unique.

genes[‘Class’].unique() # Prints the unique elements in ‘Class’ column

The 2 unique classes are labelled as ‘+’ (Promoter) and ‘-’ (Non-Promoter).

genes.head() # Displays first 5 rows of ‘genes’ dataframe

Some of the ‘Sequences’ begin with ‘\t’ special character which denotes a tab space. The preprocessing step would be to remove these special characters so that there are only bases(a,g,c,t).

# Removing ‘\t’ in ‘Sequence’ column for i in range(genes.shape[0]):

genes[‘Sequence’][i] = genes[‘Sequence’][i].replace(‘\t’, ‘’)

Let us verify this by checking the sequence and length of sequence of 5 randomly selected sequences.

for i in np.random.randint(0,106,5):

print(genes[‘Sequence’][i])

print(len(genes[‘Sequence’][i]))

print(‘

’)

‘\t’ characters have been removed and length of each sequence is 57.

Only the ‘Sequence’ column is important in classifying the gene. Let us make a dataframe of the ‘Sequence’ column.

# Initializing a dataframe ‘nucleotide_sequence’ of size (106,57) nucleotide_sequence = pd.DataFrame(np.random.randn(106,57)) # Storing each base of ‘Sequence’ in ‘nucleotide_sequence’ for i in range(genes.shape[0]):

nucleotide_sequence.loc[i] = list(genes[‘Sequence’][i]) # Storing 'Class' column of 'Sequence' as a column in 'nucleotide_sequence' nucleotide_sequence['Class'] = genes['Class']

Let us look at the dataframe ‘nucleotide_sequence’

nucleotide_sequence.head() # Displays first 5 rows of ‘nucleotide_sequence’ dataframe

plt.figure(figsize=(8,8))

plt.xticks(fontsize=20)

plt.yticks(fontsize=20)

plt.xlabel('Class',fontsize=20)

plt.ylabel('count',fontsize=20)

sns.countplot(nucleotide_sequence['Class']) # Displays count of each element in 'Class' column

There are equal number of Promoters and Non-Promoters in the dataset given.

Machine learning algorithms can be applied only on numerical values. Hence we convert the string characters of ‘nucleotide_sequence’ into numerical values using ‘get_dummies’ function.

# Switch to numerical data using pd.get_dummies() function numerical_nucleotide = pd.get_dummies(nucleotide_sequence)

Let us print the ‘numerical_nucleotide’ dataframe.

print(numerical_nucleotide.head()) # Prints first 5 rows of 'numerical_nucleotide'

There are 2 class columns :- ‘Class_+’ and ‘Class_-’. The class label ‘Promoter’ or ‘Non-Promoter’ can be found by just looking at one of the column (either ‘Class+’ or ‘Class-’). So one of them is dropped and the other is renamed as ‘Class’.

# Remove one of the class columns and rename the other to simply ‘Class’ numerical_nucleotide.drop(‘Class_-’,axis = 1,inplace = True)

numerical_nucleotide.rename(columns = {‘Class_+’:’Class’},inplace = True)

Now let us print the ‘numerical_nucleotide’ dataframe.

print(numerical_nucleotide.head()) # Prints first 5 rows of 'numerical_nucleotide'

Below given is the snippet of code that serves the same purpose as get_dummies() function (Optional)

Now that we have processed the data, the next step is train the data using machine learning classifiers.

The dataset is split into training and test sets.

# Import train_test_split from sklearn.model_selection import train_test_split # X is input and y is output X = numerical_nucleotide.drop(‘Class’,axis = 1)

y = numerical_nucleotide[‘Class’] # Split into train and test sets X_train,X_test,y_train,y_test = train_test_split(X,y)

The necessary classifiers and metrics are imported.

# Performance comparison of 9 classifiers from sklearn.neighbors import KNeighborsClassifier

from sklearn.neural_network import MLPClassifier

from sklearn.tree import DecisionTreeClassifier

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier

from sklearn.naive_bayes import GaussianNB

from sklearn.svm import SVC

from sklearn.metrics import classification_report,accuracy_score

from sklearn import model_selection

The Cross validation scores of the classifiers are calculated.

The Cross validation scores of classifiers are displayed.

Classifier rankings based on Cross validation scores :-

(1 Neural Net (MLP Classifier) & SVM Linear

(2 Naive Bayes

(3 AdaBoost

(4 SVM rbf

(5 K Nearest Neighbors

(6 Decision Tree

(7 Random Forest

(8 SVM Sigmoid

Neural Net (MLP Classifier) & SVM Linear have the best cross validation score of 0.9625. But it is not right to generalize the performance of classifier on new unseen data based on cross validation score. Hence we check the performance of classifier models on the test set.

for i in range(len(models)):

a = models[i].fit(X_train,y_train) # Fitting the model

y_predict = a.predict(X_test) # Predicting using trained model

print(accuracy_score(y_predict,y_test))

print(classification_report(y_predict,y_test))

K-Nearest Neighbors

Neural Net (MLP Classifier)

Decision Tree

Random Forest

AdaBoost

Naive Bayes

SVM Linear

SVM rbf

SVM Sigmoid

Classifier rankings based on Test scores :-

(1 Neural Net (MLP Classifier), Naive Bayes & SVM Linear

(2 K-Nearest Neighbors & SVM rbf

(3 AdaBoost & SVM Signoid

(4 Decision Tree

(5 Random Forest

Neural Net (MLP Classifier), Naive Bayes & SVM Linear have the highest accuracy score and f1-score.

The performance of the classifier models can be further improved by hyperparameter tuning. If you are curious, please do read hyperparameter tuning of Neural Networks.

Happy Reading!",https://medium.com/datadriveninvestor/machine-learning-in-bioinformatics-d81b1b3eaba2,['S Joel Franklin'],2019-12-22 19:38:26.165000+00:00,915,"machinelearning, bioinformatics, genes, classification, promotergenesequences"
How to Create a Simple Dashboard With Plotly,"Photo by Negative Space from Pexels

Introduction

In order to have a better visualization of your data, you may want to gather everything in one place, creating a dashboard. Plotly Express is a tool that will let you to create awesome interactive graphs easily and, with Plotly Dash, you can use them to compose a dashboard.

In this article, I’ll show you how to create two graphs and put them together in a simple dashboard. We’ll be using COVID-19 data around the world as an example.

Method

The first graph we’ll create is a scatter plot on a world map showing the evolution of cases through the weeks of 2020. The data on COVID-19 worldwide is from Our World in Data COVID-19 dataset. We’ll also need coordinates of each country, which I got from this dataset on Kaggle.

import pandas as pd

import plotly.express as px covid = pd.read_csv('full_data.csv')

covid = covid[['date', 'location', 'total_cases', 'total_deaths']] countries_geo = pd.read_csv('countries.csv')

countries_geo = countries_geo[['latitude', 'longitude', 'name']]

countries_geo.columns = ['latitude', 'longitude', 'location']

Now that we imported both datasets, let’s merge them. After merging, I also converted the date column to datetime format and filled null values with 0.

countries = pd.merge(covid, countries_geo, how='inner', on='location') countries['date'] = pd.to_datetime(countries['date'])

countries = countries[countries.date.dt.year == 2020].copy()

countries.fillna(0, inplace=True)

The last preparation we need to do with our data is to organize them by week. I achieved this by selecting only the rows with data from Sundays and then created a new column with the corresponding week number using the isocalendar method.

countries_week = countries[countries.date.dt.weekday == 6].copy()

countries_week['week'] = countries_week['date'].dt.isocalendar()['week']

Now, we can create our first graph using the Plotly’s scatter_geo function.

fig_map = px.scatter_geo(

countries_week,

lat = 'latitude',

lon = 'longitude',

size = 'total_cases',

hover_name = 'location',

hover_data = [""total_cases"", ""total_deaths""],

title = ""COVID-19 Evolution in 2020"",

animation_frame = ""week"",

color=""total_cases"",

labels={

'total_cases': 'Cases',

'total_deaths': 'Deaths',

'week': 'Week',

'latitude': 'Latitude',

'longitude': 'Longitude'

},

projection = 'natural earth'

) fig_map.show()

The result is a world map with points with size proportional to the total amount of cases in each country. The color also changes according to that number and you can see historical data using the slide at the bottom. If you hover over a point, it shows some extra infos.

The next graph we’ll create is a line plot showing the evolution of cases in the current 10 countries with highest amount of confirmed cases. For this, we need to know what countries are these. Therefore, we’ll select the rows of the last week, sort them by total_cases and extract only the first 10 as a list.

top10 = countries_week[countries_week.week == countries_week.week.max()].sort_values('total_cases', ascending=False).iloc[:10, 1].to_list()

Now, we can create the graph comparing COVID-19 evolution in these countries.

fig_line = px.line(

countries_week[countries_week.location.isin(top10)],

x=""week"",

y=""total_cases"",

log_y = False,

color='location',

hover_name = 'location',

hover_data = [""total_cases"", ""total_deaths""],

title = ""COVID-19 Evolution in 2020 (Top 10 Countries)"",

labels= {

'total_cases': 'Cases',

'total_deaths': 'Deaths',

'week': 'Week',

'latitude': 'Latitude',

'longitude': 'Longitude',

'location': 'Country'

}

) fig_line.show()

The result is a simple line plot, on which you can select lines to hide and zoom in sengments of the graph.

The last thing we’ll do is creating a dashboard web page with these two graphs using Plotly Dash and Ngrok. Plotly Dash is a framework for building machine learning anddata science web apps. Ngrok is a tool that gives us a public URL for our localhost (you’ll need this if you’re running your code using Google Colab like me).

First, we’ll need to install Plotly Dash on our Colab machine.

!pip install dash

!pip install dash-html-components

!pip install dash-core-components

!pip install dash-table

Now, we can import everything we’ll need.

import os.path

import sys, json

import requests

import subprocess import numpy as np

import pandas as pd

import plotly.express as px from requests.exceptions import RequestException

from requests.adapters import HTTPAdapter

from requests.packages.urllib3.util.retry import Retry from collections import namedtuple

You can define the following function to download Ngrok.



if not os.path.isfile('ngrok'):

!wget

!unzip -o ngrok-stable-linux-amd64.zip

pass def download_ngrok():if not os.path.isfile('ngrok'):!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip !unzip -o ngrok-stable-linux-amd64.zippass

We’ll also use a function to set up Ngrok’s tunneling.

Response = namedtuple('Response', ['url', 'error']) def get_tunnel():

try:

Tunnel = subprocess.Popen(['./ngrok','http','8050'])

retry = Retry(connect=3, backoff_factor=0.5)

adapter = HTTPAdapter(max_retries=retry)

session.mount(' session = requests.Session()retry = Retry(connect=3, backoff_factor=0.5)adapter = HTTPAdapter(max_retries=retry)session.mount(' http://' , adapter)

res.raise_for_status() res = session.get(' http://localhost:4040/api/tunnels' res.raise_for_status() tunnel_str = res.text

tunnel_cfg = json.loads(tunnel_str)

tunnel_url = tunnel_cfg['tunnels'][0]['public_url'] return Response(url=tunnel_url, error=None)

except RequestException as e:

return Response(url=None, error=str(e))

The last step is to create a script that will create and serve the dashboard. We’ll use %%writefile on top of the code so it’ll be written on disk.

%%writefile my_dash_app.py

import dash

import dash_core_components as dcc

import dash_html_components as html

from dash.dependencies import Input, Output

import pandas as pd

import plotly.express as px external_stylesheets = [' https://codepen.io/chriddyp/pen/bWLwgP.css' app = dash.Dash(__name__, external_stylesheets=external_stylesheets) # THE PREVIOUS CODE TO CREATE THE GRAPHS COMES HERE # You can edit this layout to include more graphs

app.layout = html.Div(children=[

html.Div([

html.H1(children='COVID-19 Evolution in 2020'), dcc.Graph(

id='graph1',

figure=fig_map

),

]),

html.Div([

dcc.Graph(

id='graph2',

figure=fig_line

),

]),

]) if __name__ == '__main__':

app.run_server(debug=True, use_reloader=True)

Now, run the function to download Ngrok if you haven’t done it yet and run the server with the following commands.

tunnel = get_tunnel()

print(tunnel)

!python my_dash_app.py

You should see something like this.

Click on the first link to open your dashboard and there it is!",https://medium.com/swlh/how-to-create-a-simple-dashboard-with-plotly-b705cefeb564,['Eduardo Sarmento'],2020-11-25 07:16:53.223000+00:00,762,"data visualization, dashboard, Plotly Express, Plotly Dash, COVID-19 data"
Acrotrend and Nuffield Health’s Strategic Partnership,"Thanks to the recent updates Acrotrend has helped us make to our organisation, we are now more competitively positioned to provide our customers with an enhanced service. DAVE ANKERS, TECHNOLOGY, STRATEGY & PLATFORM DELIVERY DIRECTOR, NUFFIELD HEALTH

We have been working as a strategic partner to Nuffield Health — the UK’s largest not-for-profit healthcare provider — to create a more amenable data environment for their current and future analytics needs and capabilities.

Nuffield Health holds enormous quantities of data. This data was creating data silos and making it difficult not only to obtain a true and consolidated view of customers, but to pull business-critical reports required to guide their key strategy of ‘Connected Health’.

Acrotrend’s team worked with Nuffield Health’s key business stakeholders to understand their functional and business goals from an insight and analytics perspective and mapped these requirements onto a fit-for-purpose and ready-for-future tech stack.

Our architecture and technology selection instantly made the data environment more amenable for their analytics needs and capabilities, and together with the implementation roadmap, made it really clear on how Nuffield Health could get the most of the proposed solution.

Fill out the form here to read the full case study and find out more about how we did this.

To find out more about how Acrotrend can help your business become truly data-driven, please contact us today to request a free consultation.",https://medium.com/acrotrend-consultancy/acrotrend-and-nuffield-healths-strategic-partnership-ad5ea18d1d4b,['Acrotrend Consultancy'],2019-11-06 11:34:53.061000+00:00,224,"Nuffield Health, Dave Ankers, Technology, Strategy, Platform Delivery"
The age of autonomous transportation,"Today is the beginning!

Here are the first 10 chapters, with the related 96 Companies

Apollo

Argo AI

Aurora

AutoX

Cruise

Oxbotica

Tesla

Voyage

Waymo

Zoox

AImotive

Dataspeed Inc.

Didi

Ford

Mobileye

Motional

PonyAI

Renovo

StreetDrone

Yandex

Einride

Embark

Peloton

Plus.ai

Starsky Robotics

Tesla

Torc Robotics

TuSimple

Volvo Trucks

Waymo

2getthere ZF

Apollo

Aurrigo

Coast Autonomous

Cruise

EasyMile

Hochbahn

Navya

Local Motors

Sensible 4

Amazon

FedEx

LMAD_EU

Neolix

Nuro

StarshipRobots

Udelv

Yandex

Aurora

Autox

Dataspeed

Phantom Auto

Renovo Auto

Torc robotics

Voyage

Waymo

AEye

Benewake

Blickfeld

Cepton

Innoviz

LeddarTech

Luminar

Quanergy Systems

RoboSense

Velodyne Lidar

aeva

Hesai

Ibeo

Ouster

LivoxTech

Lumotive

MicroVision

Sense Photonics

SICK

XenomatiX

Airobotics

Drone Delivery Canada

Elroy Air

Flytrex

Matternet

ParaZero Drone

Percepto

Skydio

SkyX Systems

Zipline

AirspaceSystems

Altitude Angel

Amazon

Azur Drones

DroneUp

Iris Automation

Sabrewingair

Ups

Wing

XSun",https://medium.com/@guidaautonoma/the-age-of-autonomous-transportation-1b40b34cf48,[],2020-12-24 14:40:26.500000+00:00,37,"Autonomous Vehicles, Robotics, AI, Automotive Industry, Self-Driving Cars"
Cloud APIs For a Rainy Day,"Cloud APIs For a Rainy Day

Writing, Deploying, and Sharing Computations Have Never Been Easier With the Wolfram Language

Photo by Nick Scheerbart on Unsplash

A (long) while back I had the good fortune to take a hiking trip to the western part of Ireland, near Galway and through the Connemara National Park. The scenery was spectacular, but it was the ever-changing weather patterns that really caught my attention. It was not unusual for a single day of hiking to have multiple rain-sun cycles and many days had afternoon rainbows.

Located near the Atlantic Ocean, a warm stream of water arrives from the Gulf of Mexico and the Caribbean Sea. This makes the climate of Ireland milder than can be expected based on its latitude, and also limits extreme temperature swings year-round. But it rains, a lot, in Ireland.

To get an idea of the amount of rainfall in Ireland, I visited the web site of the Irish Meteorological Service (Met Éireann). They offer both current weather information and historical data. From the historical data page, I downloaded a dataset with time-series data for 1850–2010. This dataset includes rainfall for 25 stations across Ireland.

In this story, I will discuss how you can use this data and create a callable API that acts on this data to visualize it.

Each station has its own CSV file. Here is a sample for the station at University College, Galway:

(image by author)

This raw data is best represented in the Wolfram Language with a TimeSeries object. We take the data fields from the cvs expression, generate the monthly dates with DateRange, and construct the TimeSeries (quick note: a notebook with complete code is provided at the end of this story):

rain = Flatten[csv[[4 ;;, 2 ;;]]]; dates = DateRange[

DateObject[{1850, 1}, ""Month""],

DateObject[{2010, 12}, ""Month""]]; ts = TimeSeries[Thread[{dates, rain}]]

Using this time-series object we can create a visualization function:

(image by author)

Which allows us to create a plot for any time range, for example, the 1950s:

(image by author)

At this point, it is very easy to deploy this function as an API. We define an APIFunction which defines the connection between URL parameters and functional arguments:

api = APIFunction[{

""y1"" -> ""Integer"", ""m1"" -> ""Integer"",

""y2"" -> ""Integer"", ""m2"" -> ""Integer""},

ExportForm[GalwayRainPlot[{#y1, #m1}, {#y2, #m2}], ""PNG""] &

];

Next, we deploy this APIFunction to a CloudObject with CloudDeploy:

CloudDeploy[api, CloudObject[""GalwayRainPlot.api""], Permissions -> ""Public""]

At this point, the API is publicly available. You can change the permission settings to keep your APIs more private. Here is an example to call the API for rainfall in Galway for the 1960s.

(image by author)

You can of course follow this workflow for any computation you want to share with others in the cloud. The concept of deploying any computation to the Wolfram Cloud in just a few lines of code is very powerful. For more information check out the following guide page:

As for hiking in Ireland, I would love to go again someday. But if you’re planning to go, make sure to bring a raincoat and an extra pair of dry socks!",https://towardsdatascience.com/cloud-apis-for-a-rainy-day-f4f06468346,['Arnoud Buzing'],2020-09-24 18:33:54.101000+00:00,492,"Cloud APIs, Wolfram Language, Cloud Deployment, API Functionality, Rainfall Data"
Python for loops—enumerate & zip,"Two Simple Ways to Loop More Effectively in Python

Use enumerate and zip to write better Python loops

Many loops. Photo by David Streit on Unsplash

The Python range function is very powerful, but it can often be replaced with other built-in functions that make your loops easier to write and read. In this article, I’ll show you when you can replace range with enumerate or zip .

Use enumerate(object) instead of range(len(object))

Problem 1: You often have objects like lists you want to iterate over while also keeping track of the index of each iteration. Given the list below, how would you use a for loop to generate the desired output?

my_list = ['apple', 'orange', 'cat', 'dog'] # desired output

Item 0: apple

Item 1: orange

Item 2: cat

Item 3: dog

Solution 1: Use for i in range(len(my_list))

for i in range(len(my_list)):

print(f""Item {i}: {my_list[i]}"") Item 0: apple

Item 1: orange

Item 2: cat

Item 3: dog

Better solution: Use for i, value in enumerate(my_list)

for i, value in enumerate(my_list):

print(f""Item {i}: {value}"") Item 0: apple

Item 1: orange

Item 2: cat

Item 3: dog

Explanation: enumerate loops over the iterator my_list and returns both the item and its index as an index-item tuple as you iterate over your object (see code and output below to see the tuple output). We unpack the index-item tuple when we construct the loop as for i, value in enumerate(my_list) .

for i in enumerate(my_list):

print(i) (0, 'apple') # tuple, which can be unpacked (see code chunk above)

(1, 'orange')

(2, 'cat')

(3, 'dog')

Problem 2: Given the same list as above, write a loop to generate the desired output (ensure the first index begins at 101 instead of 0).

my_list = ['apple', 'orange', 'cat', 'dog'] # desired output

Item 101: apple

Item 102: orange

Item 103: cat

Item 104: dog

Solution 2: Use for i, value in enumerate(my_list, 101)

The function enumerate(iterable, start=0) lets you start counting the index at any desired number (default is 0).

for i, value in enumerate(my_list, 101):

print(f""Item {i}: {value}"") Item 101: apple

Item 102: orange

Item 103: cat

Item 104: dog

Takeaways

The enumerate built-in function loops over an iterator and returns the index and the item as an index-item tuple from the iterator as you go

built-in function loops over an iterator and returns the index and the item as an index-item tuple from the iterator as you go Use enumerate(object) instead of range(len(object)) for more concise and readable code

instead of for more concise and readable code Provide a second parameter to indicate the number from which to begin counting (0 is the default)

Use zip to iterate multiple objects in parallel

Problem 3: You have multiple lists or objects you want to iterate in parallel. Given the three lists below, how would you produce the desired output?

my_list = ['apple', 'orange', 'cat', 'dog']

my_list_n = [11, 12, 25, 26]

my_list_idx = [1, 2, 3, 4] # desired output

1. apple: 11

2. orange: 12

3. cat: 25

4. dog: 26

Solution 3: Use range(len(my_list)) to get the index

for i in range(len(my_list)):

print(f""{my_list_idx[i]}. {my_list[i]}: {my_list_n[i]}"") 1. apple: 11

2. orange: 12

3. cat: 25

4. dog: 26

Better solution: Use zip(my_list_idx, my_list, my_list_n)

for i, obj, count in zip( my_list_idx , my_list , my_list_n ):

print(f""{i}. {obj}: {count}"") 1. apple: 11

2. orange: 12

3. cat: 25

4. dog: 26

Explanation: You can use zip to iterate over multiple objects at the same time. zip returns tuples that can be unpacked as you go over the loop. See examples below to understand how this function works.

zip returns tuples:

for i in zip(my_list_idx, my_list, my_list_n):

print(i) (1, 'apple', 11) # 3-item tuple

(2, 'orange', 12)

(3, 'cat', 25)

(4, 'dog', 26)

Each element within the tuple can be extracted manually:

for i in zip(my_list_idx, my_list, my_list_n):

print(f""{i[0]}. {i[1]}: {i[2]}"") # i is a 3-item tuple 1. apple: 11

2. orange: 12

3. cat: 25

4. dog: 26

Takeaways

The zip built-in function can iterate over multiple iterators at the same time.

built-in function can iterate over multiple iterators at the same time. zip creates a lazy generator that produces tuples

Conclusion

Using the built-in Python functions enumerate and zip can help you write better Python code that’s more readable and concise.

If you are interested in improving your data science skills, the following articles might be useful:

For more posts, subscribe to my mailing list.",https://towardsdatascience.com/two-simple-ways-to-loop-more-effectively-in-python-886526008a70,['Hause Lin'],2020-08-20 13:48:20.588000+00:00,657,"python, loops, enumerate, zip, range"
Data Science and Machine Learning books that you should read in 2020,"Data Science and Machine Learning books that you should read in 2020

“Until I feared I would lose it, I never loved to read. One does not love breathing.” — Harper Lee

It’s accepted truth that reading books provides more insights and knowledge than any videos or article. Taking that into consideration this time my article it is not about anything related to data science or machine learning techniques or algorithm, rather about books where you can find all the contents in the arena.

Source : Pexels Images by Min An

So, In this article I will be talking about some of the most read books and frequently referred one by experts in the field. So what to wait for, let’s dive in further.

1. An Introduction to Statistical Learning with application in R.

This book details about various statistical learning techniques such as graphs, variance distributions etc. along with various techniques on which foundation of various machine learning algorithms such as linear regression, its variants such as lasso and ridge regression are laid upon, Including some of the classification techniques.

This book could help us to make a strong statistical foundation if you want to set up in the field of data science and machine learning.

Click here to download PDF.",https://medium.com/an-idea/data-science-and-machine-learning-books-that-will-take-you-from-zero-to-hero-dca91470f197,['Shobhit Srivastava'],2020-10-08 05:43:02.095000+00:00,204,"Data Science, Machine Learning, Books, Reading, Statistical Learning"
Analyzing Ethereum Classic with Google BigQuery,"I’m happy to announce, through collaboration with Allen Day and Evgeny Medvedev of Google, Ethereum Classic is now part of the Google BigQuery dataset. There’s even a CoinDesk article about it.

This means that now, it’s more easier than ever to query the Ethereum Classic network using regular SQL which allows for more seamless data analysis. You can also download the datasets directly in Kaggle to use in your notebooks for analysis.

Why is this a Big Deal?

Blockchains are most accessible to cryptography and blockchain engineers, who are most familiar with the inner workings of the client and viewing the data. On the finance side, analysts mostly observe market data. For your average data scientist or entrepreneur who wants to do quick analysis of block data, they’re stuck with running their own Geth or Parity node and trying to parse it. Even if that’s successful, they need to continue doing so to keep up to date with more recent Ethereum Classic data.

With the Ethereum Classic dataset being constantly available on Google BigQuery and continuously updating daily, researchers, entrepreneurs and stakeholders can quickly analyze Ethereum Classic’s network and blocks without having to worry about the data engineering aspects or their cloud infrastructure or node setups. They can just focus on the data science and let us do all the rest!

How Did This Project Come About?

We at the ETC Cooperative always believed the most important stories can be told with data, and Ethereum Classic is no exception. We were planning on quantifying Ethereum Classic’s decentralization for a while now, along with many other analysis we want to explore. For your average data scientist, getting blockchain data can be tricky, if not daunting.

It immediately seemed clear to us there was no easy way to query Ethereum Classic’s blockchain history, so we set about looking for solutions.

Apache Airflow is used to update the dataset daily

I’ve stumbled upon the Ethereum-ETL library developed by Evgeny Medvedev to parse EVM based data, and quickly worked with him on adding more features to allow it to parse Ethereum Classic nodes. We also used Apache Airflow, a powerful workflow library, to update the Ethereum Classic dataset daily. It was all adapted from the original project for Ethereum dataset. If you would like to know how it was built or you’d like to build your own, check out this post.

Analyzing The Ethereum Classic Dataset

The Ethereum Classic dataset contains several tables, like blocks , transactions and traces which contains lots of interesting information about the blockchain activity. Furthermore, you can even analyze traces and smart contracts (who wants to analyze the DAO smart contract before and after the hack?). For more ideas on how what you can analyze with smart contracts, check out this post.

In this Medium post, we will show our analysis of a few interesting things we found in Ethereum Classic.

We set out to quantify decentralization in Ethereum Classic. For that, we will measure the Gini Coefficient of Ethereum Classic using BigQuery. The Gini coefficient is a measure of the income or wealth distribution of a population.

We will use it in two instances here:

1) Daily Top 10K Account Balances

2) Daily Mining Rewards.

Furthermore, we will be using Balaji Srinivasan’s “Nakamoto Coefficient” for further analysis of Ethereum Classic, as discussed in his blog post Quantifying Decentralization. Nakamoto Coefficient is a proposed measure by Balaji regarding a blockchain’s subsystem. It measures what is the minimum number of entities that can influence more than 51% of a subsystem. To check out that, head on over to Analyzing ETC With Nakamoto Coefficient section below. Over there, we also introduce the nakamoto Python library I’ve built for data analysis.

Running SQL on BigQuery

The following queries and plots have full source code in this Kaggle notebook that you can clone and run on your own.

With BigQuery, we can run a query to get the average daily hash rate as shown in the following plot.

We can plot the output of the following query for the hash rate using Plotly with the following graph shown here, measured in Giga hash.",https://medium.com/@yazanator/analyzing-ethereum-classic-with-google-bigquery-df55822ec6a6,['Yaz Khoury'],2019-02-06 15:06:51.036000+00:00,669,"Ethereum Classic, Big Query, Google Big Query, EVMData, Blockchain Data Analysis"
How to import Google Sheets data into a Pandas DataFrame using Google’s API v4 (2020),"Importing Google Sheet data to a Pandas DataFrame

Google Sheets is a useful way to share data and collaborate remotely. But transferring the data to environments such as Python on a regular basis can be burdensome. This post will cover how to set up the latest Google Sheets API, v4 as of June 2020, for Python. We’ll also cover how to extract data from a Google Sheet range (or even an entire sheet) into a Pandas data frame.

Before you start:

Before you start you’ll need the following:

Python 2.6 or greater (Python 3 recommended)

Pip/pip3 package management tool (comes standard with Python 2 >= 2.7.9 or Python 3 >= 3.4)

A Google Account (and Google Sheet containing your data-of-interest).

API Setup:

First, you’ll need to enable the Google Sheets API on your Gmail account, where the Google Sheet is stored. Login to your Gmail account and visit the Google Sheets API QuickStart Guide for Python. You’ll see a blue “Enable Google Sheets API” button. Click on the button (marked as [1] in the image below):

Image by author

Select “Desktop App” from the dropdown menu (marked as [2] in the image below) and click “Create” (marked as [3] in the image below). This will create a client configuration, which we’ll need to set up the initial connection via the API:

Image by author

Click the blue “DOWNLOAD CLIENT CONFIGURATION” button (marked as [4] in the image below).

Image by author

You should now have a file named “credentials.json” downloaded. You’ll need to move this file to your working directory.

Install Google’s Client Library:

Next we’ll need to install Google’s Client Library using pip:

pip install — upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib

NOTE: Rather than running the sample Python code Google provides in their guide, we’ll be using a modified version of their script.

Generating our API user tokens:

First, we’ll set up our gsheet_api_check() function. It looks for an existing token.pickle file (which stores our user access and refresh tokens). If no token.pickle file is found, the function will prompt you to log into your Google Gmail account. The credentials.json must be present in your working directory to initiate token.pickle creation/refresh. The function will specify and return the credentials we’ll use to make API calls:

import pickle

import os.path

from google_auth_oauthlib.flow import InstalledAppFlow

from google.auth.transport.requests import Request def gsheet_api_check(SCOPES):

creds = None

if os.path.exists('token.pickle'):

with open('token.pickle', 'rb') as token:

creds = pickle.load(token) if not creds or not creds.valid:

if creds and creds.expired and creds.refresh_token:

creds.refresh(Request())

else:

flow = InstalledAppFlow.from_client_secrets_file(

'credentials.json', SCOPES)

creds = flow.run_local_server(port=0) with open('token.pickle', 'wb') as token:

pickle.dump(creds, token) return creds

NOTE: After completing the initial credentials setup we can discard gsheet_api_check() and load our tokens directly during future API calls. Alternatively, it may be beneficial to keep gsheet_api_check() because the function checks if our tokens are expired and/or missing. If the tokens are expired/missing, gsheet_api_check() will then initiate a refresh of the user tokens (by prompting you to re-login to your Google account). You can decide whether you want to continue using gsheet_api_check() or load the tokens directly.

Making the API call:

Next we’ll define a function that makes the API call and pulls the data we want from Google Sheets. The pull_sheet_data() function establishes the API call and pulls the data we want. If no data is found the function will print “No data found.”, otherwise it will confirm the data has been retrieved by printing “COMPLETE: Data Copied” and return our data:

from googleapiclient.discovery import build def pull_sheet_data(SCOPES,SPREADSHEET_ID,RANGE_NAME):

creds = gsheet_api_check(SCOPES)

service = build('sheets', 'v4', credentials=creds)

sheet = service.spreadsheets()

result = sheet.values().get(

spreadsheetId=SPREADSHEET_ID,

range=RANGE_NAME).execute()

values = result.get('values', [])



if not values:

print('No data found.')

else:

rows = sheet.values().get(spreadsheetId=SPREADSHEET_ID,

range=RANGE_NAME).execute()

data = rows.get('values')

print(""COMPLETE: Data copied"")

return data

NOTE: pull_sheet_data() can be modified to define other API tasks such as appending data to a google sheet, updating existing data, or creating new spreadsheets.

Running our API call and saving the data as a Pandas DataFrame:

Next we will need two pieces of information. First, we need to find and copy the ID of the spreadsheet-of-interest. This can be found in the URL of your Google Spreadsheet (marked as [5] in the image below):

Image by author

Second, we’ll need the name of the spreadsheet tab from which we’ll pull the data from. Alternatively, you can explicitly define the range of cells you want to retrieve if you wish to pull specific sections of data from the spreadsheet (examples provided below):

#Pulls data from the entire spreadsheet tab.

RANGE_NAME = 'spreadsheet_tab_name!' or #Pulls data only from the specified range of cells.

RANGE_NAME = 'spreadsheet_tab_name!A2:C6'

Finally, we’ll bring all of our code together by specifying our pull_sheet_data() parameters, running the functions, and storing the retrieved data into a Pandas DataFrame. Be sure to replace ‘spreadsheet_url_ID ’ with the spreadsheet ID you copied, and replace ‘spreadsheet_tab_name!’ with your spreadsheet tab name (and range if necessary):

import pandas as pd

SPREADSHEET_ID = 'spreadsheet_url_ID'

RANGE_NAME = 'spreadsheet_tab_name!' SCOPES = [' https://www.googleapis.com/auth/spreadsheets' SPREADSHEET_ID = 'spreadsheet_url_ID'RANGE_NAME = 'spreadsheet_tab_name!' data = pull_sheet_data(SCOPES,SPREADSHEET_ID,RANGE_NAME)

df = pd.DataFrame(data[1:], columns=data[0]) df

And now you’re ready to explore your retrieved data within Python, all without manually downloading or importing the dataset!",https://towardsdatascience.com/how-to-import-google-sheets-data-into-a-pandas-dataframe-using-googles-api-v4-2020-f50e84ea4530,['Erik Yan'],2020-12-11 06:50:41.406000+00:00,794,"Google Sheets, Python, Pandas Data Frame, API Setup, Client Library Installation"
Big O Notation,"Big O Notation

Arrays and their Built-in Methods

Photo by Oliver Schwendener on Unsplash

In my previous Big O Notation blogs, we discussed Time and Space Complexity and Objects. Now we are going to look at Big O and how it relates to arrays and their built-in methods.

An array is an ordered data structure and can be used to store data of any type. Or, in more technical terms, “When we initialize an array in a programming language, the language allocates space in memory for your array, and then points that starting variable to that address in memory. Then the program assigns a fixed amount of memory for each element” (Learn). Below is a simple example of an array with multiple data types.

const array = [ ""a"", {}, 7, [], false]

In terms of Big O, arrays are favorable when we need fast access to elements. In the example above, we can access each element by its index; array[0] = “a” and array[4] = false. Since the elements are indexed, our computers know exactly where the element is and can go directly to the element. Moreover, an array with 2 elements or 2,000 elements has the same time complexity, O(1), for accessing methods.

Arrays can also be fast in some insertion or removal methods. These types of methods have either an O(1) or O(n). The difference comes from where in the array elements are inserted or removed. If elements are inserted or removed from the end of an array, these methods have an O(1) because just the last indexed element is impacted. However, if elements are inserted or removed from the beginning of an array, all elements are impacted. For example, if we add an element to the beginning of the array, the added element takes over index 0, and the element that previously was in index 0 is now at index 1 and so forth. Since the number of operations grows with the number of elements in the array, these methods have an O(n).

Lastly, Arrays have an O(n) for searching methods. This is because the worst case scenario for a search method is checking every element in the array.

Array Built-in Methods

Arrays come with many different built-in methods, but the methods we will look at here are push, pop, shift, unshift, concat, slice, splice, sort, and map.

The first two methods push and pop involve adding or removing an element from the end of the array. As we mentioned earlier, these types of methods have an O(1) because they only impact the array’s last element.

Shift and unshift, involve adding or removing from the beginning of the array. As explained earlier, the run-time of these methods grow with the inputs or O(n) because they impact all indexes in the array.

Similar to shift and unshift, concat, slice, and splice have an O(n). Concat is used to combine two or more arrays into a new array. Moreover, as the number of inputs in each array that you are combining grows, so does the run-time in a linear fashion. Slice involves making a copy of an array and, also, has a run-time that grows in a linear fashion with inputs. Splice is a method that, “changes the contents of an array by removing the existing elements and/or adding new elements” (MDN). Although splice can remove/add elements from anywhere in the array it has an O(n) because, in the worst case scenario, all array inputs would be impacted.

Sorting methods have an O(n * log n) which is worse than O(n). This is because sorting methods require comparisons between the elements and elements may need to be visited more than once.

The last built-in method we will discuss for arrays is map. This method has an O(n) and works by creating, “…a new array populated with the results of calling a provided function on every element in the calling array” (MDN). The reason that this function has an O(n) is due to fact that it must interact with every element in the array at least once. So, the size of the array directly impacts the run-time.

See below for a summary of the method types and built-in methods discussed in this blog and how they relate to Big O Notation.

Summary Table based on lecture material presented in Colt Steele’s Udemy Course

Thank you for taking the time to learn more about arrays through the lens of Big O notation. This is the last blog in my Big O notation series, I hope you now understand the importation of Big O notation and can apply it as you write and interpret code.

Resources

Steele, C. (n.d.). JavaScript Algorithms and Data Structures Masterclass. Online Course.

Arrays Underneath. (n.d.). Retrieved October 30, 2020, from https://learn.co/lessons/arrays-underneath

“Array.prototype.splice().” MDN Web Docs, developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/splice.

“Array.prototype.map().” MDN Web Docs, developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/map.

Gain Access to Expert View — Subscribe to DDI Intel",https://medium.com/datadriveninvestor/big-o-notation-14fa1e4538a1,['Matthew Sedlacek'],2020-11-07 16:32:51.102000+00:00,791,"bigonotation, arrays, builtinmethods, timecomplexity, spacecomplexity"
Machine Learning-based Market Map Solution For A Leading Financial Data Provider,"This is a case study on automated service that leverages Machine Learning to provide a user with a list of companies according to the requested parameters.

Overview

This project was developed for a leading financial data provider that covers the global venture capital, private equity, and public markets. We remained its business partner for over 13 years, starting with a small team and growing into a primary full-cycle technology provider. A product manager from a financial data provider analyzed the market and while their competitors already had some kind of Market Map solution, we were tasked with perfecting this idea and developing it as a separate service with a visual interface, flexible functionality, and additional features.

Challenge

The client has a database with descriptions of the companies and their verticals. Each company is assigned a number of keywords. For a Market Map solution, we should take these descriptions, verticals, and keywords and clusterize them. The process of clusterization must be repeated numerous times, because each company operates in different areas. Each industry and vertical has its own hierarchy with multiple levels. Depending on the required extent of details, hierarchies must be adjustable.

We must turn company descriptions into vectors and clusterize them with Machine Learning algorithms. But this is where the first challenge comes in — each group must be named correctly. Manual processing will take a lot of time; hence, picking and adjusting the right ML algorithms is the only option. The second challenge is choosing the right hierarchy among industry codes, sectors, and verticals. Additionally, not all companies have a full or completely accurate description — it’s hard to group such companies correctly.

Read the full case study here: https://spd.group/cases/market-map-solution-for-a-financial-data-provider/",https://medium.com/@spd-group/machine-learning-based-market-map-solution-for-a-leading-financial-data-provider-49625a287bd0,[],2020-12-23 12:35:40.140000+00:00,276,"machinelearning, marketmapping, financialdata, clusterization, hierarchy"
Latest picks: In case you missed them:,"Sign up for The Variable

By Towards Data Science

Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.",https://towardsdatascience.com/latest-picks-modern-recommender-systems-a8c09fea7db0,['Tds Editors'],2021-01-26 14:27:38.879000+00:00,36,"Data Science, Towards Data Science, The Variable, Sign Up, Thursday"
Submit Kaggle Solutions From Command-line Using Phantomjs,"Submit Kaggle Solutions From Command-line Using Phantomjs

Allowing you to submit programatically, even from your remote machine.

You might have participated in some Kaggle competitions where the dataset was in hundreds of megabytes (compressed!). You most probably used a machine in the cloud that can load this amount of data, train some model, and predict the also-quite-large test set. At this point you’ve become eager to submit your predictions to see your position on that leaderboard. So what do you do? Well, instead of downloading this large submission file to your machine and then uploading it to Kaggle, wouldn’t it be easier if you could just upload that submission from the cloud to Kaggle directly? Or even better, automate the whole submission process?

You can do exactly that, by installing my node package kaggle-submit:

npm install kaggle-submit

then running:

kaggle-submit -s <submission> -u <user> -p <pass> -c <competition>

and that’s it! Head over to your submissions page to see the results.

The package uses phantomjs to launch a headless browser, a.k.a a browser without a window. The package then takes your credentials and logs in to Kaggle just like you would normally do to submit your predictions, it uploads your file to the competition you specified.

It is also worth mentioning kaggle-cli which is a Python library providing an unofficial command line tool, to download and upload data. However, a quick look at the issues and commit history will tell you that it’s functionality has been on and off for some time. This is natural when developing unofficial APIs, since the only way to maintain it is keeping up with the website’s UI/API changes.

We shall do our best efforts in maintaining and developing this one. We hope you find it useful and, please suggest more features, fixes, and improvements on the github repo.",https://medium.com/optima-blog/submit-kaggle-solutions-from-command-line-using-phantomjs-df307d302087,['Muhammad Atef'],2016-06-21 03:58:49.708000+00:00,294,"kaggle, command-line, phantomjs, programatically, submission"
The Risks & Rewards of Emerging Technologies within Public Services,"By Brandie Nonnecke, Director, CITRIS Policy Lab & Camille Crittenden, Executive Director, CITRIS and the Banatao Institute

Investments in digital infrastructure in the public sector have lagged for years. The COVID-19 pandemic has torn back the curtain to reveal a dilapidated IT framework that undergirds many of the services that millions rely on for education, food, and public safety. Within the first three months of the pandemic, over 44 million Americans filed for unemployment, overwhelming current government software systems and public service workers. Now is the time to remediate patchy systems and strengthen the tools and platforms needed to meet the demand for public services likely to continue well into the future.

The pandemic only highlights a long-standing need to improve public sector processes. With decades of rising workload demands, worker shortages, and budget constraints, many public sector institutions have been ramping up deployment of emerging technologies to support productivity. Machine learning-powered tools are increasingly used to support decision-making in classrooms and child welfare offices, chatbots can field common questions from the public and offer appropriate resources in law enforcement and food assistance programs, and robotic process automation (RPA) bots assist to streamline social service applications.

While emerging technologies such as natural language processing, machine learning, and RPA promise to make the public sector more efficient, effective, and equitable, they also pose ethical challenges in implementation. Emerging technologies, especially AI-enabled tools, can present risks to the public by reinforcing biases, making costly errors, and creating privacy and security vulnerabilities from data collection and collation. For public sector workers, implementation of inefficient, inaccurate, or ineffective technologies can overburden and undermine their efforts.

The public sector is at a pivotal moment in its digital transition. While the pandemic has acted as a catalyst, jumpstarting the rollout of emerging technologies in services, full integration into the sector is still in the early stages. The appropriate modernization of the sector requires proactive and thoughtful consideration of the benefits and risks of deployments and careful analysis of the effects of these early applications to inform appropriate technology and policy strategies. Doing so will better ensure that future applications maximize benefits and mitigate harms to the public sector workforce and the beneficiaries of its programs.

The CITRIS Policy Lab, with support from Microsoft, has released a report investigating the effects of emerging technologies within three public service sectors: K-12 education, social welfare services, and law enforcement. The report explores implications of emerging technologies on efficiency, effectiveness, and equity in each sector and provides specific technology and policy recommendations for each. These analyses are used to formulate broad recommendations to guide adoption of emerging technologies in ways that mitigate harms and maximize benefits for workers and the public. Among the recommendations: implement frequent reviews to ensure technology deployments are adequately meeting the needs of the workforce and public; develop appropriate training mechanisms to equip workers with the technical skills necessary to use and evaluate the effects of new technology; and adjust procurement processes to confirm that gains in efficiency and effectiveness from implementation do not outweigh equity concerns.

The public sector is rife with antiquated IT infrastructure in dire need of being updated. The COVID pandemic and related economic disaster have accelerated the need to implement better technology-powered solutions. Fortunately, innovative tools incorporating machine learning, virtual reality, and robotics are ready to be put into service in the sector. With appropriate consideration of their effect on the workforce and the public, emerging technologies can be leveraged to provide more efficient, effective, and equitable outcomes for public service professionals and the democracy they serve.",https://medium.com/citrispolicylab/the-risks-rewards-of-emerging-technologies-within-public-services-e56bcc72b845,['Citris Policy Lab'],2020-09-09 17:22:18.508000+00:00,590,"Public Sector, Digital Transition, Emerging Technologies, Efficiency Effectiveness Equity, COVID-19"
Introduction to Data Science with R,"Introduction to Data Science with R

R is the name of a programming language as well as software for processing data and graphics.

R is very popular today because of three things, namely:

Many data processing options with a very complete number of features.

It is faster to learn and run to process data compared to other languages.

R is free and open source, which means there is no need for licensing fees which are usually very expensive for data processing software .

R is so popular also because it is used by the world’s big companies.

Some of them are AirBnB for data science, Uber for statistical analysis, Facebook for behavior analysis, and others.

R “Hello World”

Input

Output

Calculation of Numbers with R

Input

Output

Note:

Where 15 is the result of 10 + 5.

Variable in R

The use of variables in programming is very important to accommodate numbers and text in R with a name.

Input

Ouput

Note:

This means that variable a is filled with the number 5.

print() used to print the contents of the variable a

Comment on R

Comment is text to add information to our code, so we'll remember what we did when we opened the code again.

Comments are not considered executable code.

In R the use of comments is to precede a text with a '#' sign.

Input

Output

From this process it can be seen that the comments are not processed by R, so only the mathematical calculation results are displayed.

Vector on R

Vector is a data structure that can store more than one data used in R.

Its use is very simple, namely using the c function along with the data you want to store.

Input

Output

You can see that the c command (5, 10, 15) creates three series of numbers, namely 5, 10 and 15 which are stored and displayed together in a vector.

Input

Output

You can see that the c command (1:10) creates a vector with ten series of numbers starting with 1 and ending with the value 10.

Using the Summary Function

R is an analysis function called summary that can be used to deduce the data that we are processing again.

Input

Output

This means that from the vector there is a minimum number 1 (Min), a maximum number 10(Max), an average number 5.50(Mean), and a middle number 5.50(Median). For 1st Qu and 3rd Qu we just ignore it.",https://medium.com/@zidanealdanifr/introduction-to-data-science-with-r-ff6fe7722422,['Zidane Aldani Fitrah Ramadhan'],2020-12-19 04:01:06.865000+00:00,366,"RR, Data Science, Processing Data, Open Source, Vector"
Topic Analysis to Identify (and Classify) Environmental Policies,"Analyze the Policy Landscape

Collecting all available online policies, by web scraping, in a country can result in a database of thousands of documents, and millions of text fragments, all contributing to the policy landscape in the country or region.

When we are faced with thousands of potentially important documents, where do we start from?

We have several options to solve this problem, for example, we can select a couple of documents and start from there. Of course, we can read the abstract if one such exists, but in real life, we may not be that lucky.

Another approach is using the bag-of-words algorithm; this is a simple technique that counts the frequency of the words in a text, allowing to deduce the content of the text from the highest-ranking words. (In this project we used CountVectorizer from sklearn to get the document-term matrix), which can then be displayed in a word cloud (using Wordcloud), for an easy, one-look summary of the document, like the one below.

Figure 3: A word cloud for Sembrando Vida — an incentive program from Mexico.

This way we can get a quick answer to the question “What is the document about?”.

However, faced with thousands of documents, it is impractical to do word clouds for them individually. This is where topic modeling comes in handy. Topic Modeling is a technique to extract the hidden topics from large volumes of text. Latent Dirichlet Allocation (LDA) is a popular algorithm for topic modeling.

The LDA model is a topic classification model developed by Prof. Andrew Ng et al. of Stanford University’s NLP Laboratory. It is a generative model for text and other forms of discrete data that generalizes and improves upon previous models of the past, such as Bayes, unigram, and N-gram models.

Here’s how it works: Consider a corpus that comprises a collection of M documents, and each document formed by a selection of words (w1 w2, …, wi, …, wn). Additionally, each word belongs to one of the topics in the collection of topics (z1, z2, …, zi, …, zk). By estimating machine-learning weighted parameters, the per-document topic distributions, the per-document word distributions, and the topic distribution for a document, we can calculate the probabilities to which certain words are associated with certain topics, characterizing the topics and word distributions. Then, we can generate a distribution of words for each topic.

The LDA package outputs models with different values of the number of topics (k), each giving a measure of topic coherence value, a rough guide of how good a given topic model is.

Figure 4: Coherence score vs. Number of topics

In this case, we picked up the one that gives the highest coherence value, without giving too many or too few topics, that would mean either not being granular enough, or difficult to interpret. ‘K’=12 marks the point of a rapid increase of topic coherence; a usual sign of meaningful and interpretable topics.

For each topic, we have a list of the highest-frequency words constructing the topic, and we can see some overarching themes appearing. Naming the topic is the next step, with the explicit caveat that setting the topic name is massively subjective, and the assistance of a subject matter expert is advisable. The knowledge of topics, and the keywords, is necessary because the topic should reflect the different aspects of the issues within the study or problem. For example, forest restoration can be seen as operating in the intersection of the following themes, defined by the LDA. Below is an example of a model with 12 topics, which happened to be the one with the most coherence, and the subjectively determined Topic Labels (Table 1).

Table 1. Topic labels (12) and their respective keywords in the selected LDA model

We can see that one of the topics, “Forestry and Resources”, reflects closely the topics we are interested in, so the documents within it may be of particular relevance. The example document we saw before, “Sembrando Vida”, was assigned topic 8: “Development”, which is what it is expected from a document outlining the details of a broad incentive program. Some of the topics (e.g. Environmental, Agriculture) are related to the narrow topic of interest, whereas others (e.g. Food Production) are more on the periphery, and documents with this topic can be put aside for the time being. Thus topic modeling allows sifting the wheat from the chaff and zooming straight into more relevant documents.

The challenge of LDA is how to extract good quality topics that are clear, segregated, and meaningful. This depends heavily on the quality of text preprocessing and the strategy of finding the optimal number of topics, as well as the subject knowledge. Being familiar with the context and themes, as well as with different types of documents, is essential for this. Followed up with data visualizations, and further processing, like comparison, identifying conflicts between ministries, change of theme over time, zooming into the document, etc.",https://medium.com/omdena/topic-analysis-to-identify-and-classify-environmental-policies-e327626faf08,['Gijs Van Den Dool'],2020-11-25 16:50:52.544000+00:00,812,"policy landscape, topic modeling, Latent Dirichlet Allocation (LDA), bag-of-words algorithm, word cloud"
TensorFlow Save & Restore Model,"Keras API provides built-in classes to save models regularly during model fitting. To save a model and restore it later, we can create a callback ModelCheckPoint passed to model.fit, and the model will be saved regularly.

In the example above, models are saved epoch. In the configuration below, save_best_only is True. Therefore, the model is saved only when validation loss is the lowest so far.

In the configuration above, a new checkpoint overwrites the old one because it uses the same checkpoint name. Here is another example where the checkpoint file includes the epoch number so it will not be overwritten.

Here are what saved under training_2 directory. The configuration above saves the model every 5 epochs.

Save the whole model v.s. weights only

There are two options to save a model — weights only or include the training states as well as the model architecture also. If save_weights_only flag is True in creating ModelCheckpoint, the model will be saved as model.save_weights(filepath). This saves the model weights only. If it is False, the full model is saved in the SavedModel format.

By default, a model will be saved every epoch. But it can be overridden with save_freq in ModelCheckpoint.

save_freq= int(NUM_OF_EPOCHS * STEPS_PER_EPOCH)

model.save_weights

If the model is saved with weights only, we need to instantiate a new model first before restoring the weights. Likely, we call the original Python code (create_model in our example) to create a model instance. Then we load the weights of the model with model.load_weights. The latest checkpoint can be located by tf.train.latest_checkpoint.

Without the ModelCheckpoint callback, we can call model.save_weights to save the model weights manually.

model.save

To save the complete model, we use model.save(filepath) to save it as a SavedModel. As later explained, it contains the state of the optimizer and the dataset iterator such that the whole training can be resumed from the last saved point. Since the model architecture and configuration are also saved, the model can be restored directly without creating a model instance.

When a model is saved, all the model’s tf.Variable are saved and all @tf.function annotated methods are also saved as a graph. Below is a model saved as dnn_model.

We don’t need the original Python code anymore. TF executes the graph directly. In fact, this reduces possible mistakes during production deployment. Below is what the directory my_model contains now:

But that requires all methods needed by any custom layers to be covered by @tf.function annotation.

CheckpointManager

We can also use the CheckpointManager to save models if we want to use the lower level Keras API. The code below is the boilerplate code for creating a toy dataset and a model. It also contains codes for the training step.

To save a checkpoint, we create a CheckpointManager with a Checkpoint. This Checkpoint contains the model, the optimizer, training state (step), and the dataset iterator. So before the training starts, we can restore the checkpoint with the latest stored checkpoint. This loads the model weights and restores the state of the optimizer, the dataset iterator, and the training steps. In short, we resume the training state when the model is last saved — not just the model weights.

Restore a training session

Finally, we will look a little bit deeper into what is saved in SavedModel and how a training session is restored. The checkpoint in the previous section does not save the model parameters only. It also contains the state of the optimizer (learning rate, decay) and any parameters related to the trainable parameters, for example, the momentum (m). It also contains the state of the training including the training step and the save counter appended to the name of the checkpoint file. Hence, when the checkpoint is restored, it also restores the state of the optimizer and the checkpoint’s states. It also checkpoints the progress of the dataset iterator. Therefore, the iterator can be resumed from where it stops instead of starting from the beginning.

checkpoint.restore restores variable values for any matching path from a checkpoint object, i.e. we can just load a subsection of the checkpoint. For example, we can recreate part of the model only, and in the example below, we just load the bias weights from the self.l1 dense layer checkpoint.

Copy Weights

The code below copy weights from one layer to another.

In the code below, even though functional_model_with_dropout contains an extra dropout layer compared to functional_model, the dropout layer does not contain any weight. So we can still copy weights from functional_model to functional_model_with_dropout using model.set_weights.

Credits & References

The code in this article is mostly originated from TensorFlow Guide.",https://jonathan-hui.medium.com/tensorflow-save-restore-model-75a1e6d3b9a6,['Jonathan Hui'],2020-12-26 00:53:54.153000+00:00,743,"keras, modelcheckpoint, save_weights_only, savedmodel, checkpointmanager"
Understanding the Inference Mechanism of RCNs,"RCN

Understanding the Inference Mechanism of RCNs

Adapting the generative model for classification to break CAPTCHA

If you take a look at Stanford’s AI Index report of 2019, you will notice that the performance of models on famous challenges is starting to saturate [1]. For that reason, I believe that we need to shed a light on new ideas to advance deep learning even further than it has reached. And, one of the fields that I think should eagerly strive for progress is computer vision because, as Fei-Fei Li said, understanding vision is really understanding intelligence [2].

The new idea we explore in this column of articles is Recursive Cortical Networks (RCNs) [3], the brainchild of the company Vicarious which has attracted the attention of investors like Musk & Zuckerberg [4]. In this article, we talk about how RCNs are adapted to perform classification rather than generation.

An RCN model classifying the letter ‘A’ | taken from Vicarious’ Blog

This article assumes an understanding of the structure of RCNs and how they perform generation which you can gain from reading the supplementary material of [3], or from reading my previous article.

Single Object Detection

Given an image with a certain object in it, we want the RCN to tell us which class the object belongs to. RCNs achieve that by posing the question: if we assume that the input image was actually generated by the RCN, which channel in the topmost layer is the most likely channel to have generated it? In RCNs, answering that will not only identify the class of the object but also its location. To answer it, we could try to build a joint probability distribution model of all the states of all the random variables (the channels) we have in the RCN, a model that computes the probability of each certain full assignment of states to all the channels. Having that, we would condition the model on the input image, find the full assignment that has the maximum probability, and then find the channel in the topmost layer that is set to ‘true’.

However, that is intractable since we have a ton of random variables. Luckily, RCNs are inherently graphical models which means that they exploit the conditional independence structure between their random variables to make that task more tractable. To be more clear, we need to notice that a channel can directly know the state it should be in (e.g true or false in case of feature channels) if it knows the state of the channels that are directly wired to it; it doesn’t need to know the state of all channels in the RCN. In other words, a channel is conditionally independent of all channels given the channels that it is wired to. This saves us from a huge amount of computation that a full joint distribution model would have needed.

There are a lot of efficient algorithms that are able to exploit the conditional independence we just talked about, namely belief propagation algorithms which the authors use. Explaining belief propagation is out of the scope of this article and isn’t a new thing, but the gist of it is that we are going to propagate information (called messages) from the bottom-most layer, since the evidence is there, to the top-most layer, layer by layer. The information each channel sends is just a single number for each state its parent can be in. This number doesn’t have much of a meaning in my opinion; it came from simple algebraic manipulations of the equations required to get the max probability state of a certain random variable in the graphical model. So, running a forward pass of the algorithm (bottom-up) will give us the probability of each state of each channel in the top-most layer. Choosing the channel with the highest (state = ‘true’) probability answers our question.

Object Reconstruction

Unlike ConvNets, RCNs can naturally handle multiple objects in the scene and also reconstruct them. For reconstruction, we are going to make a belief propagation backward pass from the channel of the detected class in the top-most layer to the channels in the bottom-most layer. This time, however, the backward pass messages will be able to give us the most likely assignment to all the channels in the RCN, a global approximate MAP solution. If we have that, we would be able to know which channels in the bottom-most layer should be in (state = ‘true’) and thus construct an edge map using their patch descriptors. This will construct the whole object even if a part of it is occluded.

Multi-Object Detection

For detecting multiple objects in the scene, instead of just picking the most probable channel in the top-most layer after the forward pass, we want to pick a set of candidate channels that best explains the image. The authors develop a scene scoring function that can score the reconstruction done by a set of candidates which makes us able to compare between sets. They acquire the best set of candidates by finding the set that optimizes the scoring function using an approximate dynamic programming method.

That’s all for this article. If you want to learn more about RCN, you can check its paper [5] and the accompanying supplementary material document, or you can read the rest of my articles talking about the learning and the results of applying RCNs on different datasets.",https://medium.com/swlh/understanding-the-inference-mechanism-of-rcns-ba1f00416b63,['Ahmed Maher'],2020-02-03 14:57:21.631000+00:00,884,"RCN, Inference Mechanism, Generative Model, Classification, CAPTCHA"
"Simple Dataflow Newsletter #1: FastAPI, Git LFS, tesseract, numpy, word2vec…","Simple Dataflow newsletter. Issue #1

I pick the articles which I read myself and which can help developers. No useless stuff.

Topics: machine learning engineering, python, software development

Check it as my blog post on simpledataflow.com:",https://medium.com/@simpledataflow/simple-dataflow-newsletter-1-fastapi-git-lfs-tesseract-numpy-word2vec-8d7121c16364,[],2020-12-25 10:09:58.550000+00:00,33,"machine learning engineering, python, software development, dataflow, newsletter"
Does data make humans beautiful?,"Does data make humans beautiful?

UX Design for Cybernetic Applications

Figure 1 @lilianpereir on unsplash

Corporation ‘X’ thinks there is a huge market for a new product in endurance sports such as swimming, biking, and running. How should they design one? Determining ‘Worth’ is a great place to start. How important would the product be to society?

Product Worth.

Let’s conjecture why most people participate in endurance sports, including all the training it requires. Do they want to…

A. Become a professional athlete?

B. Grow into a healthier and more beautiful versions of themselves?

C. Socially engage others, including obtaining acceptance and personal meaning?

D. All of the above?

Assuming some or all of these are important to a lot of people, then a product could have significant worth.

The worth could be demonstrated to them by revealing the beauty of how they use data to achieve more.

The human and data are working together to reach an optimized performance, which feels brilliant. Think of the last time you ran faster, felt strong and beautiful, and posted your performance to a social application?

Should data be designed Intrinsically or Extrinsically?

Photo by Natasha Connell on unsplash.com https://unsplash.com/photos/byp5TTxUbL0

We have at least two options for the design of a product: intrinsic and extrinsic data interaction. In short, data interaction can feel as if it is a natural part of our existence but it can also feel separate and outside of our existence. This is important because it determines how the product works with the human body as a means to measure, visualize, and suggest insights into how to become better versions of ourselves.

An Intrinsic interaction could be biologically integrated. This could be done perhaps by implanting an electrode that receives electrical signals from our brain [1]. An Extrinsic interface could gather data from sensors. This could be through contact the outside of the body [2].

Although we can assume that most people we talked to prefer extrinsic design, do you find intrinsic design unsettling? Why?

Intrinsic and Extrinsic Design for Humans and Machines

Art from undraw.co

Determining answers to how people will interact with the products the next step after we find out the worth of the product to people. This is because although the product may seem important to them it doesn’t mean it will make them feel its importance. There are many reasons for this. Much of it has to do with the human sensory system(s).

Designers need to determine how to relate digital processes to our brains. This is because they are essentially creating inputs by which we learn. These must match what we pay attention to which is principally driven by our sensory motor cortex and pain vs pleasure feedback [3]. This is done in our email and calendars when we quickly create appointments. Creating the appointment can take longer if we are using a mouse or finger to physically touch a device but we receive a sensory signal that it happened and thus they seem more extrinsic. This can even be true if we ask Siri or Alexa to do something. There is audio feedback. Less extrinsic interactions occur when we don’t have the sensory signal for the prompting of the calendar event creation. There is less perceived agency when the system asks us if we would like to complete the task based on the information it maintains about us. The question and confirmation make it sufficiently extrinsic.

What if the system had something distinct and seemingly independent about it such as making calendar decisions without asking us to confirm them and providing no notification of the occurrence? That is, when our own internal thought processes are automated in systems outside of ourselves.

At this point we simulate intrinsic design when we forget we are dispatching the externalized things, but the design is still inherently extrinsic, since it is removed from our body’s biological processes.

For instance, what if an application on your phone learned how you like to bank based on transfers you make, when you check your account, and maybe when you purchase things? Psychologically it diminishes a sense of agency or causality for the human and a greater awareness of the system.

What if your colleagues and friends were all doing the same thing? They relegated choices to electronic systems so that their calendars were no longer being planned on their selections but rather on their general preferences. Would these preferences change based on how the network’s preferences changed? This could lead to new and seemingly bizarre social patterns, especially if you didn’t have access to the system.

How should we arrange things that seem discreet and to exist independently of us? Should we feel physically connected? Should we only be able to sense them through some external cue whether through hearing, seeing, or feeling?

In the next discussion, we will form a hypothesis as to what product would create product worth:",https://uxdesign.cc/does-data-make-humans-beautiful-c3205c11d70d,['Micah Tinklepaugh'],2020-06-25 21:33:16.332000+00:00,793,"Data Interaction Design, Intrinsic Design, Extrinsic Design, Human Sensory Systems, Product Worth"
Visualizing the News with Watson Discovery,"This article is co-authored by John Carpenter, artist and spatial interaction designer at Oblong Industries.

Our ongoing exploration of the ways that IBM Watson® services can make sense of large datasets led us to a demonstration of exploring a live corpus of English language news stories from around the world. During a single weekday, an average of approximately 20,000 stories are fed into our system, the Discovery News application. In order to process all of this unstructured information quickly, the ability to understand language at scale is required.

The team’s goal for this project was to broaden people’s typical field of vision for news, to encourage curiosity, and to facilitate research within the ever expanding landscape of news. We wanted to invigorate the familiar routine of browsing a daily news feed while demonstrating the power of a Watson service, Watson Discovery News. One of the strengths of this service is its ability to analyze unstructured text to extract meta-data from content. This is made possible with embedded natural language processing capabilities. The service provided data such as: concepts, entities, sentiments, semantic roles, authors, publication dates, and relevant keywords with associated confidence scores. Visualizing news stories using this information help users find stories they might not have in a typical day of browsing the news.

The Immersion Room

The application is designed for a large scale 320° immersive room and visitors are invited to experience the news inside of it. Imagine you are controlling a UI that is an entire room, as opposed to a browser window. The software is built in Oblong’s g-speak spatial operating environment, and runs in real-time across 5 computers and 45 screens — which means that we’re driving 93 million spatially and temporally synced pixels. Another advantage of the spatial operating environment is that it provides true spatial tracking for interactions, allowing for input events to seamlessly pass (like 3D pointing or a 4D gesture) into the application’s environment. This is useful for both navigating a large complex spatial datasets and for interacting with the immersive, dynamic UI.

The Immersion Room is designed for collaborative group engagement. While one person drives the UI with a spatially tracked ultrasonic wand, additional inputs such as mobile devices, are integrated into the system to conduct searches within the application. The wraparound space, dynamic interface, and ease of discovering new content has been found to facilitate discussion and generate insight into the day’s events amongst our visitors.

Data Visualization and UI

Given this large scale form factor and a very real possibility of visitors not knowing where to focus upon entering the space, we decided to anchor the stories to their positions on the globe using locations extracted from articles. By plotting points geographically, we were able to see concentrated areas of news activity and identify locations of high activity over time.

Next we extracted concepts and entities, semantic roles and relations, along with sentiments from the daily feed of news stories. These relationships are mapped and visualized to reveal the rate of co-occurrence between topics as well as the average sentiment. The more frequently we see a concept or entity in the news, the more prominent it becomes in the UI. Very simply, the frequency of those topics and the positive and negative feelings associated with them gives us a quick glance at what a given day’s news is about.

A simple control panel allows users to easily explore this rich live dataset. The flexible UI accommodates a variety of views as users filter on concepts and sentiment, see high activity spikes, explore global reaches of concepts and drill down to a single news story. Users are encouraged to fully explore and immerse themselves in the room-sized visualizations. When designing and building this application, the team opted to allow users free rein to go inside of and even pass through the globe, progressively revealing new concepts and meta data as they move closer and closer.

UI elements unfold in the space to facilitate exploration. When a concept is selected the control panel opens up to a related concepts diagram, which opens up to a scrollable grid of filtered news stories, which in turn launch a real-time web view of the current story. With this system, users can quickly navigate from 20,000 to just a few stories of interest with a couple of gestures and clicks.

The highly dynamic UI of the News Discovery application provides users with a myriad of ways to explore different facets of an enormous body of aggregated news stories; inspiring a vision of how quickly teams can use this Watson service to find patterns and trends on topics they care about.

This project was a collaboration with the IBM Immersive Experiences team and Oblong Industries. It can be viewed in a Watson Experience Center.

Jenny Woo is a Design Lead of Watson Immersive Experiences at IBM, based in New York City. The above article is personal and does not necessarily represent IBM’s positions, strategies or opinions.",https://medium.com/design-ibm/visualizing-the-news-with-watson-discovery-13e8be538428,['Jenny Woo'],2019-06-10 21:46:42.224000+00:00,818,"IBMWatson, Oblong Industries, Discovery News, Natural Language Processing, Data Visualization"
Web Scraping in a corporate environment — The Scraper,"Photo by Stephen Dawson on Unsplash

Project — The Scraper

I decided to to bundle my efforts, at least the tangible code part, under the code name The Scraper. Working for an engineering company, I have this natural urge to automate & engineer my own work processes.

Let’s talk about my project. Basically I have a web based reporting system that doesn’t allow exports/downloads nor allow me to operationalise and execute on the data. In this case it is not a feature issue, but it is how the system is designed and setup based on corporate guidelines. The reporting system is Oracle Business Intelligence 12 Enterprise Edition (EE).

Now, the good thing is, the reports are basically dynamically generated HTML pages — including HTML tables, so I started thinking about writing a web page scraper — to scrape the data from the webpage and store it somewhere else. Keep in mind that I just want the data from the displayed table!

What better language to use then Python! Python has gained huge popularity over the last couple of years, not in the last place because of it’s simplicity and huge eco system of modules. Also it is a very popular language for data science purposes. For this exercise I am using Python 3.7.

Enter “Requests” & “BeautifulSoup” Python Modules

Python has an excellent module called “Requests” that allows to make calls to webpages. This is article is not going to talk in depth about these technologies as there are plenty of articles out there. I want to share how I applied the technologies in a real life context and some of the specific challenges I encountered.

Let’s start coding…note that you need to have the requests module installed, wich can be done with:

pip install requests

After the module is installed we can fire up our code editor and start with importing the requests module:

import requests

Python Modules Import in the source code of the program

As you can see I also import “getpass”, “time” and “BeautifulSoup”. We will need these later on, so again use pip install [module name] to install them on your system.

BeautifulSoup is one of the most frequently used HTML parsers. It converts HTML in a python nested object. This allows for easy navigation through HTML and finding specific parts in the HTML code based on HTML TAGs. We will use it later, because we first need to get the page to be parsed.

Calling the application page

As you could have guessed, we will use requests to call the page we want. Let’s imagine the base URL is https://bi.yourcompany.com — but off course this will only get you to the homepage. Not practical. The good thing about Oracle BI EE is the fact that you can create the page view on the data that you are interested in and create a bookmark link that will remember your data selection options and hence, you only have to call this bookmarked URL to get the data you want. This will normally be a long string like (note: I shortened the string a lot where you see the “….”:

https://bi.yourcompany.com /analytics/saw.dllPortalGo&PortalPath=%2Fshared%2FSales%20Forecast%20and%20Pipeline.......ghvgo90negq&options=bsmrdle

But before getting the URL, we need to do two more things:

Setting the header so we appear to be a normal user instead of a programmatic interface to the server — some web-servers block scraper based access Setting up a “session” so we can store information (e.g. cookies) between requests as we have to deal with authentication

So we can set the header like the following:

headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux i686; rv:7.0.1) Gecko/20100101 Firefox/7.0.1'}

In this case we are using a fairly common header, used by the Firefox browser, to communicate with the server.

Now that we have done this, we will create a session object, which we can do like this:

s = requests.session()

Next we will update the headers values based on this newly created session object:

s.headers.update(headers)

Now we call the full URL for the first time:

r = s.get('https://bi.yourcompany.com /analytics/saw.dllPortalGo&PortalPath=%2Fshared%2FSales%20Forecast%20and%20Pipeline.......ghvgo90negq&options=bsmrdle')

In this case, if you have read the requests documentation, you know that “r” will be the response object and, after a successful request, will contain all the response data from the server, including the raw HTML we need.

You can inspect the content by doing this:

print(r.text)

There are much more things you can do with the response object, but this is out of scope of this post.

Getting the data from the response and write it to file

Finally we get to the end of our exercise. At this moment it is enough to just store the HTML so we can process it later with BeautifulSoup. Writing the data is pretty straightforward.

The code to do it is:

with open('output.html', 'w') as f:

f.write(r.text)

What we do is open/create/close a file with the name “output.html” and as we want to “write” to the file, we use the “w” flag. The object name that we will use to interact with the file is “f”. Then we simply call the write operation on the “f” object and use “r.text” data to write to the file — et voila! Double click on the “output.html” file to open it in your browser and see your locally stored data.

What’s next

I can hear you thinking, but what about the other modules you mentioned like “time”, “getpass” and “BeautifulSoup”? We will deal with those in subsequent posts. Also, you didn’t think it was this easy right to get the data? Right. I made this example simple, for the sake of the exercise. We have to deal with authentication, HTML parsing, waiting for pages to load, error handling etc. Stay tuned for more…and thanks for reading. Leave comments, questions and feedback if you want.",https://medium.com/@arno-schots/web-scraping-in-a-corporate-environment-the-scraper-573dc22c1c87,['Arno Schots'],2020-03-18 13:57:45.188000+00:00,922,"Python, Requests Module, Beautiful Soup, HTML Parsing, Oracle Business Intelligence 12 EE"
Change Management Analytics,"Data Driven Methodology for Managing Business Transformation Initiatives

In this article I will present a data driven methodology created as part of my knowledge management team’s mission supporting global business transformations for a Fortune100 company.

Our said objectives for the project were:

Measure employee effectiveness before/after implementation of change

Map corporate departments, roles, and business processes + related KPIs

Lists of CM processes & best practices for enabling for the absorbance of change

Leveraging Existing Business Communication Flows

Our approach was based on using existing business conversations and analytics to build a real time picture of work accomplished and track progress while keeping the data as anonymous as possible, focusing on work segments and business processes instead of individual employees or managers.

In order to accomplish this goal, we needed to establish key corporate and department goals and map them against our given change management timelines, initiatives, and objectives. In practice, our deliverable included the following main components:

Lists of department managers, trainers, knowledge subject matter experts,

Manager/Department Credibility

Company Perceived Credibility

Manager CM Journey

CM Transformations Timeline

Locally validated ideas enabling global transformations at scale

Identify new CM Tracks, User Personas, Talent Management Journeys

Enabling for absorbed change

Increased credibility among the workforce

Root cause analysis for negative impact on workforce effectiveness

Change Enablement Program Creation:

Internally, we worked with our company’s management consulting teams to organize into the following service-delivery consulting lines of business:

Communications & Training

Subject matter experts + smart request/inquiry tools

Business absorption risk mitigation

Service introduction & Stakeholder integration

Change Enablement Communications

Learning & Knowledge Management

Overview of project management and issues/requests workflow & tools

Stakeholder integration & Strategic Solution

Discuss business risk, delivery, and privacy concerns

Key Aspects of Knowledge Management Capabilities

As I mentioned before, this project was owned by our knowledge management team, and in support of the above objective, we ‘codified’ the following internal structure for the team:

KM Process

Each knowledge management engagement follows an account management process composed of the following general sections:

Business Understanding Data Ingestion & Analysis Model Development Client Deployment & Training Acceptance & Data Quality

KM Project Structure

Project Charter & Scope Definition Raw Data Reports, Documents, and Discovery Analytics Model Development Data Features/Dimensions & Measures Business Context Model Analysis Model Performance Internal deployment, integration, and testing Client deployment & integration with other analytics and operational initiatives

Change Management Analytics

Change Management Analytics are proprietary methods & measurements leveraging the client’s existing data capabilities to define, schedule, and manage the workforce impact of upcoming business transformation, providing our clients with a combination of organizational agility & consistency simultaneously.

All of the business entities measurements and data flows are processed through machine learning models and data lakes and presented to corporate management via a corporate knowledge portal (mockup):

Mockup Dashboard for Realtime Corporate Knowledge

Part of our CM analytics assets is a funnel of web pages allowing different members of the organization to create, score, and elaborate on new initiatives.

We also combined the company’s elaborate system of both open and structured employee/management surveys, incorporating them into our data driven change management models:

Combining employee surveys with data driven models

We guided our client stakeholders in building the metrics, tools, and context for discussing global initiative priorities — critical tools for automating CM communication and strategic discussions about upcoming transformation initiatives.

Workforce Performance Measurements

The following is a list of measures & entities sourced from existing enterprise systems and data assets used to quantitatively measure productivity and success of business transformation initiatives:

KM Information Architecture / Documents Structure

We also organized our internal documentation resources into the following sections, we greatly improved our ability to manage increasing complexity:

Project Deliverables

Data Raw Processed Production Models

Documents

Project Data Reports Model Reports Data Quality Management

Code

Queries & Transformations AI & Machine Learning Deployment & Analytics

Conclusions

The resulting organization structure and client centric processes combined into a powerful knowledge management center driving both executive level strategy and tactical business execution for both our own company as well as our clients:",https://medium.com/@yeshivaventures/change-management-analytics-fbd0680351c8,['Ronnie Moshe Rendel'],2020-12-20 10:44:49.393000+00:00,619,"data driven methodology, knowledge management, business transformation initiatives, change management analytics, workforce performance measurements"
November 2020 Election Violence Risk Briefing,"Burkina Faso and Myanmar have the highest risk of election violence, while our low estimates of election violence risk in the United States may understate the country’s true level of risk.

Apple Harvest by Camille Pissarro

The Electoral Violence Intelligence System (ELVIS) is a machine learning driven forecasting system that estimates the risk of election-related violence for every national election every month. These posts will provide updates on the outputs of our forecasting model, give an in-depth view of what to expect in the coming month and note any technical changes/updates.

Feel free to reach out to either myself (mfrank@oneearthfuture.org) or Clayton Besaw (cbesaw@oneearthfuture.org) for any questions regarding our ELVIS platform or analyses contained in the updates.

November 2020 ELVIS report

Data and algorithm updates:

Updated precipitation (SPI) estimates using NOAA’s September 2020 PREC/L release (https://www.esrl.noaa.gov/psd/data/gridded/data.prel.html).

Added new ground-truth and projected GDP estimates using the IMF’s October 2020 World Economic Outlook projections and the World Bank’s 2019 estimates.

Added new population data based on the IMF’s October 2020 World Economic Outlook projections.

Added previously missing election events for the following countries: Algeria

Risk forecast for November 2020:

Burkina Faso has the highest risk for election violence this month, as political violence overall remains high in the country and threatens to limit the number of citizens who will be able to cast their ballots. This, in turn, is raising concerns over the popular legitimacy of the electoral outcome.

Next highest this month is Myanmar where a United Nations human rights investigator has noted that the military has been jailing journalists and students in the lead up to the election.

Moreover, in Rakhine state and other areas of the country experiencing conflict elections have been cancelled, meaning that large portions of the country will not be able to participate in the election. These undemocratic moves have led to concerns that many will not recognize the popular legitimacy of the elections, which could further inflame conflict and insurgency in the country.

The United States is also in the process of holding elections as we are preparing to publish these updates. ELVIS gives the United States a low probability of election violence based on the structural factors we use to predict violence around the polls, however, there is significant reason to be concerned about election violence in the US.

Myself and Clayton Besaw wrote about why this is the case and what the longer-term potential implications for election violence could be here on Datayo.

Turning our attention to the rest of the year, are just four more elections slated for December, although all of them have an estimated probability of election violence greater than 50%.",https://medium.com/the-die-is-forecast/november-2020-election-violence-risk-briefing-39bdd2febb18,['Matt Scott Frank'],2020-11-04 19:07:57.892000+00:00,425,"Burkina Faso, Myanmar, Election Violence, ELVISPlatform, November2020ELVISReport"
Breaking CAPTCHA Using Machine Learning in 0.05 Seconds,"Breaking CAPTCHA Using Machine Learning in 0.05 Seconds

Machine learning model breaks CAPTCHA systems on 33 highly visited websites. The concept bases on GANs

December 19, 2018, by Roberto Iriondo — Updated May 5, 2020

Everyone despises CAPTCHAs (humans, since bots do not have emotions) — Those annoying images containing hard to read the text, which you have to type in before you can access or do “something” online.

CAPTCHAs (Completely Automated Public Turing tests to tell Computers and Humans Apart) were developed to prevent automatized programs from being mischievous (filling out online forms, accessing restricted files, accessing a website an incredible amount of times, and others) on the world wide web, by verifying that the end-user is “human” and not a bot.

Nevertheless, several attacks on CAPTCHAs have been proposed in the past, but none has been as accurate and fast as the machine learning algorithm presented by a group of researchers from Lancaster University, Northwest University, and Peking University showed below.

Figure 1: Overview of the approach. The researchers first use a small set of non-synthesized CAPTCHAs to train a CAPTCHA synthesizer. (1) the CAPTCHA synthesizer is then used to generate synthetic CAPTCHAs, which at the same time (2) the synthetic CAPTCHAs are used to train a machine learning base solver, (3) which is refined to build a fine-tuned solver of non-synthesized CAPTCHAs. | [1]

One of the first known people to break CAPTCHAs was Adrian Rosebrock, who, in his book “Deep Learning for Computer Vision with Python,” [4] Adrian goes through how he bypassed the CAPTCHA systems on the E-ZPass New York website using machine learning, where he used deep learning to train his model by downloading a large image dataset of CAPTCHA examples to break the CAPTCHA systems.

The main difference between Adrian’s solution and the solution from the research scientists from Lancaster, Northwest, and Peking is that the researchers did not need to download a large dataset of images to break the CAPTCHAs system, au contraire, they used the concept of a generative adversarial network (GAN) to create synthesized CAPTCHAs, along with a small dataset of real CAPTCHAs to create an extremely fast and accurate CAPTCHA solver.

Generative adversarial networks, introduced by Ian Goodfellow along with other researchers [2], are deep neural net architectures comprised of two neural networks, which compete against the other in a zero-sum game [3] to synthesize superficially authentic samples. These are especially useful in scenarios where the model does not have access to a large dataset.

Figure 2: Targeted CAPTCHA security features. Sample examples were collected from Baidu, Sina, Microsoft, and JD captcha schemes. | [1]

The researchers evaluated their approach by applying 33 text-based CAPTCHA schemes, 11, which are currently being used by 32 of the world’s most popular websites ranked by Alexa. Including CAPTCHA schemes being used by Google, Microsoft, eBay, Wikipedia, Baidu, and many others. The machine learning model used to attack these CAPTCHA systems only needed 500 non-synthesized CAPTCHAs instead of millions of examples as other attacks before this one (such as Adrian’s) have proposed.

Figure 3: List of text-based captcha schemes used as training data, along with testing of the machine learning CAPTCHA solver. | [1]

Once the model was initialized with the CAPTCHAs security parameters in mind shown in Figure 2, it was used to generate a batch of synthetic CAPTCHAs to train the synthesizer with the 500 real CAPTCHAs obtained from the various CAPTCHA schemes shown in Figure 3. The researchers used 20,000 CAPTCHAs to train the pre-processing model along 200,000 synthetic CAPTCHAs to train the base solver.

The machine learning prototype was implemented using Python. The pre-processing model is built using the Pix2Pix framework, which was implemented using Tensorflow. The fine-tuned solver was coded using Keras. [1]

Figure 4: Real Google CAPTCHAs and the synthetic versions generated by the researchers’ CAPTCHA synthesizer | [1]

After the generative adversarial networks were trained by using the synthesized and real CAPTCHA samples, the CAPTCHA solver was used then to solve CAPTCHAs from highly visited websites, such as Megaupload, Blizzard, Authorize, Captcha.net, Baidu, QQ, reCaptcha, Wikipedia, and others. The unique approach of this method is that most of the sites CAPTCHAs were solved with over 80% success rate, exceeding 95% on sites like Blizzard, Megaupload, and Authorize.net, an attack method that has proven to have better accuracy on all other prior methods to solve CAPTCHAs, which used sizeable non-synthesized training datasets.

Figure 5: Compares the researchers’ CAPTCHA solver against four prior attack methods to solve CAPTCHAs. | [1]

Other than enhanced accuracy, the researchers mentioned on their paper that their approach was not only more accurate but also more efficient and less expensive to implement that other methodologies proposed [1]. Besides being the first GAN based solution for text-based CAPTCHAs, it is an open door for attackers to use, hence their effectiveness and inexpensiveness to implement.

Nevertheless, the approach has some limitations, such as the use of CAPTCHAs with variable numbers of characters. The current approach uses a fixed number of characters — if it’s extended, the prototype breaks. Another is the use of variable characters on the CAPTCHA. While the prototype can be trained to support this change, it currently does not as is.

It is crucial for highly visited websites to use more robust ways to protect their systems, such as bot-detection measures, cyber-security diagnoses, and analytics, along with multiple layers of security such as device location, types, browsers, and others. — as they are now and even easier target to attack.

DISCLAIMER: The views expressed in this article are those of the author(s) and do not represent the views of Carnegie Mellon University, nor other companies (directly or indirectly) associated with the author(s). These writings are not intended to be final products, yet rather a reflection of current thinking, along with being a catalyst for discussion and improvement.",https://medium.com/towards-artificial-intelligence/breaking-captcha-using-machine-learning-in-0-05-seconds-9feefb997694,['Roberto Iriondo'],2020-08-31 14:17:50.972000+00:00,954,"CAPTCHA, Machine Learning, GANs, Robust Security Measures, Cyber Security Diagnoses"
NLP Text Preprocessing and Cleaning Pipeline in Python,"Through language we express the human experience. Language is how we communicate, express sentiment, listen, think and converse. Over the past decade, tremendous progress has been made in natural language processing(NLP) where computers can classify, generate, and respond to language like a human can. These models and algorithms have started to give computers the tools to understand the human experience. The most important reason that these areas have grown is because of the exponential increase in data that can be feature engineered and then fed into models.

Feature Engineering:

The data scientists, PhDs, machine leaning engineers, and data engineers who have been on the front lines of the advancements in natural language processing spend most of their time cleaning and exploring datasets.

Garbage in Garbage out

Models will produce state of the art results if they are fed state of the art data; models will produce garbage if they are fed garbage, therefore data cleaning is one of the most important parts of the entire machine learning process. It does not matter if you are training a state of the art transformer like BERT or old-school word2vec. The quality of data that you feed the model will determine the quality of the results that you get.

garbage in garbage out; data on fire in data on fire out

Feature engineering and data cleaning are not tedious and pointless tasks — they are important filtering mechanisms that work the same way the human brain does when it processes a language. When humans are young they learn how to separate noises from a language and then find meaning in that language through the most important words and phrases in that language. This process is similar to text preprocessing. Text preprocessing breaks down a corpus into smaller parts then extracts the most important information from those parts which a model will then derive meaning from.

#tldr show me the code!

Steps in the Pipeline

📜 ,👀, 📊 (Load, Look at, and Analyze the Data):

The first step of any machine learning pipeline is to load the data! For good measure we will also take a peek at the data to make sure everything loaded correctly and collect a few basic stats like the number of words, the number of lines, and the number of characters. This will give us a good idea of what we are working with for the next steps in the pipeline. I am using the Reuters dataset that can be found here.

data_folder = Path(""/Users/emilyelia/Downloads/reuters/reuters/reuters/training"")

file_to_open = data_folder / ""104""

f = open(file_to_open) print(f.read())

num_lines =0

num_words =0

num_chars =0

with open(file_to_open, 'r') as f:

for line in f:

words = line.split() num_lines += 1

num_words += len(words)

num_chars += len(line) print (""numbers of words"", num_words)

print(""number of lines"", num_lines)

print(""number of chars"", num_chars)

Tokenization:

Tokenization is the next preprocessing step. It takes the text corpus and it splits it into “tokens” (words, sentences, etc.).

This process is not as simple as using some kind of separator. There are a lot of different situations where separators don’t work such as abbreviations with dots like “Dr.” or periods “.” at the end of sentences. A more complex model is needed to properly tokenize, but don’t worry because tokenization is a built-in feature in commonly used NLP libraries such as nltk.

from nltk.tokenize import sent_tokenize, word_tokenize

f = open(file_to_open)

#use built in tokenize to seperate each indiviual word

nltk_words = word_tokenize(f.read())

print(f""Tokenized words: {nltk_words}"")

Cleaning:

Cleaning is the process of removing all unnecessary content from the corpus. The unnecessary content includes stop words and punctuation since they do not add any value or meaning to the overall corpus.

Punctuation

Punctuation removal is an important step since punctuation does not provide any additional value or insight into the overall corpus and the vectorization of the corpus. Removing punctuation is best done after the tokenization step because doing so before might cause some unforeseen results.

Stop Words

Stop words are the most common words in the language that you are using. There are between 50–100 stop words depending on the library that you are using and they are words that don’t add meaning like “the”, “an”, and “it”. Removing these words will not change the meaning of the corpus that you are working with and it will lead to better results because the remaining words will be the most important to determining the meaning of the corpus.

print(nltk_words)

punctuation = list(string.punctuation)

stopWords = set(stopwords.words('english'))

filter = []

for w in nltk_words:

if w.lower()not in stopWords and w not in punctuation:

filter.append(w)

Normalization

Normalization is the process of returning to a standard form or state. In terms of text preprocessing, it means taking numbers, abbreviations, and special characters, and converting them to text. This process uses the same associations that our brains do when processing special characters, misspellings, and abbreviations by taking them and assigning them to the words that we say, read, or think in the language of our choice. I am using the library normalise that can be found here.

The most common features to normalize are dates, numbers, abbreviations, currency, percents, and misspellings. In normalise you have to list the abbreviations that you want to spell out and then call the normalise function to perform the rest of the normalizations.

from normalise import normalise

abbr = {

""lb"": ""pounds"",

""USDA"": ""United States Department of Agriculture"",

""cts"": ""cents"",

""U.S."": ""United States""

}

normalise(text, user_abbrevs=abbr)

nt =normalise(filter, user_abbrevs=abbr)

display( ' '.join(nt))

These normalizations ensure that the model will be able to understand all of the text that is has to process because the numbers, special characters, misspellings, and abbreviations will have the same representation as everything else in the corpus.

Check out the entire project here!",https://towardsdatascience.com/nlp-text-preprocessing-and-cleaning-pipeline-in-python-3bafaf54ac35,['Emily Elia'],2019-11-05 06:01:38.436000+00:00,894,"natural language processing, NLP, feature engineering, data cleaning, tokenization"
An intro to Advantage Actor Critic methods: let’s play Sonic the Hedgehog!,"A2C and A3C

Introducing the Advantage function to stabilize learning

As we saw in the article about improvements in Deep Q Learning, value-based methods have high variability.

To reduce this problem, we spoke about using the advantage function instead of the value function.

The advantage function is defined like this:

This function will tell us the improvement compared to the average the action taken at that state is. In other words, this function calculates the extra reward I get if I take this action. The extra reward is that beyond the expected value of that state.

If A(s,a) > 0: our gradient is pushed in that direction.

If A(s,a) < 0 (our action does worse than the average value of that state) our gradient is pushed in the opposite direction.

The problem of implementing this advantage function is that is requires two value functions — Q(s,a) and V(s). Fortunately, we can use the TD error as a good estimator of the advantage function.

Two different strategies: Asynchronous or Synchronous

We have two different strategies to implement an Actor Critic agent:

A2C (aka Advantage Actor Critic)

A3C (aka Asynchronous Advantage Actor Critic)

Because of that we will work with A2C and not A3C. If you want to see a complete implementation of A3C, check out the excellent Arthur Juliani’s A3C article and Doom implementation.

In A3C, we don’t use experience replay as this requires lot of memory. Instead, we asynchronously execute different agents in parallel on multiple instances of the environment. Each worker (copy of the network) will update the global network asynchronously.

On the other hand, the only difference in A2C is that we synchronously update the global network. We wait until all workers have finished their training and calculated their gradients to average them, to update our global network.

Choosing A2C or A3C ?

The problem of A3C is explained in this awesome article. Because of the asynchronous nature of A3C, some workers (copies of the Agent) will be playing with older version of the parameters. Thus the aggregating update will not be optimal.

That’s why A2C waits for each actor to finish their segment of experience before updating the global parameters. Then, we restart a new segment of experience with all parallel actors having the same new parameters.

This schema is inspired by this article.

As a consequence, the training will be more cohesive and faster.

Implementing an A2C agent that plays Sonic the Hedgehog

A2C in practice

In practice, as explained in this Reddit post, the synchronous nature of A2C means we don’t need different versions (different workers) of the A2C.

Each worker in A2C will have the same set of weights since, contrary to A3C, A2C updates all their workers at the same time.

In fact, we create multiple versions of environments (let say eight) and then execute them in parallel.

The process will be the following:

Creates a vector of n environments using the multiprocessing library

Creates a runner object that handles the different environments, executing in parallel.

Has two versions of the network:

step_model: that generates experiences from environments train_model: that trains the experiences.

When the runner takes a step (single step model), this performs a step for each of the n environments. This outputs a batch of experience.

Then we compute the gradient all at once using train_model and our batch of experience.

Finally, we update the step model with the new weights.

Remember that computing the gradient all at once is the same thing as collecting data, calculating the gradient for each worker, and then averaging. Why? Because summing the derivatives (summing of gradients) is the same thing as taking the derivatives of the sum. But the second one is more elegant and a better way to use GPU.

A2C with Sonic the Hedgehog

So now that we understand how A2C works in general, we can implement our A2C agent playing Sonic! This video shows the behavior difference of our agent between 10 min of training (left) and 10h of training (right).

The implementation is in the GitHub repo here, and the notebook explains the implementation. I give you the saved model trained with about 10h+ on GPU.

This implementation is much complex than the former implementations. We begin to implement state of the art algorithms, so we need to be more and more efficient with our code. That’s why, in this implementation, we’ll separate the code into different objects and files.

That’s all! You’ve just created an agent that learns to play Sonic the Hedgehog. That’s awesome! We can see that with 10h of training our agent doesn’t understand the looping, for instance, so we’ll need to use a more stable architecture: PPO.

Take time to consider all the achievements you’ve made since the first chapter of this course: we went from simple text games (OpenAI taxi-v2) to complex games such as Doom and Sonic the Hedgehog using more and more powerful architectures. And that’s fantastic!

Next time we’ll learn about Proximal Policy Gradients, the architecture that won the OpenAI Retro Contest. We’ll train our agent to play Sonic the Hedgehog 2 and 3 and this time, and it will finish entire levels!

Don’t forget to implement each part of the code by yourself. It’s really important to try to modify the code I gave you. Try to add epochs, change the architecture, change the learning rate, and so forth. Experimenting is the best way to learn, so have fun!

If you liked my article, please click the 👏 below as many time as you liked the article so other people will see this here on Medium. And don’t forget to follow me!

This article is part of my Deep Reinforcement Learning Course with TensorFlow 🕹️. Check out the syllabus here.

If you have any thoughts, comments, questions, feel free to comment below or send me an email: hello [at] simoninithomas [dot] com, or tweet me @ThomasSimonini.",https://medium.com/free-code-camp/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d,['Thomas Simonini'],2019-02-05 19:21:53.937000+00:00,941,"A2C, A3C, Advantage Function, TD Error, Asynchronous"
K-Means Clustering: How It Works & Finding The Optimum Number Of Clusters In The Data,"Python working example

For this example we will create artificial data i.e. artificial clusters. This way we will know in advance the ground through i.e. the exact number of clusters in our dataset.

Let’s start with importing the required python libraries:



from sklearn.cluster import

from sklearn.metrics import silhouette_score



import matplotlib.pyplot as plt

import matplotlib.cm as cm

import numpy as np from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples import matplotlib.pyplot as pltimport matplotlib.cm as cmimport numpy as np

Next, let’s create some artificial data containing 500 samples, 2 features/variables, and K=4 clusters.



# This particular setting has one distinct cluster and 3 clusters placed close together.

X, y =

n_features=2,

centers=4,

cluster_std=1,

center_box=(-10.0, 10.0),

shuffle=True,

random_state=1) # Generating the data# This particular setting has one distinct cluster and 3 clusters placed close together.X, y = make_blobs (n_samples=500,n_features=2,centers=4,cluster_std=1,center_box=(-10.0, 10.0),shuffle=True,random_state=1)

We know that we have K=4 clusters in the data however, in order to understand how the Silhouette Score works we will fit the model using a range of different number of clusters.

Each time, we will estimate the Silhouette Score and also plot the data with the final (converged) centroids. All these are done by the following code:

range_n_clusters = [3, 4, 5]

# Create a subplot with 1 row and 2 columns

fig, (ax1, ax2) =

fig.set_size_inches(18, 7)



# The 1st subplot is the silhouette plot

# The silhouette coefficient can range from -1, 1 but in this example all

# lie within [-0.1, 1]

ax1.set_xlim([-0.1, 1])

# The (n_clusters+1)*10 is for inserting blank space between silhouette

# plots of individual clusters, to demarcate them clearly.

ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])



# Initialize the clusterer with n_clusters value and a random generator

# seed of 10 for reproducibility.

clusterer =

cluster_labels = clusterer.fit_predict(X)



# The silhouette_score gives the average value for all the samples.

# This gives a perspective into the density and separation of the formed

# clusters

silhouette_avg =

print(""For n_clusters ="", n_clusters,

""The average silhouette_score is :"", silhouette_avg)



# Compute the silhouette scores for each sample

sample_silhouette_values =



y_lower = 10

for i in range(n_clusters):

# Aggregate the silhouette scores for samples belonging to

# cluster i, and sort them

ith_cluster_silhouette_values = \

sample_silhouette_values[cluster_labels == i]



ith_cluster_silhouette_values.sort()



size_cluster_i = ith_cluster_silhouette_values.shape[0]

y_upper = y_lower + size_cluster_i



color = cm.nipy_spectral(float(i) / n_clusters)

ax1.fill_betweenx(

0, ith_cluster_silhouette_values,

facecolor=color, edgecolor=color, alpha=0.7)



# Label the silhouette plots with their cluster numbers at the middle

ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))



# Compute the new y_lower for next plot

y_lower = y_upper + 10 # 10 for the 0 samples



ax1.set_title(""The silhouette plot for the various clusters."")

ax1.set_xlabel(""The silhouette coefficient values"")

ax1.set_ylabel(""Cluster label"")



# The vertical line for average silhouette score of all the values

ax1.axvline(x=silhouette_avg, color=""red"", linestyle=""--"")



ax1.set_yticks([]) # Clear the yaxis labels / ticks

ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])



# 2nd Plot showing the actual clusters formed

colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)

ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,

c=colors, edgecolor='k')



# Labeling the clusters

centers = clusterer.cluster_centers_

# Draw white circles at cluster centers

ax2.scatter(centers[:, 0], centers[:, 1], marker='o',

c=""white"", alpha=1, s=200, edgecolor='k')



for i, c in enumerate(centers):

ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,

s=50, edgecolor='k')



ax2.set_title(""The visualization of the clustered data."")

ax2.set_xlabel(""Feature space for the 1st feature"")

ax2.set_ylabel(""Feature space for the 2nd feature"")





""with n_clusters = %d"" % n_clusters),

fontsize=14, fontweight='bold')



() for n_clusters in range_n_clusters:# Create a subplot with 1 row and 2 columnsfig, (ax1, ax2) = plt.subplots (1, 2)fig.set_size_inches(18, 7)# The 1st subplot is the silhouette plot# The silhouette coefficient can range from -1, 1 but in this example all# lie within [-0.1, 1]ax1.set_xlim([-0.1, 1])# The (n_clusters+1)*10 is for inserting blank space between silhouette# plots of individual clusters, to demarcate them clearly.ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])# Initialize the clusterer with n_clusters value and a random generator# seed of 10 for reproducibility.clusterer = KMeans (n_clusters=n_clusters, random_state=10)cluster_labels = clusterer.fit_predict(X)# The silhouette_score gives the average value for all the samples.# This gives a perspective into the density and separation of the formed# clusterssilhouette_avg = silhouette_score (X, cluster_labels)print(""For n_clusters ="", n_clusters,""The average silhouette_score is :"", silhouette_avg)# Compute the silhouette scores for each samplesample_silhouette_values = silhouette_samples (X, cluster_labels)y_lower = 10for i in range(n_clusters):# Aggregate the silhouette scores for samples belonging to# cluster i, and sort themith_cluster_silhouette_values = \sample_silhouette_values[cluster_labels == i]ith_cluster_silhouette_values.sort()size_cluster_i = ith_cluster_silhouette_values.shape[0]y_upper = y_lower + size_cluster_icolor = cm.nipy_spectral(float(i) / n_clusters)ax1.fill_betweenx( np.arange (y_lower, y_upper),0, ith_cluster_silhouette_values,facecolor=color, edgecolor=color, alpha=0.7)# Label the silhouette plots with their cluster numbers at the middleax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))# Compute the new y_lower for next ploty_lower = y_upper + 10 # 10 for the 0 samplesax1.set_title(""The silhouette plot for the various clusters."")ax1.set_xlabel(""The silhouette coefficient values"")ax1.set_ylabel(""Cluster label"")# The vertical line for average silhouette score of all the valuesax1.axvline(x=silhouette_avg, color=""red"", linestyle=""--"")ax1.set_yticks([]) # Clear the yaxis labels / ticksax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])# 2nd Plot showing the actual clusters formedcolors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,c=colors, edgecolor='k')# Labeling the clusterscenters = clusterer.cluster_centers_# Draw white circles at cluster centersax2.scatter(centers[:, 0], centers[:, 1], marker='o',c=""white"", alpha=1, s=200, edgecolor='k')for i, c in enumerate(centers):ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,s=50, edgecolor='k')ax2.set_title(""The visualization of the clustered data."")ax2.set_xlabel(""Feature space for the 1st feature"")ax2.set_ylabel(""Feature space for the 2nd feature"") plt.suptitle ((""Silhouette analysis for KMeans clustering on sample data """"with n_clusters = %d"" % n_clusters),fontsize=14, fontweight='bold') plt.show ()

Code output in the console (image created by the author)

We observe that the average/mean Silhouette Score is the highest in the case of K=4 clusters.

This verifies that the Silhouette Score is a good measure of K-means fitting goodness.

We also produced these Figures:

Silhouette score & the data with the final (converged) centroids (image created by the author)

Silhouette score & the data with the final (converged) centroids (image created by the author)

Silhouette score & the data with the final (converged) centroids (image created by the author)

The vertical line is the average silhouette score of all the values.

Again, we can also visually verify that the Silhouette Score is a good measure of K-means fitting goodness for the specific example.",https://towardsdatascience.com/k-means-clustering-how-it-works-finding-the-optimum-number-of-clusters-in-the-data-13d18739255c,['Serafeim Loukas'],2020-09-09 21:03:09.610000+00:00,919,"python, working example, artificial data, sklearn library, k-means clustering"
The Multiclass Definitions,"Photo by Tuân Nguyễn Minh on Unsplash

One of the first lessons a budding machine learning programmer learns is about binary classification. It is the idea that you and your model are trying to classify an input as one of two outcomes. It is a hot-dog or not, should you reject of accept someone for a loan, or do you think a student will pass or fail a class.

Once you get the hang of that, courses start to then teach about multi-class classification. This is the idea that inputs can be classified as one of many outputs — this represents the world more closely. Maybe you would like to predict an image of a number as between 0–9. Or maybe you are curious about what kind of flower that pretty one on your neighbor’s lawn is so you train a model to find out. Some first key lessons include things like one-hot encoding or label encoding.

We are all here to learn, and I recently found the different multiclass and multilabel classifications that I would like to share with everyone. So, let’s dig in.

Multilabel Classification

This set of algorithms can be thought of as classifying your input as part of one or many classes. Think of Medium articles for one second. When authors are getting ready to publish their article, they have to decide a set of tags that represent their article. These could be tags like ‘artificial intelligence’, ‘dumb story’, or ‘Towards Data Science’.

Now, Medium or someone with some time could train a model that could learn how people tag their articles by doing some natural language processing on their article itself. Their model would then be able to predict or recommend the top 5 ‘labels’ or tags that a new article should have.

The idea here is that there is no mutual exclusivity, and the output can classify the input as one of many things.

Multiclass Classification

Conversely, multiclass classification does have mutual exclusivity. If we extend the Medium analogy a little bit further, the same model would only predict or recommend one of the tags instead of many.

This type of problem is more commonly talked about within the machine learning guides out there because the training sets may have a defined ground truth. For example, if you have a classifier that is predicting dog breeds, you would want the model to choose one output instead of two.

Interestingly, there are a couple of sub-classes within this set of methods. There are one vs. one and one vs. all/rest classification. Here are a couple of links that succinctly explain these differences. In essence, the are smart ways to divide the multi-class classification into easier sub-problems, particularly binary classification problems.

Multioutput Regression

This classification method is similar to multiclass classification but instead of a class that the model is predicting, the model is spitting out a number or continuous variable for the result. If you are looking to create a model that outputs stock price of Apple as well as the momentum of the next move, this may be the way to go.",https://towardsdatascience.com/the-multiclass-definitions-356d2de7ef20,['Danilo Pena'],2018-04-13 20:27:26.706000+00:00,505,"machine-learning, binary-classification, multiclass-classification, multilabel-classification, multioutput-regression"
https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212…,"Learn more. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more

Make Medium yours. Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore",https://medium.com/power-and-representation-2020-new-creativity/why-does-a-i-facial-recognition-favor-lighter-skinned-indiviudals-4f9e81092d95,['Erick Hernandez'],2020-12-07 14:33:18.913000+00:00,64,"the latest stories from across the platform, and stay up to date with all Medium has to offer.Medium, Open Platform, Insightful Thinking, Dynamic Thinking"
Powerful AI Starts With a Diverse Teacher,"Noelle had a very interesting opportunity to collaborate with data scientists at MIT and curators of the Met Museum of Art. The museum was trying to “tag” their art in the most inclusive way they could. “We knew that in order to solve any problems for the museum, we had to get the person with the problem involved to solve it. You can’t just “empathize enough”. You need someone who knows the problem.”

So with that, they used sources and data scientists from all over the world as well as specific to the pieces of art that they were tagging to get the most accurate and robust information that they could. What they ended up doing was fascinating.

Through data collection and machine learning, the AI was taught to, for example, point out to the art viewer .. during x century while in drought season the y society considered z as art. It could pick out information sometimes faster and more efficiently than a skilled viewer could do with their naked eye.

As a human, you only know the words in your head.

The importance of this study, in my opinion, is that when a curator tags the same word for “chair” on all art, what happens? Without a cultural perspective, they couldn’t know what a specific culture or time period may call a “chair”. So by calling it a “chair”, those searching for an alternate word couldn’t find it. The AI tagging could then expose more of an audience to the accurately portrayed artwork.

By including multiple perspectives and diversifying their data scientists, the museum was able to augment the curator’s work. This deep dive into archiving work was never meant as a replacement to those curators, but rather as an enhancement to the art. With all the research in the world, one human is bound to miss things.

Noelle took this study and created her own Alexa Skill with the fundamental principles. She wanted to help people with special disabilities navigate art. The user asks, “Alexa, what’s the art of today”. Based on the comments of the users, the skill generates work that the AI perceives as art the recipient would appreciate.

Theoretically, this AI could learn to draw out information and insight that it knew would be pleasing to the viewer’s taste as the AI learned what that viewer’s unique interests may be.

As a looker but not an understander of art, this is groundbreaking! I have no idea what art I may or may not like. However, if someone or something was to give me suggestions…? That could potentially make an art appreciator out of me.

Having this accessibility could bring new patrons into the museums that wouldn’t have come otherwise.",https://medium.com/swlh/powerful-ai-starts-with-a-diverse-teacher-2e1c5be1dd2d,['Jeanna Isham'],2020-10-21 06:27:06.617000+00:00,446,"data-science, machine-learning, AI, cultural-perspective, Alexa-Skill"
An NLP Analysis: What is Happening About the Presidents on Twitter?,"Data Exploration (EDA Process)

1- Visualizing numeric data (‘length’) as frequency:

2- To see how many tweets have been posted in which language:

3- Min/max tweets:

To view the longest and shortest tweets, we want to see the max and min length values using describe ():

min:7 & max=459 character

min = ‘Merkel!’

max = ‘Merkel: Germany’s …

4-Dividing data into clusters, positively and negatively:

This division process will have optimized the data for future studies. We want to have a ready-made data set when we want to view WordCloud or positive / negative comments separately. So let’s first divide them:

Positive dat set: top 5 tweets

Negative dat set: top 5 tweets

5- Positive and negative as a string:

Next, we will try to combine negative and positive sentences into a single statement. For this we will first use tolist () and then join ().

Negative sentences as one string

Positive sentences as one string

6-WordCloud

Projecting a multi-language data set in WordCloud may not seem meaningful. We will add new words to the words in StopWords to make this a little bit better. For example, when we call stopwords for german:

Now let’s do the additions mentioned. Since the most English tweets are posted after German tweets, let’s add the stopwords in English. There may be different ways to do this. I will prefer to display the stopwords for english first and then update my stopwords list by adding these words. In that case:

And now I select all of them and update them with the update () function. However, for stopwords words like “why’s” that have a single quotation mark in the word again, I have to change all quotes to be binary. Otherwise we will get an error.

Thus, the number of words in total stopwords increased from 232 to 404.

Apart from the unnecessary words, Merkel is already mentioned in every tweet. And it doesn’t make much sense to actually see this in WordCloud. So we added “Angela, Frau and Merkel” to this group of words as above. Now we can focus on other phrases.

Negative WordCloud

If we wonder how many times some words occur in this WordCloud, we can use a code like the one below. For example, I would like to learn how many times the words ‘Adolf’, ‘Hitler’ and ‘Nazi’ are used in tweets about Merkel according to our data set:

Next, we will consider displaying positive tweets in a WordCloud. But here, unlike the above, we will make a Merkel’s mask appear in it. For this, the np.array() values of the image are checked first. And we look at whether the values need to be optimized or not.

In this case, it must be ensured that 0 values are 250 feet. We will use transform for this. The function we will use to transform our mask should be:

Congratulations! Now we can show WordCloud in the image.

wc = WordCloud(stopwords=stopwords, width=3000, height=2000, background_color=""black"", max_words=1000, mask=transformed_merkel_mask,

contour_width=3, contour_color='firebrick').generate(positive_sentences_as_one_string) plt.figure(figsize=(15,10))

plt.imshow(wc)

7- Language Group

In pandas, we can evaluate together one or more data we select between columns by grouping method. Now let’s apply this to the language.

What we want to visualize now is to better understand the average length of negative tweets posted in which language. So let’s see the mean of the group with mean ():

We have not applied any exceptions here. For example, in some languages, only 1 posted tweet is included in this data set. Therefore, graphical values of “bosnian, catalan, crotian, czech, slovenian, finnish” are not very determinant since a single tweet determines the average.

To visualize the most positive / negative tweets sent according to the languages:

plot the top 5 languages for “positive”

plot the top 5 languages for “negative”

8- Issue genereal distrubition

We will now observe the results of the labeling among the negative tweets by subject. First of all, if we consider negative tweets categorically:

Now let’s look at what percentages of tweets labeled “conspiracy theory — insult — political criticism”, by language. Getting all the code here will be a bit confusing. So you can reach this by following the link for the code I will share at the end of the article. If we look at the graph of the percentage stack for the negative issue:

Here we have taken 6 languages that have more data. Because this way we can have a more balanced average.

Let’s examine in which languages the positive content tweets are more intense with ‘Do Not Plot’ According to this:

Thus, we completed the graphical evaluation process according to the subjects.

Now NLP is coming …",https://medium.com/datadriveninvestor/will-the-president-go-home-due-to-the-corona-crisis-the-epidemic-and-the-changing-sentiments-on-14bd99943f1a,['Kurt F.'],2020-12-14 17:06:01.149000+00:00,730,"Data Exploration, EDA Process, Visualizing Data, Frequency Analysis, Language Grouping"
10 Python Skills for Beginners,"#7— Apply a condition to multiple columns

Let’s say we want to identify which Bach-loving plants also need full sun, so we can arrange them together in the greenhouse.

First, we create a function by using the def keyword and giving it a name with underscores between words (e.g. sunny_shelf). Appropriately, this naming convention is called snake case 🐍

The function sunny_shelf takes in two parameters as its inputs — the column to check for “full sun” and the column to check for “bach.” The function outputs whether both these conditions are true.

On line 4, we .apply() this function to the DataFrame and specify which columns should be passed in as parameters. axis=1 tells pandas that it should evaluate the function across columns (versus axis=0 , which evaluates across rows). We assign the output of the .apply() function to a new DataFrame column called ‘new_shelf.’

Alternatively, we could use the np.where() function for the same purpose:

This function from the numpy library checks the two conditions specified above (i.e., that the plant is a lover of full sun and Germanic classical music) and assigns the output to ‘new_shelf’ column.

For these tips on .apply(), np.where(), and other incredibly useful code snippets, check out Chris Albon’s blog.",https://towardsdatascience.com/10-python-skills-beginners-3066305f0d3c,['Nicole Janeway Bills'],2020-11-23 14:06:04.934000+00:00,200,"pandas, np.where(), .apply(), Data Frame, snake_case"
10 tecnologías exponenciales explicadas,"Learn more. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more

Make Medium yours. Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore",https://medium.com/bbsc-blog/10-tecnolog%C3%ADas-exponenciales-explicadas-136cbc79a063,['Biscay Bay Startup Campus'],2020-12-23 11:57:09.416000+00:00,64,"Medium, Open Platform, Insightful Thinking, Dynamic Thinking, Expert Voices"
Review: SqueezeNet (Image Classification),"Review: SqueezeNet (Image Classification)

AlexNet-Level Accuracy with 50× Fewer Parameters

In this story, SqueezeNet, by DeepScale, UC Berkeley and Stanford University, is reviewed. With equivalent accuracy, smaller CNN architectures offer at least three advantages

Smaller Convolutional Neural Networks (CNNs) require less communication across servers during distributed training. Smaller CNNs require less bandwidth to export a new model from the cloud to an autonomous car. Smaller CNNs are more feasible to deploy on FPGAs and other hardware with limited memory.

This is a technical report on arXiv in 2016 with over 1100 citations. (Sik-Ho Tsang @ Medium)",https://towardsdatascience.com/review-squeezenet-image-classification-e7414825581a,['Sik-Ho Tsang'],2019-04-22 02:46:13.031000+00:00,92,"squeezenet, deepscale, UCBerkeley, Stanford University, CNN"
Artificial intelligence in biotechnology,"Although talking about artificial intelligence sounds futuristic, there are already applications that are changing industries, one of these is biotechnology.

From the research and development of new medicines to the sequencing of the human genome, artificial intelligence has disruptively entered biotechnology.

What is biotechnology?

In a very simplified and general way, biotechnology is:

“An area of ​​biology that focuses on the use of organisms or living beings for the generation of products.”

Although the definition may sound a little strong or ethically compromised, this has been the greatest advancement of the 20th century, which has allowed people to extend their lives to more than 80 years.

The application areas of biotechnology can be:

Medicine (Health)

Agriculture

Industry

Ecology

Space exploration

Bioengineering

Bioengineering is an application of biotechnology that is based on the use of technological tools other than biological, mainly derived from informatics to be able to carry out processes.

One of the main barriers that may exist within biological research is the analysis of information and certainty in data.

Computer science has found that by using related methods and technologies, research and clustering of data can be optimized to produce better results.",https://medium.com/dataseries/artificial-intelligence-in-biotechnology-ff43a50c4c36,['Carlos Villegas'],2020-06-09 08:17:03.709000+00:00,178,"Biotechnology, Artificial Intelligence, Bioengineering, Medicine, Agriculture"
How Does a Computer Draw a Smooth Line?,"How Does a Computer Draw a Smooth Line?

Visualizing some of the intuition behind a statistical technique used to trace a line through data points

If I asked you to draw a smooth line between a bunch of points, you could probably do a pretty good job. And it’s also something journalists do all the time using computer graphics to illustrate trends in their data. But how do we get from that first example to the second — how does a computer replicate the intuition we exercise when tracing a line?

One such method is called kernel regression (or more specifically, Nadaraya-Watson kernel regression), which estimates a dependent variable y for an input x with the following equation:

If you’re the type of person to abandon an article once a mathematical equation pops up, I urge you to stick around, since this is actually simpler than it appears, and we’ll be done with the math after this next paragraph.

All the above equation says is that we can estimate any new point by taking a weighted average of the existing points. The x-xi term measures the distance between our new point and one of the old ones, and the K function (the kernel) assigns a weight based on that distance before multiplying it by the accompanying y-value. Add up all of those weighted points, and you get an estimate for a new point based on the values of all the old ones (the denominator ensures the weights will sum to 1).

This of course is a more explicit, technical approach to what we instinctually do while drawing a line through data: take close points into account while ignoring the rest for the most part, especially those farthest away.

If that still doesn’t make sense then…good! The goal of this post is get a visual grasp on how a kernel works, not a mathematical one. Our final deliverable will be a GIF that clearly illustrates a kernel in action as it draws a smooth line. From hereon we’ll use R to write some code, which you can follow if you’re familiar with the language or otherwise ignore.

First we need some fake data. We’ll count to 100 for our x-values, and in order to create a decently curvy response, our y-values will be the product of x squared and sine of x (scaled down to a single period). And, naturally, we’ll add some noise to shake the points off their underlying function.

Using the R package ksmooth, we can easily draw a smooth line for this data using the very kernel regression we just defined. The “normal” term specifies what type of kernel we use while bandwidth sets its sensitivity—the smaller the bandwidth, the more the smoothed line prioritizes extremely close points.

Now the kernel package automatically implements the earlier equation before we can see the intermediary steps — that’s the purpose of statistical software, after all — but we need the actual value of those weights for each point so that we can visualize their influence on the curve.

Since we’re using a Gaussian kernel, the weight of surrounding points is distributed normally, peaking at a distance of zero (that is, at the new point in question), which we can calculate using R’s dnorm function. At the risk of doing some classic mathematical hand-waving, I would ignore the “scale” line below—in short, it’s the way in which we convert the ksmooth bandwidth, defined by quartiles, into the appropriate normal distribution.

Now that we have a way of expressing the weight of every surrounding point that goes into the kernel’s estimate, we can visualize it using the size and color of points. More specifically, for any new point on the smooth line, the influence of the existing points is reflected by how large and red they are. For example, here is the point x = 50 :

Essentially, if we track the smooth line to where it lands over the x-value of 50, we get an average of the surrounding points with the various weights illustrated by their size and color. The nearby points exact great influence on the smoother, and are thus big and bright red; further points are all but negligible, and are therefore small and blue.

Let’s get fancy. By doing the same calculations and visualizations for every x-value between 1 and 100, we can generate a bunch of images and then stitch them together into a pretty little GIF. It’s as simple as executing the above code in a for-loop, and then uploading the individual images into some free online software to create the GIF:

if the GIF doesn’t load for some reason, it’s also here

This was a somewhat expedient examination of kernel regression. Surely we could unpack the equation more or play with different kernels and different bandwidths.

But our final GIF is nonetheless illustrative of the concept behind a smoothing kernel. Plus it kind of feels like were peeking into a computer’s brainwaves as it decides which points to consider as it draws a line. And in turn that process feels very familiar to anyone who’s done something like this the old-school, manual way.

Thanks for reading. Most of my other statistical thoughts can be found on my blog, perplex.city. The full code is here.",https://towardsdatascience.com/how-does-a-computer-draw-a-smooth-line-a484176ac24e,['Walker Harrison'],2017-07-27 16:42:09.692000+00:00,863,"Computer Graphics, Kernel Regression, Nadaraya-Watson Kernel Regression, Statistical Technique, Visualizing Intuition"
Predictions for AI Developments in 2018,"Entering the new year, we are wondering what to expect from the new AI technologies and which directions to look at.

In this brief review, we offer some predictions for AI developments in 2018.

Since all indicators show the likely increase in investment into the development of AI and, particularly, machine learning, technologies, the world is expecting a new wave of AI applications. Most importantly, these applications are finally reaching beyond computers learning to beat humans at chess and TV game shows. In 2018, machine learning and neural network technologies will continue to refine themselves and take on more routine tasks — in all aspects of our lives.

Robotics

Yes, robots. In the Blade Runner universe, people have already mastered replicants manufacturing by 2019. On the verge of 2018, we are still far from that, but our coexistence with robots is already taking place.

2017 was all about toy robots, programmable robots and other kinds of educational robots for kids. In 2018, we’ll see programmable robot platforms, designed for adults, which are more sophisticated in their capabilities and offer more interesting and versatile robotics functionality. Additionally, more consumer robots will enter our lives as home helpers to remove friction from our daily routine.

Healthcare

Advancements in big data analysis and AI will play a major role in healthcare.

Maybe, the present-day encroachment of AI is still almost invisible to patients. Yet, new technologies will push the industry forwards, changing the way it operates and treats patients by optimizing workflows both within in-patient and out-patient scenarios. Behind the scenes of health examination, image recognition algorithms are already being used in pilot projects to spot warning signs buried in medical images and to decipher hand-written doctors’ notes. With these pilots being quite successful, we are likely to expect a boost of similar projects in the coming year.

Another important, and definitely more visible, development in healthcare is a greater involvement of robots in looking after the health and well-being of patients. Caregivers and companion robots are expected to gain popularity and could begin to become an everyday reality in 2018.

Communication

The next year is likely to change our interaction with machines, and conversational interfaces will become common when it comes to interacting with technology in a business environment. This year’s estimates are that in 2018 about 20% of firms will look to add voice enabled interfaces to their existing point-and-click dashboards and systems.

Natural language generation and natural language processing algorithms are constantly learning to become more tuned to understanding us, and talking to us in a way we understand. In 2018 they will continue to improve and we should get used to robots which we can converse with in a more human-like manner.

In the nearest future, we are likely to see an increased focus on bot sensitivity training, which will allow humans to offload even more work on chatbot shoulders both in business and in the everyday life: while a new virtual assistant by X.ai called “Amy” can respond to messages regarding meetings, meals, and calls, Amazon’s Alexa recently began syncing with Outlook and Google to help families keep up with their schedules.

Data-Driven Machines

Big data may sound less exciting android creation, but it’s what will be the focus of most companies’ work next year. Pushed by the steady growth of data produced by the Internet of Things, businesses will turn to machine learning to process, trend, and analyze the received information. In 2018, machine learning will become an absolute must-have for companies wishing to make sense of structured or unstructured data. With humans unable to process this overwhelming data flow, the key strategy will be to call AI with large-scale analytics into play.

Processing of big data will change the way businesses are run, simultaneously improving the security monitoring by anomaly detection and prediction of stock-exchange and market trends. Enabling more precise clustering of customer groups, and developing smart cross-sale by tracking the sales vs. ranks for all products in various categories, machine learning will boost the sales and proclaim a new era for commerce.

Whatever changes in our lives we’ll see in the coming months, 2018 will definitely be an exciting year to live!",https://medium.com/sciforce/predictions-for-ai-developments-in-2018-d06193c905db,[],2018-01-17 09:06:28.257000+00:00,681,"AI, Machine Learning, Robotics, Healthcare, Communication"
3 Learnings From 3 Years In Applied AI,"Since 2017, we have been working in the field of applied Artificial Intelligence. Our projects ranged across various industries and across all sizes of corporations. But still, there have been recurring patterns that transformed into condensed learnings. Those learnings happened to be very helpful for early client discussions. That is why we would like to share them.

Learning 1: Expectation management is crucial

Before you start an AI initiative, please consider what your expectation is. By doing this pre-assessment, you make sure not to fall into the “magic-trap”. Don´t get me wrong, AI is a powerful hammer. But not every business challenge is a nail. Be sure, on which of the following three dimensions you start.

Many decision makers want to start at the biggest piece of the pie (Things you don´t know that you don´t know). The notion of “we give you all our data and you tell us our new strategy” seems seductive, but does not work. In this case, an explorative data analysis does the job. But AI initiatives work best, when you put them in context with your strategic objectives, test your hypothesis and build a solution around the problem you solved with AI.

Learning 2: Everyone wants to know the future

At one point, we sat down and analysed all the 100+ use cases we had assessed with clients so far. Those included not only the ones we actually did, but also the ones that clients put on the table as desired future functionalities, products or services.

By far, prediction use cases are the most requested category. Predicting customer churn, predicting logistics capacities or predicting demand peaks in a given period. This showed us that apparently, we all are fascinated by knowing the future. But on a second look, we also observed a big overweight on data-driven use cases. This may be due to the fact, that Machine Learning was (and still is) by far the most prominent representative of the AI technology family. Often times, ML is used synonymously with AI. Most of the other use case groups build upon the given predictions. Based on the prediction, you want your resources to be allocated correctly. This includes scheduling of tasks, planning and allocating space in a delivery truck or dynamically schedule machine utilisation in your factory.

Based on your resource allocation, you want to quickly identify and anticipate exceptions. And down the line, the actual work also needs to be done in terms of running a process on an IT system, adjusting master data or generating and triggering an email workflow. Here, we are more on the Automation side. Less data-heavy, less looked at but not less important if you plan to create a powerful AI solution.

Learning 3: Powerful AI solutions consist of two sides: A data side and an automation side.

What is it worth knowing that your customers might churn or that your machine is about to overheat…if you do not act upon it?

Right — almost nothing. That is why you need both a data side and an automation side. Detection and Action.

Our third key learning was the symbiosis of detection and action. Mixing the data world with the process automation world. Similar like Kahneman´s System 1 and System 2, a holistic AI solution also needs one part that quickly grasps information and detects a scheme, classifies it accordingly and hands it over to the reasoning side, where rule-based decision-making takes over. This approach does not only match the human decision-making, it also creates an explainable AI.

BONUS: Learning 3.5: For scalable (!) AI use cases, solid data integration is key and serves as a foundation for a companies data- and AI-strategy

By far the biggest time eater in most AI projects is data integration. This includes sourcing, cleaning and organizing of appropriate data sets. Depending on a company´s degree of data savviness, this number can go way up than 61%. That is why it makes sense to invest some time to be clear about what you want to achieve with your AI project. Starting with smart data integration and a solid data pipeline eases the way down the road significantly. Otherwise, scalability is at stake and your AI project will have a hard time leaving the PoC stage.

In summary, corporate AI frontrunners have already embraced the need for explainable AI and built powerful products, services and business models around it. With the right use case, diligently assessed on technical feasibility and strategical relevancy, every company can jumpstart their AI journey.

About Norders: NORDERS is a consulting firm specialising in the development of use case specific Artificial Intelligence solutions. Based on advanced analytics models and alternative data sources, we help companies to measure the so-far immeasurable. Combined with intelligent automation technologies, we help companies to create lasting competitive advantages.",https://medium.com/@norders/3-learnings-from-3-years-in-applied-ai-d08fd816b0c7,[],2021-01-25 15:10:03.564000+00:00,786,"AI, machinelearning, expectationmanagement, predictionusecases, dataintegration"
GPU Accelerated Cyber Log Parsing with RAPIDS,"By: Bianca Rhodes US, Bhargav Suryadevara, and Nick Becker

Security Operations (SecOps) and IT departments are collecting, managing, and attempting to analyze more data than ever before. Employees are likely to connect their own devices to corporate networks, further widening an already heterogeneous attack surface. With the average time to detect a data breach at 196 days and the cost of a data breach to a US company at $7.91M, it is absolutely necessary to collect, ingest, and make available to SecOps teams salient cybersecurity logs and data feeds. Parsing that raw data quickly and staging it in such a way that makes it readily available not only to human operators but also machine learning models is a key factor in a strong, layered security model.

We introduce a RAPIDS use case to address this issue. Our workflow focuses on two RAPIDS libraries: cuDF and dask-cudf. cuDF the (GPU dataframe library within RAPIDS) is modeled after the Pandas API, allowing the user to leverage GPUs almost seamlessly by editing a python import statement. Dask-cudf allows us to execute tasks or code on a partitioned dataframe across GPUs. Incorporate dask-cudf to easily scale your data processing to multiple GPUs. In this post, we show how using RAPIDS to parse Windows Event Logs (WinEVT logs) provides a speed increase and affords immediate benefits of integration with machine learning techniques. Benchmarks for general end-to-end data processing and machine learning using RAPIDS can be found at RAPIDS.AI.

By the end of this tutorial, we’ll be able to parse raw Windows Event Logs containing authorization data.

Using RAPIDS for Log Parsing

Let’s start with importing the necessary RAPIDS libraries.

cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.

dask-cuDF is a library used to create partitioned GPU-backed dataframe, using Dask.

Raw Windows Event Logs

Below are sample windows event record samples of type 4624 and 4625, provided by Los Alamos National Laboratory (LANL). Event code 4624 represents a successful logon event. Event code 4625 represents a failed logon event. While the LANL data is provided already parsed in JSON format, we created raw WinEVT data from this parsed format in order to test the workflow (shown in the “Raw” column). This data closely matches how a raw WinEVT log would appear after creation. The script used to create this raw representation of the data can be found here.

Importing the Data into a Dataframe

Let’s take these sample records and begin the parsing process. We can import the data rather easily from a CSV file. Follow along by saving the above data into a csv file called “sample.csv”.

Data Preprocessing

The data must be preprocessed to remove the non-printable characters (e.g., newlines and tabs), replacing newlines with a “|”. We do this by creating a function called preprocess_logs that accepts a dataframe and preprocesses the “Raw” column.

Then we run this function on our dataframe

A sample output of the first record after preprocessing is shown below:

Applying Regular Expressions to Capture Key-Value Pairs

After preprocessing is complete, we use regular expression (regex) dictionaries to extract all of the key-value attribute pairs. Because each Windows Event Log of a different event code type contains different key-value pairs, we must apply different regex to each log type. To help with this, we created the functions filter_by_pattern, which filters the data by event code type, and process_log_type.

The function process_log_type below performs several operations needed for processing. It:

Creates an empty dataframe with the list of superset columns to hold parsed data, Retrieves the list of regex pattern keys for a given event code, and Iterates over each key to use the regex extract operation on raw records to pull an attribute’s corresponding value.

We execute this on our dataframe for WinEVT code 4624.

These steps can be repeated for other event codes. The parsed output for event code 4624 is shown below.

Bringing It All Together

Let’s now tie all of these steps together with a pipeline function. This function preprocesses the logs then applies filtering and regex to each log based on event code.

Optimizing the Pipeline for Large Datasets

For ease of use, we created additional functions to read in regex configurations from yaml files. This creates a well-formatted regex dictionary that we apply to a larger dataset with various event codes. A sample yaml file can be found here.

Before we run our pipeline on a large dataset, we first define our input and output columns. As of this post, cannot concatenate dataframes that have different column names. For the time being, we pre-define the output columns to ensure each new dataframe has the same columns.

We incorporate dask_cudf to execute our pipeline function. The following parameters are configurable and may be modified to suit your needs.

AUTH_INPUT_PATH = Path to input file containing LANL data.

AUTH_REGEX_CONF_PATH = Path to regex config files

AUTH_EVENT_CODES_OF_INTEREST = An array of event codes we are interested in

AUTH_REQUIRED_COLS = Required columns from input csv file needed for parsing and analytics.

A sample output of auth_gdf is shown below when the pipeline is run on a larger dataset.

Conclusion

As demonstrated, we utilize RAPIDS to parse event log data utilizing the GPU. The first step of any analytic is to parse the data, and we utilize this parsed data to create a network map in a future post. View a completed Jupyter notebook, which includes network mapping, and execute it in your own environment. Input data that matches data used in this post is provided. Instructions for configuring a Docker environment are available in the README of the Github repository.

The processing time for the Jupyter notebook referenced above can be found in the table below.

We invite you to contribute to RAPIDS and submit issues or feature requests as we continue development.",https://medium.com/rapids-ai/gpu-accelerated-cyber-log-parsing-with-rapids-10896f57eee9,['Bianca Rhodes Us'],2019-06-28 20:06:25.328000+00:00,936,"RAPIDS, Data Processing, GPU Dataframe, WinEVT Logs, Machine Learning"
Building an Image Classification Model with CNN,"First Model

As our baseline model, we will build a simple convolutional neural network that takes in images after resizing them to be a square matrix and normalizing all pixel values to range from 0 to 1. The full step is shown below.

Now I will explain each step in detail.

1. Scaling Data

keras.image.ImageDataGenerator() takes images and create augmented data based on the parameters. Here we are just asking it to rescale all pixel values to be 0 to 1 without specifying any other augmentation parameters. Combined with flow_from_directory , the generator calls images from the directory in the assigned format, then creates rescaled data.

2. Building a Model Architecture

keras.models.Sequential() initiates a sequential model. This model will sequentially process the added layers.

Conv2D layers are the convolutional layers, which takes the input and runs them through the assigned number of filters. Kernel size refers to the dimension of filters. So in this example, each consecutive group of 3*3 pixels in our 256*256*1 image (1 referring to the number of channels, RGB images have 3 channels, while grayscale images have 1 channel) will run through 32 filters generating 32 feature maps with a size of 256*256*1.

padding = ‘same' is used to add equal paddings around our window since 256 is not divisible by 3.

activation = 'relu' means that we are assigning the rectified linear unit as our activation function. Simply put, we are telling the layer to convert all our negative values to 0.

We then feed these outputs from convolutional layers into the pooling layer. The MaxPooling2D layer abstracts the convolved outputs by only leaving the maximum value of each of the 2*2 matrices of the convolved output. Now we would have 32 feature maps with the size of 128*128*1.

Now we need to narrow down these 4-dimensional outputs to a single number that can tell us whether to classify the image as pneumonia or normal. We do this by first flattening the layer into a single dimension then running them subsequently through smaller and smaller dense layers. A sigmoid function is applied as an activation function on the final layer because we now want the model to output a probability of whether the output is pneumonia or not.

3. Configuration

We have defined the architecture of our model. The next step is to decide the goal of this model and how we want it to get there. Using model.compile , we tell the model to minimize the binary cross-entropy loss (the log-loss, think logistic regression) using the gradient-descent. Here we are using the RMSprop algorithm to optimize this process by adaptively decreasing the learning rate. In the later model, I used the AMSGrad algorithm, which performed better for our problem.

4. Fitting Data

Finally, we finished constructing our model. It’s time to fit our training data! Each epoch will run 32 batches by default. We are setting EarlyStopping to prevent overfitting. This model will stop running if our validation loss is not reduced for 5 consecutive epochs. I set the restore_best_weights to be true so it will revert to the highest performing weights after those 5 epochs. It tests its performance on the validation generator we created earlier.

5. Evaluation

Our first model showed 94% accuracy predicting the class of validation data with the log loss of 0.11. Based on the below graph, we can see that training loss has room to improve so we can probably increase the complexity of the model. Also, the validation loss seems to be hovering around 0.1. We can try to improve generalizability by imitating adding more data using data augmentation.

Here’s a full code to plot the loss graph and accuracy graph from the fitted model.",https://towardsdatascience.com/detecting-pneumonia-using-convolutional-neural-network-599aea2d3fc9,['Eunjoo Byeon'],2020-12-15 16:23:16.427000+00:00,600,"Generate Tagsconvolutional neural network, keras.image.Image Data Generator(), keras.models.Sequential(), Max Pooling2D, binary cross-entropy loss"
Machine Learning — K-Nearest Neighbors(K-NN),"Statistic How To (2019)

Definition

The K-Nearest Neighbors (K-NN) algorithm is one of the most fundamental and simple classification scheme which can be used for both classification and regression problems. However, the algorithm is mostly used for classification predictive problems in industry. ‘Feature similarity’ is used to predict the values of new data points which we wish to classify. The ‘feature similarity’ suggests that the new data point will be assigned a value which depends on the ‘closeness’ of it with the data points in the training set.

2. The Algorithm

Suppose now we have a dataset, we would like to implement K-NN algorithm, we have already known that the algorithm depends on the ‘closeness’ of the test sample and the specified training samples, thus, we should find a way to measure the distance between them. There are several distance metrics could be used to calculate the distance, the most commonly used method is called Euclidean distance. The function is defined as follows:

Other distance measure functions includes the Manhattan and the Minkowski.

After we calculated the distance between the new data point and all other points, we could sort the distance values in ascending order. If we take the first row from the top which is the smallest distance value, that will be 1-Nearest Neighbor, then we will classify the new data point in the same category as its ‘neareast neighbor’.

Instead of taking one value, we can extend the idea to a K-NN rule by taking the first K (K can be any integer) rows from the sorted array. Then, we will classify the new data point based on the majority of the K-NN votes in the list.

The choice of K is somewhat arbitrary, however, K can be adjusted based on the size of the dataset and the accuracy of predicting. Some experimentation of different value of K is needed in order to get the best value of K which could minimize the misclassification rate in the validation data. If the value of K is too small, the new data point which we need to classify will be very sensitive to the classification of the closest. A larger value of K will reduce the variability, however, if the value of K is too large, it will induce bias into the classification decisions. Usually, the value of K we use will be between 1 and 20.

To sum it up, In order to implement K-NN algorithm, we could follow the steps below:

First, we got our dataset, load the train data and test data.

Choose a specified value of K, then for each data point in the test data, we measure the distance between the point and all other data points in train data.

Based on the distance values and the value of K which we chose, we classify the new data point based on the majority of the votes in the list.

Check the predicting accruacy of different values of K, then we pick the K with highest accruacy.

3. Example

Now let us implement the K-NN algorithm on the dataset in Python. The Iris dataset is used in this case.

The Iris dataset could be download here: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data

We first take a look at the dataset:

The dataset has four features (X) and the column ‘Class’ is our y.

The next step we need to normalize the data, this procedure is important in order to ensure that equal weight are given to each variable. Then we split the data into train set and test set with the help of ‘train_test_split’.

After the preparation is done, we could implement the algorithm, we will make the value of K equal to 4 first to get the model, and we will also calculate the accruacy in this case.

For the value of K equals to 4, the K-NN gives us 91.67% accuracy for the test set. The predicting result is not bad, let us see how the other values of K performs. We would range the value of K from 1 to 9.

As we can see, when the value of K equls to 8, the K-NN gives us the highest accuracy for the test set of 98.33%.

Reference

Evans, J. (2017) Business Analytics. England: Pearson.

Madhu Sanjeevi (2017) ‘Chapter 5 : K-nearest neighbors algorithm with code from scratch.’

Tutorialspoint (2020) ‘KNN Algorithm — Finding Nearest Neighbors’. https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_knn_algorithm_finding_nearest_neighbors.htm",https://medium.com/@marvinbu/machine-learning-k-nearest-neighbors-k-nn-778f445f5a48,['Bu Yifan'],2020-11-23 17:09:42.411000+00:00,703,"statistics, K-Nearest Neighbors, classification, regression, distancemetrics"
You’ll Be Scared in the Dark,"One of the most debated AI-application in recent years has been facial recognition (such capabilities were called image processing 15 years ago but are routinely termed AI today). Did you know that a study by the National Institute of Standards and Technology (NIST) found that these systems misidentified people of color more often than white people? Moreover, people of Asian and African American descent were up to 100 times more likely to be misidentified than white, middle-aged men. But the algorithms also misidentified the elderly, women, and children at a significantly higher rate, the study found. It probably doesn’t come as a surprise that middle-aged white men generally benefited from the highest accuracy rates. According to The Verge, NIST looked at 189 algorithms from 99 organizations, which counted together, powering most of the facial recognition systems in use.

Is Dystopia the Logical Consequence?

We know different ways in which unfounded fears are stirred. The various ways in which news is distorted or facts are denounced as fake news. We understand how AI applications produce fake images and distribute them to compromise deep counterfeit pictures and videos (called “deepfakes”).

Since there are no killer robots yet and the probability is more significant that humans’ technological advances against other humans seem more likely, we need to know whether and how we should be afraid or develop a fear in the first place. Or, in other words: Where is it worth taking a closer look? How do we have to educate ourselves to find out precisely what is in danger? With the advances that have been made, the threat of AI seems to affect not so much our physical integrity but rather our social norms and the freedoms as citizens. In other words: We have to look closely into Dark AI applications and the potentially disastrous consequences they might have.

Therefore, it is evident that we have to ensure that many people have a voice and contribute when we discuss AI applications. Ultimately, we must inform ourselves about the critical aspects of AI and form an opinion on how it should be used. To do so, we should try to understand AI as a political and social ideology and not only as algorithms since we are users, beneficiaries, and in part, victims of this technological advancement. It is essential to know that in the wrong hands, AI is currently a more significant danger than science-fiction killer robots. Therefore, more has to be done to ensure inclusion, equality, and equity in developing such applications. This requires our attention and cooperation as citizens, as humans, and participants in the ecosystem we live in. We don’t have a choice in this matter: machine learning, deep learning, and all other aspects of AI are here, and they are here to stay.

So what will you do?",https://medium.com/digital-diplomacy/youll-be-scared-in-the-dark-11bfe0d76427,['The Unlikely Techie'],2020-10-23 21:49:38.016000+00:00,464,"AI, Facial Recognition, Deepfakes, DarkAI, Machine Learning"
Camouflaged Object Detection Using SINet,"Now that a dataset is discussed let’s take a look at the framework itself. It consists out of two main modules: the search module (SM) and the identification module (IM). Both are inspired by hunting. First, a predator will look for a potential prey. If a prey is found, it will be identified and finally be caught.

The Search Module

Just like the human visual system, Receptive Fields(RF) are used to highlight the area close to the retinal fovea, which is a part of the eye that is sensitive to small spatial shifts. This inspired the researchers to use a Receptive Field component that mimics RF of the human visual system. The RF component contains five branches. The first four are concatenated and the fifth branch is added. After that, the whole output of the component is fed through a ReLU function.

Inside look of the RF component[1]

The RF component is used multiple times in the SM, as seen in the image below. The inputs, that originate from Resnet-50, went through multiple convolution layers, down and up-sampling layers and concatenation.

Visual representation of the SM[1]

What is also being used within the SM is the search attention (SA) function, which is, in fact, a Gaussian filter that will later cause an enhanced camouflaged map to be generated.

The Identification Module

After receiving the outputs from the search module, the partial decoder component (PDC) is used to precisely detect the camouflaged object. As shown in the image below, the PDC uses four (one optional) inputs of the SM and the output is a camouflaged object map.

A look into the PDC component[1]

As seen in the complete overview of SINet[1], the PDC is being used twice. The differences between the two are the numbers of inputs it requires to generate the camouflaged object map and the inputs themselves. Both maps are being combined through addition to create the final camouflaged object map.",https://medium.com/swlh/camouflaged-object-detection-using-sinet-b8d6fcae5cf9,['Dylan Hiemstra'],2020-10-30 10:55:27.612000+00:00,312,"SINet, Receptive Fields, ReLU, Resnet-50, Search Attention"
Top Research Papers from the ECML-PKDD 2020 Conference,"Last week I had the pleasure to participate in an ECML-PKDD 2020 Conference. The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases is one of the most recognized academic conferences on ML in Europe.

Fully online event, run around the clock — nice idea to make it accessible in all time zones. Conference schedule, neatly divided into many tracks on various flavours made it simple to dive into my favourite topics in reinforcement learning, adversarial learning and meta-topics.

ECML-PKDD brings a large number of new ideas and inspiring developments in the ML field, so I wanted to pick top papers and share them here.

In this post, I focus on research papers, which are divided according to the following categories:

Reinforcement learning

Clustering

Architecture of neural networks

Transfer and multi-task learning

Federated learning and clustering

Network modeling

Graph neural networks

NLP

Time series and recurrent neural networks

Dimensionality reduction and auto-encoders

Large-scale optimization and differential privacy

Adversarial learning

Theory for deep learning

Computer vision / image processing

Optimization for deep learning

Enjoy!

Reinforcement learning

1. EgoMap: Projective mapping and structured egocentric memory for Deep RL

Paper | Presentation

Paper abstract: Tasks involving localization, memorization and planning in partially observable 3D environments are an ongoing challenge in Deep Reinforcement Learning. We present EgoMap, a spatially structured neural memory architecture. EgoMap augments a deep reinforcement learning agent’s performance in 3D environments on challenging tasks with multi-step objectives. (…)

First author:

Edward Beeching

Twitter | LinkedIn | GitHub | Website",https://medium.com/neptune-ai/top-research-papers-from-the-ecml-pkdd-2020-conference-6a784681d467,['Patrycja Jenkner'],2020-12-16 22:08:12.233000+00:00,227,"Reinforcement Learning, Ego Map, DeepRL, 3DEnvironments, Edward Beeching"
Data Science Crash Course 1/10: Introduction,"Data Science Crash Course 1/10: Introduction

Let’s learn Data Science in 2020.

2020 is here and it’s time to learn Data Science! This is a quick course to start your journey with Data Science. If you’re looking to jump into Data Science and especially learn Python, this is a place to start. Our goal is to start programming right away on whatever computer you have right now. Here we go!

Data Science Crash Course. Introduction to Python.

Use Python for Data

Python is a perfect language for beginners and experts alike, due to its popularity and clear structure. It’s easy to pick up and thanks to community you’ll be able to use it both for your first experiments as well as the most recent machine learning research.

Basic Data Science Concepts

First of all I’ll talk about setting up your environment. In our case that means installing Anaconda on your computer, which will allow you to quickly run Jupyter Notebooks and there you’ll be running short Python programs straight away from your browser. So from now until your first program you probably need like 15 minutes. Read this post to know more about Anaconda.

Then to do data science you will need a little bit of mathematics. I will review basic terms from linear algebra and statistics that you will need along the way. Those include:

vectors and operations on them

matrices and operations on them

mean, variance, correlation

random variables, dependence, conditional probability,

Naive Bayes theorem

Have a look at this post to learn more about mathematics for Data Science.

Then we can go directly to processing data:

importing files in .txt, .json, .xml, .html

storing data in NumPy arrays, pandas DataFrames and other structures (lists, dictionaries)

I discuss processing data methods in this post on Medium.

Getting and processing Data

I will also discuss how and where you can get interesting data from. Because Data Science is about Data, so you should find a dataset which is exciting to you. Some examples include:

books and documents, for example from Project Gutenberg

scraping websites (downloading sport or finance news)

Kaggle databases

Twitter API

Read this post to learn more about getting data for Data Science.

Standard techniques in Data Science

Having data is the first step to Data Science and once we have data we can start working with. Standard techniques include:

Classification: we have 2 or more categories (labels) and we want to classify objects according to these labels. We talk here about supervised learning and standard techniques include: KNN, Naive Bayes, Decision Trees, XGBoost, neural networks. Read more about classification here.

Clustering: we don’t have any labels but still we want to classify our data. We cluster our data by different measures of similarity, thus creating labels. We talk here about unsupervised learning and standard techniques include: k-means and DBSCAN. Read more about clustering here.

Neural Networks is a separate topic in itself. The basic idea behind them is simplifying a problem into smaller pieces which can be processed separately by ‘neurons’. I’ll cover basic frameworks like Keras and Tensorflow, and then also discuss MNIST and other problems which are great to start with. Read about neural networks here.

Dimensionality Reduction is a must if you want to visualize data or better understand it. Sometimes you just want to plot your 4D data in 2D and then you have to transform it to still capture the essence of information. Standard techniques include PCA and SVM. Have a look here.

Visualizing Data is important when it comes to business applications of Data Science. In the end you want to convey your findings to others, and the best way to do that is by presenting them with a nice graph, where everything is clear. We’ll talk about plotly and Dash. Read it here.

After this series you’ll be prepared to tackle basic Data Science problems and learn more by yourself.

Let’s learn Data Science in 2020!

If you want to see a video for this course, please have a look here:",https://medium.com/data-science-rush/data-science-crash-course-1-10-introduction-4c23260194c6,['Przemek Chojecki'],2020-05-07 16:53:58.735000+00:00,637,"Data Science, Python, Machine Learning, Linear Algebra, Statistics"
Plotnine: ggplot2 in Python,"Plotnine is Python’s answer to ggplot2 in R. R users will feel right at home with this data visualization package with a highly similar syntax with minor syntactic differences.

R has been the ruler of data visualization and statistical modelling between the two while Python was the best for productionizing / monetizing data science. The ggplot2 package from R is a child of Hadley Wickham’s ingenuity. It undoubtedly allows anyone who can code in R to use a declarative approach to creating stunning visuals for their work. There are also a number of extensions that can extends ggplot2 even further.

Python has Matplotlib and Seaborn for its external plotting library. Seaborn is built on top of Matplotlib and is a dependency. Seaborn helps the user accomplish what would be done in Matplotlib with much less code and allows the user to use Matplotlib commands to manipulate the figure as well.

Undoubtedly we are spoiled for choice once we have a good grasp of both languages. However lone Python users have not been able to experience the beauty and simplicity of ggplot2 in its native Python until now (to a certain extent).

Thanks to Hassan Kibirige, the creator of plotnine. Plotnine is the competitor of R in Python. Its syntax is probably 95% similar or more to ggplot2. It has a table release and is still active on Github. It already does a lot of what users love most about ggplot2 such as access to various geoms, declarative syntax and faceting for example. There are many R users that realize the importance of Python in their skillset and vice versa. Plotnine is another way that makes a comfortable transition between the languages. Plotnine is still an infant in comparison to its counterpart ggplot2 but the potential is huge if active development continues.

Plotnine

As Python’s designated ggplot2, we shall now see how the syntax and visuals match up with each other. One does not replace the other but if different jobs can be done on different languages, certain people can get an idea on what language to conduct their data visualization tasks.

A tip about plotnine installation that some users may experience (not all): When installing plotnine make sure you are using the updated version of your package manager (i.e. pip or conda). If you are using Jupyter, for some reason it may not work but it will work on other IDE’s like Spyder. This was the case for me and I haven’t figured out the reason yet.

Below is a standard example of using plotnine. R users will notice that the syntax and output are strikingly similar.

(

ggplot(mtcars, aes(‘wt’, ‘mpg’, color=’factor(cyl)’))

+ geom_point()

+ labs(title=’Miles per gallon vs Weight’, x=’Weight’, y=’Miles per gallon’)

+ guides(color=guide_legend(title=’Number of Cylinders’)) )

There are really only two noticeable differences in the syntax:

ggplot(mtcars, aes(‘wt’, ‘mpg’, color=’factor(cyl)’)) — we can see that ‘factor(cyl)’ is a striking resemblance to the method of converting data into a category in R. There are other ways to convert a numeric column into a category but this seems a nice nod to R users.

The whole syntax is actually enclosed in parenthesis (brackets). This is due to indentation issues in Python. Because Python relies so much on indentation for functions, brackets become necessary to execute the code.

To take it to the next step, faceting the plot is also simple and easy with a minor tweak to make it in plotnine:

(ggplot(mtcars, aes(‘wt’, ‘mpg’, color=’factor(cyl)’))

+ geom_point()

+ labs(title=’Miles per gallon vs Weight’,

x=’Weight’, y=’Miles per gallon’)

+ guides(color=guide_legend(title=’Cylinders’))

+ facet_wrap(‘~gear’)

)

The last line of code has (as R users would know) facet_wrap and the only difference is the argument needs to be in colons.

A scatterplot is ideal for addding many dimensions of data. Now we shall see how to change the sizes of the points to represent horse power:

(ggplot(mtcars, aes(‘wt’, ‘mpg’, color=’factor(cyl)’, size = ‘hp’))

+ geom_point()

+ labs(title=’Miles per gallon vs Weight’,

x=’Weight’, y=’Miles per gallon’)

+ guides(color=guide_legend(title=’Cylinders’))

+ facet_wrap(‘~gear’)

)

Once again just a simple command and a barely noticeable difference. We have added the size=’hp’ argument to the aesthetics so that each point will represent a different level of horsepower. We can see that higher the weight engine, the more horsepower it has.

Any plot needs to look their best. Themes are a great way to add some default formatting in order to get started. Plotnine appears to have some of the same themes from ggplot2 as well:

(ggplot(mtcars, aes(‘wt’, ‘mpg’, color=’factor(cyl)’, size = ‘hp’))

+ geom_point()

+ theme_bw()

+ labs(title=’Miles per gallon vs Weight’,

x=’Weight’, y=’Miles per gallon’)

+ guides(color=guide_legend(title=’Cylinders’))

+ facet_wrap(‘~gear’)

)

Just by adding theme_bw() we can get a rather nicer and more aesthetically pleasing plot.

Conclusion

We can see that plotnine and ggplot2 have much in common. Transitioners to Python from R will definitely feel more at home when conducting data visualization in Python with plotnine.

One of the best things about plotnine is that is still active on Github as of July 2019 and hopefully it is a good sign that it will continue to be updated to become the ggplot2 of Python. I say that because ggplot2 has so many extensions and complementary packages that make it powerful and wanted. Plotnine is undoubtedly doing a great job and I would definitely use it myself.",https://medium.com/bitgrit-data-science-publication/plotnine-ggplot2-in-python-2b4b1fe8d4c5,['Asel Mendis'],2019-07-16 06:20:20.865000+00:00,841,"plotnine, Python, ggplot2, RR, data visualization"
What Industries Will Be Next to Adopting Data Science?,"It’s no surprise that data science will surely spread to more industries in the next couple of years. So, which of them would probably be the next ones to hire more data scientists and benefit from big data?

We looked at five very different businesses that are starting to benefit or could benefit from data science and how exactly can big data better help them achieve success in their fields.

1) Sports

If you saw the movie Moneyball you might know why big data is important to baseball and sports in general. Nowadays, for instance, many NBA teams collect millions of data records per game using cameras installed in the courts. The ultimate goal for all these sports teams is to improve health and safety, and thus performance of the team and individual athletes. In the same way that businesses seek to use data to custom their operations, it’s easy to see how these two worlds can crossover to benefit the sports world.

2) On-demand services

Uber gets attention for its growth and success that came mainly because how the company uses data. The Uber experience relies on data science and algorithms so this is a clear example of how on-demand services can benefit from big data. Uber continues to succeed because of the convenience that its data-driven product provides. Other on-demand services should look up to Uber’s example for their own good and follow up with relying more on data science.

3) Entertainment industry

In this era of connected consumers, media and entertainment businesses must do more than simply being digital to compete. Data science already allows some organizations to understand their audience.

A once content-centric model is turning into a consumer-centric one. The entertainment industry is prepared to capitalize on this trend by converting information into insight that boosts production and cross-channel distribution. From now on it can be expected that those who provide a unique audience experience will be the only ones to achieve growth.

4) Real estate agents

We continue hearing that the housing market is unpredictable, however some of the top real estate agents claim they saw the housing bubble burst coming way back (think again of movies, exactly like in The Big Short). It’s easy to obtain this information from following data and trend spotting. This is a great way for this volatile industry to be prepared for market shifts.

5 ) Restaurant owners

This business field is the epitome of how important it is being able to tell what customers want. According to the Washington, D.C.-based National Restaurant Association, restaurants face another big obstacle besides rent, licensing and personnel: critics, not only professional but amateurs who offer their opinions on social media. The importance of quality is the reason why restaurants are beginning to use big data to understand customer preferences and to improve their food and service.",https://medium.com/3blades-blog/what-industries-will-be-next-to-adopting-data-science-d385fa06ccc6,['Samuel Noriega'],2017-04-29 17:09:48.847000+00:00,463,"data science, big data, sports, on-demand services, entertainment industry"
5 The Most Exciting AI & IoT Trends of 2021,"Technology Trends

5 The Most Exciting AI & IoT Trends of 2021

… holographic meetings sound pretty cool

Photo by Maximalfocus on Unsplash

Every industrial revolution is a change and an upgrade of traditional/ industrial practices.

The 1st industrial revolution has brought us mechanization, steam and water power. The 2nd one — mass production and electricity. The 3rd revolution has brought us electronics and IT systems and was the starting point of automation. The current industrial revolution is in the process of bringing cyber-physical systems.

Each of these revolutions bears a global societal transformation at large. Even from the 1st industrial revolution, the new technologies have reformed society. It has changed not only how we work and how we live. It also reshaped the way we think, rest, communicate and how we perceive ourselves as human beings.

In the next year, we will see more and more Tech For Good applications that will bring more purpose and value to technology. Here I would like to cover 5 of the most exciting AI and IoT trends and innovations that will drive 2021 and the fourth industrial revolution.",https://medium.com/technology-hits/5-the-most-exciting-ai-iot-trends-of-2021-425a6a028efc,['Elena Beliaeva-Baran'],2020-12-15 08:51:38.291000+00:00,179,"AI, IoT, Industrial Revolution, Tech For Good, 2021Trends"
Digit Significance in Machine Learning,"Digit Significance in Machine Learning

An End to Chasing Statistical Ghosts

Standards for reporting performance metrics in machine learning are, alas, not often discussed. Since there doesn’t appear to be an explicit, widely shared agreement on the topic, I thought it might be interesting to offer the standard that I have been advocating, and which I attempt to follow as much as possible. It derives from this simple premise, which was drilled into me by my science teachers since middle school:

A general rule for scientific reporting is that every digit you write down ought to be ‘true,’ for whichever definition of ‘true’ is applicable.

Let’s examine what this means for statistical quantities such as test performance. When you write the following statement in a scientific publication:

The test accuracy is 52.34%.

you are stating that, to the best of your knowledge, the probability of success of your model on unseen data drawn from the test distribution lives between 0.52335 and 0.52345.

This is a very strong statement to be making.

Consider that your test set consists of N samples drawn IID from the correct test distribution. The number of successes is s. Success rate can be represented as a binomial variable, and its average probability p estimated from the sample mean: p ≅ s / N

Its standard deviation is: σ = √p(1-p)

Which is bounded above by 0.5 when p = 0.5.

Under the Normal approximation, the standard deviation of the estimator is: δ = σ/√N

This error δ on the accuracy estimate looks like this, in the worst case of ~50% accuracy:

In other words, in order to report the 52.34% accuracy of the example above with any confidence, the size of your test set should be at least on the order of 30M samples! This crude analysis readily translates to any countable quantity besides accuracy, though not to continuous figures like likelihood or perplexity.

Here are some illustrations on some common machine learning datasets:

How many digits of accuracy can one reasonably report on ImageNet? Accuracies are in the ~80%, and the test set is 150k images:

√(0.8*0.2/150000) = 0.103%

Which means you can almost report XX.X% figures, and in practice everyone does.

How about MNIST, with accuracies in the 99%:

√(0.99*0.01/10000) = 0.099%

Pffew, just ok to report XX.X% as well!

The biggest caveat, however, is that in most cases, performance figures are not reported in isolation, but are used to compare multiple methods on the same test set. In that scenario, the sampling variance between the arms of the experiment cancels out, and the difference in accuracy between them may be statistically significant even with a smaller sample size. A simple approach to estimating the variance of the figure is to perform bootstrap resampling. A more rigorous, and generally tighter test involves conducting a paired difference test or more generally analysis of variance.

It could be tempting to report digits beyond their intrinsic precision, since performance figures tend to be more significant in the context of a baseline to compare against, or when one considers the test set to be fixed as opposed to a sample drawn from the test distribution. This is a practice that leads to surprises when deploying models in production, and suddenly the fixed test set assumption vanishes, along with insignificant improvements. More generally, it is a practice that leads straight down the dark path of overfitting to the test set.

So what does it mean for a digit to be ‘true’ in our field? Well, it’s complicated. For engineers, it’s easy to argue that dimensions should not be reported past tolerances. Or for physicists, that physical quantities should not go past measurement errors. For machine learning practitioners, not only do we have to contend with the sampling uncertainty of our test set, but also to the model uncertainty under independent training runs, different initializations and shufflings of the training data.

By that standard, it can be very difficult to ascertain what digits are ‘true’ in machine learning. The antidote is of course to report confidence intervals whenever possible. Confidence intervals are a more granular way to report uncertainty, and can take into account all sources of randomness, as well as significance testing beyond simple variances. Their presence also signals to your reader that you’ve thought through the meaning of what you report beyond the number that your code spits out. A figure presented with confidence intervals can be reported beyond its nominal precision, though beware, you now have to contend with how many digits to report the uncertainty with, as this blog post explains. It’s turtles all the way down.

Fewer digits make for less clutter and better science.

Avoid reporting digits that are beyond statistical significance, unless you provide an explicit confidence interval for them. This can rightfully be considered scientific malpractice, particularly when used to argue that one number is better than another in the absence of paired significance testing. Papers are routinely rejected on this basis alone. It is a healthy habit to always be skeptical of accuracy figures that are reported with a large number of digits. Remember the 3000, 300k, and 30M rule-of-thumb limits on the number of samples required for statistical significance in the worst case as a ‘smell test.’ It will save you from chasing statistical ghosts.

(With thanks to a number of colleagues who contributed invaluable comments to earlier versions of this article.)",https://towardsdatascience.com/digit-significance-in-machine-learning-dea05dd6b85b,['Vincent Vanhoucke'],2020-09-17 20:32:28.332000+00:00,875,"machine learning, digit significance, statistical ghosts, test performance metrics, scientific reporting"
The Age of the Algorithm,"(Published initially on August 26th, 2016)

“Technology” is a funny word, extremely malleable: we tend to recognize as technology only the most recent artifacts of human progress. In a more than purely metaphorical sense, technology is what appears in the world after we graduate. We call the likes of Google, Facebook and Amazon technology-based firms because the last great economic and social revolution hinged on the internet, but almost two centuries ago trains were all the novelty, and the technology firms of these age made huge fortunes covering the rolling plains of the Far West with a network of railroads among indian fights and bandit robberies, immortalized in John Wayne’s movies. Boilers over wheels replacing horses and carriages. Startups are technological because that allows to compete with the establishment.

What technology provides to a startup is, mainly, scalability. This is relevant not only since startups routinely lack the deep pockets of a fully grown business and must achieve a lot with very limited resources, but also because frequently a startup’s winning strategy relies on offering a superior product at an inferior price, yet marginally profitable, and quickly obtain the economies of scale that effectively create a de-facto monopoly. There are many ways to compete, but really few to win. The best way I ever heard to summarize it is in the candid words of Jack Welch, CEO of General Electric and professor at MIT: “do it first, do it cheaper, do it better or go home”.

We have a classic example in Google: Sergei Brin and Larry Page created the company around PageRank, their thesis’ algorithm to automatically classify a website’s relevance and thus facilitate internet search. Infinitely more scalable that the technology used by the incumbents of the time: rooms full of clerks constantly browsing and manually classifying the information.

Interestingly in 1998 Google offered to sell their technology for 1 million dollars because the founders preferred to continue studies at Stanford, but both AltaVista and Yahoo declined the outrageous offer. Yahoo had yet another opportunity to acquire Google in 2002, but the agreement fell apart after weeks of hard negotiations because Terry Semel, then Yahoo’s CEO, faltered at the clearly exaggerated 5 billion dollar price tag.

As I write these lines at the beginning of September, 2016, Google and its parent organization Alphabet have a market capitalization in excess of 500 billion dollars. This is a humongous level of value creation. Meanwhile, in July this same year it was announced that Yahoo reached an agreement to sell its internet properties to Verizon for 4.83 billion dollars. It is worth remembering that in 2008 Microsoft bid 44.6 billion dollars for all of Yahoo’s businesses. That, dear readers, is value destruction.

An algorithm defeated an entire industry, and this fact has been often repeated since distributed computation is at the spearhead of technology. AirBnb, Uber y Taskrabbit are today’s reference companies worldwide, followed by a myriad of startups pursuing more specific market opportunities in sharing economy, crowdsourcing and crowdfunding. Through digital marketplaces that automatize the exchange of services among individuals, again technology allows reaching that scalability that makes possible the disruption of an established industry.

Then, what does future hold for us? We already know that predictions are only reliable when performed backwards, but it is likely that the next great technological revolution will come from the world of artificial intelligence (AI), the set of methodologies that strive to replicate human cognitive processes in machines.

Here we have an interesting blind spot: as we mentioned above technology is all that’s new in the world, but AI is veiled with the magical halo with which we tend to dress everything that surrounds matters of the human mind. Rodney Brooks, professor of AI at MIT, refers to it as the AI effect: “When we finally manage to understand a piece we say ‘Oh! That is just computation’ and we leave as AI only the unsolved problems”. We should be aware that elements of AI have been for years “infiltrating” our daily experience, often in rather subtle ways: the simple ability to automatically tag friends in pictures, the kind interactivity of a virtual assistant, the cunning behavior of an enemy inside of one of our favorite computer games.

Now both technical advances in fields like deep learning and the availability of massive amounts of data with varying levels of structure (what is usually known as big data) will allow us to perform automated analysis in areas of human activity that just a few years earlier required a high level of domain-specific knowledge and were thus confined to the hardly scalable niches of the expert and the consulting boutique.

An example that is close home: smart algorithms endow us at Ágora EAFI to automate the tedious task of analyzing and classifying fundamental company information not about a single firm, but thousands of them, unearthing relevant patterns without the need for hiring rooms full of technical analysts, the associated costs and headaches. Another example that I find really interesting is Descartes Labs, a startup based in USA that employs machine learning algorithms applied over pictures taken via low orbit satellites in order to more precisely predict crop yields. While the method employed by the department of agriculture (USDA), consisting of asking the farmers around the country, produces one estimate each month, Descartes Labs updates its own each two days. Imagine using that privileged information to speculate in the market with futures. Big deal.

And beyond? If the advances in biotechnology are up to our expectations, then maybe the algorithm will become flesh. But please stop me here before my philosophical knack runs wild and I bore you even more. Cheers and talk again soon!",https://medium.com/algonaut/the-age-of-the-algorithm-60f9b3125991,['Isaac De La Peña'],2019-06-24 10:02:29.776000+00:00,938,"Technology, Scalability, Google, Artificial Intelligence, Big Data"
Unity is strength — A story of model composition,"Introduction

Earlier this summer (August 2020), Criteo R&D teams proudly delivered the project codename “DRACuLa” into production, bringing a strong uplift of performance to our engine on our main business cases.

While trends are clearly leaning towards the “A.I.” (Artificial Intelligence) hype, this project demonstrates that a composition of simpler techniques can still beat the deep nets magic in specific cases.

Photo by Stephan Henning on Unsplash

Genesis — Why?

Since 2010, Machine Learning (ML) has been instrumental in Criteo business. While the type of ML model has its importance (Regression, Deep Neural Network, Decision Trees, …), two other major steps are of uttermost impact on the performance of the overall pipeline: the quality of the data collected in the first place and the so-called feature engineering. The later encompasses all data processing turning the history of interactions with the user, partner, publisher into a real-valued features vector suitable for consumption by the ML models themselves.

On the one hand, Criteo’s initial success relied on the Generalized Linear Model (a.k.a. Logistic Regression) for ML model and hashing-trick for feature encoding. While these tools enable Criteo to scale up and out reliably, their ML performance is now plateauing, and we believe Criteo has reached the limits of this architecture: finding useful features to keep adding to the model becomes increasingly difficult.

Figure 1: Number of features in model input over time

On the other hand features formed using counters of user events on various key dimensions and cross-dimensions combined with non-linear ML models are well known to yield improvements in performance. This architecture named DRACuLa (Distributed Robust Algorithm for Count Based Learning) has become an industrial state-of-the-art solution for search models and bidding models in advertising. It also relies on approximate counting with count min-sketch (CMS) to make the counting scalable and the models deployable online.

Anatomy — What?

Let us now review the characteristics difference between these two ML model families.

A- Logistic regression model

The historical model used at Criteo are the logistic regression ones: solving the Click Through Rate (CTR) problem by predicting the probability of a user to click on a displayed banner given the context. They have the nice properties to be easily computed (convex loss), to be able to operate in a high dimensional space using the hashing trick (thus handling high cardinality categorical features such as IDs — advertiser_id, publisher_id) and to be very fast at inference time (only needing simple dot product as seen on figure below). The huge number of examples available at training time (a few billion per day) is counterbalancing the high dimensionality of representation of the features to avoid overfitting at training time.

A simplistic way to think about these models is that they are very efficient at memorizing a given problem when exposed to enough data.

Figure 2: Logistic regression model (simplified)

The counterpart of this model family is the need for manual feature engineering on the data to achieve the best performance: numerical features must be bucketized to circumvent the linearity of the model, feature interactions (cross- features) must be found and specified explicitly by the data scientists.

B- DRACuLa models (decision trees + counters)

While decision trees (DT) are not a novelty (used in expert system for years), their application to Criteo’s context and available data is quite challenging: categorical features with many modalities are hard to handle (they would require very high depth of the trees).

Figure 3: Example of decision tree

The DRACuLa models are solving this issue by combining the decision trees with a specific encoding of the categorical features into numerical ones using mean encoding techniques: instead of using the specific modality of a given input dimension, we use the counts and ratio of the modality in regard of the target we want to optimize (eg.: click or non-click).

Table 1: Mean encoding of partnerId

C- Comparing the characteristics of each model type

In the table below, we summarize the main aspects of both model families:

Table 2: Model family comparison

Wrap up — How it went?

Our first hope was that the DRACuLa models would just plain outmatch the good old logistic regression ones: ability to express non-linear behavior & autodetection of cross-features were shaping a very promising challenger. First series of online A/B tests (pure DRACuLa vs. Logistic regression) turned out to be negative to neutral. Indeed, beating years of fine tuning and of feature engineering was not going to be an easy fight. Even if disappointing at first for our teams, they did not give up that easily: if the new models could not just outperform the old ones, would they be able to compensate each other’s weaknesses?

Following this idea a composition (here geometric mean) of both models was shaped in place using ensemble methods leaning on the hypothesis that using multiple learning algorithms would obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.

History proved this right, beating the production baseline with a whopping uplift on the whole scope of adaptative models!

Unity is strength — Open, Together, Impactful!

Credits to all R&D teams involved: ML Platform, Bidding, CAML, Arbitrage/PRE-Supply/Infra & EPMs",https://medium.com/criteo-labs/unity-is-strength-a-story-of-model-composition-49748b1f1347,['Loïc Le Bel'],2020-10-08 09:35:05.230000+00:00,833,".Criteo, Machine Learning, Logistic Regression, Decision Trees, Feature Engineering"
Review of First Bokeh Fundraiser,"July of this year was the first time that Bokeh has had a time-bound, focused fund drive, and we were impressed by the show of support: we exceeded our initial funding goal by 20%! While our code contributors have always been essential to maintaining and growing the scope and applications of the Bokeh project, the support of our users in other ways is also fundamental in driving Bokeh’s future success.

The Bokeh project has grown significantly in the last year, and some of that has meant incurring some operational costs. The funds raised will go directly to hosting costs for our documentation, demo sites, and the CDN for BokehJS, all platforms for helping our users research and discover ways to create beautiful data representations.

We received support from individuals, several who have shared some amazing visualizations using our libraries; check out the ongoing “30 Days of Bokeh” challenges on Twitter! The success of our fundraiser is a direct result of our community of data scientists, data engineers, and data communicators of all types showing their support. Thank you!

We also started a corporate sponsorship program, in which companies can support the ongoing development of Bokeh with tiered donation levels:

Bronze: $500 per year

Silver: $1,000 per year

Gold: $2,500 per year

Companies interested in becoming a Bokeh sponsor can email info@bokeh.org to start the conversation.

Here is our inaugural list of wonderful, much-appreciated corporate sponsors for 2019:

The mission of NumFOCUS is to promote open practices in research, data, and scientific computing by serving as a fiscal sponsor for open source projects and organizing community-driven educational programs.

With more than 15 million users, Anaconda is the world’s most popular data science platform and the foundation of modern machine learning. Anaconda Enterprise delivers data science and machine learning at speed and scale, unleashing the full potential of their customers’ data science and machine learning initiatives.

The RAPIDS suite of software libraries, built on CUDA-X AI, gives you the freedom to execute end-to-end data science and analytics pipelines entirely on GPUs. It relies on NVIDIA® CUDA® primitives for low-level compute optimization, but exposes that GPU parallelism and high-bandwidth memory speed through user-friendly Python interfaces. RAPIDS also focuses on common data preparation tasks for analytics and data science. This includes a familiar DataFrame API that integrates with a variety of machine learning algorithms for end-to-end pipeline accelerations without paying typical serialization costs. RAPIDS also includes support for multi-node, multi-GPU deployments, enabling vastly accelerated processing and training on much larger dataset sizes.

Quansight will help you get started with Artificial Intelligence, Machine Learning, Deep Learning, and Data management using Python or R as well as the toolboxes they link to including NumPy, Pandas, scikit-learn, Jupyter, Tensorflow, PyTorch, MXNet, Caffe, CNTK, Chainer, and more. Quansight extracts data from transactions, images, videos, weblogs, audio, and more. Access to their community and tools (which produced projects such as NumPy, SciPy, PyData, Numba, Conda, JupyterLab, Dask, Bokeh, XND, and many others) are now available directly to your business or project.

REX was created in 2015 to bring residential real estate into line with today’s expectations by using AI and big data to push past the outmoded practices of traditional real estate brokers to provide a superior outcome for both buyers and sellers at one-third the cost. Rex uses data modeling and machine learning to match sellers and buyers of homes as accurately and speedily as possible on Zillow, Google, Facebook and more.

Become a Contributor!

We learned a lot about how to run a fundraiser with this first event, and we look forward to future successes. If you’d like to become a sustaining supporter of the Bokeh project, the donation form is still accessible! Other ways to contribute to the project include:

Code contributions. If you know of a way in which Bokeh’s feature set could be improved, go for it!

Sharing your examples of Bokeh at work. We’re so proud of all the plots we see on social media and in our gallery!

Take part in the discussions on the Bokeh Discourse site . It’s a great format for getting support for your Bokeh questions, and helping others with your knowledge.

Thanks again for your support, Bokeh community- happy plotting!",https://medium.com/bokeh/review-of-first-bokeh-fundraiser-d589b291ff99,[],2020-08-24 20:28:13.048000+00:00,684,"bokeh, data-science, machine-learning, python, AI"
Understanding how Convolutional Neural Network (CNN) perform text classification with word embeddings,"CNN has been successful in various text classification tasks. In [1], the author showed that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks — improving upon the state of the art on 4 out of 7 tasks.

However, when learning to apply CNN on word embeddings, keeping track of the dimensions of the matrices can be confusing. The aim of this short post is to simply to keep track of these dimensions and understand how CNN works for text classification. We would use a one-layer CNN on a 7-word sentence, with word embeddings of dimension 5 — a toy example to aid the understanding of CNN. All examples are from [2].

Setup

Above figure is from [2], with #hash-tags added to aid discussion. Quoting the original caption here, to be discussed later. “Figure 1: Illustration of a CNN architecture for sentence classification. We depict three filter region sizes: 2,3,4, each of which has 2 filters. Filters perform convolutions on the sentence matrix and generate (variable-length) feature maps; 1-max pooling is performed over each map, i.e., the largest number from each feature map is recorded. Thus, a univariate feature vector is generated from all six maps, and these 6 features are concatenated to form a feature vector for the penultimate layer. The final softmax later then receives this feature vector as input and uses it to classify the sentence; here we assume binary classification and hence depict two possible output states.”

#sentence

The example is “I like this movie very much!”, there are 6 words here and the exclamation mark is treated like a word — some researchers do this differently and disregard the exclamation mark — in total there are 7 words in the sentence. The authors chose 5 to be the dimension of the word vectors. We let s denote the length of sentence and d denote the dimension of the word vector, hence we now have a sentence matrix of the shape s x d, or 7 x 5.

#filters

One of the desirable properties of CNN is that it preserves 2D spatial orientation in computer vision. Texts, like pictures, have an orientation. Instead of 2-dimensional, texts have a one-dimensional structure where words sequence matter. We also recall that all words in the example are each replaced by a 5-dimensional word vector, hence we fix one dimension of the filter to match the word vectors (5) and vary the region size, h. Region size refers to the number of rows — representing word — of the sentence matrix that would be filtered.

In the figure, #filters are the illustrations of the filters, not what has been filtered out from the sentence matrix by the filter, the next paragraph would make this distinction clearer. Here, the authors chose to use 6 filters — 2 complementary filters to consider (2,3,4) words.

#featuremaps

For this section, we step-through on how CNN perform convolutions / filtering. I have filled in some numbers in the sentence matrix and the filter matrix for clarity.

The action of the 2-word filter on the sentence matrix.

First, the two-word filter, represented by the 2 x 5 yellow matrix w, overlays across the word vectors of “I” and “like”. Next, it performs an element-wise product for all its 2 x 5 elements, and then sum them up and obtain one number (0.6 x 0.2 + 0.5 x 0.1 + … + 0.1 x 0.1 = 0.51). 0.51 is recorded as the first element of the output sequence, o, for this filter. Then, the filter moves down 1 word and overlays across the word vectors of ‘like’ and ‘this’ and perform the same operation to get 0.53. Therefore, o will have the shape of (s–h+1 x 1), in this case (7–2+1 x 1)

To obtain the feature map, c, we add a bias term (a scalar, i.e., shape 1×1) and apply an activation function (e.g. ReLU). This gives us c, with the same shape as o (s–h+1 x 1).

#1max

Notice that the dimensionality of c is dependent both s and h, in other words, it will vary across sentences of different lengths and filters of different region sizes. To tackle this problem, the authors employ the 1-max pooling function and extract the largest number from each c vector.

#concat1max

After 1-max pooling, we are certain to have a fixed-length vector of 6 elements ( = number of filters = number of filters per region size (2) x number of region size considered (3)). This fixed length vector can then be fed into a softmax (fully-connected) layer to perform the classification. The error from the classification is then back-propagated back into the following parameters as part of learning:

The w matrices that produced o

matrices that produced The bias term that is added to o to produce c

to produce Word vectors (optional, use validation performance to decide)

Conclusion

This short post clarifies the workings of the CNN on word embeddings by focussing on the dimensionality of matrices in each intermediate step.

References",https://towardsdatascience.com/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-d2ee64b9dd0b,['Joshua Kim'],2017-12-05 14:19:10.621000+00:00,820,"CNN, textclassification, wordembeddings, dimensions, matrices"
Clustering — Diving deep into K-Means Algorithm,"Clustering comes from the set of unsupervised learning algorithms. The core idea here is to group (partition) the unlabelled data points based on their features, such that data items in one group would have similar properties. Being acquainted with the faintest machine learning concepts, one would know the idea of features and labels. Unsupervised learning works on features, sometimes as a use-case while preprocessing to find labels. You may have at some point come across the computer program asking you to identify cars from buses, cats from dogs, or the likes of this. How you can do it is by your visual capabilities. In the case of the processor, a clustering algorithm is running underneath to identify groups for images based on their features.

Consider that you have a huge dataset with the features of students for the past ten years of your college. Features can be like marks of different technical and non-technical subjects, annual income, a grading of soft skills, participation in extracurricular activities, and a number of suspensions, whether the person likes tea or coffee and other details. In the data preprocessing step, irrelevant features can be omitted. After applying clustering algorithms, we can obtain a cluster as in the image below:

Now, this can work as target segmentation where we know based on the features which group of students we need to target for skilling programs, which group of students we can recommend for specific internships, etc.

This was a simple use-case. Another well-known example is that of Google News. After enough verification, extraction and preprocessing (can be TF-IDF, name entity recognition techniques) of various news articles, a cluster of related news articles appears to us whenever we search one particular news, a keyword, etc. Another use-case can be that of color palettes, which groups similar shades of colors under one group, a technique often used in image compression.

The clustering algorithm can be:

Flat clustering: Algorithms where the value for the number of clusters is decided by the scientist.

Hierarchical clustering: Algorithms with a top-down/ bottom-up approach, where the machine decides the number of clusters.

Let’s discuss one of the well-known algorithms:

K-Means clustering :

K-means clustering is a centroid based algorithm that assigns each point to a certain cluster based on its Euclidean distance. It is an iterative algorithm with two main steps — The assignment step and the update step. Let us have a look at the algorithm:

K <- Choose the value for the number of clusters.

Initialization techniques: Select K number of data points as centroids and assign nearest data points to their respective cluster (Forgy method) or assign a cluster randomly to each data point (Random Partition method).

Using Euclidean distance, recalculate the centroids for data points assigned to each cluster and change the assignment of data points accordingly.

Iterate till there is no change in the centroids and a balance of cluster is hence met.

Keep changing the values of K, and using the elbow method finds the best-suited values of K.

Elbow method for choosing the value of K:

Key concept: Total within-cluster sum of square (WCSS) or simply the total intra-cluster variation or “inertia” has to be minimized. For calculating total WCSS calculate the sum over the Euclidean distance of all the within-cluster data points from their centroids and divide by the number of data points. With an increase in K, cluster size gets smaller and the points are closer to their respective centroids. So with a K vs. Total WCSS graph will provide an indicator to an appropriate value of K, such that the trade-off of the computation is fair. The knee (or elbow point) is considered to suggest an optimal value of K.

Python code:

Again being a part of the scikit-learn module, below is the code of K-means with some of the hyperparameters explained:

class sklearn.cluster.KMeans ( n_clusters = 8, init = 'k-means++', n_init = 10, max_iter = 300, tol = 0.0001, random_state = None)

The already assigned values are the default values.

n_clusters: The number of clusters you want to generate. This means the same number of centroids will be generated by the algorithm.

init: These are the algorithms that specify the choice of centroid initially. Values can be = {‘k-means++’, ‘random’, ndarray, callable}

n__init: Number of times centroids are initialized and the most appropriate centroid set based on convergence is selected.

max__iter: For each time a centroid set is selected, K-means is iterated over max__iter times.

random__state: The number of times we want the machine to initialize with the same centroids.

tol: Controls tolerance with regards to the changes in the within-cluster sum-squared-error of two consecutive iterations to declare convergence.

Drawbacks:

The number of clusters is to be decided by the scientist.

The centroid initialization is random and the chances of getting stuck at a local minimum of the cost function are fairly good.

Different centroid selection will give different clusters.

K-means doesn’t perform well on data points of varying density and sizes.

Outliers can stretch the centroids; in fact, outliers are assigned a centroid separately in some cases.

More dimensionality is a problem with K-means as the concept of convergence of the most suitable set which was dependent upon the distances becomes less distinguishing and more confusing.

K-means assumes the spherical shape of the cluster and is not good with clusters of complicated geometric shapes.

K-means is confusing when there are overlapping data points between clusters.

To overcome these, other advanced algorithms have been designed which will be discussed in future blogs.

Stay tuned. Happy learning :)",https://medium.com/devtorq/clustering-diving-deep-into-k-means-algorithm-18305a37291,['Hafsa Farooq'],2020-08-09 08:39:43.043000+00:00,889,"Clustering, Unsupervised Learning, K-Means Clustering, Features, Labels"
Latest picks: In case you missed them:,"Sign up for The Variable

By Towards Data Science

Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.",https://towardsdatascience.com/latest-picks-deep-few-shot-anomaly-detection-284b0e8e8b75,['Tds Editors'],2020-11-10 14:27:29.506000+00:00,36,"Data Science, Towards Data Science, Sign Up, The Variable, Weekly Newsletter"
Fake News Detection Using Machine Learning and Natural Language Processing,"— by Vineet Maheshwari, Palak Tiwari, Deepankar Kansal

Fake news is misleading news stories that come from non-reputable sources which are having a significant impact on our social life. Fake news detection is an emerging research area which is gaining interest but involved some challenges due to the limited amount of resources (i.e., datasets, published literature) available.

To make sure that our machine learning models give accurate results, we need to check the dataset on which we are training our models i.e. dataset should be unbiasedly containing an equal number of fake news instances and real news instances. Today we will look into this problem and propose some machine learning algorithms, that are helpful in predicting whether a news is fake or not.

Unbiased Dataset (Good for training the models)

Biased Dataset (Not give accurate results on future data)

So we will see the results of various machine learning models, for which we first apply the natural language processing techniques on the text data of the unbiased dataset.

Text Preprocessing

For all the below functions, you can use libraries like ‘nltk, re, etc.’

Tokenization: First we create tokens of the text data. Removing Stopwords: Remove the stopwords like “a, an, the, then, etc.” Because we know that these words have no or very less impact on whether a news is fake or not. Stemming: Converting the word into its base form, for example: converting ‘worked’ to its base form which is ‘work’.

After the above preprocessing, we can apply the ‘Bag of Words’ feature extraction technique using CountVectorizer class from the sklearn library “sklearn.feature_extraction.text”. This class can also build ‘Bag of Words’ using Unigram and Bigram combinations. For training the model we are using these Unigram and Bigram combinations.

We also apply RNN’s(recurrent neural network) variant ‘Bi-LSTM’, for which we will use ‘Glove embeddings’ to convert the words into their embedding.

Models

Machine Learning Models that we used:

Logistic Regression Support Vector Machines Multinomial Naive Bayes (Naive Bayes Variant) Gradient Boosting Bi-LSTM (Bidirectional Long Short Term Memory)

Why we choose Text+Title corpus for training the models?

We try multiple variations of the corpus such as choosing the title of the news, the text of the news and both together. We train the logistic regression(LR) model using all the three variants of corpus and observed that Text+Title combination of the corpus is giving good results. We can observe the accuracies for the same from the below table:

So, for further training, we have applied Text+Title corpus on the models mentioned above and achieved encouraging accuracies mentioned in the below table and also visual from the plot:

Table for accuracies on mentioned models

Conclusion

Using the above-mentioned algorithms, i.e. Naive Bayes classifier, Support Vector Machine and Logistic Regression, Gradient Boosting and Bidirectional LSTM. The maximum accuracy obtained is using Text+Title with Gradient Boosting Classifier. LR and Gradient Boosting were giving almost similar accuracy, SVM and Bidirectional LSTM were little less, however, the accuracy obtained from Naive Bayes was relatively lesser than other models. LR and Gradient Boosting were performing better because the features that we are extracting from the corpus are well separated that both logistic regression and gradient boosting are able to capture the context.

Link to the source-code:

https://github.com/VineetMaheshwari/FakeNewsDetection

Dataset Used from Kaggle and Codalab’s competition

Blog Authors and Contributions:

Deepankar Kansal (linkedin.com/in/deepankar-kansal-255911152): Literature Survey, Unigram & Bigram combinations for Bag of words, Logistic Regression, Multinomial Naive Bayes, Report & Analysis of models.

Vineet Maheshwari (linkedin.com/in/vineet-maheshwari-9117b3180): Literature Survey, Datasets Handling, SVM, Glove+Bi-LSTM, Report & Analysis of models.

Palak Tiwari (linkedin.com/in/palak-2810-tiwari): Literature Survey, Text Preprocessing, Gradient Boosting, Data & Results Visualization, Report & Analysis of models.

Under the guidance of

Professor: linkedin.com/in/tanmoy-chakraborty-89553324 Prof. Website: faculty.iiitd.ac.in/~tanmoy/ Teaching Fellow: Ms Ishita Bajaj Teaching Assistants: Pragya Srivastava, Shiv Kumar Gehlot, Chhavi Jain, Vivek Reddy, Shikha Singh and Nirav Diwan.

References",https://medium.com/@deepankar20007/fake-news-detection-using-machine-learning-and-natural-language-processing-68e8e485f05d,[],2020-12-23 06:43:49.540000+00:00,607,"fakenews, dataset, machinelearning, textpreprocessing, bagofwords"
Learning a XOR Function with Feedforward Neural Networks,"What is the “Exlusive Or” function?

While a regular “or” has a truth table like this:

An “exclusive or” has a truth table like this:

If we translate this to a matrix we get the input:

…and the output:

Before we specify exactly which model, cost function, optimization procedure and data, let’s review some basics, nearly all deep learning algorithms consist of a few parts:

Dataset

Cost function

Optimization procedure

A model

Overview

We have the real XOR function, represented by:

Our task is to learn this function’s behavior, we pretend to not know how it works. Now let’s represent our version of the XOR function, and it looks like this:

for our function, the theta represents all the parameters that we adapt to make f as similar as possible to f*. This is our main task, in order to accomplish this task we will jump to a few mini lessons and sub-tasks. Let’s find out what the inputs or the x’s are…

Inputs

There can only be 4 inputs, the truth value of statement A & B inside one vector, in set notation it looks like:

We call our set of inputs fancy capital X. We can’t get a statistic generalization on such a small input space, all we can do is to fit the the dataset.

Cost Function

We use a MSE for the cost function because it has a simpler math. But you can use more different types of functions.

For our dataset we get:

Remember that our number of examples m = 4 and that we are trying to minimize the distance between the actual outputs and the predicted outputs. Next we choose our model, specifically what function the f will be.

Model

Let’s say we choose a linear model, again this part is interchangeable with many other functions but we start with a simple one:

… every input gets multiplied by a weight and adjusted by a bias term, this means that our parameters are:

We need to find what w is first. We will do that by using the Normal Equations. Normal equations are an analytical way to solve for the parameters. It’s the better option when you have a smaller data set to work with. Let’s see what Normal Equations are and what w and b are too.",https://medium.com/swlh/learning-a-xor-function-with-feedforward-neural-networks-74ba3841f1c0,['Jake Batsuuri'],2020-02-24 20:25:20.501000+00:00,361,"Exclusive Or, XOR, Boolean Logic, Truth Table, Cost Function"
15 Jobs That Will Be Gone in 10 Years,Text above was generated by AI © inite.io,https://medium.com/@aivanouski/15-jobs-that-will-be-gone-in-10-years-7f101e4dc514,['Andrei Ivanouski'],2020-12-19 17:31:37.267000+00:00,8,"AI, Machine Learning, Natural Language Processing, Text Generation, Inite.io"
"Hola, SEAT Data Office!","At SEAT and CUPRA, we foresee a world where our customers, partners, providers, employees, and even our society are connected to foster lifestyle, user experience, productivity, and creativity. We believe this connection can be created by picking data and converting it into useful information that can positively impact everyone's day-to-day.

We manage tons of data — trust me — more than you can imagine. It is not only because of manufacturing our great products, or our effective logistics processes or our new mobility services but also because our products equip the most advanced connectivity systems.

+12M parts per day are tracked and monitored by the Supply Chain Control Tower

By leveraging data, the SEAT Data Office contributes directly to the success of SEAT and CUPRA. However, for us, that is not enough 😉.

We can help build better infrastructures by knowing which roads are darker, which roads are bumpier, or analyzing air pollution as few examples. We can leverage our data to contribute to everyone’s safety. We can also help other companies increase their productivity by improving their logistics tracking, tuning their quality process, and predicting customer demand. The SEAT Data Office is willing to help others.

SEAT Data Office’s short-term mission is to contribute to SEAT and CUPRA’s success by optimizing processes and finding new business opportunities around Data. The Data Office’s long-term mission is to leverage all such knowledge to contribute to a better society and a more productive Spanish industrial ecosystem.

SEAT:CODE Mobility Plaftorm tracking the SEAT Mótosharing fleet in Barcelona

Considering all the great talent within SEAT IT combined with our Center of Software Development, SEAT:CODE, the success is guaranteed. If you or your company share our same vision and want to work together to find opportunities, send us an email to connecting-the-dots@code.seat.

I want to thank all the people that worked hard to make the SEAT Data Office a reality. I’ll do everything possible and impossible to lead this Data Office how it’s deserved. Let’s build a better future together.",https://medium.com/seat-code/hola-seat-data-office-3bcf43eee9be,['Carlos Buenosvinos'],2020-09-24 10:10:13.899000+00:00,327,"Data Office, SEATData Office, SEATCODE, Connecting The Dots, Mobility Platform"
[Chapter-2] Data Preparation for End-to-End Machine Learning Project — part-3🧙🏻‍♂️,"Data Cleaning 🧓

Most of Machine Learning algorithm cannot work with missing features, so let’s create a few function to take care of them. In previous blog we have seen that total_bedrooms attribute has some missing values, so let’s fix it 👨‍🏭. In order to fill these missing values we have three options:

Get rid of the missing values.

Get rid of the whole features.

Set the values to some values (zero, the mean, the median, etc.)

First things 👨‍🏫, getting rid of the missing values might lost our one of the most critical scenario, so in the worst case we should do this and also goes for the same with getting rid of the while features. We do unless and until, we do not have any options left.

If you choose 3 👩‍💼, you should compute the median value on the training set, and use it to fill the missing values in the training set, but also do not forgot to save the median somewhere 👩‍🏫, it will be handy later to replace missing values in the test set when you want to evaluate your system, and also once the system goes live (into production)👷‍♂️ to replace missing values in new data.

housing.dropna(subset=[""total_bedrooms""]) # option 1

housing.drop(""total_bedrooms"", axis=1) # option 2

median = housing[""total_bedrooms""].median() # option 3

housing[""total_bedrooms""].fillna(median, inplace=True)

Another things, in Scikit-Learn provides a handy class to take care of missing values: SimpleImputer 😻. Here it how to use it. First, you need to create a SimpleImputer instance, specifying that you want to replace each attribute’s missing values with the median of that attribute:

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy=""median"")

housing_num = housing.drop(""ocean_proximity"", axis=1)

imputer.fit(housing_num)

Since the median can only be computed on numerical attributes, we need to create a copy of the data. And then use fit() method on dataset 🤗.

The imputer has simply computed the median of each attribute and stored the result in its statistics_instance variable. Although we know only total_bedrooms attribute had missing values, still it would be safer to implement on all numerical attribute before system goes live to 😎production.

>>> imputer.statistics_

array([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409]) >>> housing_num.median().values

array([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409]) X = imputer.transform(housing_num) housing_tr = pd.DataFrame(X, columns=housing_num.columns)

By using transform() method we implement transformation on dataset using imputer object 🙌. The result is a plain NumPy array containing the transformed features.",https://medium.com/analytics-vidhya/chapter-2-data-preparation-for-end-to-end-machine-learning-project-part-3-%EF%B8%8F-2a5c00d7b520,['Vishvdeep Dasadiya'],2020-12-26 15:59:32.789000+00:00,395,"data-cleaning, machine-learning, missing-values, data-transformation, Simple Imputer"
How to Ace the Most Underrated Certification for Data Scientists,"Everything I recommend doing to study for the CDMP

📘 Buy the DMBOK 2nd ed. right now. The exam is open book and the DMBOK is legitimately super useful as a reference for Data Management work. The DMBOK occupies a prime position on my desk — I frequently find myself referencing my highlights and sticky notes to address clients’ questions.

❓ Buy the CDMP exam right now. You have an unlimited amount of time to schedule the test date, and paying for the exam gives you access to a test bank of 200 questions that simulate the real exam.

🔖 Highlight and sticky note the DMBOK. Okay, this one’s obvious for an open book test. You’ll definitely want to use a highlighter and sticky notes to indicate key concepts. I put all sticky notes on one side of the book for easy referencing. I also recommend orienting your writing sideways on the tabs so they don’t stick out of the book as far.

❗️Indicate the start of each chapter. Use wide sticky notes or notes of a specific color to mark the start of each chapter. This was invaluable during the test to quickly find content associated with a specific topic.

🔨 Work with the chapter framework. All the chapters of the DMBOK follow the structure Introduction, Activities, Tools, Techniques, Implementation Guidelines, Governance, Works Cited / Recommended. Focus your studying on the overview and technical sections that get more play on the exam relative to implementation and organizational sections.

💙 Read in order then review by priority. The topics of the DMBOK are arranged in an intuitive order — the sequence you might assess these topics during a Data Management engagement. However, the proportion of each of the 14 topics tested on the exam ranges from 11% for foundational areas such as Data Governance to 2% for advanced activities such as Big Data. On your second reading of the DMBOK, I recommend studying in the priority order covered in this post.

⭐️ Join the CDMP Study Group. Use this community to compare notes, ask questions, and find study partners.",https://towardsdatascience.com/cdmp-exam-d65e1255016b,['Nicole Janeway Bills'],2020-11-19 16:44:38.212000+00:00,342,"CDMPExam, DMBOK2nd Ed., Highlight Sticky Notes, Chapter Framework, Read Review Priority"
Linking Trade Data to Company Information,"Making smart business decisions is dependent on a company’s ability to use its data effectively.

While internal company data has a lot to offer, in a world of complex connections with a fast-growing ocean of accessible data, it is naïve to think that useful and competitive information can be harvested without benefiting from external data sources. As more and more data becomes available, businesses are developing a new way of thinking:

Every piece of the puzzle is needed in order to see the big picture.

Matching these external sources to internal data becomes an entity resolution and data linkage question, and it’s one that businesses in every industry are struggling to answer. At ThinkData, we help businesses find and leverage new sources of data and match them to their own.

For example, one of our banking clients is interested in ingesting new streams of import/export data that we provide on Namara. By linking this external import/export data to their internal client data, the bank will be able to uncover new client trade activities that were otherwise unknown, identify true wallet size, investigate trade trends, monitor growth, map clients, and ultimately gain insight into a previously opaque part of their business.

Every one of these advantages is dependent on linking the bank’s internal client data to the external data feed provided by ThinkData.

UK Companies Data on Namara

To link this valuable export/import data feed to the bank’s internal client data, a third external data source is needed. This third data feed is an extremely useful public repository of all businesses in UK which can be linked to external trade data feeds. This connection provides us with the key we need to connect to the bank’s internal data. This data set was harvested and organized by ThinkData and can be accessed on Namara.

The UK companies data set has about 4 million rows of company registration information in 55 columns including basic information such as company names and addresses. Since some of the companies appear in more than one row with similar information, the data set needs to be deduplicated. After omitting the repetitive information, we can then assign a unique ID to each company in this data set. This unique ID can be used later in connecting to the import/export data.

Global Exports Data Feed

Let’s consider a scenario in which we want to use the export data feed, filtered on UK based companies, and connect this data to the UK companies information on Namara. First, let’s take a look at this data set. A sample of this data feed contains various trade information in 20 columns, such as:

Date Exporter information (name, ID, address) Consignee information (name, ID, address) Information on exported goods (weight, quantity, value, etc.) etc.

Note that not all of the columns are populated for every record. This plays an important role in choosing the columns we want to use for the matching exercise.

Results: Connecting Global Export Data to UK Companies Data

Since both the global export data and UK companies data are missing information in some columns, we decided to use the company name field in the UK companies data and the consignee name (representing UK companies involved in the deal) field in the export data feed to make the connection.

Since these two columns are always populated in both data sets, the first attempt in connecting these data sets was based on matching these unstructured text fields. An unstructured text field could contain anything and is not necessarily cleanly recorded company names.

The first step is performing the required preprocessing steps to make these data sets ready for the matching pipeline. We designed and performed a fuzzy matching algorithm which includes the following steps:

An indexing algorithm is used to extract a good subset of similar fields. This helps with the performance of the algorithm and saves time by choosing pairs from two data sets that are more likely to be a match. The candidate pairs are then processed by the text-matching algorithm. Our approach breaks down the names and takes into account many different scenarios in which a specific name can appear. This algorithm computes a score of match for each record in the global export data set to its closest match in the UK companies data. The generated scores reflect the probability of the match which then is used to extract the most likely matches. The final result would be the global export data set with an extra column which has the unique ID of the corresponding company in the UK companies data set.

The following image shows 20 randomly selected records with the corresponding match score. We can see that the score appropriately reflects the level of confidence in match and the matching algorithm has a good tolerance for small typos and variations.

Based on this ID, the two data sets are then linked and ingested to Namara. The resulting data set can then be used by the banking client to merge this information with their own internal client data.

This process adds new transaction level data to the bank’s internal data, providing them with competitive and valuable information on their current and prospective clients — information that was not available before linking to external trade and company data sets on Namara. It helps them understand not only who’s linked to trade and where they are, but what they’re importing/exporting, and how much.

Data-Driven Value

For modern companies, data is the key to unlocking major opportunities, better understanding your market, and developing advanced analytics. As we move toward AI and ML-driven tech, the quality of insight will be reflected in the quality of the data that is being used. By enriching internal company data with external feeds of data, fast-moving businesses are establishing a pipeline that can deliver them actionable information on demand, giving them the time and resources needed to grow their business.",https://medium.com/thinkdata/linking-trade-data-to-company-information-e9520f37993c,['Thinkdata Works'],2019-08-08 15:17:18.382000+00:00,962,"data-driven value, external data sources, internal company data, entity resolution, data linkage"
Arguing with Edward Snowden,"Arguing with Edward Snowden

A Data Scientist’s take on defending Machine Learning models

Introduction

I’ve recently read Edward Snowden’s Permanent Record during my holiday. I think it is a great book that I highly recommend for basically anyone, however it is particularly interesting for IT-folks for the obvious reasons. It is a great story about a guy growing up together with the internet, starts to serve his country in a patriotic fervour after 9/11, and becomes a whistleblower when he notices the US has gone too far violating privacy in the name of security. Moreover, a paradox I found most interesting is something a Data Scientist can easily relate to.

The systems that collect data about one’s browsing on the internet (basically anything you do on the internet) was an engineering masterpiece. It surely did something the NSA has no mandate for, but when building something brilliant, it can be easy to miss the big picture, and help malignant actors by handing over great tools for them.

Think about this in terms of machine learning! I am quite sure — although I cannot know — that the Chinese Social Credit System’s mass surveillance network makes use of some state-of-the-art Deep Learning concepts. Or they even do some things more brilliantly, than publicly available research can suggest. But the system it is used for at best raises some super serious questions about individual rights. Being IT professionals, we cannot miss the big picture and have to be mindful of what is the consequence of our work!

When discussing the threats of massive private data collection done by governments, Mr Snowden makes some controversial statements about data science as well. Firstly, he incorrectly suggests that machine learning models in general are total black boxes, and their decisions cannot be explained afterwards — thereby making a point that algorithms make obscure decisions that people should make in a transparent way. Secondly, he states that recommendations are just ways to put pressure on an individual to buy popular products. I aim to argue against both of these statements.

Model explainability

There is an example in the book about COMPAS, a widely used risk-assessment algorithm in the judicial system of the USA. In this case, the point is that an algorithm made a decision having a substantial effect on someone’s life — and neither we nor the algorithm can even explain why so. I think this is an inherently wrong and ill-disposed point of view.

There are models which are explainable by nature which is one of the main reasons practitioners use them. If you think about linear regression for a regression problem: the product of the feature value and the corresponding beta gives you the amount this feature contributed to the prediction of the target value. In a classification problem, widely used logistic regressions behave almost the same way, as they are a special linear regression.

A decision tree produces the exact series of decisions the algorithm learned to be useful determining the target value. However, bagging and boosting algorithms make use of numerous trees built simultaneously or sequentially (Random Forest, Xgboost, Extremely Randomized Trees, LightGBM, etc). These voting trees, or high-dimensional data in case of Support Vector Machines are harder to interpret or visualize concisely. Moreover, in a deep Neural Network millions of matrix multiplication take place — keeping track of it sounds intimidating of course.

However, there are several methods that make these algorithms more transparent. This article shows how deep neural networks can be more interpretable in breast cancer research. Another great article elaborates on different model explainability techniques: permutation importance, partial dependence plots and SHAP-values. There is room for improvement in non-technical human readable interpretations of complex machine learning models, but there are techniques to explain why a model predicted such an output.

Consequently, if a model is not explained well, it is almost certainly arising from an omission or failure of a human actor. On top of that, algorithms being biased in terms of socio-economic factors is an accusation appearing increasingly often. It is important to note again, that this a failure of the modeler not the model. The data these models are trained on are reported to contain bias — accounting for that is a challenging task we as modellers surely have to overcome. Luckily, the theoretical foundations and tools are there to assess the “fairness” of algorithms for example, between two racial groups.

Advertisements and recommendations

A second argument I did not like in the book was about recommendations in general. The author states that recommendations are just about softly pressuring the customer to buy what others did buy. I think this argument misses the real point here.

There are advertisements everywhere. I would certainly agree that advertisements — apart from conveying information about a product — are means of putting some pressure on the subjects to buy a product. Nonetheless advertisements are natural and necessary in a market-driven economy, and in a world packed with so many products and services.

But if we accept the premise, that some sort of advertising is going to exist, which one would you prefer? The one with no personalization whatsoever, or one where the advertisers’ goal to make you buy happens to cause that you are getting more relevant advertisement about products that you may actually need? I’d prefer the latter. A sophisticated recommender system takes your history into account, along with other people’s history that have a similar record to you. If done right, they are much more than just popular product recommendations.

Conclusions

In general, I really liked the book. I also admire the bravery of Mr. Snowden that started a discussion about privacy, and the trade-off between privacy invasion and crime prevention. But I also think that the book expresses a negative attitude towards everything in connection with using large amounts of data. Opposing this, I believe that statistical models built on top of massive datasets can greatly benefit humanity — if used for the right purposes, transparently and responsibly.",https://medium.com/starschema-blog/arguing-with-edward-snowden-2e21d553c056,['Mor Kapronczay'],2020-03-12 14:42:16.869000+00:00,985,"Edward Snowden, Machine Learning, Privacy Invasion, Data Science, Model Explainability"
Data Visualization using Matplotlib & Seaborn,"# Setting the data

years = [2013,2014,2015,2016,2017,2018,2019,2020,2021]

Sho = [0,0,0,0,0,0,1,1,1]

Chu_M = [0,0,0,0,1,2,2,2,2]

Chu_S = [1,2,4,7,8,18,18,20,22]

Oku_Nosyu = [0,0,0,1,1,3,4,4,4]

Oku_Shin = [0,0,0,0,3,4,8,10,10] # set bar width

width=0.15 # axis index

years_index = np.arange(len(years)) # clear reset plt style

plt.style.use('default')

plt.style.use(['ggplot'])

plt.figure(figsize=(8,5)) # plt interface approach for bar charts

plt.bar(years_index-2*width, Sho, color='#c6ddf1', label='Sho', width=width, linewidth=0.4,edgecolor='darkgrey') plt.bar(years_index-width, Chu_M, color='#99b6ce', label='Chu_M', width=width) plt.bar(years_index, Chu_S, color='#6d91ad', label='Chu_S', width=width) plt.bar(years_index+width, Oku_Nosyu, color='#416e8c', label='Oku_Nosyu', width=width) plt.bar(years_index+2*width, Oku_Shin, color='#004c6d', label='Oku_Shin', width=width) plt.legend() plt.title('Iaito model ownership',fontsize=ts)

plt.xlabel('Year',fontsize=xls)

plt.xticks(ticks=years_index, labels=years)

plt.ylabel('Count',fontsize=yls)

plt.yticks(list(np.arange(df_cumsum['count'].max()+1))) plt.show()

Image by Author

Depending on the message to convey from the data, the characteristics of the data, and the target audience, certain plots might be more suited. For example, the following catplot might convey the number of saya designs across the five models and their additional costs. But as the number of data points increases, that may not be scalable. It would be better to split the message, for example, the popularity of saya designs by quantity and distribution of saya design prices. This was what I did with a radial plot for the latter direction. The code snippets are after the catplot.

ax = sns.catplot(x=""model"", y=""saya(cost)"", data=df_saya1,

hue=""saya"", palette= 'tab20b', kind=""swarm"", s=10,

height=4.5, aspect=2.5)

ax.fig.suptitle('Saya across models',fontsize=ts)

ax.fig.subplots_adjust(left=0.1, top=0.9)

ax.set_xlabels('')

ax.set_ylabels('price (JPY)', fontsize=yls)

ax.set_xticklabels(fontsize=xts)

ax.set_yticklabels(fontsize=yts)

ax._legend.set_title('Saya') plt.show()

Image by Author | Scalability is an issue as data points increases

# replicating saya color is impractical from experience. Use a sequential color scheme

palette_saya = ['#7eedff','#6dd7ed','#5dc2dc','#4dadc9','#3e99b7','#3085a5','#217192','#125e7f','#004c6d',

'slateblue','rebeccapurple','purple','indigo'] # initialize the figure

plt.figure(figsize=(10,10))

ax = plt.subplot(111, polar=True);

plt.axis('off')

plt.title('Saya ranked by price (JPY)',y=.9,fontsize=ts) # set coordinate limits

upperlimit = 100

lowerlimit = 30 # compute max and min of dataset

max_ = df_saya2['saya(cost)'].max()

min_ = df_saya2['saya(cost)'].min() # compute heights (conversion of saya_charge into new coordinates)

# 0 will be converted to lower limit (30)

# max_ converted to upper limit (100)

slope = (max_ - lowerlimit)/max_

heights = slope * df_saya2['saya(cost)'] + lowerlimit # width of each bar

width = 2*np.pi / len(df_saya2.index) # compute angle each bar is centered on

indexes = list(range(1, len(df_saya2.index)+1))

angles = [element * width for element in indexes] # draw

bars = ax.bar(x=angles, height=heights, width=width, bottom=lowerlimit,

linewidth=1,edgecolor=""white"",color=palette_saya) # padding between bar and label

labelPadding = 1000 # label

for bar, angle, height, label in zip(bars,angles,heights,df_saya2['saya_']):

# specify rotation in degrees

rotation = np.rad2deg(angle)



#flip some labels upside down for readability

alignment = """"

if angle >= np.pi/2 and angle < 3*np.pi/2:

alignment = ""right""

rotation += 180

else:

alignment = ""left""



# add label

ax.text(x=angle, y=lowerlimit + bar.get_height() + labelPadding,

s=label, ha=alignment, va='center', rotation=rotation, rotation_mode=""anchor"",size=12) plt.show()

Image by Author | Unconventional but works in this case

Last but not least, xkcd styles. xkdc plots with their balance of humor, scientific jargon, and perspectives hold a special place in the minds of many. This plotting style can be called out from matplotlib via plt.xkcd() like so. The output is the first image for this article. It comes as a surprise that a simple-looking plot in xkcd’s style requires a fair amount of planning and thought, particularly annotation locations and arrow positioning.",https://towardsdatascience.com/data-visualization-using-matplotlib-seaborn-97f788f18084,[],2021-05-22 05:29:01.443000+00:00,444,"data visualization, data analysis, plotting, bar graphs, catplot"
Dressing Up with Data Science (Part 1),"Picture from: Medium

Stage 1: Designing

“So, who is the new designer? AI or Human?”

To begin with, the designing stage is the root of all fashion trends. It is where designers scratch their heads just to produce the one masterpiece after hundreds, or even thousands of editings. Designing is more than just sketching garments. More accurately, it encapsulates many components of the design process like researching, drawing, coloring, conceptualizing, pattern making, etc.

In recent years, famous designers and top fashion companies have started utilizing big data in creating product designs that align with the consumers’ preferences in terms of styles, patterns, and colors. With that, fashion companies are able to strategize their design planning to take place either ahead of the fashion trend or during the season.

[Ahead of the trend]

In this first case scenario, where we are predicting future fashion trends (“trend forecasting”), we require mass data collection and analysis on the past sales, as well as comparison of trends up to one to two years in advance. While most fashion retail companies have the financial capability and capacity to develop models to extract insights from their customer base, some of them also leverage external platforms to gauge market movement to quickly respond to the fluctuating fashion trends.

There are online big data tools and trend forecasting platforms that provide data insights ranging from the fashion industry at-large (such as fashion trends), competitive analysis and price fluctuations, to the nitty-gritty details such as color codes of a particular trend with the extensive use of big data analytics and machine learning. For example, EDITED is a data analytics tool that uses visual pattern recognition, optical character recognition, and text crawling to collect fashion product data and provide insights such as the product activity of certain design style and distribution of patterns. These platforms help businesses identify and develop strategies in terms of product designs and sales based on the key success factors of other companies, instead of relying on guesswork.

Picture from: EDITED

Apart from analyzing from a business perspective, trend forecasters typically identify trends from Fashion Weeks and social media using image recognition and hashtag detection. Images of fashion wear from runways and social media platforms are categorized and analyzed to project the popularity, as well as the spread of trends given a particular region at a fixed period of time. However, one shortcoming of image recognition technology is that it does not provide information about customers acceptance and perception towards a certain trend. It turns out that hashtags on social media platforms are able to make up for this as it can provide information about consumers sentiment, as well as the engagement between a fashion brand and its customers. As the forecasters closely monitor such data online, they are able to predict trends in the near future be it a comeback or new fashion, allowing fashion retailers to plan their design process ahead of time.

Picture from: intelistyle

[During the season]

In the second case scenario, where garments are designed and produced during the season, fashion retailers usually conduct real-time analysis on product sales and consumer sentiment such that they are able to identify changes in consumer preference and purchasing behavior to refine product designs and alter production. Companies adopting such strategy usually produce relatively smaller batches of product for each given design with flexible and easily accessible distribution channels.

For example, Zara has created a central management system that constantly analyzes and evaluates their customers’ fashion sensibility by consolidating all types of engagement with their customers. At offline stores, their personnel actively collect feedback from customers, ranging from the overall shopping satisfaction to the tiniest detail like color, zips, pattern and sizes of apparel. The feedback would then be reported to regional managers through the system on a daily basis, thus allowing the managers to further analyze customer preferences across the region. Zara operates in a way that they only send a small number of new products to the local stores bi-weekly with more than 350 in-house designers authorized to make product decisions. This allows Zara to continuously improve their product design, quickly adapt to changes and even predict their customers’ preferences in the near future.",https://medium.com/ds3ucsd/dressing-up-with-data-science-part-1-1e0d829aeeae,['Winfrey Xian Ying Kong'],2020-12-08 22:20:44.308000+00:00,688,"Designing, Big Data, Trend Forecasting, Image Recognition, Hashtag Detection"
MerzFiles #04: I am an AI-driven troll (probably).,"MerzFiles #04: I am an AI-driven troll (probably).

I greet you, my dear 45!

So you are probably aware of my biggest sin — I begin series. And then I begin a new one, and so on. Sequential thinking? Anyway, I continue my series sometimes, oh yes I do.

So here’s a deal: when we will be 100 subscribers, I’ll tell you more about my series.

Today I just tell about “Our Research Facility”.

I started this series on Instagram (are we following us?) years ago. It was a small portal into another dimension, full of mad scientists, mysteries, and other weird stuff.

And now I will invade Medium with this stuff from an alternate reality. For example this:

Meaning of Life

Reports from our Research Facility

No more, no less.

Apropos, meaning of life. Here is my favorite, written by Artificial Intelligence (still GPT-2, back to 2019):",https://medium.com/merzazine/merzfiles-04-i-am-an-ai-driven-troll-probably-77cfc25b94a8,"['Vlad Alex', 'Merzmensch']",2020-10-29 15:29:42.240000+00:00,138,"AI-driven troll, Artificial Intelligence, GPT-2, Meaning of Life, Reports from our Research Facility."
Notes on Deep Learning — Data Loader,"Deep learning is a success because of big data.

When it comes to machine learning or deep learning 80% or more is spent wrangling with data.

The effort spent by a machine learning engineer or a data scientist to just get the data into format prepare and clean for abnormality is immense. The part of wrangling isn’t even trivial and most important part, so it can’t even be skipped. The wrangling, in fact, is given special attention as it affects the performance the most. This chapter is often revisited a couple of times to be revised.

The deep learning loves noise and learns better with noise but it cannot be yet denied that one has to prepare the data. The loading of data is one of the inevitable tasks. PyTorch helps us with load, preprocess our non-trivial datasets.

It also makes code looks nice^^ so why not do it better :)

Until this point in series, we skillfully iterated over our nice hand-written while loop. This was simple efficient but we could do a lot more than a simple iteration over our data. In particular, we could

Create batches

Shuffle the data

Load data in parallel enhancing multiprocessing

Access easy to use functions to iterate so we don’t have to worry

Use state-of-art standards and common standards across all the programs we develop

These all feature are accessible at ease if we use torch.utils.data.DataLoader

A dataset

A dataset is your data. In Pytorch it is an abstract representation of a class with functions:

len so that len(dataset) returns the size of the dataset.

so that len(dataset) returns the size of the dataset. getitem to support the indexing such that dataset[i] can be used to get ith sample

If we need to write our custom data loader we need to override the above methods",https://medium.com/datadriveninvestor/notes-on-deep-learning-data-loader-d93ab79a631a,['Venali Sonone'],2019-07-30 03:05:55.240000+00:00,287,".Big Data, Deep Learning, Machine Learning, Data Wrangling, Py Torch"
Folium: Create Interactive Leaflet Maps,"Folium: Create Interactive Leaflet Maps

In this article, we will explore Folium, a Python library which makes it easy to visualize data that’s been manipulated in Python on an interactive leaflet map.

The maps created using folium are highly interactive which makes it even more useful for dashboard building. So let’s start exploring Folium and learn on our way of exploring it.

Installing Folium

We can install folium by running the following given below command in our command prompt.

pip install folium

Creating a world map using folium

After installing folium we need to import it in our Jupiter notebook to start working. Folium is an easy to understand library by which you can create highly interactive and visually appealing maps in just a few lines of code.

Here we will learn how we can use it to create a world map. For this, we just need to call the Map() function. The code given below will help you understand this better.

#importing folium library

import folium # calling Map() function

world_map = folium.Map() #displaying world map

world_map

Map using Folium with just one line of code.

The map created above is interactive i.e. you can actually zoom-in and zoom-out of the map using the ‘+’ and ‘-’ or by just using the cursor of your mouse.

We can pass the ‘location’ argument which contains the longitude and latitude of a particular location to Map() function to display the map of the desired location. For example, you can see the code below I used to create the map of Russia using its coordinates. Here I have also passed an attribute named ‘zoom_start’ which already zoom-in accordingly.

mah_map = folium.Map(location=[61.5240, 105.3188], zoom_start=3)

mah_map

Russia’s Map created using Folium

Further, you can use folium for creating different types of maps. Some of them I have explained below.

Stamen Toner Maps

These are the high contrast Black & White maps. They are perfect for data mashups and exploring river meanders and coastal zones.

For creating this we just need to add an attribute to Map() function named ‘tiles’ and set it to Stamen Toner. The code below is used to create a Stamen Toner map of India.

india_map = folium.Map(location=[20.5937, 78.9629 ], zoom_start=4, tiles='Stamen Toner') india_map

Stamen Toner Map of India.

Stamen Terrain Maps

These maps show hill shading and natural vegetation colors. They also showcase the advanced labeling and linework generalization of dual-carriageway roads.

For creating this we need to set the value of attribute tiles as Stamen Terrain. The below code will produce the Stamen Terrain map of India.

india_map = folium.Map(location=[20.5937, 78.9629 ], zoom_start=4, tiles='Stamen Terrain') india_map

Stamen Terrain Map of India.

Other than this there are many more tile options like ‘Mapbox Bright’ etc. that you can explore and learn.

Folium also has a marker function that marks the desired location of your choice of given coordinates. We can also select the icon for the marker.

Below Given code is used to create a map and displaying the marker on the desired location.

my_map = folium.Map(location=[20.5937, 78.9629], zoom_start=4, tiles='Stamen Terrain') folium.Marker([28.73158, 77.13267], popup='MyLocation',

marker_icon='cloud').add_to(my_map)

my_map

Marker showing pop-up.

Conclusion

In this article, we explored a beautiful library Folium use to create leaflet maps that are interactive and visually appealing. We can use these maps in our dashboards or as needed. Folium is easy to understand the library and creates a map in just one line of code. This article is just a basic understanding of folium, go ahead, and explore folium for many more features.

Before You Go

Thanks for reading! If you want to get in touch with me, feel free to reach me on hmix13@gmail.com or my LinkedIn Profile. You can also view the code and data I have used here in my Github.",https://towardsdatascience.com/folium-create-interactive-leaflet-maps-919def6d6c63,['Himanshu Sharma'],2020-07-04 14:33:51.903000+00:00,587,"Folium, Python, Leaflet Maps, Visualization, Interactive Maps"
GPU-accelerated Docker containers in Windows using WSL2,"What is WSL?

The Windows Subsystem for Linux, or WSL for short, lets developers run a GNU/Linux environment — including most command-line tools, utilities, and applications — directly on Windows, unmodified, without the overhead of a traditional virtual machine or dual boot setup.

You can:

Choose your favourite GNU/Linux distributions from the Microsoft Store.

Run Bash shell scripts and GNU/Linux command-line applications including:

Tools: vim, emacs, tmux

Languages: NodeJS, JavaScript, Python, Ruby, C/C++, C# & F#, Rust, Go, etc.

Services: SSHD, MySQL, Apache, MongoDB, PostgreSQL.

Install additional software using your own GNU/Linux distribution package manager.

Invoke Windows applications using a Unix-like command-line shell.

Invoke GNU/Linux applications on Windows.

What is different in WSL 2?

WSL 2 is a new version of the Windows Subsystem for Linux architecture that powers the Windows Subsystem for Linux to run ELF64 Linux binaries on Windows. Its primary goals are to increase file system performance, as well as adding full system call compatibility.

This new architecture changes how these Linux binaries interact with Windows and your computer’s hardware, but still provides the same user experience as in WSL 1 (the current widely available version).

Individual Linux distributions can be run with either the WSL 1 or WSL 2 architecture. Each distribution can be upgraded or downgraded at any time and you can run WSL 1 and WSL 2 distributions side by side. WSL 2 uses an entirely new architecture that benefits from running a real Linux kernel.

How to install WSL and configure WSL2

* To install WSL, you need to be on Windows 10/11 version 2004 or newer (Build 19041 or newer). Check which version you have with WIN+R and starting winver. You should see something like this:

Open a PowerShell prompt as administrator and enter the following commands:

dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart

dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart

Restart your system to enable WSL and download the WSL kernel update. Run the updater as administrator and just click next until the installer in finished.

If there is an error telling you that WSL needs to be enabled first, you probably skipped the part where you rebooted your computer. If that doesn’t help, look for “Windows Features” in your Windows settings and manually check the “Windows Subsystem for Linux” checkbox.

Reboot again if you had to do these steps and run the kernel updater after login.

You should see this if the installation was successful:

You just enabled WSL!

Changing WSL version to 2

The default installation is still for the original WSL, but we want WSL2 because it provides far better performance. This article shows the performance difference between WSL, WSL2 and Ubuntu 20.04 LTS. To set the WSL version to 2, open PowerShell as administrator again and paste this command:

wsl --set-default-version 2

Configuring WSL2 system resources

By default WSL has access to all available cores and threads in your system as well as half of the available amount of system memory. For example a 4 core / 8 thread laptop with 16 GB of memory will provide WSL with 8 threads and 8 GB of memory. You can change these values to whatever values you prefer by creating a “.wslconfig” file. This file needs to be placed in the root directory of your Windows user like this: C:\Users\<yourUserName>\.wslconfig . Here's what a possible .wslconfig file would look like:

[wsl2]

memory=4GB # Limits memory in WSL2 to 4 GB

processors=3 # Makes WSL2 use up to 3 threads

The full list of possible configurations can be found on https://docs.microsoft.com/en-us/windows/wsl/wsl-config.

Install Ubuntu from the Microsoft Store

Open the Microsoft Store and look for Ubuntu. You should pick the latest LTS release to install.

Other versions are supported by Docker as well but if you want to enable GPU acceleration, pick Ubuntu 20.04 LTS. Ubuntu 18.04 LTS and Debian have been tested but failed to use the GPU to render applications and instead used software rendering.

After installation is complete, you should be able to launch the application which will ask you for a UNIX username and password. You can pick these however you like, they don’t affect your Windows login. Just be sure to remember your password.

If you can launch Ubuntu and get this screen, you can proceed to installing Docker.

Docker Desktop configuration

Download and install Docker Desktop. After installation reboot your computer and start Docker. Configure your settings to match the following: (You can choose to start and open on login if you want to, doing so will consume system resources even when not using Docker or WSL actively):",https://medium.com/@jimvekemans/gpu-accelerated-docker-containers-in-windows-using-wsl2-ce9a5d9a0041,['Jim Vekemans'],2021-09-05 06:49:04.001000+00:00,721,"wsl, windows subsystem for linux, linux distributions, bash shell scripts, GNU/Linux command-line applications"
The Ultimate NumPy Guide for Beginners,"Getting Started

It would help if you had Python installed and PIP. If you have already done this, you can start working with NumPy. Before you can use NumPy, we have to install it with the following command:

C:\Users\Your Name>pip install numpy

Then open up a Python script or file and import the library:

import numpy

Now, NumPy is imported and ready for you to use. NumPy usually is imported as np instead of NumPy:

import numpy as np

We can check if NumPy works by adding two simple lines of code:",https://medium.com/better-programming/the-ultimate-numpy-guide-for-beginners-93a0ef6d6c89,['Bryan Dijkhuizen'],2020-11-25 15:43:07.687000+00:00,85,"python, pip, numpy, installation, getting started"
Know Yourself More from Your WhatsApp Chat History,"If we see carefully, in terms of the number of lines, party B replies with more line that party A. This might lead to a conclusion that Party A tends to sum up the reply in a whole line, while party B tends to send it in different lines.

The number of words and characters that party B sends on each reply is relatively lower than party A, meaning party A writes longer than party B. This is also supported by the duration variable, where party A has more duration in writing a reply than party B. So, party A writes longer than party B.

In the previous question, we know that party B takes a longer time to respond. I assume that’s because party B writes longer than A. In fact, A writes longer. And B even writes faster than A. So there’s no excuse for B haha.

5. Word Frequencies

One of the most popular ways to inspect word data is by visualizing its word frequency using a word cloud.

I remove several stopwords to remove the meaningless frequent word. Since I use Bahasa Indonesia on my chat, I use Elang to remove Indonesian stopwords. Now, using word cloud, take a look at these words

That’s a lot of “haha” and “wkwk”, Indonesian-considered laughing word. Using regex, I remove the laughing words and here’s the final word cloud.

I’m sorry if you don’t understand the words, It’s Indonesian, and it’s mostly non-formal words (who use formal words in chat anyway).

6. How often do we laugh

Previously I discovered that the laughing words take up a lot of my word cloud space. Are we that frequent in laughing (via text)? To find out, I extract all laughing words and compare its number with the number of lines to get the probability.",https://towardsdatascience.com/know-yourself-more-from-your-whatsapp-chat-history-8ec4b69c12c5,['Iqbal Basyar'],2020-03-27 15:18:43.536000+00:00,294,"wordfrequency, wordcloud, stopwords, regex, bahasaindonesia"
Neuroscience to AI: Transitioning Laterally,"In my junior and senior year of undergrad, I came to my advisors to gauge their opinion about continuing school to get a masters or bachelors in software engineering. To my luck, they encouraged me not to go to a masters/bachelors program (for software engineering), but to go to a coding bootcamp instead. To get into a career in AI engineering, it could take years from a traditional schooling standpoint. Whereas attending coding bootcamp, it could be done in less than a year.

I chose Flatiron School because of their unique immersive education system and undeniable belief in their students; through the coding prep-work, I never felt isolated, my questions were always positively accepted and well-answered by patient coaches, and, with hard work, I was able to retain the material being taught. If you are a current student in a biologically related field or are interested in immersing yourself into the coding world, I encourage you to first research and weigh your options, but at least check out Flatiron Coding Bootcamp. While the pre-med track is competitive and can be difficult to navigate and network, I’ve never experienced such thorough encouragement and support from my peers and teachers like I have Flatiron.

As a Flatiron student, we have algorithm exercises for each student to practice for coding interviews, algorithm club, lecturers, special guest speakers, AI tech interview events and prep.

While Flatiron prepares you for real world interactions and expectations of software engineers, Flatirons’ gauge of empathy breakdown and compassion towards their students is incomparable: everyday at 5:30 the cohort exercises ‘Stand down’. ‘Stand down’ allows students to talk about their struggles of the day and to share with their classmates their feelings that resonate throughout the group. The purpose of this exercise is to not only understand that imposter syndrome is real, but to understand, through each other, that imposter syndrome is not a solo journey through the construction of a community.",https://medium.com/@crystal-villanueva/neuroscience-to-ai-transitioning-laterally-18cb88f24c92,['Noelani Villanueva'],2020-12-24 07:41:57.669000+00:00,320,"Software Engineering, Coding Bootcamp, AIEngineering, Flatiron School, Algorithm Exercises"
Storage + Commercial Buildings: Load Forecasting,"The energy storage market has yet to explode, with technical & cost challenges yet barring widespread deployment. However, we’re very close to the inflection point, which will open the doors to a whole new technology category that will forever change all facets of the broader energy sector. Starting with this project, we seek to better understand this massive technology wave (so we don’t miss it!) 🌊

Project Objective

As we seek to better understand the challenges & opportunities in the commercial storage sector, we’re modeling the dispatch of a generic battery storage unit in a commercial building setting.

Specifically, we’re trying to answer the following questions:

What are the key determinants of commercial building load (kW)? In which regions is the value proposition of commercial storage strongest? What’s preventing existing commercial-scale storage companies from hyper-growth? What are the key factors & constraints to consider when dispatching batteries in large office buildings?

We’ll first explore commercial building load, then use machine learning techniques to generate a forecast for our geographic location of interest (this post). Then, we’ll use the building load forecast as an input to test & refine our storage dispatch algorithm (coming soon ⏳).

Overview: Commercial Load Forecasting

One of the most important variables for a behind-the-meter energy storage unit is the building’s hourly energy consumption (in kWh). In order to appropriately model the optimal dispatch of a behind-the-meter energy storage unit in large commercial buildings, we need a multi-year forecast of the building’s load.

We employed supervised learning techniques (specifically polynomial regression) to predict this commercial building load. The rest of this article walks through the details of the process.

1. Identify Key Drivers 💭

We intend on using supervised learning techniques (specifically polynomial regression) to predict this commercial building load. Commercial building load depends on a variety of internal & external factors, including:

Hour

Temperature

Size (space volume)

Lighting type & consumption

HVAAC type & consumption

No. of employees

Working hours

# of electrical vehicles (EV)

# of rooms

and so on…

Unfortunately, most of this data is incredibly hard to come by in training sets. For the purposes of this analysis, we’re taking a simplistic approach to commercial load forecasting for a single building by using Outside Air Temperature (OAT) and hour as the independent variables.

Specifically, we’ll be using DoE’s Reference Building 7 (“Office”) Long-term energy consumption & outdoor air temperature dataset (source), where they’ve published hourly modeled building load data, along with a corresponding timestamp and OAT.

2. Explore Relationships 🔎

Just by digging into the relationships between our independent (hour-of-day, temperature) and dependent (building load) variables, we can identify clear patterns & insights.

Time Series (temperature, load)

To get a birds-eye view of the training data, we plotted the hourly time series data for both temperature & load. When looking at the two charts together, you can already start to draw the relationships between them (extreme temperature = higher building load).

Temperature vs Load

Things get more interesting when we observe the relationship between temperature & building load. Generally, building load seems to peak during extreme weather (0–20 ℉ and 80–100 ℉), most likely due to increased cooling & heating. This is unsurprising, as HVAC systems (Heating, Ventilation, and Air Conditioning) comprise of ~28% of total energy consumption for office buildings.

We do notice numerous occasions where despite extreme temperatures, energy consumption stays relatively flat. This is somewhat counterintuitive and will be worthwhile to investigate in the future.

Hour vs Load

The relationship between hour & building load is also fairly intuitive (though the pattern is relatively less “clean-cut”). Energy consumption levels are higher during typical working hours (9 a.m. — 5 p.m.) and drop significantly starting at 6 p.m. (a.k.a. “Happy Hour” 🍸).

One unexpected observation is the energy consumption levels between 5 a.m. & 8 a.m. are noticeably higher than during night/dawn hours. One hypothesis is that office buildings could be starting the warming/cooling a few hours before employees arrive (kinda doubt it, as it means buildings need 4 hours to cool/heat 😒). Either way, definitely worth a look later 🕵️‍♀️

3. Modeling 🤖

Based on the exploratory analysis in #2, it’s clear that the relationships between the dependent and independent variables are non-linear. Thus, we’ll use polynomial regression with regularization as our model.

To identify the best combination of hyperparameters (i.e. degree-of-polynomial, regularization alpha), we used scikit-learn’s GridSearchCV feature.

[More details on evaluation metrics coming soon… 🔃]

4. Results 📊

To forecast building load for our location of interest (Seattle, WA), we needed to obtain hourly temperature data for that city. Fortunately, this was readily available for various cities in the Historical Hourly Weather Data 2012–2017 dataset from Kaggle (source). We used this data (after some preprocessing) as inputs to the regression model we trained in #3 to predict the corresponding hourly building load.

Though the resulting hourly load shapes differ somewhat from our training dataset from OpenEI, the general trends are in line with the observations we made in #2.

Load peaks during working hours:

Load peaks during extreme weathers (mostly during cold winters):

Conclusion

Since energy consumption in commercial buildings tends to peak during working hours (especially during extreme weather events), we can make a preliminary conclusion that metro cities where temperatures get really hot (e.g. Arizona, Austin, Denver) or really cold (e.g. New York, Chicago) are more likely to be attractive markets.

Given the predicted values generally make sense and are in line with the training data, we start testing the storage dispatch model using this dataset as inputs. In parallel, we intend to further refine & test our modeling to generate more refined hourly load shapes.

As noted in the beginning, this is definitely an incredibly simplistic load forecasting model for commercial sites, as in reality there are many other variables that dictate a building’s energy consumption. While we can maintain the same assumptions by the DoE’s reference building for practical purposes of this project, it may be worthwhile extending our regression model to consider additional variables (to the extent that it’s possible).

The GitHub repo for the analysis above can be found here.

Project Contributors: Jae Beom Bae, Niel Patel",https://towardsdatascience.com/storage-commercial-buildings-part-1-load-forecasting-8d3c4d1e083d,['Jae Beom Bae'],2019-12-23 06:09:38.803000+00:00,978,"energy storage, commercial storage, energy consumption, forecasting, machine learning"
Gradient Descent Update rule for Multiclass Logistic Regression,"Gradient Descent Update rule for Multiclass Logistic Regression

This is continuing off of my Logistic Regression on CIFAR-10 article, so skim that for some details —especially the intuition on cross-entropy and softmax.

Here are a few details about the CIFAR-10 image dataset this article is being written in the context of.

CIFAR-10 has ten possible classes.

CIFAR-10 has 3072 features per example (color pixel values in an image)

I’d recommend a good knowledge of derivatives and simple neural network functions, like backpropagation for this part. Even if you do know those things, you won’t understand just from reading this. Work through the problem yourself, and solve the problems you encounter.

A second disclaimer — out of simplicity, all of this will be assuming a single training example.

To derive the update rules of the parameters, it helps to think of logistic regression as a simple neural network, with zero hidden layers.

Much of this proof is from this fantastic lecture:",https://ai.plainenglish.io/gradient-descent-update-rule-for-multiclass-logistic-regression-4bf3033cac10,['Adam Dhalla'],2020-11-29 09:17:16.875000+00:00,152,"Gradient Descent, Logistic Regression, Multiclass Classification, CIFAR-10, Derivatives"
Detecting Brain Tumours From MRI Scans Using a CNN,"The Canadian Cancer Society presents the following statistics related to brain tumours in 2020:

Approximately 3,000 Canadians are to be diagnosed with brain and spinal cord cancer

Canadians are to be diagnosed with brain and spinal cord cancer An estimated 2,500 Canadians will die from brain and spinal cord cancer

Canadians will die from brain and spinal cord cancer Approximately 1,700 men will be diagnosed with brain and spinal cord cancer

men will be diagnosed with brain and spinal cord cancer Out of these 1,700 men, 1400 of them will die from the cancer

1,350 women will be diagnosed with brain and spinal cord cancer

women will be diagnosed with brain and spinal cord cancer Out of these 1,350 women, 1,050 will die from it

The Brain Tumour Charity also states that brain tumours are the biggest cancer killer of children and adults under the age of 40 worldwide.

Now, these are some scary facts.

Unfortunately, most brain tumours are only diagnosed when patients see their doctors because they are experiencing certain symptoms that may be brain tumour related. Then doctors perform a bunch of tests and it is then determined whether the patient is experiencing brain tumour-related symptoms.

Intraoperative pathology analysis takes time. A sample must be processed, stained, and overlooked by a pathologist, and the surgeon and patients must wait a long period of time for the results.

On the positive side of things, CNNs are proving to actually be an extremely effective and efficient way to diagnose brain tumours.

The journal of Nature Medicine reveals a study that shows a new method of modern optical imaging and CNNs. being used to diagnose brain tumours. The researchers at NYU hope that this new method will provide a better and more accurate diagnosis in comparison to our current methods and will also initiate the correct treatments instantly.

In terms of cancer treatment, the earlier cancer has been detected, the earlier the patients can start treatment, and early detection usually improves the outcome of a patient.

This newly proposed method that utilizes optical imaging and CNNs has shown research the method of detection can have a 94.6 percent accuracy in comparison to the 93.9 accuracy that comes alongside pathology-based methods. Additionally, it is much faster to use this new method in comparison to having to run many tests and have them analyzed by pathologists, thus possibly detecting and diagnosing brain tumours at an earlier stage.

This new method is still improving, and the more data that can be provided and the more a model trains increases the likelihood of this detection having an even higher accuracy in the future.",https://ai.plainenglish.io/detecting-brain-tumors-from-mri-scans-using-a-cnn-ae3c2913bde7,['Ashley C'],2020-12-28 19:30:18.153000+00:00,424,"Brain Tumours, Cancer Treatment, CNNs, Optical Imaging, Pathology Analysis"
Improving Operations with Route Optimization,"Improving Operations with Route Optimization

Contributors: Feiko Lai, Michal Szczecinski, Winnie So, Miguel Fernandez

This story was originally published at GOGOVAN Technology Blog Please find our academic paper on the topic here.

Every day, GOGOVAN drivers arrive at warehouses across Asia to pick up thousands of orders that our business partners have asked us to deliver to their clients.

These orders can be a range of things — from that long-awaited new phone, to an anniversary present that was ordered last minute. All of them will be of different sizes, shapes and weights. For each one of them, there will be a person waiting and hoping this time the courier company gets there on time…

This is why at GOGOVAN, we do everything in our power to ensure smooth and timely deliveries with quality of service that will amaze our customers. Every delivery route is carefully planned manually and double-checked by our Operations Team, to make sure we never fail.

MANUALLY?!

DIDN’T YOU JUST SAY THAT YOU HAVE THOUSANDS ORDERS DAILY?!

Yes, that is correct.

In the past, Operations Team would have to manually sort out the delivery routes, usually on the morning of pickup, and ensure we meet all the delivery time requirements for that day. As you might imagine, that is not a particularly exciting or easy task :)

It took one person approximately 1 hour to create a sub-optimal route for 100 waypoints. For requests bigger than that, this time grew exponentially.

We instantly realised that this process was simply begging for some automation.

Not only we felt sorry for the Operations Team who had to do such mundane work each early morning, but we also knew that as our order volume grew, that task would slowly become mission impossible. We saw it as an opportunity to develop a cutting-edge technology that will become a core component of GOGOVAN’s Data Science stack.

How did we start?

We are very client and driver-centric. Consequently, we always try first to analyse a problem from their perspectives in order to understand how our solution could impact and benefit them. After a lot of brainstorming, these are the goals we came up with:

All orders need to be delivered on time .

. Ensure drivers are not rushed to make it on time by using buffer times and real-time distance .

. Save fuel by reducing the distance driven.

by reducing the distance driven. Minimise idle time for drivers — no one likes waiting with a trunk full of packages.

— no one likes waiting with a trunk full of packages. Improve vehicle utilisation .

. Fully automate the process.

The algorithm needs to be able to grow with us — supporting different types of deliveries, vehicles and countries.

Having determined our main objectives, we decided to explore the world of academia and open source — there’s no point in rediscovering the wheel. We realised that the problem we faced was widely known as the Vehicle Routing Problem.

What is Vehicle Routing Problem?

Vehicle Routing Problem (VRP) can be described as the problem of creating a set of optimal routes from one, or many, depots to multiple customers, subject to a set of constraints. The objective is to deliver goods to all customers, at the same time minimising for the cost of the routes and the number of vehicles.

This problem is NP-hard, as proven by Lenstra and Rinnooy Kan¹. However, there are still some exact solution methods, using a branch-and-bound², or dynamic programming³, however, as described in the papers above, they only seem to be working for up to 150 waypoints.

Currently, the state-of-art solutions are obtained using the metaheuristics: Genetic Algorithms⁴, Tabu Search⁵ and Ant Colony Optimization⁶. These are the methods mainly used in the field nowadays.

For in-depth review of the field, we recommend this brilliant thesis⁷ by Xiaoyan Li.

Our solution

With VRP being a widely recognised problem, there are indeed a lot of companies out there who seem to be tackling the problem.

However, we somehow did not feel satisfied with their solutions…

We just knew that if we combined our operations know-how, data science and research expertise, large volumes of data, and open source state-of-the-art contributions, we can arrive at a robust in-house solution that:

Is more up-to-date, performant and has customisable algorithms and iteration logic.

Is cheaper, more efficient and more scalable.

Allows developing tangible intellectual property assets and building competitive advantage around it.

Allows us to guarantee to our clients that their delivery data will go no further than GOGOVAN.

Having thought about it for quite a time, we decided that if we want to be the leaders in the field, we need to do it our way, not use some blackbox solution.

So we got going…",https://towardsdatascience.com/improving-operations-with-route-optimization-4b8a3701ca39,['Kamil Bujel'],2020-01-08 08:49:44.145000+00:00,757,"route optimization, delivery optimization, logistics optimization, vehicle routing problem, NP-hard problem"
[Paper] ProxylessNAS: Direct Neural Architecture Search on Target Task (Image Classification),"[Paper] ProxylessNAS: Direct Neural Architecture Search on Target Task (Image Classification)

Left: Conventional NAS, Right: ProxylessNAS

In this story, ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware (ProxylessNAS), by Massachusetts Institute of Technology (MIT), is presented.

In conventional NAS:

The prohibitive computational demand (e.g. 10⁴ GPU hours) makes it difficult to directly search the architectures on large-scale tasks such as ImageNet.

such as ImageNet. They need to utilize proxy tasks, such as training on a smaller dataset , then use the learned cells on large-scale target tasks.

, then use the learned cells on large-scale target tasks. However, architectures optimized on proxy tasks are not guaranteed to be optimal on the target task.

In this paper:

ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms, to address the high memory consumption issue of differentiable NAS and reduce the computational cost.

This is a paper in 2019 ICML with over 500 citations. (Sik-Ho Tsang @ Medium)",https://medium.com/@sh-tsang/paper-proxylessnas-direct-neural-architecture-search-on-target-task-image-classification-73c35ebd8aed,['Sik-Ho Tsang'],2020-11-14 12:43:27.098000+00:00,154,"ProxylessNAS, Neural Architecture Search, Image Classification, ConventionalNAS, DifferentiableNAS"
Will Automation Replace Content Writers?,"What happens when you let an AI text writer write a post for you? The answer is pretty simple. They’ll make a mistake, and you’ll be able to find it. That mistake could come in several forms. A small edit is all you need to get things back on track, and sometimes a complete rewrite is needed if it really doesn’t make sense.

This post is mainly written using Text Synth, with gentle nudging to keep the post going in the right direction. But sometimes it takes you back to a good old fashioned hand-wrote blog post. That’s fine! Keep it going, and post again tomorrow!

Have you ever had the dreaded writers block? I know it has happened to me in the past; it’s fairly common. If you needed a little help to get going, you can use a text completion language model to give you suggestions on how to continue your writing. But don’t use it as a substitute for real-time feedback!

When you use software like this to complete text, sometimes its easy to tell.

Photo by Shahadat Rahman on Unsplash

Here is how to use Text Synth:

Go to Text Synth.com Enter a few words or even a paragraph into the text field and press the generate button. Watch what the output is, then add what you like to your original paragraph and repeat.

In this manner, you are pretty well letting a computer write a post for you, while you have the choice of using the output, or making it write another. It is pretty easy.

This way you still have control over the flow of the post, and still get to tell it to “do something else.”

Try a bit of editing!

At this point you should have a skeleton of a post, now it is time to revise your work. Re-read your post and ask yourself “does this make sense?” Feel free to edit as needed, make it your own.

You will get random topics like:

What if…? Who is that? When was the last time someone spoke? In our world of endless distractions, being “wrong” on things can be a bad thing, for us. The more you know about it, the better off you will be in knowing your own thoughts.

and by pressing generate again, we get this output:

Here is your template for what you might write back to the computer, at this point you should be able to understand a lot of what the computer may have seen. Here are a few things the computer could have seen: A guy with his laptop outside and has been reading up on philosophy A girl reading some stuff on philosophy A woman reading some stuff on philosophy

The story can really go sideways if you let it, but it really helps if you are struggling for ideas moving your story along. At the minimum it is good for a laugh.

If you write a story this way you start to understand the importance of your own thoughts. It will become easy for you and your thoughts to become a part of the story you have created.",https://blog.prototypr.io/will-automation-replace-content-writers-4a8bb6c0404,['L Hall'],2021-06-17 10:46:12.647000+00:00,509,"AI, Text Writer, Writers Block, Text Completion Language Model, Text Synth"
All You Need to Know About the Lambda Functions in Python,"All You Need to Know About the Lambda Functions in Python

Practical examples and real usage of lambda functions in Python

Photo by Clint Patterson on Unsplash

If you’ve heard of lambda functions in Python but not sure about how to use them, you’re in the right place.

This article will provide all you need to know about the lambda functions in Python:

What are lambda functions in Python, how they differ from the normal functions?

Why lambda functions are useful and when to use them?

Practical examples and real usage of lambda functions in Python

What are lambda functions in Python, how they differ from the conventional functions?

Lambda functions are defined without a name so they are also called anonymous or nameless functions. We do not have to assign a name to lambda function as we do when defining normal functions.

# lambda function syntax

# lambda arguments: expression

lambda x,y: x+y # conventional function definition

def add_x_y(x,y):

return (x+y)

In addition, with lambda functions, we can have any number of arguments but we can only define a single line expression. It is not possible to define multi-line expressions as we can do when creating normal functions.

Why lambda functions are useful and when to use them?

When you need a single expression function which will be used once in your code, there is no need to define a normal function. You can define a lambda function whenever and wherever it is needed.

Lambda functions are also used when a function is needed as an argument to another function. Instead of defining a normal function, using a lambda function is more convenient and simple for such cases.

Practical examples and real usage of lambda functions in Python

You can find below several examples of how lambda functions are used.

# use of lambda functions

# x is the argument

# x * 2 is the expression which will be returned by lambda function double_x = lambda x: x * 2

print(double_x(4))

# output 8

# use of lambda functions

# x and y are the arguments

# x * y is the expression which will be returned by lambda function mult_x_y = lambda x,y: x * y

print( mult_x_y (4,5))

# output 20

Lambda functions are generally used in higher-order functions (filter(), map(), reduce() are very good examples of such functions) which take in another function as an argument. Let’s see how lambda function is used in such cases.

Example use with filter()

Filter() function takes a function and a list as arguments, then it returns the list items for which the function evaluates True.

# Use of lambda functions with filter() function

# Program to filter out only the items which are greater than 5 my_list = [1, 2, 3, 4, 5, 6, 7, 8]

new_list = list(filter(lambda x: x > 5 , my_list))

print(new_list) # output

[6, 7, 8]

In the example above, the filter function gets my_list and lambda function as arguments. It evaluates every list item with the expression ( x > 5) provided in the lambda function. If the evaluation result is True, then the new list contains this specific element. If not, the function filters out the element and exclude it from the new list.

Instead of the lambda function, we could be providing a normal function as an argument. But this would increase the complexity of the code. Whenever it is possible, using a lambda function makes the code simpler.

Example use with map()

Similarly, the map() function takes a function and a list as arguments. The function provided as an argument is called for each list item and returned results are contained in a new list.

# Use of lambda functions with map() function

my_list = [1, 2, 3, 4, 5, 6, 7, 8]

new_list = list(map(lambda x: x * 5 , my_list))

print(new_list) # output

[5, 10, 15, 20, 25, 30, 35, 40]

Example use with reduce()

reduce() function also takes a function and a list as arguments. This time, instead of returning a list, it returns a single value by applying the lambda function cumulatively to all the items in the list provided.

my_list = [1, 2, 3, 4, 5, 6, 7, 8]

result = reduce(lambda x,y: x + y , my_list)

print(result) # output

36 my_list = [1, 2, 3, 4, 5, 6, 7, 8]

result = reduce(lambda x,y: x * y , my_list)

print(result) # output

40320 my_list = [1, 2, 3, 4, 5, 6, 7, 8]

result = reduce(lambda x,y: x if x > y else y , my_list)

print(result) # output

8

Summary and Key Takeaways

In this short article, I have explained what lambda functions are and how to use them in Python. The key takeaways are;

Lambda functions are defined without a name so they are also called anonymous or nameless functions .

. We can have any number of arguments in lambda functions but we can only define a single line expression .

in lambda functions but we can only define a . You can use lambda function when you need a single expression function which will be used once in your code.

when you need a which will be used once in your code. Lambda functions are also used when a function is needed as an argument to another function.

I hope you have found the article useful and you will start using lambda functions in your own code.",https://medium.com/python-in-plain-english/all-you-need-to-know-about-the-lambda-functions-in-python-53db0fa7d02e,['Erdem Isbilen'],2020-12-25 19:45:48.365000+00:00,843,"lambda functions, Python, anonymous functions, nameless functions, single line expression"
Python Numpy and Matrices Questions for Data Scientists,"I’ve been preparing for Data Science interviews for a while, and there is one thing that struck me the most is the lack of preparation for Numpy and Matrices questions.

Often, Data Scientists are asked to perform simple matrix operations in Python, which should be straightforward but, unfortunately, throw a lot of candidates off the bus!

Me included!

One time, I was asked by a FAANG company to perform a multiplication of two matrices, which I didn’t know back in time.

I find the best way of preparing these types of interviews is to find a niche area and write a post on the topic. It’s a win-win situation for me and my fellow readers.

So, here it goes. In this post, I walk through 4 Numpy/Matrices questions that often come up in DS interviews and code it up in Python.

Photo by Brittany Bendabout on Unsplash

Question 1:

Given a 4x4 Numpy matrix, how to reverse the matrix?

# step 0: construct a 4*4 Numpy matrix

[[ 1 2 3 4]

[ 5 6 7 8]

[ 9 10 11 12]

[13 14 15 16]]

A side-note, np.arange(1,17) returns a Numpy array, and reshape(4,4) generates a 4*4 matrix.

# step 1: to flatten the numpy array

array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])

There are two ways to flatten a matrix depending on the data type. For Numpy arrays, we use np.array.flatten() command; for non-array matrices, we use matrix.ravel(). Please try it out.

# step 2: read the elements backward into a new matrix

array([16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1])

After flattening out, we read the matrix backwards starting from the last element.

# step 3: reshape the matrix back into a 4*4 matrix

Last, we reshape the matrix and turn it back into a 4*4 matrix, as shown:

array([[16, 15, 14, 13],

[12, 11, 10, 9],

[ 8, 7, 6, 5],

[ 4, 3, 2, 1]])

The unique attribute about Numpy arrays or matrices is that we have to flatten it out before reversing it, and there are two ways of doing so depends on the data format (Numpy or not).

Photo by Pete Walls on Unsplash

Question 2:

How do you multiply two matrices?

This type of question could be tricky!

Two matrices? Numpy or not?

The solution largely depends on the data type that it refers to. Overall, there are two solutions: the dot method for Numpy arrays and nested for loops for non-arrays.

Solution 1: Numpy arrays

# step 1: numpy array

# A

array([[0, 1, 2],

[3, 4, 5],

[6, 7, 8]]) # B

array([[10, 11, 12],

[13, 14, 15],

[16, 17, 18]]) # Step 2: dot method

[[ 45 48 51]

[162 174 186]

[279 300 321]]

The one-liner dot method easily solves the multiplication question for Numpy arrays, but I doubt interview questions would be so easy.

So, it’s more likely to do multiplication questions for non-arrays.

Solution 2: Non-Arrays

Step 0: construct two matrices.

Python does not have a built-in data type for matrix, and we use a list of a list (nested lists) instead.

# step 1: create a zero matrix to store results

The reason why we create an empty matrix is to store the multiplication results in the following nested for loop.

# step 2: nested for loop

[[14, 20, 31], [21, 30, 46], [21, 30, 44]]

Let’s disentangle the nested for loops:

for i in range(len(X)): returns the number of rows for j in range(len(Y[0])): returns the number of columns of Y. A side-note: we use Y[0] to access matrix columns and Y to access matrix rows. for k in range(len(Y)): to iterate rows of Y Z[i][j] += X[i][k]*Y[k][j]: fill in the values in Z by the sums of element-wise multiplication.

Recall, the sequence of two matrices multiplication: the elements in the first row from X multiply the elements in the first column from Y, and we add the sums up. Repeat the process until the end.

The above nested loop simply follows the same procedure of calculating matrix multiplication as we normally do. A quick refresh of linear algebra here.

Photo by Jonny Gios on Unsplash

Question 3:

How to transpose a matrix?

Again, there are multiple solutions depends on whether you are allowed to use Numpy.

Solution 1: nested for loop (don’t use Numpy)

# step 1: create matrices

# step 2: nested for loop

[12, 1, 3]

[7, 2, 4]

Solution 2: using zip() for a list of tuples (don’t use Numpy)

# step 1: create a list of tuples

[(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12)]

We use a list of tuples to create a matrix.

# step 2: unzip and zip in one line

(1, 4, 7, 10)

(2, 5, 8, 11)

(3, 6, 9, 12)

# Solution 3: use Numpy array

3.1 np.transpose()

If we can use Numpy, a one-liner can get the job done.

3.2 np.array.T

array([[12, 1, 3],

[ 7, 2, 4]])

Alternatively, we can change the matrix (list) into a Numpy array and use array.T.

Photo by Eduardo Drapier on Unsplash

Question 4:

How to add two Numpy matrices together?

Again, there are two solutions depends on whether we can use Numpy.

Solution 1: np.array()

[[17 15 4]

[10 12 9]

[11 13 18]]

Numpy handles element-wise addition with ease.

Solution 2: nested for loops for ordinary matrix

[17. 15. 4.]

[10. 12. 9.]

[11. 13. 18.]

The code is pretty self-evident, and we have covered them all in the above questions.

On a related note, there are two variations of the question.

4.1 How to stack matrices vertically?

array([[12, 7, 3],

[ 4, 5, 6],

[ 7, 8, 9],

[ 5, 8, 1],

[ 6, 7, 3],

[ 4, 5, 9]])

4.2 How to stack matrices horizontally?",https://towardsdatascience.com/python-numpy-and-matrices-questions-for-data-scientists-167af1c9d3a4,"['Leihua Ye', 'Ph.D. Researcher']",2020-12-30 00:18:10.177000+00:00,880,"array([[12, 77, 33, 44, 55"
Data visualization using Pandas,"Data visualization using Pandas

This article will help you to use in-built Pandas methods for visualizing data and drawing insights.

How to import the packages?

Numpy and Pandas package is imported. Along with this the magic function ‘%matplotlib inline’ is mentioned to make sure that the plots are displayed in the notebook.

>>> import numpy as np

>>> import pandas as pd

>>> %matplotlib inline

For the purpose of understanding, a dataset is taken which has random values.

>>> df1 = pd.read_csv('dataset2.csv')

>>> df1.head()

How to create a histogram plot?

A histogram plot can be generated by using the method ‘hist’ on a column of a dataframe. The number of bins can also be specified.

>>> df1['b'].hist(bins=15)

In case you want to view the plots in the seaborn style then import the package of seaborn, set the style and run the code again.

>>> import seaborn as sns

>>> sns.set_style('whitegrid') >>> df1['b'].hist(bins=15)

How to deal with general plot kinds?

You can also call the ‘plot’ method off the dataframe and mention the kind of plot needed.

>>> df1['c'].plot(kind='hist',bins=10)

You can also call the hist method directly through the plot method.

>>> df1['c'].plot.hist()

How to create area plots?

It plots the area of the dataframe columns. The alpha keyword can be specified to tweak the transparency.

>>> df1.plot.area(alpha=0.7)

How to create bar plots?

The ‘bar’ method plots the bar plot.

>>> df1.plot.bar()

In case you do not want the bars to be plotted separately and instead be stack on each other, pass the value True to the stacked keyword.

>>> df1.plot.bar(stacked=True)

How to create line plots?

A line plot can be drawn by calling the line method and pass the x and y values.

>>> df1.plot.line(y='d')

The figure size and line style can also be changed like linewidth.

>>> df1.plot.line(y='a',figsize=(5,3),lw=2)

How to create scatter plots?

The scatter method is called and the x and y values are passed.

>>> df1.plot.scatter(x='b',y='d')

If you want to compare with another column then mention the column name to the ‘c’ value. You can also change the color of the plot using the cmap keyword.

>>> df1.plot.scatter(x='b',y='d',c='a',cmap='rainbow')

How to create box plots?

The box method can be called to create box plots.

>>> df1.plot.box()

How o create hex plots?

This is just like the scatter plot but the data points are represented as hex cells.

>>> df1.plot.hexbin(x='b',y='d',gridsize=15)

How to create KDE plot?

KDE stands for kernel density estimation. The kde method can be called to plot it.

>>> df1['d'].plot.kde()

>>> df1.plot.kde()

For more detailed information on Pandas data visualization, check the official documentation here. Refer to the notebook for code here.",https://medium.com/nerd-for-tech/data-visualization-using-pandas-cfcde72807b1,['Jayashree Domala'],2020-12-27 02:51:56.204000+00:00,386,"Data Visualization, Pandas, Numpy, Matplotlib, Seaborn"
NeurIPS 2020 Sharing,"We can all agree that 2020 has been a wild year. Even though being under such a tough condition, there’s still some silver lining and NeurIPS hosted from Dec 6th ~to 12th being one of them. This the first time for me attending NeurIPS conference. Usually a ticket to NeurIPS is quite difficult to get. NeurIPS 2018 tickets were sold in 12 minutes and 2019 it converted to perform a lottery ticketing system. This year, it is hosted entirely online while simulating the poster sections, roundtables and live Q&A with Gather.town virtual avatars and Rocketchat where you can actually “meet” people.

Walking around sponsored booths talking to company representatives.

Because this being the first year, I wan’t fully prepared for what to expect during the conference. This year, there were 7 invited keynotes, 7 virtual poster sessions, 16 tutorials, 60 workshops, 20 demonstrations, 16 competitions as well as multiple tracks of contributed oral and spotlight presentations.

With so much content, I found it impossible to keep track of what’s happening without constantly checking the main schedule. The good thing though, is that most sessions are both live and recorded. Even if you missed part of the talk, you can still catch up by dragging the video back. By doing this, you can attend both sessions even if they are hosted at the same time. It’s almost like entering parallel worlds while you can listen to several different sessions at the same time.

There’s also multiple social talks and events via Zoom and Gathertown where there aren’t recorded. I am happy that I didn’t miss those sessions. Especially the roundtable session from Women in Machine Learning Workshop taken place on Wednesday (12/9) 3:30pm EST. There were 50 roundtables to choose from covering varies topics in research or career life and advice. I attended Navigating the job search, Putting machine learning research into practice, and Non-traditional paths to machine learning. Each topic took 30 minutes and the event was from 3:30pm to 5pm so I can at most attend 3 topics. It was a really nice way to get interaction with mentors and I hoped there were more time since there’s still a lot more sessions I wish I could go to.

Attending “Navigating the job search” roundtable on Gather town interface.

The entire conference was more research focused (at least compare to KDD, the conference I went least year), so the participants are mostly PhDs or working in research institutes/departments. I often found myself sitting with a roomful of PhDs asking to apply for internship at research labs so I do feel out of place sometimes. I feel like most people has years of experience in research and lots of publications while I just mostly read papers and filter out the closest ones that fit our needs and bring them into our ecosystem. The New in ML workshop is meant to help with people just beginning so that draws my attention. I found the Q&A session Prof. Anima Anandkumar (from California Institute of Technology and NVIDIA) quite inspiring.

The conference covers a wide range of research topics, but can be roughly categorized into four main trends:

Below is some highlights that I will go over more in depth on separate posts:

There’s also some cools sessions that might not be as main streams such as the Machine Learning for Creativity and Design 4.0 Workshop. Seeing people using machine learning to produce music, analyze dance movements and create artwork is really fascinating. That’s also something I’m personally very interested in.",https://medium.com/@tzuruey/neurips-2020-sharing-9e8f8496d520,['Jenny Ching'],2020-12-24 05:51:05.823000+00:00,579,"2020, NeurIPS, machinelearning, researchtopics, conferences"
Understanding Regression Models,"Using SKlearn for creating linear, ridge, lasso regression models.

Photo by Markus Winkler on Unsplash

What is a linear regression?

Linear regression comes from the equation for a line, y=mx+c, where y is the dependent variable which changes according to x, which is the independent variable. C is the y intercept and m is the slope. In ML terms coefficient of x is the weight and y intercept or c is the bias. “m” and “c” are the variables that our machine will try to learn.

In this article, we will see how we can use scikit-learn to perform different types of regression like linear, ridge, lasso, etc. In order to start let us first import the required libraries.

Importing required libraries

We will start by importing pandas to load the dataset that we require, we will use matplotlib and seaborn for visualization, and similarly, we will import regression models from scikit-learn as and when required.

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

Loading Required Dataset

The dataset we are going to use here is the Boston dataset which can be easily downloaded from Kaggle. This dataset contains the median value of house prices along with all the features on which the price depends. Let us load the dataset and see what are the different columns it has.

df = pd.read_csv(""C:/Users/DELL/boston.csv"")

df.head()

Boston Dataset(Source: By Author)

Performing Data Analysis

In order to prepare the dataset for machine learning, we need to perform some basic data analysis on our dataset. We will start by dropping any unnecessary column like “Unnamed:0” as we can see in the image above.

df.drop('Unnamed: 0', axis=1,inplace=True)

Now let us see some statistical properties of our dataset using the describe function of data frames.

df.describe()

Statistical Properties(Source: By Author)

Now let us visualize the relationship of our target variable that is “MEDV” with all other feature variables. For this, we will use matplotlib and seaborn.

plt.figure(figsize=(15,30))

cnt=1

for i in df.columns[:-1]:

plt.subplot(10,3,cnt)

sns.regplot(x=df[i],y=df['medv'], color = 'green')

cnt=cnt+1

plt.tight_layout()

plt.show()

Regression Plot Visualization(Source: By Author)

After analyzing all this we can start with the modeling process. For this we need to divide our dataset into testing and training datasets also we need to import the models from sklearn.

from sklearn.model_selection import train_test_split

#Splitting the dataset

X_train, X_test, y_train, y_test = train_test_split(df[['crim','zn','indus','chas', 'nox','rm','age','dis','rad','tax','ptratio','black','lstat']], df['medv'], test_size=0.3, random_state=109) #Importing Regression Models

from sklearn.linear_model import LinearRegression, Ridge, Lasso

Linear Regression Model

We will start by creating a linear regression model and fit the data into it. To know how accurate our model is we will find the model score for testing data.

linreg = LinearRegression()

linreg.fit(X_train, y_train)

print(""Coefficients: "", linreg.coef_)

print(""Intercept: "", linreg.intercept_) #y intercept

Model Coefficients(Source: By Author)

Now let us check the accuracy of the model using the model score.

print(""Accuracy score for training Data"", linreg.score(X_train, y_train))

print(""Accuracy score for testing Data"", linreg.score(X_test, y_test))

Accuracy( Source: By Author)

Similarly, we can use the Ridge and Lasso models also for regression.

Lasso and Ridge Regression

In linear regression, if we have a large number of predictor variables and these predictor variables have a high correlation with each other then we can say that our model encounters a large variance and makes the model unreliable.

In order to overcome this problem, we use regularization techniques. Ridge regression is a technique which aims at decreasing the size of the coefficient using l2 penalty(sum of squared coefficients) while Lasso regression uses l1 penalty(absolute value of coefficients).

Creating Lasso Regression Model

lassoreg = Lasso()

lassoreg.fit(X_train, y_train)

print(""Accuracy score for training data"", lassoreg.score(X_train,y_train))

print(""Accuracy score for test Data"", lassoreg.score(X_test,y_test))

Accuracy(Source: By Author)

Creating Ridge Regression Model

ridgereg = Ridge()

ridgereg.fit(X_train, y_train)

print(""Accuracy score for training data"", ridgereg.score(X_train,y_train))

print(""Accuracy score for test Data"", ridgereg.score(X_test,y_test))

Accuracy(Source: By Author)

In this article, we saw how we can create different regression models and check their accuracy using a model score. Go ahead try this and in case you find any difficulty please let me know in the response section.

Before You Go

Thanks for reading! If you want to get in touch with me, feel free to reach me on ipsitashee4@gmail.com or my LinkedIn Profile. You can view my Github profile for different data science projects and packages tutorials.",https://medium.com/analytics-vidhya/understanding-regression-models-c9ac4f2184dc,['Ipsita Shee'],2020-12-14 16:43:11.532000+00:00,639,"Linear Regression, Ridge Regression, Lasso Regression, SKlearn, Scikit-Learn"
What is Clustering?,"Today, we are going to cover clustering method. Clustering means group, or crowd. As the derivation of word shows, clustering methods split sample datasets into several groups. But what is difference from algorithm I introduced, such as regression or K-NN. The main difference between clustering and them is that clustering is one of unsupervised learning. In supervised learning, the answer which is compatible with each sample data is prepared, so that we can calculate the accuracy of the learning model. On the other hand, unsupervised learning has no answer data compatible with sample data. Therefore, the algorithm tries to find possible grouping(clusters) from sample datasets.

Unsupervised learning

There are some types of unsupervised learning as supervised learning had. The main unsupervised learning is below.

K-Means clustering — clustering sample data into a number (K). K parameter is crucial to this algorithm. This is totally different from K-NN method!

Hierarchical clustering — clustering data into parents and children. The result looks like hierarchical tree!

Probabilistic clustering — clustering data into clusters based on probability.

K-Means clustering

Firstly, I am going to introduce K-Means clustering. As the list above, K-Means is one of clustering method.

How K-means clustering works

In the beginning, all data are plotted into x-y plane. Then K centroids are randomly selected. Centroid refers to the centroids of cluster. After random centroids picked up, it performs iterative(repetitive) calculation to optimeze the position of each centroids.

You can image the flow in this video.(https://youtu.be/Coa5IbDfDSI)

To determine the most effective K parameter

Elbow method — using the SSE figure to find the effective k -parameter. SSE is the sum of square errors. If SSE shows big number, the model’s estimates have big difference between sample data. The reverse is also true.

In elbow method, we have to make a diagram of SSE number, then try to find the edge of line. The point before the edge is taken the best k-parameter.

Silhouette analysis — using silhouette coefficient to find the best k parameter. Silhouette coefficient means how far each cluster is positioning from each clusters. Coefficient ranges from -1 to 1. Closer to 0, cluster is close to other clusters. Hence, -1 or 1 is better for clustering model. Our aim!",https://medium.com/@ryotennis0503/what-is-clustering-9e0c8a3c316c,[],2019-11-10 01:21:10.087000+00:00,356,"Clustering, Unsupervised Learning, K-Means Clustering, Hierarchical Clustering, Probabilistic Clustering"
Data Scientist Job Recommendation System,"Objective

Recommendation systems are a class of data science utilities that involve suggesting a product to a user based on a user’s or multiple users’ activities. This project is a similar approach that recommends data science jobs to users based on their activity by applying a classification machine learning based recommender system.

Steps involved

Data Extraction Data Cleaning Exploratory data analysis Data Preparation for Modeling Machine learning classifier model implementation Optimizing model for better validation Hyperparameter Tuning Prediction using the best fit model and checking validation metrics Recommendation system based on predictions for different users Productionalizing into a web application (future scope)

Step 1: Data Extraction

For this data science job recommendation project, I am extracting 1000 data science jobs in the USA from multiple states, and creating a pandas data frame. The scraper code has been referenced from the following article

And the data extraction process looks like below

Our extracted data frame looks as follows:

Please Note: Download the exact chrome driver for the chrome version that you have installed on your local PC using the following link https://chromedriver.chromium.org/downloads",https://medium.com/@anishnitin/job-recommendation-system-ccb3a5d5c5cc,['Anish Nitin Somaiah'],2020-12-15 05:28:35.327000+00:00,173,"Data Extraction, Data Cleaning, Exploratory Data Analysis, Data Preparation, Machine Learning Classifier"
How Will Predictive Analytics Affect Your Business?,"Broadly speaking, predictive analytics is the science of using historical data to predict the likelihood of future outcomes. While this is one of the oldest problems faced by businesses, recent advances in AI have made it possible to achieve much better accuracy and address a much wider range of applications than ever before.

Recommender Systems

The most famous recommender system is probably Netflix: based on your viewing patterns and the patterns of other viewers, Netflix recommends new movies to you. There was even a famous contest called the Netflix Prize where they challenged researchers to come up with new techniques for recommending movies that could significantly outperform the current systems.

These systems use a variety of techniques, but at their core they rely on some common ideas. First, users are likely to prefer items similar to items they’ve already enjoyed. Second, they’re also likely to prefer the favorite items of other users who have similar preferences to themselves. AI techniques can improve the performance of these systems, for example by removing the need for manual feature engineering and letting the algorithm extract features automatically.

Marketing Analytics

Similar to recommender systems, marketing analytics uses your existing customer data to predict what new offers and products to market to customers, and how. Tools such as Klaviyo and Mailchimp tie together AI and analytics with your marketing funnel, allowing you to predict when your customers will make new purchases, how much, and how often, and helping you craft the messages and campaigns that will increase those numbers.

Supply Chains

Accurately forecasting inventory can increase margins and reduce waste. According to McKinsey, AI will produce up to $2 trillion in impact on the supply chain and manufacturing sectors, completely transforming them.

AI-powered forecasting tools like Llamasoft, AnyLogistix, JDA / Blue Yonder, E2Open help enterprises optimize their supply chain. Employing AI in these forecasts allow them to adapt more quickly to changing market conditions as well as automatically taking into account external variables like weather.

For example, Proctor & Gamble has adopted an end-to-end supply chain model to help scale its distribution and manufacturing across the 130 manufacturing sites serving 180 countries across the world.

Demand Forecasting

Many modern companies now rely on real-time data to forecast demand for resources like drivers. A company like Uber has to decide how many drivers it needs on the roads and offer the right incentives to encourage that number to sign in to drive at a given time. One of the approaches they took to forecast demand in extreme events (such as New Year’s Eve) is to apply neural networks to historical data in order to predict demand for drivers in future events, achieving gains in performance of 2%-18%.

How to use predictive analytics in your business?

You’re most likely already using predictive analytics in your planning, whether you know it or not. Marketing tools such as Mailchimp, business tools like Quickbooks, and more already incorporate predictive analytics as part of their offering and their recommendations for actions.

Going to the next level and making more direct use of predictive analytics requires building and maintaining a data pipeline. First, identify which data you capture from your customers and your operations. Then, clean and normalize the data, and finally use analytics software to visualize and create predictions from them. Tools from companies like Microsoft and SAS can help you analyze the data, but usually the most challenging part is the data collection and organization.

Building your own data pipeline usually requires deep knowledge about the specifics of your business, and needs to be custom-tailored for your situation. Research shows that while many businesses have invested heavily in analytics software, many have not simultaneously invested in the talent needed to make best use of the software. When planning your business’s analytics strategy, make sure to keep the human element in mind and not let the technology outpace your team’s ability to utilize it to its full potential.

This is the fourth in our series on how AI will affect your business. Check out our other posts to learn more.",https://medium.com/@bensenai/how-will-predictive-analytics-affect-your-business-f91fe4a7ead8,['Bensen Ai'],2020-12-14 14:39:45.804000+00:00,663,"Predictive Analytics, Recommender Systems, Marketing Analytics, Supply Chains, Demand Forecasting"
10 Minutes to Building a Machine Learning Pipeline with Apache Airflow,"10 Minutes to Building a Machine Learning Pipeline with Apache Airflow

Why ML pipelines matter, and how to build a simple ML pipeline using Apache AirFlow

Photo by Jaimie Phillips on Unsplash

Often, when you think about Machine Learning, you tend to think about the great models that you can now create. If you want to take these amazing models and make them available to the world, you will have to move beyond just training the model and incorporating data collection, feature engineering, training, evaluating, and serving.

On top of all that, you will also have to remember that you’re putting a software application into production. That means you’ll have all the requirements that any production software has, including scalability, consistency, modularity, testability, and security.

ML code is only one piece of a ML system

A ML pipeline allows you to automatically run the steps of a Machine Learning system, from data collection to model serving (as shown in the photo above). It will also reduce the technical debt of a machine learning system, as this linked paper describes. This segues into the fields of MLOps, a fast-growing field that, similar to DevOps, (aims to automate and monitor all steps of the ML System.

This tutorial will show you how to build a simple ML pipeline that automates the workflow of a deep learning image classifier for dandelions and grass built using FastAI, and then served as a web app using Starlette. We’ll use Apache AirFlow, out of the many workflow tools like Luigi, MLFlow, and KubeFlow, because it provides an extensive set of features and a beautiful UI. AirFlow is open-source software that allows you to programmatically author and schedule your workflows using a directed acyclic graph (DAG) and monitor them via the built-in Airflow user interface. At the end of the tutorial, I’ll show you further steps you can take to make your pipeline production-ready.

Requirements: Since you will run this tutorial on a VM instance, all you will need is a computer running any OS, and a Google account.

This tutorial will be broken down into the following steps:

Sign up for Google Cloud Platform and create a compute instance Pull tutorial contents from Github Overview of ML pipeline in AirFlow Install Docker & set up virtual hosts using nginx Build and run a Docker container Open Airflow UI and run ML pipeline Run deployed web app

1. Sign in to Google Cloud Platform and Create a Compute Instance

Signing up for GCP is free!

If you haven’t already, sign up for Google Cloud Platform through your Google account. You’ll have to enter your credit card, but you won’t be charged anything upon signing up. You’ll also get $300 worth of free credits that last for 12 months! If you’ve run out of credits, don’t worry — running this tutorial will cost pennies, provided you stop your VM instance afterward!

Once you’re in the console, go to Compute Engine and create an instance. Then:",https://towardsdatascience.com/10-minutes-to-building-a-machine-learning-pipeline-with-apache-airflow-53cd09268977,['Binh Phan'],2020-06-29 22:49:36.256000+00:00,483,"machine learning, ML pipeline, Apache Air Flow, MLOps, Google Cloud Platform"
"Getting into Deep Learning, what is Computer Vision? Let’s create a face recognition app!","Star Wars + Han Solo + Artificial Intelligence!

Recently, I read on an article that someone used Artificial Intelligence to insert Harrison Ford into the movie: “Solo: A Star Wars Story”. It uses deep learning artificial intelligence, that analyzes a large collection of photos of a given person, in this case, Harrison Ford, then creates a database of those images in multiple positions and poses. This technology then uses that database to intelligently perform an automatic face replacement on a source clip. Hello New World!

What started as an interest in learning Data Science, Python libraries and its algorithms to solve business problems, became a desire to understand Machine Learning, Computer Vision and recognize faces in images!

Everytime I read an article about Data Science, saw these new words to me. So I decided to start this challenge of learning more about this amazing world of Artificial Intelligence.

But what is Machine Learning?

We may not see it, but Machine Learning is in our phones, in the products we use today. Tagging people in pictures sounds familiar? Video recommendation systems? Big companies like Netflix and Facebook are using this technology in all their products. Machine Learning consists in a set of algorithms that parse data, learn from them, and then apply this knowledge to make intelligent decisions. You feed the computer with a set of rules and tasks, then it will find a way to complete those tasks.

On the other hand, Computer Vision is an Artificial Intelligence that allows machines to capture, process and analyze real world images and videos to extract meaningful and contextual information from the physical world.

Nowadays, Computer Vision is used in different fields such healthcare, agriculture, banking, automotive and industrial, among others. Computer Vision technology is helping healthcare professionals to accurately classify conditions or illnesses that may potentially save patients’ lives by reducing or eliminating inaccurate diagnoses and incorrect treatment. Farmers are beginning to adopt computer vision technology to monitor the health of crops. Banks are using image recognition applications that apply machine learning to classify, extract data, and authenticate documents.

Last night I read an article about Artificial Intelligence and it said that a school in Hangzhou, China, is using facial recognition to monitor the behavior of their students. This technology classifies the students based on their range of emotions, from antipathy to happy. The system also cross-checks the faces of all students against the school database to mark the attendance and has the ability to predict if a student is feeling sick.

So, after trying for hours, getting the right python libraries, installing the video codecs and getting some videos, I will show you the results of running face recognition on a video file.

I used a video of Brad Pitt and Jennifer Aniston. Got a picture of each one to teach the machine how to recognize them in the video:

What an incredible result! Never imagined machines could learn to do this, but the reality of Computer Vision and AI today is that machines need human help for better results.",https://medium.com/datadriveninvestor/getting-into-deep-learning-what-is-computer-vision-lets-create-a-face-recognition-app-cb7349d05d64,['Viridiana Romero Martinez'],2018-10-25 04:17:11.172000+00:00,498,"Star Wars, Han Solo, AI, Machine Learning, Computer Vision"
How air pollution is measured in Ulaanbaatar,"How air pollution is measured in Ulaanbaatar

4 different sources, 2 different methods

Air pollution is a big issue during the winter months in Ulaanbaatar, Mongolia. The effects of air pollution are quite severe, with those living in the most polluted areas suffering from lowered lung function, increased rates of respiratory infection, and shortened lifespans. In an attempt to solve this issue, the government has been focusing on the source of air pollution, the peri-urban ger districts surrounding the city center. In these areas, houses burn coal for both cooking and heat.

Beginning in May 2019, burning raw coal in Ulaanbaatar was made illegal. To replace the fuel the government coordinated the manufacture and supply of “enhanced charcoal briquettes”, which is supposed to reduce air pollution in Ulaanbaatar by 40–50%.

Along with this change in fuel, the Ministry of Environment and Tourism made a rule change in October 2018 (and implemented in February 2019) to how the air quality index (AQI) is calculated. This change has two impacts. First, physical values of pollution are no longer reported, only AQI. This means researchers and open data platforms (like openaq.org) can no longer reliably use the data reported. Second, instead of the previous used 1 hour average of air pollution being reported, a 3-hour moving average is used to calculated AQI. This change smoothes out peaks and troughs, and, as I will show later, can easily make it appear that air pollution is reducing.

Before we get into that, let’s take a look at the quite messy situation of air pollution data in Ulaanbaatar.

Data Sources

Most people following air pollution are aware of Agaar.mn, the air quality monitoring site that updates hourly showing air quality at 12 stations around the city. There are 4 different sources of air quality data in Ulaanbaatar:

Agaar.mn — part of the Ministry of Environment and Tourism — 12 stations

Tsag-agaar.gov.mn — managed by the National Agency of Meteorology and Environmental Monitoring — 4 stations

StateAir.mn — operated by the US Embassy in Ulaanbaatar — 1 station

People in Need — new air quality stations were launched in 2019 by the international NGO — 16 stations

To make things more confusing, the air quality stations run by the Mongolian government use the new reporting standard mentioned above. This means 16 stations are using the new Mongolian reporting standard and 17 stations using the US EPA reporting standard. Here is a table breaking down each station, the source, and the type of air quality monitoring each station performs.

To understand this visually, I put together an interactive map that will allow you to see how these stations are distributed. One of the biggest criticisms of the government air quality sensors is that relatively few are located in the ger district, the area that has the worst air pollution. People in Need has done an excellent job filling in the gaps in the government network.",https://medium.com/mongolian-data-stories/how-air-pollution-is-measured-in-ulaanbaatar-6ba76eec9585,['Robert Ritz'],2019-10-15 11:35:34.901000+00:00,474,"Air Pollution, Ulaanbaatar, Mongolia, AQI, Charcoal Briquettes"
Will technology save us from climate change?,"It goes without saying that I believe technological innovation has a great role to play in curbing humanity’s emissions and reversing some of the damage we’ve already done.

That’s why I do what I do, and why we started Carbon Re.

But in this rush towards technology-based solutions, we should not forget about the role of nature-based solutions, as well as the impact of technological solutions on the environment. I want to give two examples here to highlight these issues:

If whale numbers (1.3 million today) returned to pre-whaling numbers (4–5 million), this would enable the removal of 1.7 Gigaton of CO2 every year (global emissions are 51 GT/year). This is a result of the C02 absorbed by whales and, more impactfully, the growth in pythoplankton they enable. Thank you Professor John Shawe-Taylor for talking about this. You can read more about it here.

Climate Tech solutions are not risk-free for nature; they can have a substantial impact on the environment, and we need to take these into consideration. We all know that wind farms can be harmful to birds, for example, but these harms can be mitigated by good planning and designs (see this excellent article by the RSPB) and as the RSPB points out, climate change is “the single greatest long-term threat to birds and other wildlife”.

We can’t hope to save the planet from runaway climate change without new technologies, but increasingly, many have been pinning their hopes on direct air capture (DAC) solutions.

DAC plays a small but important role in the IPCC’s modelling of mitigation pathways. Whilst important, an over-reliance on DAC comes with its own challenges:

Relying on direct air capture to meet climate mitigation goals, could lead to 300 exajoules of energy being required by 2100, more than half of global demand today and a quarter of projected demand by the end of the century.

It would also require building 30,000 large-scale DAC installations. To put this in perspective, there are fewer than 10,000 coal-fired power plants stations in the world.

For more, read this excellent analysis by Carbon Brief and a comprehensive paper in Nature Communications.

So what should we do?

Focus on two things: cutting emissions at the source and restoring the natural environment.

The first is so obvious that it needs re-stating: we need to cut emissions at the source and we need to do this in the so-called ‘hard to abate’ sectors. For example, according to the International Energy Agency, construction and buildings are responsible for 39% of global CO2 emissions. But they are too often overlooked. We hear so much about wind, solar and cars as the vehicles (pardon the pun) for decarbonization but how often do we see coverage of decarbonization in cement, steel, or heating and cooling in buildings? My take: nowhere near as much as we should.

This needs to change. The only reason we would need DAC at an unimaginable industrial scale is because we do not decarbonize our biggest sources of emissions. So instead of figuring out how to remove carbon from the atmosphere in 2100, we should aim to remove emissions from sectors such as buildings, shipping and aviation quickly.

The second area of focus should be restoring the natural environment. I didn’t think nearly enough about the environmental and ecological aspect of climate change until I interviewed Will Marshall, CEO of Planet, at last year’s CogX. Will talked about an ecosystem emergency ecosystem. He also wrote about this in his Medium blog, highlighting some of the horrifying statistics:

82% of the wild animals by mass already gone… 50% of live corals already gone… 75% of freshwater ecosystems gone…

When we look at the impact of restoring whale population levels on climate mitigation, the huge potential in afforestation or reforestation, or the threat to global food security posed by declining bee populations, it becomes evident that nature is as at least as important as technology to our climate problem, if not more so.

Until recently, it was generally assumed that people and businesses would be resistant to the short term costs of decarbonization as the impact would only be felt generations later. The idea that there won’t be much suffering from climate change for decades now looks increasingly foolish. The heat dome that has caused so much misery and deaths in Western Canada and the Western USA is one of numerous yearly reminders that the planet’s climate is already dangerously disrupted. Scientists used to be reluctant to assign specific events to climate change; this is rapidly changing.

We’re already late, let’s at least show up.",https://medium.com/@carbonre/will-technology-save-us-from-climate-change-5f83ad24de49,['Carbon Re'],2021-07-06 06:44:27.823000+00:00,745,"Climate Change, Nature Based Solutions, Direct Air Capture, Decarbonization, Ecosystem Emergency"
An Overview of Building End-To-End Big Data Reporting & Analytics Systems,"In the 21st century, it’s not a hidden secret that Big Data is driving business and leading to growth and development across different industries such as e-commerce, health, finance, etc., in an unprecedented way. We all have heard the catchphrase “Data is Gold,” to not to leave behind the competitors’ industries and corporations are capturing and storing as much data as they can. Big data reporting is captured by businesses & enterprises across different streams such as transactional data, browsing data, users’ interactions data, and through different touchpoints i.e. web, mobile & other smart devices, etc.

Building pipelines to collect, record, & collate information is a first step towards building scalable analytics & intelligent big data reporting processing systems. Once the data is stored, the next main complex task is to derive insights and perform analytics over these huge piles of data being collected on a regular basis. The typical workflow in major industries & corporations for big data reporting involves having dedicated teams for performing analytics and reporting the information stored in structured format e.g. SQL tables. The main challenge faced today is the rate and the scale at which the data is captured and stored periodically. This alarming rate of data arrival also pushes for the rate & the necessity to dive deeper into deriving insights to make effective business decisions. To handle the scale & rate of information and to dive deeper into deriving insights instantly, the major question that needs to be addressed is: Can we build effective, scalable industry-grade end-to-end big data systems to perform automatic reporting and descriptive analytics over the captured data?

This problem of building an automatic End-to-End system with big data reporting has been a topic of interest in the research community and has been an area of active research under the theme of Natural Language Interfaces to Database [NLIDB], with research papers dating back to 1980s [1]. A rough abstraction of such an automatic NLIDB system is shown in the image below [Figure 1]. There are two main aspects of these NLIDB systems:

Converting Natural Language (NL) to Structured Query Language (SQL)

Fetching results from Databases (DB) using the structured query

Figure1: Image Source [Soumya, MD and Patil, BA, 2017]

One of the main complex tasks of building such an automated system for big data reporting is to understand Natural Language queries effectively which is an open area of active research. Once we have converted Natural Language to a Structured Query Language, the task becomes easier to execute the SQL statement, fetch, & present results.

Natural Language Understanding: Human language is quite complex and ambiguous, sometimes it becomes hard to understand the other person in communication, making machines understand natural language is a daunting task.

Let’s look at a simple example to show the ambiguous nature of human language, e.g. “ I saw a girl with a telescope.” This sentence with the same set of words, without any change, can have two different interpretations:

First Interpretation: I saw a girl, using a telescope I had.

Second Interpretation: I saw a girl who had a telescope.

Image Source: Link

Thus aiming to convert a free-flowing natural language to a structured query language, where we can have multiple variations of the input query to represent a single SQL statement is a complex challenge.

The main questions that need to be addressed while building such a scalable industry-grade reporting and descriptive system are:

How to build a robust & comprehensive pipeline to capture the nuances effectively for an industrial use-case setting?

How to handle Natural Language Queries effectively?

If some of these questions interest you, and you want to learn more on it: check out our upcoming talk at ODSC India 2020 during my talk, “Natural Language Querying for Industry Grade Data Analytics Systems.”

For more information on the topic, you can check information in the suggested reading material or contact the author.

References

1. C. Raymond Perrault, and Barbara J. Grosz. “Natural-language interfaces.” Exploring Artificial Intelligence. Morgan Kaufmann, 1988. 133–172.

2. Soumya, M. D., and B. A. Patil. “An interactive interface for natural language query processing to database using semantic grammar.” International Journal of Advance Research, Ideas and Innovations in Technology 3, 2017. 193–198.

3. Salil Rajeev Joshi, Bharath Venkatesh, Dawn Thomas, Yue Jiao, and Shourya Roy. “A Natural Language and Interactive End-to-End Querying and Reporting System.” In Proceedings of the 7th ACM IKDD CoDS and 25th COMAD, pp. 261–267. 2020.",https://medium.com/@ODSC/an-overview-of-building-end-to-end-big-data-reporting-analytics-systems-98640c606206,['Odsc - Open Data Science'],2020-12-10 12:02:46.664000+00:00,718,"Big Data, Data Analytics, NLIDB, Natural Language Processing, Machine Learning"
What I learnt from giving 120+ Data Science Presentations,"What I learnt from giving 120+ Data Science Presentations

6 points to help navigate the world and daily technical practices to become better data communicators

Photo by Anggi Nurjaman on Unsplash

Reflecting back on the various roles I have had in a Data Science team, it has occurred to me that I am one of the few people who is often asked to present.

From a whole host of meeting invites, to slides in various Teams channels and countless versions of presentations on my laptop, I realised that I had presented more than 120 versions of 18 projects over the last six years — that’s roughly one presentation every three weeks! Through this, I can safely say that I have sat through and given presentations that did not land well with various audiences.

One would argue that this many presentations were unnecessary for technically apt stakeholders in a data literate organisation, but these individuals are few and far between where I work.

Giphy — Come through, Karamo 👏🏽👏🏽👏🏽

Many of my presentations have been time-consuming and cumbersome to prepare, but they have been invaluable in my attempt to become a better Data Science communicator. Regardless, I am thankful of my introspective moments which have led to confidence in my content and communication skills.

To date, being a keynote speaker and attending a panel at Financial Times Energy Summit on ML in the energy industry has been a highlight. Whilst I have so much more to learn, I now know that I can drive an engaging conversation with those who are listening.

So, here are six points that I think are valuable and some resources I have used when considering presenting my projects.

1. Create a narrative

Regardless of the project you are presenting, whether it is explorative, a production ready solution or research, a narrative is necessary to coherently articulate the progress made. Whilst there are many approaches to this, there are two crucial components which are a must.

Context

Most of the time, our projects won’t make sense when they land in front of us. We have to painstakingly construct models, insights and outcomes from mediocre data. To make sense we dissect and explore parts of data over and over to find meaning within the bigger picture. Essentially, we become scientists.

Fortunately for our audience, we are in a position to frame our data within a bigger picture and to become the story-teller. We can include history, similar trends and approaches others may have taken. We can use visualisations from our explorative stages to describe potential challenges, silver linings and our inspirations.

Whether you present to very specific teams or bigger, diverse groups, a tailored presentation with more relevant facts and background is beneficial. I tend to structure my slides following a fairly high-level narrative and draw attention to the detailed slides when necessary. Doing this allows the audience to remain focused on the challenge and the solution, whilst I help them appreciate the progress I have made.

Timeline

It is important to stick to a clear and organised timeline (preferably linear) — Data Science stories shouldn’t feel like a Christopher Nolan film.

I have often seen presentations start instantly with an outcome and solution. Whilst these are impactful, they also lead to a lot of “what does that mean?” moments, which can potentially alienate your audience.

A strong start is essentially your background and the context you provide. A valuable middle section would include a concise explanation of your method, findings, insights and necessary information to help the audience understand what was done. At the end, you should wrap up with model outcomes, potential improvements, use and challenges with data, strategies to production and wider impact if the project was to continue.

If you are able to, make room for questions. This gives the audience the opportunity to ‘dive deeper’ into the content and anything they have become particularly interested with.

2. Don’t make the audience listen and read.

Put yourself in the shoes of a stakeholder sitting through a ML slide deck, trying to listen to an expert, read their blurb on the slide and understand the value their work has on a business domain. This is a lot to juggle.

Instead, limit the amount of text to few bullet points and statements per slide to avoid cramming text. There are number of steps you can take to make concise and clear points, articulated well by Guy Kawasaki — 10/20/30 rule which provides an excellent template for pitching. Though it doesn’t translate directly for ML presentations, much of it is applicable.

3. Visualisations are important. If you are making charts, use the right visual tools.

This is probably the most important point for me, however, it is also the hardest to achieve. These principles may be easy to discuss but are much harder to execute, consistently.",https://towardsdatascience.com/what-i-learnt-from-giving-120-data-science-presentations-ce18b16cf334,['Das Wijesundera'],2020-10-20 20:32:37.540000+00:00,784,"Data visualisations are key to communicating complex ideas and models. It is important to use the right type of chart for the right data. This will help you explain your findings and outcomes, with clarity and precision.I have used tools such as Tableau, PowerBI, Matplot"
Demystifying AI/ML Microservice With TensorFlow,"This tutorial will walk you through how to build and deploy a sample microservice application using the Red Hat OpenShift Container Platform. The scope of this service is to perform prediction of handwritten digit from 0 to 9 by accepting an image pixel array as a parameter. This sample application also provides an HTML file that offers you a canvas to draw your number and convert it into an image pixel array. The intention of this article is to give you a high-level idea, the same concept can be taken to the next level for addressing many other complex use cases.

Image recognition is one of the major capabilities of deep learning. In this article, we will be identifying our own handwritten digit. However, in order to accurately predict what digit is it, learning has to be performed, so that it understands the different characteristics of each digit as well as the subtle variations in writing the same digit. Thus, we need to train the model with a dataset of labelled handwritten digit. This is where MNIST dataset comes handy.

Sample dataset

MNIST dataset is comprised of 60,000 small 28x28 square pixel gray scale images and 10,000 test images. These are handwritten single digit images from 0 to 9. Instead of downloading it manually, we can download it using Tensorflow Keras API

We will be performing following steps in this exercise

Build and train the model using MNIST dataset Saving the learned model in persistent storage Build and launch microservice for prediction using the provided dockerfile Generate image pixel array of your handwritten digit using the provided HTML file Perform prediction using microservice by providing image pixel array as a parameter to this service.

You can find the dockerfile and python code by clicking the following git repository.

https://github.com/mafzal786/tensorflow-microservice.git

Model Training

Below is the python code that performs building, compiling, training, and saving the model based on MNIST dataset. This code can also be executed in Jupyter Notebook. It is important to save this model in a persistent storage once the model training is successsfully completed so that it can be used by the microservice application for prediction. Also make sure that saved model file should be accessible by the container running the microservice.

Prediction microservice with Flask

Following instructions will help you building the microservice using Red Hat OpenShift Container Platform. For the sake of simplicity, it’s a single source file and written in python. This source file is copied inside the container image via Dockerfile during the container image build process, which will be discussed later in this article. The sole purposes of this microservice to is to predict the handwritten digit using the already learned model. It simply takes the image pixel array as a parameter and predict it.

Dockerfile

Below is the dockerfile that will be used to initiate the image build and launch the container in OpenShift platform. requirements.txt file contains the pre-requisite packages for this microservice to work. requirements.txt file contains the following.

Flask==0.12.1 tensorflow==2.3.0 scikit-learn==0.22.1

Launch the microservice

Now login to your Red Hat OpenShift cluster. Click Developer and then click Add to create an application. Click “From Dockerfile”. This will import your dockerfile from your git repo and initiate the image build and deploy the container.

Supply the Git Repo URL. The dockerfile for this project is located at https://github.com/mafzal786/tensorflow-microservice.git. Also give name to your application. Click create.

This will create the build config and build will start. To view the logs for your build, click “Builds” under Administrator tab and click the build as shown below. Once the build is completed, container image is pushed to your configured image registry.

After the build is completed and image is pushed to the registry, OpenShift will launch the container.

OpenShift will create a route for this service to be exposed by giving an externally reachable hostname. Service end point will be available in Networking →Routes tab. Clicking on the Location as shown below will launch the microservice in the browser.

Below figure shows when microservice is launched in the browser.

Or run the following CLI as below

# oc get routes/ms-handwritten-digit-prediction-git

Canvas for writing digit

Below html file offers you a canvas to draw digit. Copy below code and save it as html on your computer and then launch it in the browser. Click “Convert” button to convert the image into image pixel array. Then click “Copy Image Pixel Array in Clipboard” button to copy the pixel array in your clipboard for providing it to the microservice as a parameter for prediction.

Draw Your Number

Perform prediction using the microservice

Now pass this image pixel array to the microservice as a parameter as show below.



<h1>Your handwritten digit is: 3</h1> # curl http://`oc get routes/ms-handwritten-digit-prediction-git — template=’{{.spec.host}}’`/predict?pixelarray=0,0,0,0,0,0,0.03137254901960784,0.5764705882352941,1,1,1,1,1,1,0.17254901960784313,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.011764705882352941,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.984313725490196,0.07450980392156863,0,0,0,0,0,0,0,0,0,0,1,1,1,0.6196078431372549,0,0,0,0,0,0,0,0,0,0,0,0.8980392156862745,1,1,1,1,0,0,0,0,0,0,0,0,0.48627450980392156,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.8470588235294118,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0784313725490196,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.07058823529411765,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.7098039215686275,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.7568627450980392,1,0.7843137254901961,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0.3176470588235294,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.4470588235294118,1,1,1,0,0,0,0,0,0,0,0,0,0.8980392156862745,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.03137254901960784,0,0,0,0,0,0,0,0,0,0,0,0,0,0.06274509803921569,0.07058823529411765,0.07058823529411765,0.803921568627451,1,1,0.8941176470588236,0.07058823529411765,0.06666666666666667,0,0,0,0.0392156862745098,1,1,1,0.5098039215686274,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.9254901960784314,1,0.6901960784313725,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5254901960784314,1,0.07450980392156863,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.9254901960784314,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.3803921568627451,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.24705882352941178,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.34509803921568627,1,0,0,0,0,0,0.9803921568627451,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0.3686274509803922,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0.9882352941176471,1,1,1,1,0.6784313725490196,0.08235294117647059,0,0,0,0,0,0.4235294117647059,1,1,1,1,1,1,1,0.3333333333333333,0,0,0,0,0,0,0,0,0,0.10588235294117647,0.8117647058823529,1,1,1,1,1,1,1,1,1,1,1,1,0.21568627450980393,0.12941176470588237,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 Your handwritten digit is: 3

You can run the same in the browser as follows.",https://medium.com/swlh/demystifying-ai-ml-microservice-with-tensorflow-cb640c829385,['Afzal Muhammad'],2020-12-03 16:59:22.808000+00:00,779,"http://`oc get routes/ms-handwritten-digit-prediction-git — template=’{{.spec.host}}’`/predict?pixelarray=0, 00, 00, 00, 00"
Data Traffic Control with Apache Airflow,"by Nicolas GOLL PERRIER (Data Engineer)

Leboncoin is the 7th most accessed website in France; this web traffic activity generates a lot of different events that need to be collected, transformed, reconciled, analysed and understood in order to fully grasp our customers and market, and improve upon our classified ads services.

In order to handle the diversity, volume, and complexity of events, we build workflows to ensure that our data is processed in an orderly and coherent way.

A good analogy for this job is the flight traffic control at the core of any airport. Each transiting dataset is like a plane carrying valuable information requiring to go from point A to point B. Our responsibility is to ensure on time and accurate transit of those “planes”, but we can neither control the “weather”, nor the behavior of those precious “passengers”.

So we need to be vigilant and establish the best flight paths for the streams and batches of data we ingest and transform, re-shuffle the priorities, stall some in-flight datasets at a certain “altitude” (postponing non critical information availability), while re-allocating “fuel reserves” (CPU, Memory, Bandwidth…) depending on the actual and current business priorities of the company.

From Airstrip to Airport

In the beginning, we were able to somewhat cope with the growth of the site with traditional Business Intelligence tools and practices (good old Extract Transform Load tools, cron-based orchestration, limited dashboards…). But as Leboncoin kept growing, we had to adapt our data-operations accordingly.

Therefore, we had to completely rethink our data architecture several years ago to:

Scale our data pipelines to process workloads of up to several terabytes a day efficiently

Adapt to the platform’s technological and organisational shift to a micro-service architecture

Quickly detect failures and inconsistencies in the many data processes run each day/hour

Respond to internal or external faults without impacting the quality, conformity, and availability of actionable information to our business users

Flying through the data-hurricane

Nowadays, we have different kinds of data processing batches running every hour of the day: Spark transformations, Kafka topics S3 archiving processes, Data-Warehouse loads, dataset indexing, business extracts, regulatory extracts, Machine Learning training and predictions… Too many to manually ensure that all our data stores are properly up to date and missing no data-points.

Moreover, data engineering and analytics are downstream the information system. Every incident, failure, bug, inconsistency will at some point end-up in our pipelines: so some input dataset may have changed its format without notice, a third-party data-source might become unavailable during the night, a real-life event triggered an unexpected increase in a specific customer activity, which produces more events and takes longer to process…

An information system is a living organism. We might prevent some of those things from happening, and we have contingency plans, but we can’t predict everything, especially in a quickly growing and evolving organisation.

So when dealing with our large set of data transformations, we need to guarantee to our customers, both internal and external, that those transformation will run consistently and quickly enough to provide relevant, accurate, and up to date insights. And if something were to fail, quickly assess the impact of the failure on all depending systems, reports, and dashboards.

Several tools such as Amazon S3, Apache Spark or Redshift have helped us tackle most the scaling issues, but as data processing workflows kept accumulating and were getting more and more complex, we needed a proper way to ensure that those ran like clockwork. And we were fortunate enough to try on Apache Airflow early on, just after it was open-sourced.

Airflow: Pre-flight checklist

Airflow has been developed with data-engineering challenges in mind. It is above all a DAG (Directed Acyclic Graph, a fancy way of saying “a workflow that does not loop”) management platform, or a worfklow orchestrator if you prefer.

Airflow

Airflow enables you to define, schedule, run, monitor and manage your workflows in pure Python code, while also providing the tools and UI to handle those workflow operations

It provides:

Retry mechanisms to ensure that each and every anomaly can be detected, and automatically or manually healed over time (with as little human intervention as possible)

to ensure that each and every anomaly can be detected, and automatically or manually healed over time (with as little human intervention as possible) Priority aware work queue management , ensuring that the most important tasks are run first and complete as soon as possible

, ensuring that the most important tasks are run first and complete as soon as possible Resource pooling system to ensure that, in a high concurrency environment, thresholds can be set to avoid overloading input or output systems

system to ensure that, in a high concurrency environment, thresholds can be set to avoid overloading input or output systems Backfill capabilities to identify “missing” past runs, and automatically re-create and run them

capabilities to identify “missing” past runs, and automatically re-create and run them Full history of metrics and statistics to view the evolution of each task performance over time, and even assess data-delivery SLAs over time

to view the evolution of each task performance over time, and even assess data-delivery SLAs over time An horizontally scalable set of alternatives to the way tasks are dispatched and run on a distributed infrastructure

set of alternatives to the way tasks are dispatched and run on a distributed infrastructure A centralized, secure place to store and view logs and configuration parameters for all task runs

Fig 1 — Example representation of a DAG in Airflow

All these features allow us to run more than 10,000 automated tasks each day without breaking a sweat. Even in the case of a major failure, recovery requires very little human labor, as the system eventually heals itself automatically.

Airflow’s architecture relies on several components, and a few auxiliary tools:",https://medium.com/leboncoin-engineering-blog/data-traffic-control-with-apache-airflow-ab8fd3fc8638,[],2020-07-09 14:07:41.961000+00:00,938,"Data Engineering, Airflow, Data Pipeline, Data Architecture, Business Intelligence"
Building Chatbots using Python — Part 0,"Creating a personality

Creating an engaging personality is a critical part of chatbot development. It’s one of the key differences as compared to any other kind of software.

Why creating a personality for the Chatbot is so essential? — Let us suppose, with our Chatbot, all we could do was type precise instructions to our bot. We would just have a command-line application and not a chatbot.

Most chatbots are rooted in a messaging app that people are comfortable using to talk to their friends. And we can expect that the users of our will want to make a bit of smalltalk before trying out any functionality that they came for. It’s not much effort to code up some responses to common questions and is worth it for the improved user experience.

Smalltalk and Including variables

# Define variables

name = ""Shrusti""

weather = ""sunny"" # Define a dictionary with the predefined responses responses = { ""what's your name?"": ""my name is {0}"".format(name), ""what's today's weather?"": ""the weather is {0}"".format(weather), ""default"": ""default message"" } # Return the matching response if there is one, default otherwise def respond(message): # Check if the message is in the responses if message in responses: # Return the matching message bot_message = responses[message] else: # Return the ""default"" message bot_message = responses[""default""] return bot_message

The simplest thing we can do is use a python dictionary, with user messages as the keys and responses as the values. For example, say we define a dictionary called ‘responses,’ with the messages “what’s your name?” and “what’s today’s weather” as keys, and suitable responses as values. Next, we define a function called ‘respond,’ which accepts a message as a single argument. This function tests if a message has a defined response by using the ‘in’ keyword, that is, “if message in responses.” This statement only returns ‘True’ if the message corresponds to one of the dictionary’s keys. Notice that this will only work if the user’s message exactly matches a key in the dictionary. In later parts of this blog, we will build much more robust solutions. Notice that if there isn’t a matching message, the ‘return’ keyword will never be reached so that the function will return None.

Since the world outside is always changing, our bot’s answers have to be able to as well. The first thing that we can do is add some placeholders to the responses. For example, instead of “The weather is sunny,” you can have a template string, like “it’s {} today.” Then later, you can insert a variable ‘weather_today’ by calling format weather today.

Choosing responses

# Import the random module

import random name = ""Shrusti""

weather = ""sunny"" # Define a dictionary containing a list of responses for each message responses = { ""what's your name?"": [ ""my name is {0}"".format(name), ""they call me {0}"".format(name), ""I go by {0}"".format(name) ], ""what's today's weather?"": [ ""the weather is {0}"".format(weather), ""it's {0} today"".format(weather) ], ""default"": [""default message""] }

# Use random.choice() to choose a matching response def respond(message): if message in responses: bot_message = random.choice(responses[message]) else: bot_message = random.choice(responses[""default""]) return bot_message

It gets dull hearing the same responses over and over again, so it’s an excellent idea to add a little variety! To return completely different responses, we can replace the values in the responses dictionary with lists. Then when we are choosing a response, we can randomly select an answer from the appropriate list. To do this, import random, and use the ‘random.choice’ function, passing the list of options as an argument. For now, we are still relying on the user message matching our predefined messages exactly, but we’ll soon move to a more robust approach.

Asking questions

import random responses = { ""question"": [ ""I don't know :("", ""you tell me!"" ], ""statement"": [ ""can you back that up?"", ""tell me more!"", ""oh wow!"", "":)"", ""how long have you felt this way?"", ""I find that extremely interesting"", ""why do you think that?"" ] }

def respond(message): # Check for a question mark if message.endswith(""?""): # Return a random question return random.choice(responses[""question""]) # Return a random statement return random.choice(responses[""statement""]) # Send messages ending in a question mark send_message(""what's today's weather?"") send_message(""what's today's weather?"")

# Send messages which don't end with a question mark send_message(""I love building chatbots"") send_message(""I love building chatbots"")

output

A great way to keep users engaged is to ask them questions or invite them to go into more detail. This was actually one of the things which made the ELIZA bot, so fun to talk to. Instead of using a bland default message like “I’m sorry, I didn’t understand you,” you can use some phrases that invite further conversation. Questions are a great way to achieve this. “Why do you think that?”, “How long have you felt this way?”, and “Tell me more!” are appropriate responses to many different kinds of message, and even when they don’t quite match are more entertaining than a boring fallback",https://medium.com/@shrustighela/building-chatbots-using-python-part-i-84bf34b29cad,['Shrusti Ghela'],2020-12-11 16:52:51.630000+00:00,806,"Chatbot development, Creating a Personality, Smalltalk, Variables, Choosing Responses"
The Best Books for Machine Learning Beginners,"The Best Books for Machine Learning Beginners

4 of the Best Machine Learning books out there and why they will make you the next big Data Scientist.

Machine Learning is the hottest topic currently in the atmosphere of data science. With it’s unique applications, start ups and larger companies are hiring more and more data scientists to implement these models to get a better understanding of their business and their customers.

For anyone that is getting involved in data science, Data Scientist is the career job we all dream of but you have to know your stuff, and that starts with understanding the concepts of Machine Learning!!

Therefore I’m going to give you the 4 best Machine Learning books to read if your a beginner and looking to become a Data Scientist in the future, or just interested to learn more about the topic.

Photo by Kimberly Farmer from Unsplash

1. Introduction to Machine Learning with Python: A Guide for Data Scientists.

If your just getting started with Machine Learning this is a must read. It focuses mostly on the Scikit-Learn library with an in-depth tour of some of the most useful methods in Machine Learning— classifying, regression, a bit of clustering, PCA, and all the different ways to measure the outcome of your model. Introduction to Machine Learning with Python is easy to understand and will explain thoroughly all the necessary steps to create a successful machine-learning application with Python.

The Unanimous book to read for those starting machine learning.

Link: https://amzn.to/3b6ygSZ

2. The Hundred Page Machine Learning Book

The quintessential book for those looking to learn machine learning fast. This book can be read in one night and has all the information you would need to create your own models with machine learning. It is clear, concise, and probably the best machine learning book I've read with respect to number of pages and quality of content.

Link: https://amzn.to/3fqMr8y

3. Python Machine Learning

This is a fantastic introductory book in machine learning with python. It provides enough background about the theory of each (covered) technique followed by its python code. One nice thing about the book is that it starts implementing Neural Networks from the scratch, providing the reader the chance of truly understanding the key underlaying techniques such as back-propagation. Even further, the book presents an efficient (and professional) way of coding in python, key to data science.

I strongly recommend it to those with a moderate level of understanding of machine learning principals and python.

Link: https://amzn.to/2L604fM

4. Hands-On Machine Learning with Scikit-Learn and TensorFlow

The author Aurelien Geron does a great job with explaining different concepts with the prime focus on the practical implementation of Scikit-Learn and TensorFlow. The book is split into two, with the first half covering Scikit-Learn, it is a good mix of practical with theoretical. The Scikit-Learn section is a great reference and has nice detailed explanations with good references for further reading to deepen your knowledge. The second half dives deep into deep learning with TensorFlow, the next step to understanding Machine Learning to its fullest. Deep learning is explained using the easy to learn Keras library with the combination and power from TensorFlow.

Link: https://amzn.to/3fq1foc",https://towardsdatascience.com/the-best-books-for-machine-learning-beginners-b2317d1ee27c,['Christopher Zita'],2020-05-08 17:23:11.001000+00:00,516,"Machine Learning, Data Science, Scikit-Learn, Python, Tensor Flow"
How to run Spark/Scala code in Jupyter Notebook,"Photo by Ilya Pavlov on Unsplash

Jupyter Notebook is the most widely used tool for putting code and text together in Data Science World. It’s a great tool for practicing data analysis and performing machine learning techniques in Python. This is a must for every Data Analyst and Data Scientist.

Apart from the data analyst and data scientist, this tool can also be useful for a data engineer. If you don’t know about Data Engineering, It’s a way of collecting/transforming data into a common data lake from where Data Analyst and Data Scientist can use this data to perform data analytics.

In the Big Data domain, there are various tools and technologies we use to collect and process the data. Apache Spark is one of the frameworks which allows distributed computing on a large data scale.

Apache Spark is a data processing framework that can quickly perform processing tasks on very large data sets, and can also distribute data processing tasks across multiple computers, either on its own or in tandem with other distributed computing tools.

Apache Spark code can be written in the following 3 languages — Java, Scala, and Python. Scala is the most preferred programming language for Apache Spark as Spark itself is written in Scala.

Hence in this article, I am going to tell you how you can use the same Jupyter notebook to write Spark code in Scala language.

Step 1: Install the package

pip install spylon-kernel

Step 2: Create a kernel (scala)

This will allow us to select scala kernel in the notebook

python -m spylon_kernel install

Step 3: Install and use Jupyter Notebook

pip install jupyter

And in the notebook, we select New -> spylon-kernel. This will start our scala kernel.

However, for this to work, you need to make sure that SPARK_HOME is set.

To set the SPARK_HOME in linux/mac OS, put the below statements in the home .bash_profile file.

export SPARK_HOME=/Users/vishalmishra/spark-3.0.1-bin-hadoop2.7

cd ~ vi .bash_profile",https://vishalmishra2k20.medium.com/how-to-run-spark-code-in-jupyter-notebook-using-scala-as-a-language-3d2cdcbce3de,['Vishal Mishra'],2020-11-10 00:28:15.974000+00:00,306,"Jupyter Notebook, Data Science, Data Analysis, Machine Learning, Apache Spark"
A Dialogue on Quantum Computing,"We are in the middle of a quantum computing race. China’s investment in this technology is believed to be worth $10 billion, and European Union has launched a €1 billion ($1.1 billion) quantum master plan.

In the United States, President Donald Trump signed the National Quantum Initiative Act in December 2018 to provide $1.2 billion to promote quantum information science over a five-year period.

But within the academic world there is an ongoing debate regarding the future of this technology. After Galileo, let’s call the proponents of the two main opposing positions Simplicio the physicist and Salviati the engineer, with Sagredo as the neutral layperson who has interest in Mullah Nasruddin stories.

Simplicio: Quantum computing will be one of the key technologies that decide which nation gets to be the leading technological power of the twenty-first century. Quantum computers operate in a mode where several pieces of information are simultaneously processed, making them faster than current-day machines. When built, quantum machines will provide an edge in code-breaking and the solution of certain optimization problems.

Juggling pins on top of an inverted pyramid

Salviati:While the mathematics at the basis of quantum algorithms is fine, there are practical engineering reasons why such computers will never be built.

Unlike a classical computer which uses 0s and Is (these could be charges or voltages on different points in a circuit), a quantum computer processes states that may be seen as waves together with their phases. Like waves, quantum bits (qubits) exist in superposition.

The quantum waves interact with the environment and the computer can hopelessly decohere in a fraction of a second and so the computation must be completed in very quick time, placing extreme constraints on the system. Without correction of random errors that are inevitable in any physical system the results will be worthless.

Information is physical [1] and the qubits themselves are evolving. One could say that the loading of the information on the quantum computer is like balancing an inverted pyramid, and this balancing is to be done with objects that are themselves flying about in all different directions. The juggler must mount this pyramid and continue juggling the pins while ensuring he doesn’t fall down.

Furthermore, a computing machine must correct all small random errors for the computation to be correct. But not all qubit errors can be corrected [2]. Thus errors within the encoder, introduced before the encoding entanglements are introduced, cannot be corrected.

Sagredo: Believe in the future of technology. Remember Archimedes said: Give me a lever and a place to stand, and I shall move the Earth.

Simplicio: NASA, Google, IBM, and other organizations are testing quantum computing systems with around 20 qubits. I know that is far behind the milepost of 50 qubits that was set up by DARPA for 2012. But one must be patient.

Salviati:I read this assessment by Michel Dyakonov in IEEE Spectrum [3]: “There is a tremendous gap between the rudimentary but very hard experiments that have been carried out with a few qubits and the extremely developed quantum-computing theory, which relies on manipulating thousands to millions of qubits to calculate anything useful. That gap is not likely to be closed anytime soon.”

He added: “Such a computer would have to be able to manipulate — on a microscopic level and with enormous precision — a physical system characterized by an unimaginably huge set of parameters, each of which can take on a continuous range of values. Could we ever learn to control the more than 10³⁰⁰ continuously variable parameters defining the quantum state of such a system? My answer is simple. No, never.”

Sagredo: Once Mullah Nasruddin promised his king that he can teach his donkey to read. He said all that he needed was 1,000 dinars and ten years. The king agreed but insisted that if the donkey failed the test, Nasrullah would be put to death. When he reached home with the money, his wife was furious. Relax, Nasruddin told his wife, how does anyone know that a donkey cannot be taught to read. Furthermore, even if a donkey can’t be taught remember that ten years is a long time. During that period, the king may be dead, the donkey may be dead, or he may be dead. If they are not dead and if they are unable to teach the donkey, they can flee the kingdom in the tenth year.

Simplicio: Look for the new unexpected research findings that will be obtained as a side result of the quantum computing initiative.

Quantum cryptography

Simplicio: Quantum cryptography is a breakthrough technology. Two users can exchange keys without anyone able to break the system. Surely, this is a fundamental advance of the greatest importance both for civilian communications and military applications [4]. And isn’t it great that such key exchange can be done using wireless between satellites and with naval ships?

Salviati: But quantum cryptography is used only for key distribution (QKD). First, the two parties must be authenticated, which can only be done classically because the world of conscious agents is classical [5]. Since a chain is only as strong as its weakest link, there is no point doing QKD, for the authentication process itself can be used to exchange keys.

Simplicio: We cannot yield the leadership in this technology to competitors who don’t have commitment to democracy and open society.

Sagredo: One day Mullah Nasruddin was in a mischievous mood. He assembled a crowd, and told them a made-up story about gold in the abandoned house beyond the village. When everybody ran to get their hands on the gold, Nasruddin started running with them. When asked by his friends why he was doing so, he said “So many people believe it, that I think it may be true!”

References

[1] R. Landuaer, The physical nature of information. Physics Letters A 217: 188–193, 1996.

[2] S. Kak, General qubit errors cannot be corrected. Information Sciences 152:195–202, 2003.

[3] M. Dyakonov, The case against quantum computing. IEEE Spectrum. 15 November 2018. https://spectrum.ieee.org/computing/hardware/the-case-against-quantum-computing

[4] D. Cardinal, Quantum cryptography demystified: How it works in plain language. Extreme Tech. March 11, 2019.

[5] S. Kak, Epistemic view of quantum communication. In Quantum Foundations, Probability and Information, A. Khrennikov and B. Toni (editors). Springer-Verlag International, pp. 119–128, 2018.",https://medium.com/@subhashkak/a-dialogue-on-quantum-computing-4a4e325fdd2f,['Subhash Kak'],2019-10-14 01:38:20.791000+00:00,1019,"Quantum Computing, Race, China, Europe, United States"
Does Facebook Facial Recognition Violate the Law? Class Action Suit Moves Forward,"The case against Facebook for use of biometric data is moving forward. An Illinois judge dismissed Facebook’s plea to toss it. The class action suit –ispotentially worth billions if successful. Illinois law can tag each violation with fines up to $5,000.

Illinois has a law called the Biometric Information Privacy Act. In short, the Act says businesses must get prior consent before collecting biometric data. Proponents argue that Facebook violates Illinois law by using facial recognition for photos.

If you are a Facebook user, you’ve no doubt had the experience where you go to post a picture and it somehow knows who is in it and suggests their name. The social network is accessing and storing the biometric data to accomplish this, which the suit says violates the Illinois law.

One of Facebook’s claims is that there has been no allegation of harm in the way it uses the data. Last month, however, an Illinois Appellate Court reversed a lower court ruling on a similar issue. A tanning salon in Illinois used fingerprint data and biometric identifiers for users to gain entry to the salon. The suit, Sekura v. Krishna Schaumburg Tan, Inc., claims the company stored the data, failed to provide disclosures and required written release, and disclosed the fingerprint information to a third-party vendor. The lower court had dismissed the suit on the grounds that there was no provable harm. The appellate court held that you can sue for a violation of the law without proving additional harm.

The Internet Association — which represents a range of some of the biggest names in tech — filed a friend-of-the-court brief supporting Facebook. The organization worries about the chilling effect the case could have on future tech developments.

“Biometric technologies serve many useful purposes in our society — from providing new authentication features that enhance security (like the ability to unlock one’s phone with a fingerprint), to facilitating the organization and sharing of photographs (like the ability to quickly retrieve photographs stored in a private account), to promoting health and wellness (like allowing for digital patient check-ins at the emergency room)…These are applications that millions of people already enjoy, and that offer great potential in the future. It is in no one’s interest that the lawful development and use of biometric technologies be artificially chilled.” — The Internet Association in its court brief

In December, Facebook announced that it will notify users if someone uploads a photo with you in it (whether the user tags you or not). That gives users the option of tagging it themselves, refuse to be tagged, or report the photo. That’s still not prior notice in this case, though, because the facial recognition used to identify you had already occurred prior to the consent.

How To Disable The Tech

If you want to stop Facebook from looking for you with facial recognition, The Verge has a good article on the steps you need to take to disable the tech.

Facebook can also gather additional data from photos that are uploaded, according to PetaPixel:

location from geotag data

date

device ID of your phone

cellular/Internet service provider nearby Wi-Fi Beacons/cell towers (which can be used to triangulate locations)

battery level

cell signal strength

A portion of Illinois’ Biometric Information Privacy Act:

No private entity may collect, capture, purchase, receive through trade, or otherwise obtain a person’s or a customer’s biometric identifier or biometric information, unless it first: (1) informs the subject or the subject’s legally authorized representative in writing that a biometric identifier or biometric information is being collected or stored; (2) informs the subject or the subject’s legally authorized representative in writing of the specific purpose and length of term for which a biometric identifier or biometric information is being collected, stored, and used; and (3) receives a written release executed by the subject of the biometric identifier or biometric information or the subject’s legally authorized representative. No private entity in possession of a biometric identifier or biometric information may sell, lease, trade, or otherwise profit from a person’s or a customer’s biometric identifier or biometric information.

You can read the initial ruling as well as arguments by both sides here.

Tagging Image By Jimmy answering questions.jpg: Wikimania2009 Beatrice Murchderivative work: Sylenius (Jimmy answering questions.jpg) [CC BY 2.0 (https://creativecommons.org/licenses/by/2.0) or CC BY 3.0 (https://creativecommons.org/licenses/by/3.0)], via Wikimedia Commons",https://medium.com/digital-vault/does-facebook-facial-recognition-violate-the-law-class-action-suit-moves-forward-8ba1a97f8bfd,['Paul Dughi'],2018-10-18 16:15:17.164000+00:00,703,"biometric data, Facebook, Illinois Biometric Information Privacy Act, facial recognition, class action lawsuit"
Predicting IPL-2020 Winner,"Classification and Regression are the two branches of Supervised Learning in the field of Machine Learning. These are the basic topics that one should learn when starting their journey with Machine Learning. Doing projects is the only way through which one can learn and master these topics.

In my previous blog, I did a classification project. Having learnt some of the concepts in Classification, the time has arrived for me to make my hands dirty with Regression.

You can find the link for my previous classification project here.

https://medium.com/analytics-vidhya/cricket-analysis-and-prediction-a7e9a7f3ebef

Now, without any delay, lets dive into Regression.

Regression: In simple terms, regression is a technique used to predict the unknown value of an object, using the characteristics of the object and by having some information about other similar objects for which we know the value.

Example: If we were to predict the price of a new car, we have to know the features of the car and the price of other similar cars. The features we can use are, mileage of the car, number of seats, horsepower of the car, boot space of the car, leg space of the car, kind of the fuel used etc….

Using this information, we can estimate the price of the new car. The difference between the actual price and our predicted price is the error in our prediction.

Goal: Our aim of this project is to Predict the Winner of IPL-2020.

Approach:

Given the player statistics to a machine learning model, the model outputs the rating points for that player based on his statistics.

Once the model has generated ratings for all the IPL players, we choose the best playing XI of a team by using an algorithm and sum all the points of the best XI players to get the total rating of the team.

This algorithm then outputs the winner of two teams using the team rating points that the machine learning algorithm had predicted.

We call this algorithm for all the matches in the league stage. Thus, after the league stage, the algorithm outputs the top four teams which goes into qualifiers.

In the qualifier stage, again the same algorithm is run between two teams and the top two teams will be selected for finals.

In the final stage, we are left with top two teams. We call the algorithm for one last time and it outputs the Final Winner

Dataset:

We have scraped the data from ICC’s T20 top 100 players and Cricbuzz websites using Beautiful Soup module and obtained the data for Batsmen, Bowler and All-Rounder separately.

Features used for Batsmen:

Innings

Runs Scored

Batting Average

Batting Strike Rate

Fifties

Fours

Sixes

Features used for Bowlers:

Innings

Wickets

Economy

Bowling Average

Bowling Strike Rate

Features used for All-Rounders:

Innings as Batsmen

Runs Scored

Batting Average

Batting Strike Rate

Fifties

Fours

Sixes

Innings as Bowler

Wickets

Economy

Bowling Average

Bowling Strike Rate

Now that we have our data, it is time for us to observe and understand the data.

Data Pre-Processing:

We have plotted the histogram of the data and observed that the data is highly right skewed. To overcome the skewness, we have log-transformed the data, so that the data is now approximately normal and our models work well.

Right Skewed Data

After applying log, approximately Normally distributed Data

We have observed that some records have 0 values for some of the features. We have dropped those records from our datasets. Most probably they are the players who played very less number of matches.

Model Building:

Now that we have processed and cleaned our data, we have to build the machine learning models on this cleaned data. We have implemented SGDRegressor, KNN-Regressor,Linear Regression using Least-Square Estimates, Weighted KNN-Regressor and compared our models with the Scikit learn’s model and we have achieved almost similar results to that of the Scikit Implementation.

SGD-Regressor:

We have provided the training data as the input to the models and found the best number of epoch using Cross Validation Data. AS we can see in the plots, the training error decreases as the epochs increases which means we are going towards convergence. We can increase the number of epochs to reduce the error furthermore, but we have stopped at 2000 epochs as the error fall rate is almost steady at this point.

We have got the best number of epochs using cross validation. Now, we can run the model with this best number of epochs on test data and we can obtain the predicted values. We can then compare our predicted values with the actual values to get the error.

After running our model on the test data, we get the below results.

We can see that Scikit implementation is almost similar to our results.

KNN-Regressor:

We have provided the training data as the input to the models and found the best number of neighbours using Cross Validation Data. As we can see from the plots, the training error increases as the value of K increases, which means that smaller values of K leads to overfitting and larger values of K leads to underfitting.

We have got the best number of neighbors using cross validation. Now, we can run the model with this best number of epochs on test data and we can obtain the predicted values. We can then compare our predicted values with the actual values to get the error.

After running our model on the test data, we get the below results.

We can see that Scikit implementation is almost similar to our results.

Output:

We have provided the player statistics to the above two machine learning models and applied the algorithm to predict the Winner of IPL 2020.

As we don’t know the schedule of the IPL-2020, we have randomly generated the matches. So, the order of the matches differ with both the machine learning algorithms. We also took home-advantage into consideration while building the algorithm.

Both the models predicted that Chennai Super Kings (CSK) will be the winner of IPL-2020.

Code:

All the code, analysis, graphs are available in GitHub.

Conclusions and Areas of Improvement:",https://medium.com/analytics-vidhya/predicting-ipl-2020-winner-5c95f6cb338c,[],2020-10-08 14:46:33.553000+00:00,946,"We have successfully built the algorithm to predict the winner of IPL-2020. We can further improve our model by taking more features into consideration like, ground conditions, weather conditions etc…machine learning, supervised learning, classification"
Employee Attrition Prediction.,"The data is for company X which is trying to control attrition. There are two sets of data: “Existing employees” and “Employees who have left”.

Following attributes are available for every employee :

Last evaluation

Number of projects.

Average monthly hours.

Time spent at the company.

Whether they have had a work accident.

Whether they have had a promotion in the last 5 years.

Departments (column sales).

Salary.

Whether the employee has left

Step 1 : We need to load our csv file. Link for the file :

Download Jupyter notebook using the command pip install jupyter (for Windows). And Let’s get started!

Importing all the necessary modules :

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

import warnings

warnings.filterwarnings('ignore')

Reading our file that is to be used :

excelfile=pd.ExcelFile(""python.xlsx"")

Emp_left=pd.read_excel(excelfile,'Employees who have left')

Emp_notleft=pd.read_excel(excelfile,'Existing employees')

Emp_left.tail()

Emp_notleft.head()

Emp_left.info() <class 'pandas.core.frame.DataFrame'>

RangeIndex: 3571 entries, 0 to 3570

Data columns (total 10 columns):

# Column Non-Null Count Dtype

--- ------ -------------- -----

0 Emp ID 3571 non-null int64

1 satisfaction_level 3571 non-null float64

2 last_evaluation 3571 non-null float64

3 number_project 3571 non-null int64

4 average_montly_hours 3571 non-null int64

5 time_spend_company 3571 non-null int64

6 Work_accident 3571 non-null int64

7 promotion_last_5years 3571 non-null int64

8 dept 3571 non-null object

9 salary 3571 non-null object

dtypes: float64(2), int64(6), object(2)

memory usage: 251.1+ KB

We see how many employees are there in each department :

Emp_left.dept.value_counts() sales 1014

technical 697

support 555

IT 273

hr 215

accounting 204

marketing 203

product_mng 198

RandD 121

management 91

Name: dept, dtype: int64

Now we combine the IT , Support and Technical in one value as Technical.

Emp_left[‘dept’]=np.where(Emp_left[‘dept’]==’support’,’technical’,Emp_left[‘dept’]).astype(‘str’)

Emp_left[‘dept’]=np.where(Emp_left[‘dept’]==’IT’,’technical’,Emp_left[‘dept’]).astype(‘str’)

Emp_notleft[‘dept’]=np.where(Emp_notleft[‘dept’]==’support’,’technical’,Emp_notleft[‘dept’]).astype(‘str’)

Emp_notleft[‘dept’]=np.where(Emp_notleft[‘dept’]==’IT’,’technical’,Emp_notleft[‘dept’]).astype(‘str’)

Step 2 : We start to compare values of the employees based on each factor and the reason why they are leaving.

plt.figure(figsize=(20,12))

plt.title(""Last evaluation vs Satisfication level"")

sns.scatterplot(x=Emp_left['satisfaction_level'],y=Emp_left['last_evaluation'],hue='number_project',data=Emp_left, palette='bright')

plt.show()

Point drawn from above plot:

Satisfaction level is very low for employees who has number of projects more than 4

Last evaluation for employees having number of projects equal to 2 is low, also they have low satisfaction level.

Step 3 : We now compare the Salary vs Satisfaction level based on the time spent in the company:

plt.figure(figsize=(20,10))

plt.title(‘Salary vs Satisfaction level’)

sns.boxplot(x=Emp_left[‘salary’],y=Emp_left[‘satisfaction_level’],hue=’time_spend_company’,data=Emp_left)

plt.show()

Step 4 : Just to analyze the perfect reason we draw a boxplot for the Salary vs the Monthly hours spent based upon the number of projects that they have done.

plt.figure(figsize=(20,10))

plt.title(“Salary vs Monthly hours spent”)

sns.boxplot(x=Emp_left[‘salary’],y=Emp_left[‘average_montly_hours’],hue=’number_project’,data=Emp_left)

plt.show()

Step 5 : Some more boxplots to know more reasons for why the employees left:

plt.figure(figsize=(20,10))

plt.title(“Average monthly hours vs Promotion in last 5 years”)

sns.boxplot(x=Emp_left[‘promotion_last_5years’],y=Emp_left[‘average_montly_hours’],hue=’time_spend_company’,data=Emp_left)

plt.show()

plt.figure(figsize=(20,10))

plt.title(‘Average monthly hours vs Number of projects’)

sns.boxplot(x=Emp_left[‘number_project’],y=Emp_left[‘average_montly_hours’],data=Emp_left)

plt.show()

We now see which department has the most number of employees leaving :

Points to ponder over —

Sales and Technical employees are the maximum number who are leaving.

Step 6 : As we have seen that technical employees are the most number of people leaving the company. We now see the department vs satisfaction level of the employees based upon the time spend in the company.

plt.figure(figsize=(18,10))

plt.title(“Department vs Satisfcation level”)

sns.boxplot(x=Emp_left[‘dept’],y=Emp_left[‘satisfaction_level’],hue=’time_spend_company’,data=Emp_left)

plt.show()

Step 7 : Finding the proportion of employees leaving from the company :

# Adding both dataset from employees who left with the existing employees Emp_left[‘Left’]=1

Emp_notleft[‘Left’]=0

final_Emp=pd.concat([Emp_left,Emp_notleft],axis=0)

final_Emp.info() # Creating dummies

columns=[‘dept’,’salary’]

dummies=pd.get_dummies(final_Emp[columns],drop_first=True)

final_Emp=pd.concat([final_Emp,dummies],axis=1)

final_Emp.head() # Dropping uncessary columns

final_Emp=final_Emp.drop(columns,axis=1)

final_Emp.info() # Dividing the dataset into X and Y

X=final_Emp.drop([‘Emp ID’,’Left’],axis=1)

y=final_Emp[‘Left’]

print(“The proportion of employees left is {}% from total dataset”.format(round(final_Emp.Left.value_counts()[1]/len(final_Emp)*100,2)))

The proportion of employees left is 23.81% from total dataset.

We use the following techniques for the approach of finding out about the employees prone to leave by measuring the accuracy of each technique.

The techniques are:

Logistic regression.

Random forest classifier.

Support vector classifier.

Oversampling.

From the above observations, we found the “Random Forest Classifier” had the highest accuracy. Therefore we build a model based on that.

# Random Forest from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import KFold rfc=RandomForestClassifier(random_state=50) n_folds=KFold(n_splits=5,shuffle=True, random_state=50) parameters={'criterion':['gini','entropy'],'max_depth': range(5,30,5),'max_features': range(10,18,2),

'min_samples_split': range(2,10,2)} model_cv = GridSearchCV(estimator=rfc,param_grid=parameters,cv=n_folds,verbose=1,

return_train_score=True,scoring='recall')

final_rfc=RandomForestClassifier(criterion='entropy',max_depth=20,max_features=14,min_samples_split=2,random_state=50)

final_rfc.fit(X_train.drop('Emp ID',axis=1),y_train)

y_pred=final_rfc.predict(X_test.drop('Emp ID',axis=1)) from sklearn.metrics import classification_report

print(classification_report(y_test,y_pred))

final_rfc.feature_importances_

X_train.columns

features=np.array(X_train.drop('Emp ID',axis=1).columns)

important=final_rfc.feature_importances_

indexes_features=important.argsort()

for i in indexes_features:

print(""{} : {:.2f}%"".format(features[i],important[i]*100)) # Finding employees who are prone to leave : y_test1=pd.concat([y_test,X_test[‘Emp ID’]],axis=1)

y_test3=pd.DataFrame(y_pred) y_test3.reset_index(inplace=True, drop=True) gf=pd.concat([y_test1.reset_index(),y_test3],1) new_df=gf[gf.Left==0] new_df=new_df.drop(‘index’,axis=1) new_df.columns=[‘Left’,’Emp ID’,’Predicted_left’] Employees_prone_to_leave=new_df[new_df[‘Predicted_left’]==1]

Employees_prone_to_leave=Employees_prone_to_leave.reset_index()

Employees_prone_to_leave=Employees_prone_to_leave.drop([‘Left’,’Predicted_left’,’index’],axis=1)

FINAL OUTPUT : (EMPLOYEES PRONE TO LEAVE)

Emp ID

0 6467

1 2416

2 9176

3 9241

4 5200

5 9057

6 2802

7 3862

8 4770

Factors driving employees to leave :

Below are some important features with % contribution towards attrition.

Satisfaction level : 34.63%

Time spend in the company : 27.47%

Average monthly hours : 10.84%

Last Evaluation : 12.14%

Number of projects : 9.25%

These are the driving factors which leads an employee to leave the company.

The accuracy of this model is 93.33 percent which is pretty good ! Thank you for reading this article till the end.",https://medium.com/analytics-vidhya/employee-attrition-prediction-df77255610ef,['Rishma Mitra Dhar'],2020-07-10 16:32:37.356000+00:00,676,"attrition, employee turnover, data analysis, employee satisfaction, work accident"
"Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku","Preprocessing and Parsing of Ingredients

To understand the task at hand here, let's look at an example. The delicious ‘Gennaro’s classic spaghetti carbonara’ recipe found on Jamie Oliver’s website requires the ingredients:

3 large free-range egg yolks

40 g Parmesan cheese, plus extra to serve

1 x 150 g piece of higher-welfare pancetta

200 g dried spaghetti

1 clove of garlic

extra virgin olive oil

There is a lot of redundant information here; for example, weights and measures are not going to add value to the vector encodings of the recipe. If anything, it is going to make distinguishing between recipes more difficult. So we need to get rid of those. A quick google search led me to a Wikipedia page containing a list of standard cooking metrics e.g. clove, gram (g), teaspoon, etc. Removing all these words in my ingredient parser worked really well.

We also want to remove stop words from our ingredients. In NLP ‘stop words’ refer to the most common words in a language. For example, the sentence ‘learning about what stop words are’, becomes, ‘learning stop words’. NLTK provides us with an easy way to remove (most of) these words.

There are also other words in the ingredients that are useless to us — these are the words that are very common among recipes. Oil, for example, is used in most recipes and will provide little to no distinguishing power between recipes. Also, most people have oil in their homes so having to write oil every time you use the API is cumbersome and pointless. There are advanced NLP techniques, for example using conditional random fields (a class of statistical modeling), that can calculate the probability that a word is an ingredient, as opposed to a measure, texture, or another type of word that surrounds the ingredient word. But, simply removing the most common words seemed to be very effective, so I did this. Occam’s razor and all that jazz… To get the most common words we can do the following:

import nltk

vocabulary = nltk.FreqDist() # This was done once I had already preprocessed the ingredients

for ingredients in recipe_df['ingredients']:

ingredients = ingredients.split()

vocabulary.update(ingredients) for word, frequency in vocabulary.most_common(200):

print(f'{word};{frequency}')

We have one final obstacle to get over, however. When we try to remove these ‘junk’ words from our ingredient list, what happens when we have different variations of the same word? What happens if we want to remove every occurrence of the word ‘pound’ but the recipe ingredients say ‘pounds’? Luckily there is a pretty trivial workaround: lemmatization and stemming. Stemming and lemmatization both generate the root form of inflected words — the difference is that a stem might not be an actual word whereas, a lemma is an actual language word. Although lemmatization is often slower, I chose to use this technique as I know the ingredients will be actual words which is useful for debugging and visualization (the results turned out to be practically identical using stemming instead). When the user feeds ingredients to the API we also lemmatize those words as the lemmatized words are the words with the embeddings.

We can put this all together in a function, ingredient_parser , along with some other standard preprocessing: getting rid of punctuation, removing accents, making everything lowercase, getting rid of Unicode.

def ingredient_parser(ingredients): # measures and common words (already lemmatized)

measures = ['teaspoon', 't', 'tsp.', 'tablespoon', 'T', ...]

words_to_remove = ['fresh', 'oil', 'a', 'red', 'bunch', ...] # Turn ingredient list from string into a list

if isinstance(ingredients, list):

ingredients = ingredients

else:

ingredients = ast.literal_eval(ingredients) # We first get rid of all the punctuation

translator = str.maketrans('', '', string.punctuation) # initialize nltk's lemmatizer

lemmatizer = WordNetLemmatizer() ingred_list = []

for i in ingredients:

i.translate(translator)

# We split up with hyphens as well as spaces

items = re.split(' |-', i)

# Get rid of words containing non alphabet letters

items = [word for word in items if word.isalpha()]

# Turn everything to lowercase

items = [word.lower() for word in items]

# remove accents

items = [unidecode.unidecode(word) for word in items]

# Lemmatize words so we can compare words to measuring words

items = [lemmatizer.lemmatize(word) for word in items]

# get rid of stop words

stop_words = set(corpus.stopwords.words('english'))

items = [word for word in items if word not in stop_words]

# Gets rid of measuring words/phrases, e.g. heaped teaspoon

items = [word for word in items if word not in measures]

# Get rid of common easy words

items = [word for word in items if word not in words_to_remove]

if items:

ingred_list.append(' '.join(items))

ingred_list = ' '.join(ingred_list)

return ingred_list

When parsing the ingredients for ‘Gennaro’s classic spaghetti carbonara’ we get: egg yolk parmesan cheese pancetta spaghetti garlic. Perfect, that works fantastically!

Using lambda functions, it’s easy to parse all of the ingredients.",https://towardsdatascience.com/building-a-recipe-recommendation-api-using-scikit-learn-nltk-docker-flask-and-heroku-bfc6c4bdd2d4,[],2020-12-08 19:32:06.503000+00:00,748,"Ingredients_parsed = recipes_df.apply(lambda x: ingredient_parser(x['ingredients']), axis=1)Tags preprocessing, parsing, ingredients, lemmatization"
AI is making smiles brighter and easing dental anxiety,"One of the medical practices that many people fear more than any other, is dentistry. There’s something about sitting in a chair and having someone invade your mouth that puts people on edge. Perhaps having AI play an increasing role in whatever you’re having checked, cleaned, or drilled, will help alleviate at least a modicum of that anxiety.

“Narrow” AI solutions that are experts at specialized tasks are becoming far more common place in all manner of industries, and medical care is no exception. Just as machine learning AIs are being used to help improve the diagnosis for serious diseases like cancer, it’s also finding use in dentistry to improve the accuracy of abnormality detection in X-rays of the jaw and teeth, make recommendations for orthodontic treatments, and optimize endodontic therapy.

Although this process can still be achieved using a medical professional for a second opinion — and often is to add an added layer of human oversight to the process — algorithms like Behold.ai’s can do this far faster, anywhere in the world, for any number of practices, simultaneously. It does so providing the same level of reliable, consistent, and often-accurate expertise that is far from as readily available as more traditional clinician opinions.

This makes an AI diagnosis a perfect alternative to the traditional second opinion of a professional. Not only because of its accuracy and ease of access, but also its impartiality. Patients can rest assured that the AI brings less of a potentially implicit human bias to its diagnosis. While there are ways in which algorithms can reflect the biases of the programmers who wrote it, machine learning’s independence and self-taught design makes that less likely.

Trust will need to be built up with the machines too, though. Medical data of any kind is incredibly sensitive, and dentistry is no different. That data must be shared with the AI and in some form in turn, the company that developed and manages it. Strict data sharing practices must be in place to make that process feel trustworthy and not open to exploitation, data theft, or sale.

Still, with the potential for life-altering, permanent surgery and adjustments as part of the dentistry field, and the lifelong anxiety associated with it for many, AI holds an exciting potential to both improve the quality of the service, and remove one of the biggest reasons people don’t seek help for it in the first place.",https://medium.com/product-ai/ai-is-making-smiles-brighter-and-easing-dental-anxiety-be3450156a5f,['Jon M'],2021-11-29 22:35:49.542000+00:00,399,"dentistry, AI, machinelearning, Behold.ai, data sharing"
Understanding Partial Auto-Correlation,"Laying the foundation for PACF: The auto-regressive time series

There are many phenomena in which the past influences the present. The events of yesterday can be used to foretell what will happen today. When such phenomena are represented as a time series, they are said to have an auto-regressive property. In an auto regressive time series, the current value can be expressed as a function of the previous value, the value before that one and so forth. In other words, the current value is correlated with previous values from the same time series.

If a time series is auto-regressive it is often the case that the current value’s forecast can be computed as a linear function of only the previous value and a constant, as follows:

(Image by Author)

Here T_i is the value that is forecast by the equation at the ith time step. Beta0 is the Y-intercept of the model and it applies a constant amount of bias to the forecast. It also specifies what will be the forecast for T_i if the value at the previous time step T_(i-1) happens to be zero. Beta1 tells us the rate at which T_i changes w.r.t. T_(i-1).

The key assumption behind this simple equation is that the variance in T_(i-1) is able to explain all the variance expressed by all values that are older than T_(i-1) in the time series. It is as if T_(i-1) captures all the information associated with values older than itself.

But what if this assumption were not true? What if the variance in T_(i-1) is not able to explain all of the variance contained within T_(i-2)? In that case, the above equation will not be able to feed this unexplained portion of the variance from T_(i-2) into T_i, causing the forecast for T_i to go off the mark.

Fortunately it’s easy to fix this problem adding a term to the above equation as follows:

(Image by Author)

In this equation the extra term Beta2*T_(i-2) seeks to capture the variance contained in values that are older than T_(i-1) that could not be explained by the variance in T_(i-1). It feeds this balance amount of information directly into the forecast for today’s value T_i.

With the background established let’s build the definition and the formula for the partial auto-correlation function.",https://towardsdatascience.com/understanding-partial-auto-correlation-fa39271146ac,['Sachin Date'],2020-11-21 15:38:24.280000+00:00,373,"PACF, Auto-Regressive Time Series, Time Series Forecasting, Linear Function, Variance"
Building a Recommender Engine Part I: Problem Statement & Collecting Data with Selenium,"Problem

The world of retail is changing rapidly. Many brick and mortar locations are closing and being replaced by online stores, direct-to-consumer brands, and subscription/membership services. However, while the breadth of assortment is something that drives customers to a website, a lot of eCommerce platforms fail to sell through a high percentage of their merchandise. This is often due to poor user browsing experience. Customers can spend hours scrolling through hundreds, sometimes thousands of items of merchandise never finding an item they like. Shoppers need to be provided suggestions based on their likes and needs in order to create a better shopping environment that boosts sales and increases the time spent on a website.

This series will demonstrate a popular solution to this problem, recommender engines!

What is a recommender engine? you may ask. It is an unsupervised learning algorithm (one that does not have a target variable to measure accuracy against) mostly used to aid in consumer decision making. I’m sure you have seen them while online shopping. They also appear in places like streaming apps (aka Netflix and Hulu) to help you select a TV show or movie to watch next and on journalism/media websites like Medium to suggest other articles you may like to read, among many other uses. Obviously many e-retailers like Amazon have already been using recommender algorithms for quite some time, but many smaller or newer sites are still in need. There are different varieties of recommenders that base their predictions on different features. My goal for this project is to create one that is content-based, using elements of the products themselves, and one that is a collaborative item- and user-based using data from amazon reviews. I’ll look at cosine similarity, aiming for a score of 1 or pairwise distance, aiming for a score of 0, as metrics to measure success and check similarity between items and users.

Collecting Data

I started with 278,677 reviews of clothing, shoes, and jewelry from Amazon collected here. One review equals one row of data. After some simple EDA using Python Pandas, I determined there were 23033 unique ASINs (Amazon standard identification number, aka product id) in the dataset and turned them into a list.

Applying the unique( ) function to the end of a Pandas series will return each value in the column only once. Wrapping that code in the Python len( ) function will tell you how many values were returned. If the number matches the number of rows in your data, then you know each row is unique, if it is shorter, than you know that value is in multiple rows. In this case, it tells me each product has an average of 12 reviews.

The set( ) function also creates a non-repeating (unique) collection of items, but since it is unordered and immutable, I save the set as a variable and then turn it into a list with the list( ) function so that it can be sorted alphabetically. The ordered list becomes necessary for consistency in the future when I need to loop through and index the values numerous times.",https://towardsdatascience.com/web-scraping-recommender-systems-project-1d360fa678e4,['Rachel Koenig'],2020-01-02 01:28:15.401000+00:00,510,"e Commerce, Recommender Engines, Unsupervised Learning Algorithm, Cosine Similarity, Pairwise Distance"
Here’s why decentralization is inevitable…,"When Machines Think 💭

“Forget artificial intelligence — in the brave new world of big data, it’s artificial idiocy we should be looking out for.” — Tom Chatfield

Since the dawn of the industrial revolution and machines, there were always humans operating them and making them work. But over last few years, we have seen them becoming more of our thinking partners. Speaking of identifying faces in a CCTV’s video footage, or translating a sentence from Korean to Mandarin, or playing a match of Go, machines have continuously beaten humans.

Still, what we see now in the name of artificial intelligence (AI) is merely the tip of the iceberg. We have organizations building AI models in silos and training them on their historical data. But the Pandora’s box will open the day we can have all these organizations collaborate on AI models that do better than what any of those individual AI models could do.

We are yet to see the moment in AI along the lines of ‘the whole is greater than the sum of its parts.’

Time for an example, I believe. Almost every bank has an AI that detects the probability of payment being fraud. Each bank has been training their version of the models on their historical data. It makes some banks better at fraud detection than others, which sometimes they position as one of their strengths to their customers. While only some people might buy banking services from a bank solely based on their expertise in detecting a potential fraud, the bigger problem of fraud payments is an issue for society at large. But for banks, their business and profits are equally (if not more) important than just doing good for the community.

If in an imaginary world, banks choose to work together and build a standard fraud detection AI model that serves all the banks equally, they will not be able to capture the economic value of their data. If banks choose otherwise, there would still be people suffering from fraud payments.

Interestingly, in such scenarios, decentralization allows us to capture the economic value as well as do good for the society at large. How exactly? Here’s how — banks together can train a common AI model that is stored on a peer-to-peer network (blockchain). Anyone can take the latest copy of the model from the network, train it further with their data, and submit it back to the network with a proof-of-training. If the network accepts that the additional training improved the model’s performance, it recognizes it and marks it as latest for others to train it further. Whenever the network accepts a trained model, the trainer gets paid in the network-specific tokens that can be used in return to use the model. That’s how the trainer captures the economic value from its data, and at the same time, the common trained AI model makes it possible for every bank — even a new one — to use the trained model — therefore, doing good to the society.",https://medium.com/hackernoon/heres-why-decentralization-is-inevitable-6090a9abe813,['Mohit Mamoria'],2020-01-10 08:48:45.051000+00:00,498,"Artificial Intelligence, Big Data, Machine Thinking, Decentralization, AImodels"
Excel Maps Add In Capability,"The outbreak of COVID-19 has highlighted the importance of location, something which many businesses were already aware of but not quite addressing. The value of “where” is of utmost importance in understanding buyer behaviour, especially in the context of device mobility. Whilst many companies might exploit the core capability of an Excel add in for data analysis, there has been much less use of maps in Excel.

Geospatial data analytics has traditionally been addressed by domain experts often referred to as data scientists. However, the ability to put geospatial analytics in the hands of everyday business users who are making decisions “on the fly” would seem to be nirvana. Those users often use standard desktop applications such as Excel. The nirvana point is the immediate insight Mapcite’s geospatial data visualizations can provide that isn’t readily apparent from any other Excel add on for data analysis.

Take the example images below using the Mapcite Excel map add-in which can be downloaded free from the Microsoft App Store.

The first image shows the ability to take a list of stores and addresses in Excel and add them into a map just by clicking Add to Map.

The second image below shows the resultant map at the Australia-wide level. From this point you can begin using your Excel add in for data analysis by drilling down into the map or by using the tools available within the Mapcite map, all while remaining within Excel!

Finally, the image below shows the result of clicking the “Publish” button which produces a URL which is accessible by anyone to whom you send the link. They can then access a fully interactive map with all the Mapcite tools available, in this case the heat map capability for a specific region in NSW.

This is the new world of insight from an Excel add on for data analysis, with a focus on “where”. If this isn’t the best Excel addin, I would be very surprised.",https://medium.com/@stepwalker/excel-maps-add-in-capability-c2fc41d350a7,['Steve Walker'],2020-11-20 05:33:07.462000+00:00,321,"COVID-19, location, buyerbehaviour, Exceladdin, geospatialdataanalytics"
Introduction to Machine Learning.,"Machine learning allows the computers learn automatically without human intervention or assistance and adjust actions accordingly. Let’s think about a newly born baby. That baby learn the things around him day by day. Then perform the actions based on past experiences.

Main thing we can identified in Machine learning concept is a Classifier. Classifier is a model which we can input Features (Measurable properties of data set) and labels/targets (value to be predicted by our model) for training. Then this classifier can predict the result(label) for the given testing features.

classifier

Think about a classifier which can identify an apple and an orange. First we have to train the classifier with features and labels. height, width , color , texture are some features for that classifier. Orange, apple are the two labels for that classifier.

Machine learning is a subset of Artificial Intelligence and that is a super-set of Deep Learning.

There are 3 types of Machine learning.

Supervised Learning.-learned in the past to new data using labeled examples to predict future events. We have to train the model with a set of features and labels. As an example, when teacher gives the set of questions and answers in the students study that and in the exam when the question is given students answer it according to the previous experiences. Unsupervised Learning .-Used when the information used to train is neither classified nor labeled. Unsupervised learning studies how systems can infer a function to describe a hidden structure from unlabeled data. No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Reinforcement Learning .- Use both labeled and unlabeled data for training. useful when only incomplete labels are available. For examples, driving a vehicle or playing a game against an opponent.

Types of Machine learning based on the type of output(results or labels)

Classification- This can be used when the results are qualitative. For predicting discrete labels(class,category). This modeling is the task of approximating a mapping function(f) from input variables(X) to discrete output variables(y)or classes.Then the mapping function predict the labels.

Classification Algorithms (Supervised):

Support vector machines

Nearest Neighbors

Decision Trees and Random Forests

Gaussian naive Bayes

Classification Algorithms (unsupervised):

K-Means Clustering.

Means Shift Clustering

Gaussian Mixture Models

Spectral Clustering

2. Regression- This can be used when the results are quantitative.For predicting continuous labels(numbers between negative infinite to positive infinite).This model approximating a mapping function(f)from input variables(X) to a continuous output variable(y).

Regression Algorithms

Linear regression

Support Vector Machines

Random forest regression

3. Clustering- Set of inputs is to divided into groups. Groups are not known beforehand, making this typically an unsupervised task.

First article on Machine Learning.In the next article I will write on how to work with regression problems with examples. Touch with my future articles too.",https://medium.com/@nimesha-dilini/introduction-to-machine-learning-42851b2b2ce6,['Nimesha Dilini'],2019-05-12 17:05:08.969000+00:00,443,"machine learning, classifier, artificial intelligence, deep learning, supervised learning"
Streaming ML CI/CD in a few days: how we improved our easy-to-use code delivery system for Spark Streaming applications and ML,"Streaming ML CI/CD in a few days: how we improved our easy-to-use code delivery system for Spark Streaming applications and ML Eugene Lopatkin Mar 22·3 min read

Overview

A significant trend in the development of IT business in now days is readiness to work with hot data, the lifetime of which from the moment of its appearance can be less than a second.

Let’s say you come to a store and take out a loan to buy a phone. You want to get a loan on favorable terms. And the bank wants to give a loan to a verified client. The time window in which you need credit money is relatively short. An example from Telecom domain. You have run out of money, and at this time your wife is going to deliver a baby. You need to call an ambulance. A trust payment from an operator would be very helpful.

Cost of data for business in such situations is extremely important. This state of affairs in the market has led to the emergence of kappa architecture and streaming data handling, which should guarantee minimal latency.

Recently I wrote about our code delivery system based on Kubernetes, Airflow and Jenkins.

Among the advantages of our system:

Extremely low development costs Solving all required code delivery tasks Single point of product management Fault tolerance and monitoring point

For streaming support we want to keep the same way of automation, the same point of control as batch jobs and minimize an amount of using resources.

Concepts

We decided that automation of streaming applications should not be different from automation of batch applications. In addition, we want to build Scala application “on the fly” from repository.

The fact is that Spark Streaming applications block the console after executing the spark-submit. The option

spark.yarn.submit.waitAppCompletion = false

allows us to achieve a spark-submit behavior identical to the batch job submission. Accordingly, after submitting such an application our Kubernetes Pod, under which submitting is happened, collapses and frees up resources.

Next, we have to decide how to understand that the job is running. It is quite simple to do through YARN CLI. This bash command does it:

while yarn application -list | grep InternationalRoamingApp | grep -e RUNNING -e ACCEPTED; do sleep 10; done && exit 1 > /dev/null 2>&1

If application fails we also should to rerun spark-submit steps to restart application.

DAG generation remastering

Now when we’ve found out how Spark jobs should be handled. We should just add SSH operator generation support for our automation system which will allows to check job state.

YAML Configuration files

New structures of Single-step and multi-step files. Rework allows declaring command for remote execution (in our case it is a Hadoop cluster) and additional containers for Pod that could mix in some additional logic (in our case application building).

Single-step configuration file

Multi-step configuration

New in j2 DAG template

Method that allows to restart all steps before failed step:

Conversion for commands for internal usage in Python:

Loops for sidecar containers:

Macros for SSH Operator that executes a remote command:

Resulting Python script from a rendering of this template you could see in Appendix 1.

DAG execution remastering

At the stage of DAG execution, the first step is a spark-submit that runs a pod, which completes an assembly and a submission. After that, the usual SSH operator starts, which controls the launched application. If the application crashes, the SSH operator command crashes, and the flow restarts. We could control number of restarts and restart interval of our streaming application.

New in j2 pod generation command template

Command allows adding sidecar containers. In our case, we are using one to build Scala projects.

Rendering result of this command is in Appendix 2.

ML Model composition in Spark Streaming applications

Spark allows you to use models as:

UDF Function Transformer or Estimator inside Spark ML pipeline.

Our system allows to split model and data preparation logic, use models as separate microservices and make compositions in many different ways.

Appendix 1. DAG generation example

Appendix 2. Rendered command example",https://medium.com/@eugene-lopatkin/streaming-ml-ci-cd-in-a-few-days-how-we-improved-our-easy-to-use-code-delivery-system-for-spark-b6bbafa03149,['Eugene Lopatkin'],2021-03-22 20:24:03.062000+00:00,640,"Streaming, ML, CI/CD, Spark Streaming, Kubernetes"
ML Design Pattern #5: Repeatable sampling,"ML Design Pattern #5: Repeatable sampling

An occasional series of design patterns for ML engineers. Full list here.

Many machine learning tutorials will suggest that you split your data randomly into training, validation, and test datasets:

df = pd.DataFrame(...) rnd = np.random.rand(len(df))

train = df[ rnd < 0.8 ]

valid = df[ rnd >= 0.8 & rnd < 0.9 ]

test = df[ rnd >= 0.9 ]

The problem is that this fails in many real-world situations. The reason is that it is rare that the rows are independent. For example, if you are training a model to predict flight delays, the arrival delays of flights on the same day will be highly correlated with each other. This is called leakage, and it is an important problem to avoid when doing machine learning.

Use the Farm Fingerprint hashing algorithm on a well-distributed column to split your data into train/valid/test

The solution is to split the dataset based on the date column:

SELECT

airline,

departure_airport,

departure_schedule,

arrival_airport,

arrival_delay

FROM

`bigquery-samples`.airline_ontime_data.flights

WHERE

ABS(MOD(FARM_FINGERPRINT(date), 10)) < 8 -- 80% for TRAIN

Besides solving the original problem (data leakage), this also gives you repeatability:

FARM_FINGERPRINT is an open-source hashing algorithm that is implemented consistently in C++ (and hence: Java or Python) and in BigQuery SQL. All the flights on any given date will belong to the same split — train, valid, or test. This is repeatable regardless of things like the random seed.

Choosing split column

How do you choose the column to split on? The date column has to have several characteristics for us to be able to use it as the splitting column:

Rows at the same date tend to be correlated — again, this is the key reason why we want to ensure that all rows on the same date are in the same split. Date is not an input to your model (features extracted from date such as dayofweek or hourofday can be inputs, but you can’t use an actual input to split because the trained model will then not have seen 20% of the possible input values). There have to be enough date values. Since you are computing the hash and finding the modulo with respect to 10, you need at least 10 unique hash values. The more unique values you have, the better, of course. To be safe, shoot for 3–5x the denominator for the modulo, so in this case, you want 50 or more unique dates. The label has to be well-distributed among the dates. If it turns out that all the delays happened on Jan. 1 and the rest of the year, there were no delays, this wouldn’t work since the split datasets will be skewed. To be safe, look at a graph and make sure that all three splits have a similar distribution of labels by departure delay or some other input value. You can automate this using the Kolomogorov-Smirnov test.

Variation 1: Single query

You don’t need three separate queries to generate training, validation, and test splits. You can do it in a single query as follows:

CREATE OR REPLACE TABLE mydataset.mytable AS SELECT

airline,

departure_airport,

departure_schedule,

arrival_airport,

arrival_delay,

CASE(ABS(MOD(FARM_FINGERPRINT(date), 10)))

WHEN 9 THEN 'test'

WHEN 8 THEN 'validation'

ELSE 'training' END AS split_col

FROM

`bigquery-samples`.airline_ontime_data.flights

Variation 2: Random split

What if you want a random split, but just need repeatability? In that case, you can simply hash the row data itself. Here’s an easy way to do that:

SELECT

airline,

departure_airport,

departure_schedule,

arrival_airport,

arrival_delay

FROM

`bigquery-samples`.airline_ontime_data.flights f

WHERE

ABS(MOD(FARM_FINGERPRINT(TO_JSON_STRING(f)), 10)) < 8

Note that if you have duplicate rows, then they will always end up in the same split. If this is a concern, add a unique id column to your SELECT query.",https://towardsdatascience.com/ml-design-pattern-5-repeatable-sampling-c0ccb2889f39,['Lak Lakshmanan'],2021-07-19 22:06:35.866000+00:00,569,"ML Design Pattern, Repeatable Sampling, Farm Fingerprint Hashing Algorithm, Data Leakage, Kolomogorov-Smirnov Test"
Face Detection in 2 Minutes using OpenCV & Python,"Face Detection in 2 Minutes using OpenCV & Python

In this quick post I wanted to share a very popular and easy way of detecting faces using Haar cascades in OpenCV and Python.

The video version for those who prefer that !

First of all make sure you have OpenCV installed. You can install it using pip:

pip install opencv-python

Face detection using Haar cascades is a machine learning based approach where a cascade function is trained with a set of input data. OpenCV already contains many pre-trained classifiers for face, eyes, smiles, etc.. Today we will be using the face classifier. You can experiment with other classifiers as well.

You need to download the trained classifier XML file (haarcascade_frontalface_default.xml), which is available in OpenCv’s GitHub repository. Save it to your working location.

To detect faces in images:

A few things to note:

The detection works only on grayscale images. So it is important to convert the color image to grayscale. (line 8)

detectMultiScale function (line 10) is used to detect the faces. It takes 3 arguments — the input image, scaleFactor and minNeighbours. scaleFactor specifies how much the image size is reduced with each scale. minNeighbours specifies how many neighbors each candidate rectangle should have to retain it. You can read about it in detail here. You may have to tweak these values to get the best results.

function (line 10) is used to detect the faces. It takes 3 arguments — the input image, scaleFactor and minNeighbours. scaleFactor specifies how much the image size is reduced with each scale. minNeighbours specifies how many neighbors each candidate rectangle should have to retain it. You can read about it in detail here. You may have to tweak these values to get the best results. faces contains a list of coordinates for the rectangular regions where faces were found. We use these coordinates to draw the rectangles in our image.

Results:

Similarly, we can detect faces in videos. As you know videos are basically made up of frames, which are still images. So we perform the face detection for each frame in a video. Here is the code:

The only difference here is that we use an infinite loop to loop through each frame in the video. We use cap.read() to read each frame. The first value returned is a flag that indicates if the frame was read correctly or not. We don’t need it. The second value returned is the still frame on which we will be performing the detection.

Find the code here: https://github.com/adarsh1021/facedetection",https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81,['Adarsh Menon'],2019-04-23 01:44:44.795000+00:00,410,"OpenCV, Python, Face Detection, Haar Cascades, Machine Learning"
MuseNet and the future of AI,"Training:

MuseNet starts by being fed MIDI files from various sources and artists. MIDI files are a file that is meant for audio. However, unlike conventional .wav or .mp3 files which hold the audio itself, .mid or .midi files hold the information on what notes are there and how they should be played. From this, the AI can extract patterns in the way notes are played, with what instruments, and for how long. This data will be passed on to the next step: creating the model.

Source: towardsdatascience.com

Creating the Model:

MuseNet uses a deep neural network to identify the patterns found in different pieces from each composer. Since the model is unsupervised, Musenet is just fed the raw MIDI data and can create, train, test, and save a model to create sequences of notes that sound natural and human-generated.

Front End

On the front end, MuseNet takes in a style, which is the composer that the output should sound like. It also takes in a MIDI file, or a set of predetermined songs to take notes from. Using sequences of notes from the provided MIDI file, as well as premade data about each composer's style, MuseNet can reshape and rearrange the notes and note speeds to match both user-provided criteria.

Finale and Summary

MuseNet is a deep neural network from OpenAI

It can create original music based off of pieces and human composers

It is a large leap in AI and technology, as AI can learn to blur the lines between human creativity and artificial generation and expression of “creativity”

Source: Business Insider

Finally, let me leave you with some ideas from the great men Descartes and Alan Turing

According to Alan Turing, if a man and a machine are put in front of an interrogator where the interrogator has no means to determine which one is the machine other than by asking questions, and the machine is mistakenly identified as human, we can safely assume that machines can think

According to Descartes, even if a machine managed to imitate humans in almost every respect, we would still be able to tell the man from the machine by the machine's attempts to use words and express thoughts. (He was an opponent of machines ever being able to think)

We are finally beginning to make man and machine indistinguishable.",https://medium.com/@raghav-srinivasan/musenet-and-the-future-of-ai-f0a971fc6ed7,['Raghav Srinivasan'],2021-04-01 01:35:39.526000+00:00,374,"AI, Machine Learning, Music Generation, Deep Neural Network, Alan Turing"
Analyzing COVID19 in Sri Lanka with Python,"Analyzing COVID19 in Sri Lanka with Python

In this tutorial, we will build some graphs that will give a data-driven description of Sri Lanka’s COVID19 situation. End-to-end, it should take you less than 10 minutes.

The data is from two sources:

The COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University, which we can access at https://github.com/CSSEGISandData/COVID-19 and in aggregate at https://pomber.github.io/covid19/timeseries.json. The Health Promotion Bureau of Sri Lanka, which we can access at https://www.hpb.health.gov.lk/api/get-current-statistical.

Part 1: Getting the data

You can either pull the above data yourself. Or we can use my python library, covid19-nuuuwan, which enables convenient data access. To install run,

pip install covid19-nuuuwan

[You can find the complete source code on GitHub. Feel free to fork this code, or report issues.]

To access the complete Johns Hopkins University data set (which contains up-to-date time series of confirmed cases recoveries and deaths for 189 countries from Afghanistan to Zimbabwe),

>>> from covid19 import covid_data

>>> jhu_data = covid_data.load_jhu_data()

>>> len(jhu_data.values())

189 >>> country_ids = list(jhu_data.keys())

>>> country_ids[0], country_ids[-1] ('AF', 'ZW')

Each country entry looks like,

>>> jhu_data['LK'] {

'country_name': 'Sri Lanka',

'country_alpha_2': 'LK',

'country_alpha_3': 'LKA',

'population': 21803000,

'timeseries': [

{

'date': '2020-01-22 00:00:00',

'unixtime': 1579631400.0,

'cum_confirmed': 0,

'cum_deaths': 0,

'cum_recovered': 0,

'active': 0,

'new_confirmed': 0,

'new_deaths': 0,

'new_recovered': 0,

'cum_vaccinations': 0,

'cum_people_vaccinated': 0,

'cum_people_fully_vaccinated': 0,

},

...

...

...

{

'date': '2021-06-09 00:00:00',

'unixtime': 1623177000.0,

'cum_confirmed': 213396,

'cum_deaths': 1910,

'cum_recovered': 180427,

'active': 31059,

'new_confirmed': 2735,

'new_deaths': 66,

'new_recovered': 2168,

'cum_vaccinations': 2450965,

'cum_people_vaccinated': 2089320,

'cum_people_fully_vaccinated': 361645,

}

]

}

Similarly, the Health Promotion Bureau contains a time-series of PCR tests, information about hospitals and various cumulative statistics.

{

""success"":true,

""message"":""Success"",

""data"":{

""update_date_time"":""2021-06-11 11:33:36"",

""local_new_cases"":2738,

""local_total_cases"":216134,

""local_total_number_of_individuals_in_hospitals"":31986,

""local_deaths"":1910,

""local_new_deaths"":67,

""local_recovered"":182238,

""local_active_cases"":31986,

""global_new_cases"":421019,

""global_total_cases"":175332051,

""global_deaths"":3780265,

""global_new_deaths"":14097,

""global_recovered"":159186733,

""total_pcr_testing_count"":3690770,

""daily_pcr_testing_data"":[

{

""date"":""2020-02-18"",

""count"":""1""

},

...

Part 2: Analyzing the data

Now that we have the data, you can use your favourite analytics library to dig deeper into it. In this tutorial, we’ll build some graphs with MatPlotLib.

[You can find the source code for these examples on GitHub.]

Example 1: Deaths per 100,000 people

We can access the COVID19 time-series for Sri Lanka with,

jhu_data = covid_data.load_jhu_data()

country_data = jhu_data['LK']

timeseries = country_data['timeseries']

This data also includes the population for each country, which can be accessed with,

pop = country_data['population']

To plot a line plot with MatPlotLib,

x = list(map(

lambda d: datetime.datetime.fromtimestamp(d['unixtime']),

timeseries,

))

y = list(map(

lambda d: 100_000 * d['cum_deaths'] / pop,

timeseries,

))

plt.plot(x, y, color=country_meta_data['color'])

After some cosmetic improvements (see example code for details),

Example 1a: Active Cases per 100,000 people

Similarly, we can plot Active Cases:

Example 1b: Active Cases per 100,000 people — minus the Maldives

(Since Maldives is an outlier)

Example 2: Daily COVID19 Active Cases, Total Recovered Cases and Total Deaths in Sri Lanka

With the same JHU dataset, let’s focus on Sri Lanka. We can plot a stack plot of Active Cases, Total Recovered Cases and Total Deaths as follows:

country_data = covid_data.load_jhu_data()['LK']

timeseries = country_data['timeseries'] x = list(map(

lambda d: datetime.datetime.fromtimestamp(d['unixtime']),

timeseries,

))

y1 = list(map(

lambda d: d['active'],

timeseries,

))

y2 = list(map(

lambda d: d['cum_recovered'],

timeseries,

))

y3 = list(map(

lambda d: d['cum_deaths'],

timeseries,

)) plt.stackplot(x, y1, y2, y3, colors=['blue', 'green', 'red'])

We get,

Example 2a: Total COVID19 Deaths in Sri Lanka

If we remove y1 and y2 from above (because the red stack can, thankfully for now, hardly be seen),

Example 2b: Daily COVID19 Active Cases, Total Recovered Cases and Total Deaths in China

We can plot the above charts for any country we like by simply modifying,

country_data = covid_data.load_jhu_data()['CN']

Example 2b: Daily COVID19 Active Cases, Total Recovered Cases and Total Deaths in Israel

country_data = covid_data.load_jhu_data()['IL']

Note, the effect of vaccination.

Example 3: Daily New COVID19 Cases and PCR Tests in Sri Lanka

In addition to the JHU data for Sri Lanka, the HBP dataset also proviceds PCR Tests. The combined time-series for Sri Lanka can be accessed as follows:

from covid19 import lk_data

timeseries = lk_data.get_timeseries()

Important: lk_data , not covid_data

Plotting with MatPlotLib gives us,

Example 4: Daily New COVID19 Cases per PCR Tests in Sri Lanka

Finally, let’s plot the ratio of the two plots above. In addition to the raw Cases per Test, we also plot the 14 day moving average, for a smoother analysis.

Example 5: % People vaccinated in South Asia

x = list(map(

lambda d: datetime.datetime.fromtimestamp(d['unixtime']),

timeseries,

))

y = list(map(

lambda d: d['cum_people_vaccinated'] / population,

timeseries,

))

plt.plot(x, y, color=country_meta_data['color'])

Example 5a: % People vaccinated in South Asia — without the Maldives and Bhutan

Example 6: Total People Vaccinated in Sri Lanka

x = list(map(

lambda d: datetime.datetime.fromtimestamp(d['unixtime']),

timeseries,

))

y1 = list(map(

lambda d: d['cum_people_fully_vaccinated'],

timeseries,

))

y2 = list(map(

lambda d: d['cum_people_vaccinated'] - d['cum_people_fully_vaccinated'],

timeseries,

)) plt.stackplot(x, y1, y2, colors=['green', 'lightgreen'])

…

If you enjoyed this article, you might also like,",https://medium.com/@nuwans/analyzing-covid19-in-sri-lanka-with-python-caea03296381,['Nuwan I. Senaratna'],2021-06-11 14:27:36.029000+00:00,648,"Python, COVID19, Sri Lanka, Data Analysis, Mat Plot Lib"
Introducing Deep Learning Sessions Lisbon,"Introducing Deep Learning Sessions Lisbon

An Artificial Intelligence meetup hub in the middle of sunny Lisbon.

The Deep Learning revolution that took over the world in the last few years has changed the landscape of computation. The fascination with this technology is not restricted to academia or industry and people all around the globe have developed a growing interest in understanding the inner workings of these models.

Here in Portugal, this transformation has been as strong as ever and a while back a group of curious students and AI enthusiasts created the “Deep Learning Sessions” meetup group — A community in Lisbon that revolves around the intriguing challenge of unraveling the mysteries of Deep Learning by hosting free events to gather experts to discuss the field and share their experience.

How It All Started

The founders of our group passed us the torch after leaving to pursue their academic careers. The original creators of the group were:

Cátia Fortunato

Cátia is a biomedical engineer that found her true passion in the endless task of understanding how the brain works. In the beginning of the year, she moved to not-so-sunny London, to pursue a Ph.D. in Neuroscience where she is using computational models to enhance brain-computer interfaces. Her free time is split between climbing, sailing, and occasional dancing.

João Carvalho

João is a science enthusiast with a passion for the intersection between machine learning and biomedical technology. Currently, as a first-year Ph.D. student in the Institute for Machine Learning at ETH Zürich, he is working on improving generalizability and interpretability in supervised and semi-supervised models, as well as bringing state-of-the-art models into medical applications. His hobbies include avid gaming, guitar playing, and reading!",https://medium.com/digital-diplomacy/introducing-deep-learning-sessions-lisbon-f7dbb63a8800,['Lucas Soares'],2020-10-13 10:07:38.516000+00:00,273,"Deep Learning, AI, Lisbon, Portugal, Meetup"
TensorFlow Lite Micro,"According to Statistica, 25.6 billion units of microcontrollers were shipped in 2019. There are over 250 billion microcontrollers in the world and this number is projected to grow over the coming years. As a result of this, deep learning on embedded devices is one of the fastest-growing fields. This area is popularly known as tiny machine learning (TinyML). That said, embedded devices pose a couple of challenges, key among them being their low processing power and limited memory. Machine learning models must therefore be able to work on just a few kilobytes of memory. They must also be able to perform inferencing with the low processing power available in embedded systems. In this piece, we’ll look at TensorFlow Lite Micro (TF Micro) whose aim is to run deep learning models on embedded systems. TF Micro is an open-source ML inference framework that has been fronted by researchers from Google and Harvard University. It addresses the resource constraints faced with running deep learning models on embedded systems.

Applications of TinyML

The authors of the above TinyML article start by highlighting some of the applications of TinyML technology. Key among them being:

Wakeword detection — waking up a device with a certain phrase, e.g., ‘Ok, Google.’

Predictive maintenance — resulting from analysis and modeling of signals from microphones, sensors, and accelerometers, just to mention a few.

Acoustic-anomaly detection.

Visual object detection.

Human-activity recognition.

Challenges Facing TinyML

Next, the authors highlight the major challenges faced when implementing machine learning in embedded devices. These include:

There is a lack of a unified TinyML framework for embedded devices.

Deploying models on embedded devices is slow.

Vendors produce different hardware types, hence making it difficult to evaluate hardware performance.

There is a lack of basic features such as memory management and library support.

The proposed TensorFlow Lite Micro (TF Micro) is aimed at solving these challenges.

TensorFlow Lite Micro (TF Micro) Design

The TF Micro framework has been designed with the following principles:

Minimize Feature Scope for Portability

This means that an embedded machine learning framework assumes that the model, input data, and output arrays are in memory. The framework should handle ML computations based on those values, meaning that the library will omit features such as loading models from a filesystem or accessing peripherals for inputs. This design principle is crucial because many embedded platforms don’t have a memory management mechanism as well as library support.

Enable Vendor Contributions to Span Ecosystem

As a result of device fragmentation, developers can’t build software that runs well on different embedded platforms. In order to tackle this challenge, the authors ensure that optimizing the core library operations is easy. They aim for ensuring significant technical support for developers. The authors also encourage submissions to a library repository.

Reuse TensorFlow Tools for Scalability

Exporting a model comes with its own set of challenges. Key among them being export errors that result from developers converting a model to a format that can run on an embedded device. Another challenge is the lack of support for some operations on the target device. It is also important to note that most models are trained on floating operations and these operations consume more space and memory and would therefore be unsuitable for an embedded device. This can be addressed by converting them to a quantized representation. This, however, increases the exporter complexity. To address these challenges, a converter is built on top of the existing TensorFlow Lite toolchain. This has been extended for deeply embedded machine learning systems.

Build System for Heterogeneous Support

This feature aims at achieving a flexible build environment that doesn’t rely on any one platform. This would definitely encourage the adoption of TF Micro by developers.

TensorFlow Lite Micro (TF Micro) Implementation

Developing a TF Micro application involves a couple of steps. The first one is to create a live neural-network-model object in memory. The application developer starts by producing an operator resolver object via the Client API. The operator resolver controls which operations link to the final binary in order to reduce the size of the executable.

The second step is to provide a contiguous memory array referred to as the arena. The arena holds intermediate results and variables that are needed by the interpreter.

The third step is to create an interpreter instance. The instance arguments are the model, operator resolver, and the arena. During the initialization phase, the interpreter allocates all required memory from the arena. All communications between the interpreter and operators are handled by a C API. This ensures that operator implementations are modular and independent of the interpreter’s implementation.

The fourth step is the model execution. At this point, the application retrieves pointers to the memory regions that represent the model inputs. It then populates them with values mainly obtained from sensors or user-supplied data. With the inputs available, the interpreter is invoked so that it can perform model calculations.

Finally, the interpreter returns control to the application after it has evaluated all the operations.",https://heartbeat.fritz.ai/tensorflow-lite-micro-e3f27f1eed1b,['Derrick Mwiti'],2020-12-23 14:20:24.374000+00:00,802,"The application can then access the results from memory.TinyML, Machine Learning, Embedded Devices, Tensor Flow Lite Micro, TFMicro"
"Top Modern Data Science, Data Engineering, Machine Learning Tasks to Learn in 2021","Top Modern Data Science, Data Engineering, Machine Learning Tasks to Learn in 2021 Uniqtech · Oct 22, 2020

It used to be mandatory to be a statistician to be a data scientists. While stats knowledge is still helpful, these practical data science tasks are now must-haves for job applications and data analytics interviews. How many do you know? No worries, keep this list in mind. We will have tutorials that address each of these categories. Subscribe and follow us for updates.

Supervised Learning: classification tasks, probabilities, expected values, maximum likelihood, most likely outcomes.

Unsupervised learning: clustering, grouping similar demographics or behaviors together, segmentation

Similarity: similarity metrics, distance functions, identify similar customers, groups, characteristics, demographics

Regression: best fit line, estimation, forecasting, projection, quantitative

Tree based algorithms: decision tree

Boosting algorithms: XGBoost, CatBoost

Compression: information theory, minimize, quantizationm, reducing data size

Optimization for models, optimization search

Co-occurance, co-variance, adjacency matrix, grid

Deep learning, convolutional neural network, generative networks GANs Generative language models

Recommendation systems, collaborative filtering

Neural search, search, indexing

Network analysis, link prediction, graph algorithms

Reinforcement learning

Causal analysis: randomized controlled experiment, A/B testing, statistical testing, remove confounding factors, isolating important factors

Machine learning fairness, ethics, privacy: facial recognition technology fairness, ethics, also data imbalance. Machine learning privacy, local machine learning, data leak through queries.

Did we miss anything? Write your suggestions below, request content for 2020–2021!",https://medium.com/data-science-bootcamp/top-modern-data-science-data-engineering-machine-learning-tasks-to-learn-in-2021-e94c7962b1d2,[],2020-11-12 21:49:24.179000+00:00,208,"Data Science, Data Engineering, Machine Learning, Supervised Learning, Unsupervised Learning"
Use Your Data Skills to Make Money Online,"Online Courses

If you have been working in the field of data then you have valuable skills and knowledge that others can learn from. There are many online platforms which allow anyone with the appropriate skills and experience to create and market online courses and earn an income.

Datacamp has a network of 270 instructors which you can apply to join. They state that they currently have an audience of over 5.9 million data scientists, and course instructors can earn royalties through their revenue sharing platform. The current FAQ’s on the website state that average monthly earnings for instructors are in the region of $1,000-$2,000.

I am currently in the application stages to become an instructor here and I will update this article with actual earnings data if I am successful.

If you don’t have luck with Datacamp or it doesn’t feel right for you there are many other online course platforms where you can make money. One of these is Pluralsight. This course platform recruits data experts to create video-based, written or hands-on educational material. Similarly to Datacamp, payment is based on royalties which are linked to the number of views that your content has achieved and the volume of paying subscribers the platform has. This excellent article by Troy Hunt gives some insight into working for Pluralsight including some information on earning potential.

There are a huge number of other places where you can publish online course content and earn money. These include course platforms such as Udemy and Skillshare. You might also consider publishing a series of tutorials on a social media platform such as Youtube where you could earn money from paid advertising.

Blogging

I have been blogging and earning a modest additional income through the Medium Partner Program for the past year and a half. As far as blogging platforms go Medium is probably the easiest to get started with and earn money from. You don’t necessarily need to have a large following, if your writing is interesting and of high quality, then it is possible for Medium to distribute it in tags from which you can gain a high number of views.

I currently earn around $400 a month and spend 5–10 hours a week writing, typically publishing once a week on the platform. I’ll be writing a follow-up article soon on how, when and why I write about data science on Medium.

An alternative to Medium is to host your own blog as a standalone website. Wordpress is a great platform for this. You can start a blog for free or you can pay for premium features such as a unique domain name and more storage. If you can get enough traffic to your blog there are a variety of ways to earn money including Google Adsense and affiliate programs such as Amazon Associates.

Freelance work

The final areas I will talk about in this article are freelance agencies and websites. There are a number of platforms online where you can offer your skills, experience and services in return for payment. Upwork is probably one of the most popular sites. Here you can create your profile and apply for freelance jobs.

A quick search for data-related jobs advertised right now reveal 16,744 results.",https://towardsdatascience.com/use-your-data-skills-to-make-money-online-6afc7a32d6ba,['Rebecca Vickery'],2020-05-11 13:28:53.022000+00:00,528,"Online Courses, Data Science, Pluralsight, Udemy"
Dimensionality Reduction for Data Visualization: PCA vs TSNE vs UMAP vs LDA,"In this story, we are gonna go through three Dimensionality reduction techniques specifically used for Data Visualization: PCA, t-SNE, LDA and UMAP. We are going to explore them in details using the Sign Language MNIST Dataset, without going in-depth with the maths behind the algorithms.

What is Dimensionality Reduction?

Many Machine Learning problems involve thousands of features, having such a large number of features bring along many problems, the most important ones are:

Makes the training extremely slow

Makes it difficult to find a good solution

This is known as the curse of dimensionality and the Dimensionality Reduction is the process of reducing the number of features to the most relevant ones in simple terms.

Reducing the dimensionality does lose some information, however as most compressing processes it comes with some drawbacks, even though we get the training faster, we make the system perform slightly worse, but this is ok! “sometimes reducing the dimensionality can filter out some of the noise present and some of the unnecessary details”.

Most Dimensionality Reduction applications are used for:

Data Compression

Noise Reduction

Data Classification

Data Visualization

One of the most important aspects of Dimensionality reduction, it is Data Visualization. Having to drop the dimensionality down to two or three, make it possible to visualize the data on a 2d or 3d plot, meaning important insights can be gained by analysing these patterns in terms of clusters and much more.

Main Approaches for Dimensionality Reduction

The two main approaches to reducing dimensionality: Projection and Manifold Learning.

Projection : This technique deals with projecting every data point which is in high dimension, onto a subspace suitable lower-dimensional space in a way which approximately preserves the distances between the points.

: This technique deals with projecting every data point which is in high dimension, onto a subspace suitable lower-dimensional space in a way which approximately preserves the distances between the points. Manifold Learning: Many dimensionality reductions algorithm work by modelling the manifold on which the training instance lie; this is called Manifold learning. It relies on the manifold hypothesis or assumption, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold, this assumption in most of the cases is based on observation or experience rather than theory or pure logic.[4]

Now let's briefly explain the three techniques before jumping into solving the use case.

PCA (Principal Component Analysis)

One of the most known dimensionality reduction “unsupervised” algorithm is PCA(Principal Component Analysis).

This works by identifying the hyperplane which lies closest to the data and then projects the data on that hyperplane while retaining most of the variation in the data set.

Principal Components

The axis that explains the maximum amount of variance in the training set is called the Principal Components.

The axis orthogonal to this axis is called the second principal component. As we go for higher dimensions, PCA would find a third component orthogonal to the other two components and so on, for visualization purposes we always stick to 2 or maximum 3 principal components.

Source: Packt_Pub, via Hackernoon

It is very important to choose the right hyperplane so that when the data is projected onto it, it the maximum amount of information about how the original data is distributed.

t-SNE ( T-distributed stochastic neighbour embedding )

(t-SNE) or T-distributed stochastic neighbour embedding created in 2008 by (Laurens van der Maaten and Geoffrey Hinton) for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.

(t-SNE) takes a high dimensional data set and reduces it to a low dimensional graph that retains a lot of the original information. It does so by giving each data point a location in a two or three-dimensional map. This technique finds clusters in data thereby making sure that an embedding preserves the meaning in the data. t-SNE reduces dimensionality while trying to keep similar instances close and dissimilar instances apart.[2]

For a quick a Visualization of this technique, refer to the animation below (it is taken from an amazing tutorial by Cyrille Rossant, I highly recommend to check out his amazing tutorial.

link: https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/

Source: Cyrille Rossant ,via OReilly

LDA ( Linear Discriminant Analysis )

Linear Discriminant Analysis (LDA) is most commonly used as a dimensionality reduction technique in the pre-processing step for pattern-classification.

The goal is to project a dataset onto a lower-dimensional space with good class-separability in order to avoid overfitting and also reduce computational costs.

The general approach is very similar to PCA, rather than finding the component axes that maximize the variance of our data, we are additionally interested in the axes that maximize the separation between multiple classes (LDA)[5].

LDA is “supervised” and computes the directions (“linear discriminants”) that will represent the axes that maximize the separation between multiple classes.

UMAP ( Uniform Manifold Approximation and Projection )

Uniform Manifold Approximation and Projection created in 2018 by (Leland McInnes, John Healy, James Melville) is a general-purpose manifold learning and dimension reduction algorithm.

UMAP is a nonlinear dimensionality reduction method, it is very effective for visualizing clusters or groups of data points and their relative proximities.

The significant difference with TSNE is scalability, it can be applied directly to sparse matrices thereby eliminating the need to applying any Dimensionality reduction such as PCA or Truncated SVD(Singular Value Decomposition) as a prior pre-processing step.[1]

Put simply, it is similar to t-SNE but with probably higher processing speed, therefore, faster and probably better visualization. (let’s find it out in the tutorial below)

Use Case

Now we are going to go through the above-mentioned use case where all the three techniques will be applied: specifically, we will try to visualize a high dimensional dataset using these techniques: The Sign-Language-MNIST Dataset:https://www.kaggle.com/datamunge/sign-language-mnist

(Sign-Language-MNIST Dataset), screenshot from kaggle.com

The Data

Size of the train Data

The number of unique labels

Note that: There are 25 unique labels representing the number of distinct sign-languages. Now for better visualization (“ it is very difficult to observe all 24 classes in a single visualization”) and for faster computation. I am only keeping the first 10 labels, omitting the rest.

Implementing Plotting Functions

Standardizing the data

Implementing PCA

After applying PCA, the new dimensionality of the data now has only 3 features compared to the 784 features of the x data.

The number of dimensions has been cut down drastically whilst trying to retain as much of the ‘variation’ in the information as possible.

— PCA — 2D —",https://towardsdatascience.com/dimensionality-reduction-for-data-visualization-pca-vs-tsne-vs-umap-be4aa7b1cb29,['Sivakar Siva'],2020-12-31 14:12:39.285000+00:00,1018,"Data Visualization, Dimensionality Reduction, PCA, t-SNE, LDA"
Using Docker as a Data Scientist,"Good day everyone!

The ship with our data science project is just arriving off the port! Get ready to unload the containers and fire up the environment. Wait… what? Wish it was that easy to go? Well, it basically is. Welcome to Docker!

In this article, we are going to learn

How to install Docker Desktop

How to docker — Docker images and Docker Hub

How to Docker — Docker Containers

Run a complete data science project with Docker

Docker is a modern breakthrough. A way in which we can save and share our projects complete with their original environments. No more incompatibility shenanigans and it's quick and easy. Anyone can take a docker image from a docker image registry, download it, run it and begin working in the same environment the project was built in. I explain what Docker is and how it works in my article What the hell is Docker?!

The ability to reproduce project results over different machines is incredibly important in data science. Docker makes this incredibly easy.

Photo by Noah Carter on Unsplash

If you’re ready, let's get going",https://medium.com/analytics-vidhya/using-docker-as-a-data-scientist-8bbb203fb6b7,['Aleksandar Gakovic'],2020-06-14 18:25:41.704000+00:00,175,"!Docker, Data Science, Projects, Environment, Containers"
The problem of Simplification in Natural Language Processing (NLP),"The problem of Simplification in Natural Language Processing (NLP)

Photo by Lysander Yuen on Unsplash

There are so many extraordinary AI applications out there, but so few looking to solve our big challenges as humans. Think about education for example. What are we really doing to improve our learning systems? For the first time ever, almost all content generated by human kind is available out there. Yet, only a few can access it.

Can we use AI technologies to balance unequal access to information?

During this year CERN’s WebFest I presented SYNTHIA, the App to change education by exploiting AI technologies and making content more accessible to everyone. How? The idea is to leverage on Natural Language Processing (NLP) techniques to perform tasks like text summarization, paraphrasing, speech to text transcription, and more. You can play around with the App (it’s free!), explore its source code and propose new developments.

The straight path to reduce inequality is by rebalancing education, and for that, we need to talk about our challenges. Let’s start with the problem of content complexity.

Keep it simple

One of the biggest barriers to the learning process is the inability to access complex content. But what does “complex” mean?

Lexical complexity: happens when the document or source contains infrequent words that are unknown to the receptor.

happens when the document or source contains infrequent words that are unknown to the receptor. Syntactic complexity: happens s when the source contains long sentences that are difficult or unknown to the receptor.

happens s when the source contains long sentences that are difficult or unknown to the receptor. Semantic complexity: which accounts for the amount of background knowledge required to understand the meaning of the source.

From the 3 levels, semantic complexity is probably the least studied one. It refers to the number of “things” that we can talk about in a given domain, including all the objects in the domain, their attributes, and the relationships between them.

The semantic layer is more tacit than its syntactic structure and, as a result, its calculation is more difficult.

While there are famous and powerful academic and commercial syntactic complexity measures, the problem of measuring semantic complexity is still a challenging one.

How can we assess semantic complexity? Graphs!

Graphs are a data structure employed extensively in fields like Computer Science. Social networks, molecular structures, financial transactions, biological networks, transportation systems, they are all examples of the domain and can be modeled as graphs.

Graphs capture interactions (edges) between individual units (nodes), allowing relational knowledge to be stored, accessed and analyzed. For this reason, they are playing a key role in modern Machine Learning.

Sanja Štajner and Ioana Hulpus developed a method to automatically estimate the conceptual complexity of texts by exploiting a number of graph-based measures on a large knowledge base. By using a high-quality language learners corpus for English, they show that graph-based measures of individual text concepts, as well as the way they relate to each other in the knowledge graph, have a high discriminative power when distinguishing between two versions of the same text.

Automatic Assessment of Conceptual Text Complexity Using Knowledge Graphs. Source: Serbian AI Society

Conclusion

If we succeed in capturing content complexity, we’ll be on our way to simplify existing information and making it more accessible to everyone.

Simplification has a variety of important societal applications, like increasing accessibility for those with cognitive disabilities such as aphasia, dyslexia, and autism, or for non-native speakers and children with reading difficulties.

Having said that, content simplification as a subfield of Natural Language Processing (NLP) is in a very early stage, which results in a lack of robust data sources and techniques. In fact, the subjective nature of simplification itself is at the core of the challenge. What is simple or complex for one person might not be for another, and even if we could agree on the complexity assessment, we would find different levels of understanding (e.g. very complex vs. kind of complex).",https://medium.com/@lopezyse/the-problem-of-simplification-in-natural-language-processing-nlp-314589061581,['Diego Lopez Yse'],2021-09-05 21:42:51.152000+00:00,644,"Natural Language Processing, NLP, Simplification, Text Summarization, Paraphrasing"
Should We Be Using Lisp For Data-Science?,"General Ease Of Use

Where Lisp really falls on its face is how hard it is to use as your primary language. To be clear — this is certainly not the fault of Lisp. It is important to remember that Lisp was released as open-source software at a time when the internet didn’t exist and most people didn’t have a computer in their home. With that in mind, using packages is nowhere near as seamless as it is in Python or Julia. In Lisp, packages need to be loaded directly from files in order to be used, which is certainly not great for any sort of deployment, notebook, or data analysis.

There are some more modern options that counter-act this like QuickLisp. QuickLisp allows you to have a very light package manager ran alongside your Lisp installation. This is of course so long as you are using a compiler that supports QuickLisp, like the Steel Bank compiler.

Lisp also doesn’t have many packages pertaining to the Data Science field, despite its age. There is certainly a lot of science and a lot of great things you could potentially do with Lisp, even with the packages that are there now. However, compared to the enormous monster that is Python’s ecosystem, there is just no question.

Auto-Code

There is one fantastic thing about Lisp that is often overlooked and could really be a great reason to use the language for machine-learning:

The language is simple.

While the language’s syntax might seem a little complex and confusing at first glance, the language itself is actually incredibly simple. The language can be re-written in most languages with ease in usually under 1,000 lines of code, which is quite impressive. Simplicity is magic when it comes to meta-code and automation, and Lisp certainly hits the mark in that regard. Lisp has long been venerable for its simple, yet precise syntax that can easily be manipulated and used to meta-program automatically.",https://medium.com/chifi-media/should-we-be-using-lisp-for-data-science-13136306d273,['Emmett Boudreau'],2020-07-24 22:24:21.657000+00:00,318,"Lisp, Quick Lisp, Data Science, Auto-Code, Meta-Programming"
Haar Cascade Classifiers,"Learn and implement Haar cascade classifier in projects…

Viola Jones Algorithm

Working of Classifiers

The Viola-Jones object detection framework is a machine learning approach for object detection, proposed by Paul Viola and Micheal Jones in 2001. This framework can be trained to detect almost any object, but this primarily solves the problem of face detection in real-time. This algorithm has four steps.

1. Haar Feature Selection

Objects are classified on very simple features as a feature to encode ad-hoc domain knowledge and operate much faster than pixel system. The feature is similar to haar filters, hence the name ‘Haar’. An example of these features is a 2-rectangle feature, defined as the difference of the sum of pixels of area inside the rectangle, which can be any position and scale within the original image. 3-rectangle and 4-rectangle features are also used here.

Haar Features

2. Integral Image Representation

The Value of any point in an Integral Image, is the sum of all the pixels above and left of that point. An Integral Image can be calculated efficiently in one pass over the image.

3. Adaboost Training

For a window of 24x24 pixels, there can be about 162,336 possible features that would be very expensive to evaluate. Hence AdaBoost algorithm is used to train the classifier with only the best features.

Image by Packt

4. Cascade Classifier Architecture

A cascade classifier refers to the concatenation of several classifiers arranged in successive order. It makes large numbers of small decisions as to whether its the object or not. The structure of the cascade classifier is of a degenerate decision tree.

Architecture

Implementation

Application

Despite the arrival of deep learning(RCNN, YOLO, etc), this method is still used in many applications for face and object detection, as this is very simple yet powerful.",https://medium.com/datadriveninvestor/haar-cascade-classifiers-237c9193746b,['Om Rastogi'],2020-10-29 01:29:22.610000+00:00,281,"Haar Features, Integral Image Representation, Adaboost Training, Cascade Classifier Architecture, Viola Jones Algorithm"
Analyzing your data,"BOX PLOT:

import seaborn as sns

sns.boxplot(x=”drive-wheels”,y=”price”,data=df)

Box plot gives us information like the five lines in each box from the x-axis represents the min value, 25%, 50%, 75% and the max value respectively. The lowest line is the minimum price and the top line represents the max price. In the first box, 17000 is the median price, 25% cars have price below 13000 and 75% cars have price below 22000. Max price is 36000 and min price is 7000.

SCATTER PLOT:

Scatter plot basically represents relation between 2 features in a 2D plane.

import matplotlib.pyplot as plt

x=df[""engine-size""]

y=df[""price""]

plt.scatter(x,y)

plt.xlabel(""engine-size"")

plt.ylabel(""price"")

plt.title(""Scatter plot"")

CORELATION:

pearson coefficient — to check corelation between 2 features on the basis of corelation-coefficient and p-value.

corelation coefficient has +ve relation if it’s value is nearly equal to 1, has -ve relation if it’s value is nearly equal to -1 and no relation if it’s value is nearly equal to 0.

p-value shows strong certainty if the value is less than 0.001, moderate certainty if the value is less than 0.05, weak certainty if the value is less than 0.1 and no certainty if the value is greater than 0.1.

corelation coefficient close to 1 or -1 and p-value less than 0.001 is considered strong corelation.

import seaborn as sns

ax = sns.heatmap(df.corr(), vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=200), square=True)

Corelation coefficient values of each feature with each feature is given above. The boxes that are more coloured are more co-related and hence must be used as a feature.

DISTRIBUTION PLOT:

This is basically used to check our results and visualize training output and test output together for better understanding.

x axis has y/yhat output values and y axis has frequency

import seaborn as sns

from sklearn.linear_model import LinearRegression

lm = LinearRegression()

X = df[[“horsepower”, “curb-weight”, “engine-size”, “highway-mpg”]]

Y = df[“price”]

lm.fit(X, Y)

Yhat = lm.predict(X)

axl = sns.distplot(Y, hist=False, color=”r”, label=”Actual Value”)

sns.distplot(Yhat, hist=False, color=”b”, label=”Fitted Values”, ax=axl)

We used linear regression to fit our training set. From the figure below, we can clearly get how the price gets changed. Also, we get to know at what price the frequency is higher or lower both before and after training.

Thanks.

You can reach me via mail, linkedIn, github.",https://medium.com/analytics-vidhya/analyzing-your-data-e71489fea111,['Ashish Gusain'],2020-07-18 16:06:54.871000+00:00,340,"Box Plot, Scatter Plot, Correlation, Distribution Plot, Linear Regression"
Review: DeepLabv1 & DeepLabv2 — Atrous Convolution (Semantic Segmentation),"1. Atrous Convolution

The term “Atrous” indeed comes from French “à trous” meaning hole. Thus, it is also called “algorithme à trous” and “hole algorithm”. Some of the papers also call this “dilated convolution”. It is commonly used in wavelet transform and right now it is applied in convolutions for deep learning.

Below is the equation of atrous convolution:

1D Atrous Convolution (r>1: atrous convolution, r=1: standard convolution)

When r=1, it is the standard convolution we usually use.

When r>1, it is the atrous convolution which is the stride to sample the input sample during convolution.

The below figure illustrate the idea:

Standard Convolution (Top) Atrous Convolution (Bottom)

The idea of atrous convolution is simple. At the top of the figure above, it is the standard convolution.

At the bottom of the figure, it is the atrous convolution. We can see that when rate = 2, the input signal is sampled alternatively. First, pad=2 means we pad 2 zeros at both left and right sides. Then, with rate=2, we sample the input signal every 2 inputs for convolution. Thus, at the output, we will have 5 outputs which makes the output feature map larger.

If we remember FCN, a series of convolution and pooling makes the output feature map very small, and need 32× upsampling which is an aggressive upsampling.

Also, atrous convolution allows us to enlarge the field of view of filters to incorporate larger context. It thus offers an efficient mechanism to control the field-of-view and finds the best trade-off between accurate localization (small field-of-view) and context assimilation (large field-of-view).

In DeepLab, using VGG-16 or ResNet-101, the stride of last pooling (pool5) or convolution conv5_1 is set to 1 respectively to avoid signal from decimated too much. And atrous convolution is used to replace all subsequent convolutional layers using rate = 2. The output is much larger. We only need to have 8× upsampling to upsample the output. And bilinear interpolation has quite good performance for 8× upsampling.",https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d,['Sik-Ho Tsang'],2019-03-20 15:57:31.370000+00:00,318,"Atrous Convolution, Algorithme à Trous, Hole Algorithm, Dilated Convolution, Wavelet Transform"
Why McDonald’s is Using Artificial Intelligence to Spy on Its Dumpsters,"Somebody’s Eyes Watching

In an age where surveillance and monitoring are all too easily misused, the idea of a camera in odd places can seem a bit weird and even a little creepy.

Even when cameras are where they’re supposed to be, it can mean that you’re being watched by who knows who.

I definitely cover my webcam to keep unscrupulous eyes from peering through watching me in my own home.

So, I was a little leery when I found out that McDonald’s was putting cameras in their dumpsters.

“For what?” I said.

My first thought was that they wanted to do that to possibly catch dumpster divers, but then I thought what would somebody be trying to find that they could sell in a McDonald’s dumpster?

Then I thought well maybe they’re putting the cameras in the dumpsters to catch homeless people rummaging through them, which I thought was a rotten thing for them to do.

But neither of these is the case.",https://medium.com/technology-hits/why-mcdonalds-is-using-artificial-intelligence-to-spy-on-its-dumpsters-85705a26cb29,['Audrey Malone'],2020-12-30 05:04:15.376000+00:00,156,"Surveillance, Monitoring, Cameras, Mc Donald's, Dumpsters"
"Assessing Global Health, One 📈 at a Time","The Institute for Health Metrics and Evaluation website is a treasure trove of data. On a semi-regular basis, the institute publishes data visualizations using the daily freely available on the website.

While the data is “freely available” on the charts, most of it isn’t downloadable or “unlocked” as such. By re-plotting it in Plotly, we “free the data,” so to speak, opening it up to future investigation as other users can easily download it, play with it, and explore it.

In this post, we present five charts that speak volumes about the state of health around the globe, examining in particular key areas such as healthy life expectancy, the prevalence of overweight adults, smoking prevalence, high-risk drinking prevalence, and deaths in the United States.

Plotly graphs are now embeddable in Medium, as demonstrated below. To learn how to embed can’t-miss Plotly graphs in Medium, check out our how-to post.

Make sure your Plotly graphs look as sharp on your desktop computer as they do on your mobile device — our post on mobile charts has the details.

Healthy Life Expectancy

Did you know? With an average healthy life expectancy (HLE) of nearly 74 years, Japan leads the world. On the flip side, the Kingdom of Lesotho, a landlocked country in southern Africa, holds the distinction for having the globe’s lowest average HLE at 41.19 years.

Iceland has the 2nd highest average HLE at 72.95 years and the Central African Republic the 2nd lowest at 43.19 years.

2. The Prevalence of Overweight Adults

Kuwait holds the distinction of having the highest prevalence (79%) of overweight adults in the world (BMI ≥ 25 for adults aged 20+). The overweight and obesity epidemic has been investigated in this journal article.

Despite this, Kuwait sports a fairly average HLE of 69.56 years.

Egypt also has a near-global high prevalence of overweight adults at 70.9%. Perhaps this high value can be attributed to a high carbohydrate and calorie diet, as speculated about by The Guardian.

To the contrary, Burundi in south central Africa, has the lowest prevalence of overweight adults at 6.3%.

2a. The Prevalence of Overweight Adults vs Healthy Life Expectancy

China and the U.S. have very nearly the same HLE, but being overweight is 38% more prevalent in the states. Meanwhile, the Philippines has an overweight prevalence of 26.9% but an HLE of just 62.05 years.

HLE is likely more closely linked to things like local conditions, reliable access to healthcare, and nutrition than it is to being overweight.

3. Daily Smoking Prevalence

At 45.6%, Greenland has the highest prevalence of smokers in the world. 14% less prevalent than Greenland, Macedonia has the second most smokers.

Meanwhile, smoking is least prevalent in Panama, with only 3.5% of the population identifying as smokers. A smoker in Panama would have to spend 13.1% the of national median income to purchase 10 of the cheapest cigarettes to smoke each day.

3a. Daily Smoking Prevalence vs Healthy Life Expectancy

India has a daily smoking prevalence of 9.8% while Greece is sky high at 31.2%. Despite this, India has an HLE of 58.12 years and Greece 71.13.

4. High Risk Alcohol Consumption

Belarus in Eastern Europe is home to the world’s heaviest drinkers. High risk alcohol consumption is pegged at 28.7%. It is speculated that 80% of murders and grave injuries in Belarus are committed under the influence of alcohol. Only 11% of Belarusians completely abstain from alcohol.

Mauritania in western Africa has the distinction of having the least prevalent amount of high-risk drinkers — this is because it is a dry country.

4a. High Risk Alcohol Consumption vs Healthy Life Expectancy

It likely isn’t a coincidence that the three countries where high risk drinking is most prevalent (Russia, Ukraine, Belarus) are not setting healthy life expectancy records.

5. Deaths by State

Prevalence of all-cause death in the U.S. is highest across the mid-south, Southeast, and Appalachia. It is interesting that the region is well aligned with the CDC-identified diabetes belt. Within the diabetes belt, 11.7% of the people have diagnosed diabetes. Outside the belt, 8.5% have diagnosed diabetes.

6. Crossfilter

Crossfilter is a visual analysis technique for multidimensional data that can help clarify correlations between dimensions.

To use crossfilter, simply click-and-drag on any of the charts and maps in this dashboard. Data sharing common rows between the charts will highlight in red, helping pinpoint complex relationships between various health indicators and locations on the globe.

Crossfilter in action. Link to dashboard.

You can learn more about crossfilter and try it for yourself here.",https://medium.com/plotly/assessing-global-health-one-at-a-time-d061a144a421,[],2017-10-31 15:59:12.215000+00:00,724,"Health Metrics, Data Visualization, Plotly Graphs, Healthy Life Expectancy, Overweight Adults"
Advanced visualization techniques for time series analysis,"Step 2: Build the Markov transition matrix

Once our signal is quantized, we can build a Markov transition matrix:

How to read the Markov transition matrix? Let’s have a look at the first row: | 465 | 86 | 1 | 0 | 0 | 0 | 0 | 0 | :

The first cell tells us that 465 points that are in the bin 0 (with values between 82.4 and 118.4) are followed by a value that stays in this very bin.

The second cell tells us that 86 points that are in bin 0 transition to the next bin (with a value between 118.4 and 137.4)

… and so on…

The eighth cell tells us that no point from bin 0 transition to the last bin

Let’s now have a look at the second row: | 80 | 405 | 63 | 2 | 0 | 2 | 0 | 0 | : the first cell tells us that 80 points that are in bin 1 (with a value between 118.4 and 137.4) are followed by a value that is in bin 0 (between 82.4 and 118.4).

Last but not least, the diagonal captures the frequency at which a quantile transitions to itself (self-transition frequency).

Step 3: Compute transition probabilities

We will now normalize each bin. This matrix contains now the transition probability on the magnitude axis:

The first column correspond to the probabilities that a given value (part of a given bin) transitions to bin 0. The second column are probabilities that a given value transitions to bin 1, etc. For instance, if a given value X[i] is equal to 130.0, it is between 118.42 and 137.42 and it's binned in bin 1. Given the matrix above, there is:

A 14.5% chance that the next value will be between 82.43 and 118.42 (bin 0)

chance that the next value will be between and (bin 0) A 73.4% chance to stay within the same range (between 118.42 and 137.42 , bin 1)

chance to stay within the same range (between and , bin 1) A 11.4% chance that the next value will be between 137.42 and 156.78 (bin 2)

chance that the next value will be between and (bin 2) … and so on…

Step 4: Compute the Markov transition field

The idea of the transition field is to represent the Markov transition probabilities sequentially to preserve the information in the time domain. The MTF generation process aligns each probability along the temporal order to build the MTF for the whole signal:

How to read the Markov transition field? MTF[i,j] denotes the transition probability from the quantile q[i] to the quantile q[j] . Let's have a look at a couple values in the second row of this field:

array([68.8, 68.8, 68.8, 68.8, 68.8, 68.8, 10.7, 10.7, 10.7, 10.7])

Let’s have a look at M[1,2] :

We look in which bin falls the signal at timestep x[1] : this is X_binned[1] = 2

: this is We also look in which bin falls the signal at timestep x[2] : this is X_binned[2] = 2

: this is We then look up into the Markov transition matrix to get the probability that a value from bin 2 stays into bin 2

This value is 68.8%, hence M[1,2] = 68.8%

What does this mean? Well, the transition that occurs between timestep x[1] and timestamp x[2] has an 68.8% chance from happening when looking at the whole signal.

Let’s now have a look at M[1,6] :

We look in which bin falls the signal at timestep x[1] : this is X_binned[1] = 2

: this is We also look in which bin falls the signal at timestep x[10] : this is X_binned[6] = 1

: this is We then look up into the Markov transition matrix to get the probability that a value from bin 2 transitions into bin 1

This value is 10.7%, hence M[1,6] = 10.7%

The transition that happens between timestep x[1] and x[6] has a 10.7% chance of happening when looking at the whole signal.

Let’s now plot the transition field we just computed:

High resolution MTF: pixel colors map to transition probabilities (image by the author)

Step 5: Compute an aggregated MTF

To make the image size manageable and computation more efficient (the above MTF is a matrix of dimension 4116 x 4116), we reduce the MTF size by averaging the pixels in each non-overlapping m x m patch with the blurring kernel 1/m² . m is the image size and we set it, arbitrarily, to 48. That is, we aggregate the transition probabilities in each subsequence of length m = 48 together. Let's compute an aggregated MTF accordingly: if the signal length can be divided by the chosen image size (48), which is the case here, we can just compute the aggregate MTF with this simple function:

Otherwise, we need to compute a piecewise aggregate approximation, but don’t worry, this is fully implemented and taken care for you in the pyts package! The image obtained is exactly the same than the one shown at the top of this article:

Ta da! (image by the author)

Step 6: Extracting meaningful metrics

The diagonal of the MTF contains probabilities for the self-transitions:",https://towardsdatascience.com/advanced-visualization-techniques-for-time-series-analysis-14eeb17ec4b0,['Michaël Hoarau'],2020-12-23 21:11:29.493000+00:00,835,"the probability for a given point to stay in the same bin. We can compute metrics such as entropy and complexity from these probabilities.Markov Transition Matrix, Markov Transition Field, Quantization, Self-Transition Frequency, MTF Generation Process"
The Beauty of Big Data,"Teradata’s Art of Analytics helps people make an emotional connection with the numbers that rule their world.

In her day-to-day work, Yasmeen Ahmad tackles immensely complex datasets, deploying an arsenal of approaches and methodologies that would sound intimidating to most lay people.

From predictive modelling to text analytics, time series analysis to development of attribution strategies, few people can easily wrap their heads around what such terms mean, and even fewer are capable of drawing actionable insights from them — which is why data scientists such as herself are always in such high demand.

Ahmad worked in the life sciences industry before pivoting to commercial work, where she is now Director of Think Big Analytics, the consulting branch of IT service management company Teradata.

Over many years helping clients across a variety of industries to make sense of their data, however, she realised that the best way help them see meaning in those datasets was to literally paint them a picture.

“Visualisation is a core component of any data science and analytical project,” she explains. “It is almost always used at the beginning to understand the datasets you are working with, and can help to quickly identify anomalies, outliers and strong correlations in the data.”

As far as data is concerned, she says, a picture really is worth a thousand words, as visualisation helps to add meaning on top of data that is much easier to assimilate for humans than descriptive words or single numbers and values alone.

Her team would therefore routinely include such visuals when they presented their key results to clients, and found that even people who might not be well versed in data science or technology could still connect with them. The visualisations supported storytelling around a project, engaging business stakeholders to understand connections, relationships and associations in the data.

“As more investment goes into data platforms and analytical technologies, the artwork helps to provide a face to this investment. We had business leaders commenting on how beautiful the visualisations were. Colour, shape and layout were all dimensions that were used to convey meaning. The choice of how a visualisation was formed is actually a creative process — like creating a piece of art.”

From there, she explains, it was a short leap to the idea behind the Art of Analytics project, which brings together a range of those visualisation pieces from their previous projects.

Ahmad believes one of the main strengths of the project is its ability to bring data to life for lay audiences, creating a connection between data insights and observers and bridging the technical gap.

“The visualisations push the human to look beyond individual numbers and values, to thinking about data as a series of connections to be explored. They make it particularly easy to see associations, connections, pathways etc. The art is providing form to the complex fields of big data and data science — making them accessible to a wider audience.

Yet the usefulness of data visualisation is not limited to non-technical people. According to Ahmad, it is also a key component of a data scientist’s toolkit:

“During my life sciences research — I was working in a field where everything was abstracted. The data I analysed often came from human cell samples that could only be seen under microscopes. Hence, collecting data about these samples, analysing it to create insight and then relating the insight back to reality was somewhat difficult. Visualisation was key to help portray not only the insights, but also how they linked to human cells and biology in general.”

Art of Analytics was also an opportunity for Ahmad to bring together those creative and scientific sides. Data science, she says, is actually a highly creative discipline that combines technical know-how with lateral thinking and the ability to tease out stories from complex datasets.

“I believe that art and science need to come together to help to solve the world’s most complex problems, and the best data scientists are not only great at statistics, maths and analytical subjects, but are also creative problem solvers who can translate their work into meaningful messaging that connects with their business and commercial audiences.”

This is an on-going project, and there are plans to create new data representations as they work with new datasets they haven’t encountered before.

Ahmad is also keen to explore how other media such as animation, video and perhaps even VR could help add other dimensions to that work. By creating a video, she says, it would be possible to create another level of emotional connection with audiences, by representing how those relationships have evolved over time.",https://medium.com/edtech-trends/big-data-is-beautiful-d8397e10e4f8,['Alice Bonasio'],2017-11-22 08:57:47.281000+00:00,754,"Data Science, Data Visualisation, Big Data, Analytics, Art Of Analytics"
"Some Things Should Be Done By Hand, But Not Combining Data","Photo via Death to the Stock Photo

Marketers have a lot of tasks on their hands already, between data collection and analysis and coordinating campaigns. A simple task like comparing data from Twitter to data from Google Analytics becomes needlessly complicated when they have to be combined by hand. Don’t your hands have something better to do, like making coffee?

Not to mention the level of human error introduced when automatically generated data gets opened, altered, shared, and analyzed. That is the reason Popily does so much cleaning automatically, and now it will combine datasets for you too!

One Button, Hours Saved

I tried this out with Popily’s own Twitter and Google Analytics data and found some interesting results. As a disclaimer, this data was downloaded from Google Analytics and the Twitter Analytics pages, but I took an extra step to adjust all numbers to display as a percentage of the maximum value, and I simplified the Twitter data to one Tweet per day. It isn’t necessary for Popily to work with it, but it ensures a little privacy.

You’ll notice when you download analytics data from Google that it comes with a little informational header and a summary line. No need to trim that, since Popily understands it. Once you’ve uploaded the data sets you want to combine, click on one to explore it and then click on the little plus sign at the top.

The site walks you through the process of combining two data sets, and the next time you go to explore the data, the combined data will be included in the generated visualizations. That means a faster way to get analysis like this!

Looks like our peak site visits and peak Twitter engagement don’t really line up, but we are getting a lot of engagement. Thanks Twitter followers!

Head over to Popily to try it out! Now what will you do with all that free time on your hands?",https://medium.com/popily-weekly/some-things-should-be-done-by-hand-but-not-combining-data-78e6cb5a2ecb,['Erin Richey'],2016-02-11 19:43:29.583000+00:00,316,"Data Analysis, Marketing, Twitter Analytics, Google Analytics, Popily"
Are Data Science Tasks Destined for Automation?,"By Kat Campise

Gartner predicts that 40 percent of data science tasks will be automated in less than three years. However, as the complexity of machines increases, so will the need for the “science” side of data science.

It seems not a month goes by without a headline stating that a future shortage of data scientists is imminent. And then there is the competing statement by research firm Gartner, that 40 percent of data science tasks will be automated by 2020.

While Gartner is an often-cited authority within the technology community for its predictions, there is some data missing in its postulations. For example, have Gartner analysts sat down with actual data scientists who function within organizations of all sizes? Or is its prediction merely based on the current impetus of machine learning and artificial intelligence permeating the Internet of Things?

Sean Downes, senior data scientist at Expedia.com, led a session at Qubole’s Data Platforms conference in May 2017 entitled “Industrializing Data Science.” He walked participants through the specific issues Expedia faced with migrating its huge data infrastructure to the cloud. In between the discussion of A/B testing and microservice logs, Downes shared some speed bumps that could either substantiate or counter Gartner’s “data science automation” prediction.

Clarity and detail regarding the data are paramount to data scientists:

Who owns the data?

Who owns that field?

What is this field?

Where did that field go?

Why is this field NULL?

As Downes later stated, he was once asked at what point can one consider themselves a data scientist. He answered, “the emphasis is on science.” Scientists are persistent in their questioning and seeking solutions to human- and systems-based problems that arise. While artificial intelligence promises to automate the detail provision, the data is intended for human use. Despite the eagerness of the marketing world to predict human behavior, human emotion throws a wrench into predictive analytics. It’s akin to chasing a constantly moving target.

Why data scientists still top AI

Inevitably, data scientists will pose more questions. Until artificial intelligence can predict which question an expert is going to ask based on a multitude of factors (which means the machine must have the capability to read minds) trained data scientists will still be in short supply.

[ Related: Why The Future of Data Science Is Data Psychology ]

Data scientists are not engineers and engineers are not data scientists. However, Downes emphasized fundamental similarities in streamlining the data science process, which also aligns with data engineering:

Pick a robust standard and stick to it.

Production code matters, so document and format to provide information as to the intention of what you’re doing as well as the results you’re expecting.

Pipelines count as production code

But, he included a message to the engineers as to how they can help data scientists do their job:

Create a space for data scientists to save test and training data.

Cluster bootstrap permissions.

Provide access to S3 buckets.

Sandbox clusters are an important part of a data scientist’s ability to test models.

Based on the close interaction between data scientists, data engineers, and the rest of an enterprise, if Gartner’s prediction is correct, then it won’t be only data science tasks (to a certain extent) that will be automated. An organization will rely on machines to function as data engineers and analysts. Also, given the increasingly pervasive data breaches occurring, Gartner’s proposed “citizen data scientist” might have decreasing access to large data sets if the U.S. adopts regulations such as the General Data Protection Regulation set to take effect in 2018.

While everyone waits to see the full extent of artificial intelligence capabilities, data scientists are still in demand. As the complexity of machines increases, so will the need for the “science” side of data science.",https://medium.com/rtinsights/are-data-science-tasks-destined-for-automation-4314dd130082,['Rtinsights Team'],2017-09-04 12:01:01.424000+00:00,606,"data science, automation, artificial intelligence, data engineering, data psychology"
Five Scientific Journals to Follow as a Data Scientist,"Five Scientific Journals to Follow as a Data Scientist

How to Stay Up-To-Date With the Scientific Community

Photo by Susan Yin on Unsplash

The field of data science is advancing at an incredible pace. New scientific articles are published daily. As a student, I try to stay up-to-date with the scientific literature that is published. In this blog post, I created a list of scientific journals that I believe every data scientist should follow. The journals are presented in no particular order.

International Journal of Data Science and Analytics

The journal welcomes experimental and theoretical findings on data science and advanced analytics along with their applications to real-life situations.

The first scientific journal on the list is the International Journal of Data Science and Analytics (JDSA). This is no surprise since this was the first scientific journal in data science and analytics. The editor-in-chief is Longbing Cao from the University of Technology in Sydney, Australia.

The journal contains articles from many subdomains in data science mostly focussing on machine learning and big data. It brings together thought leaders, researchers, industry practitioners, and potential users of data science and analytics, to develop the field, discuss new trends and opportunities, exchange ideas and practices, and promote transdisciplinary and cross-domain collaborations [1].

Link to a recent edition: Volume 10, issue 4, October 2020",https://towardsdatascience.com/five-scientific-journals-to-follow-as-a-data-scientist-bc50f3590bc2,['Ruben Kerkhofs'],2020-09-06 16:55:11.308000+00:00,212,"datascience, analytics, machinelearning, bigdata, scientificjournals"
How to Efficiently Fine-Tune your Machine Learning Models,"How to Efficiently Fine-Tune your Machine Learning Models

Motivation

Have you ever wanted to experiment with different parameters for your model but find it really time-consuming to do so? Let’s say you want to use support vector machines (SVM) to train and predict the model.

You debate what could be the best value for C so you keep experimenting with different values of C manually and hopefully get good results. That is time-consuming but still not so bad.

But what if you want to first use sklearn.feature_extraction.text.TfidfVectorizer to extract important texts, then use sklearn.feature_selection.SelectKBest to find the best features, and finally use sklearn.SVM.LinearSVC to train and predict the model? There would be so many parameters to tune in. Plus you need to find a way to record the results for comparison. So your notebook can look like a mess!

Is there a better way to select the parameters, shorten the code, as well as save the results for comparison? Yes, of course! Our method will be like a delicious cake with three simple ingredients:

Pipeline to combine different models

Grid search to find the best parameters for each model

MLflow to record the results

Have all the ingredients you need? Awesome! Now we are ready to put them together.

Photo by Drew Patrick Miller on Unsplash

Pipeline

What is pipeline? sklearn.pipeline.Pipeline allows us to combine a list of transforms and a final estimator into a chain.

For example, to predict whether a tweet is aggressive or not, after processing the text, we use tfidf_vectorizer, and SelectKBest as the transformers and svm.LinearSVC as a final estimator.

The codes above could be combined into a pipeline with a list of steps represented as tuples. In each tuple, the first parameter is the name of the class, and the second parameter is the class.

Much shorter and organized! We could easily switch any of the steps to another transformer or estimator. Make sure the classes in the pipeline are from sklearn. If they are not, you could easily create customized sklearn classes with def __init__, def fit(), and def transform() like this.

Find the example source code here

Grid Search CV

So we got the pipeline ready to transform and predict our data. But how about parameter tuning? We could easily find the best parameters for transformers and estimator in our pipeline with sklearn.model_selection.GridSearchCV.

Let’s say we want to find the best value of k for SelectKBest , the best value of C for svm.LinearSVC, best values of analyzer, ngram_range, and binary for TfidfVectorizer. We could give our grid several parameters to search for. The parameters are represented as lists nested inside a dictionary.

To set the parameters of a particular class, we use class_name__parameter = [para_1, para_2, para_3]. Make sure to have two underscores between class’s name and parameter.

grid_search.fit(X_train, y_train) creates several runs using different parameters with specified transformations, and estimator. The combination of parameters yielding the best result will be chosen for the transformation step.

We could also create other pipelines with different transformers and estimators and again use GridSearchCV to find the best parameters! But how could we save the results of all the runs we perform?

It is possible to observe the results by scrolling up and down the screen or taking photos of the results. But that is time-consuming, not taking into account that we might want to look at the results later for future projects. Is there a better way to track our results? That is when we need a tracking tool like MLflow

MLflow

MLflow is a tool for managing the end-to-end machine learning lifecycle including tracking experiments, packaging ML code in a reproducible form for sharing, and deployment. We will utilize this tool to keep the log of our results.

Installation of MLflow can be just simple as this

pip install mlflow

Let’s combine what we have done so far and MLflow tracking tool

In this code, we just add three elements to log the results:

mlflow.set_experiment('name_of_experiment') to create a new experiment

to create a new experiment mlflow.start_run() inside which the codes we want to run the experiment are in

inside which the codes we want to run the experiment are in mlflow.log_param() and mlflow.log_metric() to log the parameters and metrics

Save the file above as train.py. The last step is to run the file

python train.py

After the code finish running, we could find the log of MLflow by running

mlflow ui

Access to the link http://localhost:5000 and we should see this

As we can see from above, the metrics and the parameters of the experiments are logged! We could create several experiments and log the results of those experiments for easy and efficient comparison.

Conclusion

Congratulations! You have learned how to tune parameters for your machine learning models efficiently with Pipeline, GridSearchCV, and MLflow. You could find similar example codes for this article here. I encourage you to try out these methods with your existing machine learning projects. Just a little change in your coding approach can make a big difference in the long run. Experimenting and tracking the results efficiently will not only save you time but also make it much easier to find the best parameters and learn from your experiments.

I like to write about basic data science concepts and play with different algorithms and data science tools. You could connect with me on LinkedIn and Twitter.

Star this repo if you want to check out the codes for all of the articles I have written. Follow me on Medium to stay informed with my latest data science articles like these:",https://towardsdatascience.com/how-to-fine-tune-your-machine-learning-models-with-ease-8ca62d1217b1,['Khuyen Tran'],2020-11-27 21:48:05.016000+00:00,890,"machine learning, fine-tuning, SVM, sklearn, Tfidf Vectorizer"
Lane Detection And Lane Departure-The Next Gen Technology,"Lane Detection And Lane Departure-The Next Gen Technology

Introduction

Lane detection is one significant method in the visualization based driver support structure and capable to be used for vehicle routing, cross power, crash avoidance, or lane departure warning system. Different road condition that create this difficulty more complex include dissimilar variety of lanes (straight or rounded), occlusions cause by obstacle, fog, darkness, illumination change (like nighttime), and so on. Therefore it is the method to locate lane in the picture and is a significant enable or attractive skill in different automobile application, include lane departure recognition and warning, travel control, cross-control, and self-directed driving. A lane departure warning system(LDWS) is a technology designed for warning a driver when the vehicle begins to depart from its lane. An effective lane detection system will navigate autonomously or assist driver in all types of lanes like straight and curved, white and yellow, single and double, solid and broken and pavement or highway lane boundaries. The system should be able to detect lane even under noisy conditions such as fog, shadow, and stain.

Lane Detection

According to wikipedia,

“A lane is part of a roadway that is designated to be used by a single line of vehicles, to control and guide drivers and reduce traffic conflicts.”

As I mentioned earlier, lane detection is a critical component of self-driving cars and autonomous vehicles. It is one of the most important research topics for driving scene understanding. Once lane positions are obtained, the vehicle will know where to go and avoid the risk of running into other lanes or getting off the road. This can prevent the driver/car system from drifting off the driving lane.

LANE DETECTION TECHNIQUES

Hough Line Transformation

Hough Transform is a technique used for extracting features that can be used in image analysis and digital image processing. Traditional Hough Transform is basically used for identifying lines in the images. There was a difficulty in detecting straight lines, circles etc. in automated analysis of digital images. The edge detector has been used in pre-processing stage for obtaining points on image that lie on desired curve but due to some problem in image, some of the pixels were missing on desired curve.

Hough Transform

Edge Detection

This method is based on the idea of identifying points in an image at which image brightness changes sharply. Edge is defined as organized set of curved line segments. This set consists of points at which brightness of image changes sharply. Edge Detection is a tool used in image processing for feature detection and extraction.

However it is not always possible that ideal edges can be obtained from real life images of modern complexity. An edge detection algorithm called canny edge detector is used to detect edges in an image. This method uses multiple stage algorithm and aims in discovering the optimal edge detection.

Canny Edge Detector

Code for Canny Edge Detection

img = cv.imread(‘image.jpg’,0)

edges = cv.Canny(img,100,200)

images = cv2.imshow(“result”,edges)

Types of Lanes 

1. Traffic Lane : Lane for the vehicles moving from one destination to another. 2.Express Lane : Used by faster moving traffic and has less access to exits/off ramps

3.Reversible Lane: To match the peak flow direction of vehicles is changed. Periods of high traffic flow are accommodated by this lane.

4. Auxiliary lane : Used for separating entering, exiting or turning traffic from the through traffic.

In some areas, for non-moving vehicles lane adjacent to curb is reserved

Lane Departure Warning System consists of

1. Systems which warn the driver if the vehicle is leaving its lane with visual, audible, and/or vibration warnings (lane departure warning, LDW)

2. Systems which warn the driver and, with no response, automatically take steps to ensure the vehicle stays in its lane (lane keeping assist)

3. Systems which assist in oversteering, keeping the car centered in the lane, and asking the driver to take over in challenging situations.

Benefits

1. Gives assistance and details to pedestrians and drivers

2. Uniformity of the markings is an important factor in minimizing confusion and uncertainty about their meaning

3. Allows vehicular drivers to drive safely

Limitations

1. Lane departure warning systems and lane keeping systems rely on visible lane markings. They typically cannot decipher faded, missing, or incorrect lane markings. Markings covered in snow or old lane markings left visible can hinder the ability of the system.

2. UNECE regulation 130 does not require LDWS of heavy vehicles to work under 60 km/h or to work in a curve with a radius lower than 250 meters.

3. Lane departure warning systems also face many legal limitations regarding autonomous driving.

4. As stated previously, this system requires constant driver input. Vehicles with this technology are limited to assisting the driver, not driving the vehicle. Lane departure warning systems biggest limitation is that it is not in complete control of the vehicle. The system does not take into account other vehicles on the road and cannot replace good driving habits.",https://medium.com/@nikhil108/lane-detection-and-lane-departure-the-next-gen-technology-38c4cb73c25c,['Nikhil Saste'],2020-12-19 12:31:26.288000+00:00,793,"Lane Detection, Lane Departure Warning System, Hough Line Transformation, Edge Detection, Canny Edge Detector"
NeurIPS 2020 | Conference Watch on Self-Supervised Learning,"Back in February, when AI conferences were still held in-person, Turing Award winners Geoffrey Hinton, Yann LeCun and Yoshua Bengio shared a stage in New York at an AAAI event, which Synced covered in detail. LeCun told the audience that, after decades of skepticism, he had finally joined Hinton in support of the idea that self-supervised learning may usher in AI’s next revolution.

Unlike supervised learning, which requires manual data-labelling, self-supervised learning (SSL) is an approach that can automatically generate labels. Recent improvements in self-supervised training methods have established SSL as a serious alternative to traditional supervised training. Google’s language representation model ALBERT for example utilizes a self-supervised training framework to leverage large amounts of text.

It’s no surprise then that NeurIPS 2020 (the Conference on Neural Information Processing Systems) would find itself at the forefront of this trend. First proposed in 1986, the annual machine learning and computational neuroscience conference has evolved into one of the world’s leading AI gatherings.

This year, NeurIPS is hosting two workshops dedicated to self-supervised learning: “Self-Supervised Learning for Speech and Audio Processing” from 6:50 am to 4:25 pm PT (2:50 pm to 12:25 am UTC) on Friday, December 11; and “Self-Supervised Learning — Theory and Practice” from 8:50 am to 6:40 pm PT (4:50 pm to 2:40 am UTC) on Saturday, December 12.

Workshop organizers say the machine learning community is keen to adopt self-supervised approaches to pre-train deep networks as this makes it possible to use the tremendous amount of unlabelled data available on the Internet to train large networks and solve complicated tasks.

The main active SSL research direction is in speech and audio processing, particularly automatic speech recognition, speaker identification and speech translation. Challenges in the field include the modelling of diverse speech and languages and improving audio processing. Also, most existing SSL research has been driven to improve empirical performance, proceeding at speed but without a strong theoretical foundation. NeurIPS 2020 is offering these workshops to open and encourage discussion on such unexplored territories in SSL research.

LeCun will give a talk in the Self-Supervised Learning — Theory and Practice workshop, which will feature SSL-interested researchers from various domains, including Google Brain Research Scientists Quoc V. Le and Chelsea Finn. The workshop will explore the theoretical foundations of empirically well-performing SSL approaches, and how the theoretical insights can further improve SSL’s empirical performance.

Finn is also scheduled for a talk at the Self-Supervised Learning for Speech and Audio Processing workshop, where he will be joined by Dong Yu from Tencent, Mirco Ravanelli from MILA, among other speakers.

The keyword “self-supervised” appears in the titles of 27 accepted papers at NeurIPS 2020, across topics such as visual representation, image denoising, relationship probing, speech representation, cross-modal audio-video clustering, etc.

Dots represent papers arranged by a measure of similarity. Blue dots are papers with “self-supervised” in their titles.

Most SSL papers focus on text, audio, or visual representations: Facebook AI’s wav2vec 2.0 paper proposes a framework for SSL of speech representations, while DeepMind’s BYOL (Bootstrap Your Own Latent) paper introduces a new approach to self-supervised image representation learning.

Two papers shed light on SSL for 3D applications. Researchers from Google and Saarland University in Germany present an end-to-end SSL framework for fitting 3D human models to 3D scans of dressed humans, and researchers from the Potsdam University in Germany propose 3D versions for five different self-supervised methods and demonstrate the effectiveness of their methods on three downstream tasks in the medical imaging domain.

Researchers from the Korea Advanced Institute of Science and Technology in their paper propose replacing LiDAR with self-supervised depth estimators. New York University researchers explore SSL through the eyes of a child!

Over 18,000 people around the globe are participating in the NeurIPS 2020 virtual gathering. Organizers have increased the number of oral presentations and added live Q&A sessions with participation from the oral and spotlight presenters.

The online-only format for such conferences has steadily improved over the last year as bugs and bottlenecks have been worked out and new technologies introduced. To better accommodate attendees in different time zones and with varied Internet speed and access, NeurIPS 2020 organizers designed a schedule with two six-hour sessions per day: the first starts at 5 am PT (1 pm UTC) and the second at 5 pm PT (1 am UTC). Paper authors can choose either session to make their presentations. The organizers have also enabled users to choose their preferred bandwidth.

Instead of dedicating a single Zoom room for each poster, organizers opted to hold virtual poster sessions in a common space called Gather Town. This re-thinking emerged following positive feedback from ICLR attendees and the joint affinity groups poster session at ICML, the organizers explain in a blog post. Gather Town is a video-calling space that lets multiple people hold separate conversations in parallel and walk in and out of those conversations as easily as in real life.

It’s hoped this arrangement can help simulate the physical experience of conferences of yesteryear, enabling people to walk around as little video-game avatars, bumping into each other and dropping in and out of poster sessions and conversations.

NeurIPS 2020 is also hosting Town Hall meeting today — one at 4 am PT and one at 4 pm PT (12 am and 12 pm UTC) — giving the community a great opportunity to provide feedback on the changes and discuss how this and future AI conferences might continue to favourably evolve their virtual environments.",https://medium.com/syncedreview/neurips-2020-conference-watch-on-self-supervised-learning-be34842c64aa,[],2020-12-11 16:01:43.529000+00:00,895,"AI, Machine Learning, Self-Supervised Learning, Speech and Audio Processing, NeurIPS 2020"
The Data Science Process — Step by Step,"Step 1: Frame the problem

The first thing you have to do before you solve a problem is to define exactly what it is. You need to be able to translate data questions into something actionable.

You’ll often get ambiguous inputs from the people who have problems. You’ll have to develop the intuition to turn scarce inputs into actionable outputs–and to ask the questions that nobody else is asking.

Say you’re solving a problem for the VP of Sales of your company. You should start by understanding their goals and the underlying why behind their data questions. Before you can start thinking of solutions, you’ll want to work with them to clearly define the problem.

A great way to do this is to ask the right questions.

You should then figure out what the sales process looks like, and who the customers are. You need as much context as possible for your numbers to become insights.

You should ask questions like the following:

Who are the customers? Why are they buying our product? How do we predict if a customer is going to buy our product? What is different between segments who are performing well and those that are performing below expectations? How much money will we lose if we don’t actively sell the product to these groups?

In response to your questions, the VP of Sales might reveal that they want to understand why certain segments of customers have bought less than expected. Their end goal might be to determine whether to continue to invest in these segments or de-prioritize them. You’ll want to tailor your analysis to that problem and unearth insights that can support either conclusion.

It’s important that at the end of this stage, you have all of the information and context you need to solve this problem.",https://medium.com/bakeminds/the-data-science-process-step-by-step-d5892f9ce772,['Swati Jain'],2020-10-24 12:20:04.994000+00:00,293,"problem-solving, data analysis, customer segmentation, sales process, customer insights"
From model inception to deployment,"From model inception to deployment

Machine Learning model training & scalable deployment with Flask, Nginx & Gunicorn wrapped in a Docker Container

We all have been in this position after we are done building a model :p

At some point, we all have struggled in deploying our trained Machine Learning model and a lot of questions start popping up into our mind. What is the best way to deploy a ML model? How do I serve the model’s predictions? Which server should I use? Should I use flask or django for creating REST API? What about shipping it inside docker? Don’t worry, I got you covered with all of it!! :)

In this tutorial, we will learn how to train and deploy a machine learning model in production with more focus on deployment because this is where we all data scientists get stuck.

Also, we will be using docker containers, one for flask app and another for Nginx web server shipped together with docker-compose. If you are new to docker or containerization, I would suggest reading this article.

High-Level Architecture

High level design of large scale Machine Learning model deployment

Setting up

Here is the GitHub link for this project

This is the folder structure that we will follow for this project

Let’s break this piece into three parts:

— Training a Machine Learning model using python & scikit-learn

— Creating a REST API using flask and gunicorn

— Deploying the Machine Learning model in production using Nginx & ship the whole package in a docker container

Model Training

To keep things simple and comprehensive, we will use iris data-set to train a SVM classifier.

iris_svm_train.py

Here, we are training a Support Vector Machine with a linear kernel which is giving a pretty decent accuracy of 97%. Feel free to play around with the training part, try Random Forest or Xgboost & perform hyper-parameter optimization to beat the accuracy.

Make sure you execute the ‘iris_svm_train.py’ because it will save the model inside the ‘model’ folder (refer to github repo).

Building a REST API

Creating a flask app is very easy. No kidding!

All you need to know is how a request from the client(user) is sent to the server and how the server sends back the response and a little bit about GET and POST methods. Below, we are loading our saved model and processing the new data (request) from the user in order to send predictions(response) back to the user.

app.py

We will use gunicorn to serve our flask API. If you are on windows, you can use waitress (pure-Python WSGI server) as an alternative to gunicorn.

Execute the command: gunicorn -w 1 -b :8000 app:app and hit http://localhost:8000’ in your browser to ensure your flask app is up and running. If you get the message ‘Hoilaaaaaaaaa!’, then you are good to go!!

If you want to test the predict(Post) method, use curl command or use Postman curl --header ""Content-Type: application/json"" --request POST --data'[{""sepal_length"":6.3,""sepal_width"":2.3,""petal_length"":4.4,""petal_width"":1.3}]' http://localhost:8000/predict

Deploying the ML model in production

Finally, fun part begins :)

We will use Nginx web server as a reverse proxy for Gunicorn, meaning users will hit Nginx from the browser and it will forward the request to your application. Nginx sits in front of Gunicorn which serves your flask app.

More information on why Nginx is required when we have gunicorn: link

nginx.conf

Wrapping everything inside Docker Container

Congratulations, you have made it to the last part.

Now, we will create two docker files, one for API & one for Nginx. We will also create a docker-compose file which will contain information about our two docker containers. You have to install docker and docker-compose for this to work. Let’s ship our scalable ML app and make it portable & production ready.

Docker file for API (keep it in api folder)

We have created a docker file for API which needs to be saved inside ‘api’ folder with other files including requirements.txt (containing information about python packages required for your app).

Docker file for Nginx(keep it in nginx folder with nginx.conf file)

docker-compose.yml

docker-compose.yml is the master file which binds everything together. As you can see, it contains two services, one for api & one for server(nginx). Now, all you need is just a single command to run your ML app:

cd <project/parent directory>

docker-compose up

Output of above command

Cheers! Your dockerized scalable Machine Learning app is up and running, accepting requests on port 8080 and ready to serve your model’s predictions.

Open a new terminal to run predict method using curl or use Postman

Predictions from your deployed ML model

Thank you for making it till here, comment below if you face any challenges in running the project or have any feedback. Happy Learning!!",https://medium.com/datadriveninvestor/from-model-inception-to-deployment-adce1f5ed9d6,['Akshay Arora'],2018-11-28 10:04:23.359000+00:00,746,"machine-learning, model-training, flask, nginx, gunicorn"
Latest picks: In case you missed them:,"Get this newsletter By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.

Check your inbox

Medium sent you an email at to complete your subscription.",https://towardsdatascience.com/latest-picks-exploring-convolutional-neural-network-architectures-with-fast-ai-43470abc515c,['Tds Editors'],2020-12-09 14:27:36.237000+00:00,40,"newsletter, subscribe, privacypolicy, Medium, emailconfirmation"
How to process a DataFrame with billions of rows in seconds,"How to process a DataFrame with billions of rows in seconds

Yet another Python library for Data Analysis that You Should Know About — and no, I am not talking about Spark or Dask.

Photo by Christian Englmeier on Unsplash

Big Data Analysis in Python is having its renaissance. It all started with NumPy, which is also one of the building blocks behind the tool I am presenting in this article. In 2006, Big Data was a topic that was slowly gaining traction, especially with the release of Hadoop. Pandas followed soon after with its DataFrames. 2014 was the year when Big Data became mainstream, also Apache Spark was released that year. In 2018 came Dask and other libraries for data analytics in Python.

Each month I find a new tool, which I am eager to learn. It is a worthy investment of spending an hour or two on tutorials as it can save you a lot of time in the long run. It's also important to keep in touch with the latest tech. While you might expect that this article will be about Dask you are wrong. I found another Python library for data analysis that you should know about.

Here are a few links that might interest you:

Some of the links above are affiliate links and if you go through them to make a purchase I’ll earn a commission. Keep in mind that I link courses because of their quality and not because of the commission I receive from your purchases.

In case you’ve missed my article about Dask:

Big Data Analysis in Python is having its renaissance

Meet Vaex

Vaex is a high-performance Python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. It can calculate basic statistics for more than a billion rows per second. It supports multiple visualizations allowing interactive exploration of big data.

Vaex vs Dask

Vaex vs Dask logos

Vaex is not similar to Dask but is similar to Dask DataFrames, which are built on top pandas DataFrames. This means that Dask inherits pandas issues, like high memory usage. This is not the case Vaex.

Vaex doesn’t make DataFrame copies so it can process bigger DataFrame on machines with less main memory.

Both Vaex and Dask use lazy processing. The only difference is that Vaex calculates the field when needed, wherewith Dask we need to explicitly use the compute function.

Data needs to be in HDF5 or Apache Arrow format to take full advantage of Vaex.

Install Vaex

To install Vaex is as simple as installing any other Python package:

pip install vaex

Let's take it to a test drive

Photo by Eugene Chystiakov on Unsplash

Let’s create a pandas DataFrame with 1 billion rows and 1000 columns to create a big data file.

import vaex

import pandas as pd

import numpy as np n_rows = 1000000

n_cols = 1000

df = pd.DataFrame(np.random.randint(0, 100, size=(n_rows, n_cols)), columns=['col%d' % i for i in range(n_cols)]) df.head()

How much main memory does this DataFrame uses?

df.info(memory_usage='deep')

Let’s save it to disk so that we can read it later with Vaex.

file_path = 'big_file.csv'

df.to_csv(file_path, index=False)

By reading the whole CSV directly with Vaex we wouldn’t gain much as the speed would be similar to pandas. Both need approximately 85 seconds on my laptop.

We need to convert the CSV to HDF5 (the Hierarchical Data Format version 5) to see the benefit with Vaex. Vaex has a function for conversion, which even supports files bigger than the main memory by converting smaller chunks.

If we cannot open a bigger file with pandas, because of memory constraints, we can covert it to HDF5 and process it with Vaex.

dv = vaex.from_csv(file_path, convert=True, chunk_size=5_000_000)

This function automatically created an HDF5 file and persist it to disk.

Let’s check the dv type.

type(dv) # output

vaex.hdf5.dataset.Hdf5MemoryMapped

Now, let’s read the 7.5 GB dataset with Vaex — We wouldn’t need to read it as we already have it in dv variable above. This is just to test the speed.

dv = vaex.open('big_file.csv.hdf5')

Vaex needed less than 1 second to execute the command above. But Vaex didn’t actually read the file, because of lazy loading, right?

Let’s force to read it by calculating a sum of col1.

suma = dv.col1.sum()

suma # array(49486599)

I was really surprised by this one. Vaex needed less than 1 seconds to calculate the sum. How is that possible? By using memory mapping.

Plotting

Vaex is also fast when plotting data. It has special plotting functions plot1d, plot2d and plot2d_contour.

dv.plot1d(dv.col2, figsize=(14, 7))

Virtual columns

Vaex creates a virtual column when adding a new column, — a column that doesn’t take the main memory as it is computed on the fly.

dv['col1_plus_col2'] = dv.col1 + dv.col2

dv['col1_plus_col2']

Efficient filtering

Vaex won’t create DataFrame copies when filtering data, which is more memory efficient.

dvv = dv[dv.col1 > 90]

Aggregations

Aggregation works slightly differently than in pandas, but more importantly, they are blazingly fast.

Let’s calculate a binary column where col1 ≥ 50.

dv['col1_50'] = dv.col1 >= 50

Vaex combines group by and aggregation in a single command. The command below groups data by the “col1_50” column and calculates the sum of the col3 column.

dv_group = dv.groupby(dv['col1_50'], agg=vaex.agg.sum(dv['col3']))

dv_group

Joins

Vaex joins data without making memory copies, which can save the main memory. Pandas users will be familiar with the join function:

dv_join = dv.join(dv_group, on=’col1_50')

Before you go

Follow me on Twitter, where I regularly tweet about Data Science and Machine Learning.",https://towardsdatascience.com/how-to-process-a-dataframe-with-billions-of-rows-in-seconds-c8212580f447,['Roman Orac'],2020-12-13 10:50:17.598000+00:00,853,"Big Data Analysis, Python, Vaex, Dask, Data Frames"
Why ECDF is better than a Histogram,"Every exploratory analysis always has a histogram, we seek answers but histograms are less informative and biased. This may break the hearts of many who have histograms as their first weapon to perform EDA.

In this post, we will learn to draw a histogram and an ecdf using python, and then we will explore why ecdf is a better choice as a first visualization. We will use iris dataset to draw the histogram of setosa’s petal length.

Import all the required libraries.

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.datasets import load_iris

Get the iris data and convert it into a dataframe.

data = load_iris()

iris = pd.DataFrame(data= np.c_[data['data'], data['target']],

columns= data['feature_names'] + ['target'])

Subset the data to setosa species and draw a histogram of its petal length with 6 bins.

#subsetting

setosa = iris[iris['target'] == 0] #histogram

sns.set_style('whitegrid')

_ = plt.hist(setosa['petal length (cm)'], bins = 6)

plt.title('Histogram')

plt.xlabel('setosa petal length (cm)')

plt.ylabel('counts')

plt.show()

You can see the histogram from the code above. Please notice the structure of the histogram has, the bar heights are higher between the interval 1.3 to 1.7.

Imagine the same histogram with 5 bins and notice how the distribution has changed. The way you bin your data can alter the way to understand it. This is called Binning Bias.

What’s the Alternative?

ECDF stands for empirical cumulative distribution function, which you should use more often to understand your data. The algorithm to build an ecdf is illustrated in the code. The output of the custom ecdf function, when applied to setosa dataframe contains 2 arrays or vectors.

The 1st array(e.g. setosa_sort) contains the petal length in ascending order.

The 2nd array(e.g. setosa_percentiles) contains the percentiles for the respective value of petal length in setosa_sort.

#custom function for ecdf﻿

def empirical_cdf(data):

percentiles = []

n = len(data)

sort_data = np.sort(data)



for i in np.arange(1,n+1):

p = i/n

percentiles.append(p)

return sort_data,percentiles #use the function on the setosa iris data

setosa_sort, setosa_percentiles = empirical_cdf(setosa['petal length (cm)']﻿﻿﻿)

We now use the setosa_sort and setosa_percentiles ndarrays (numpy n-dimensional arrays) to plot the ecdf, the sorted setosa data goes as the x-axis and their respective percentiles go as an input to the y-axis.

_ = plt.plot(setosa_sort,setosa_percentiles, label='Data',color = 'blue')

plt.title(r'$\bf{Empirical \ CDF}$')

plt.xlabel('setosa petal length (cm)')

plt.ylabel('percentiles')

plt.show()﻿﻿﻿

The plot above is an ecdf for setosa petal length. We may observe from the plot that the median(50th percentile) is around 1.5 cms, histograms don't tell you that. We are going to make the above ecdf a little more detailed.

#data preparation for plotting﻿, capturing important statistics

ymedian = np.empty(len(setosa))

ymedian.fill(0.5)

ymax = np.empty(len(setosa))

ymax.fill(np.max(setosa_percentiles))

ymin = np.empty(len(setosa))

ymin.fill(np.min(setosa_percentiles))

y25 = np.empty(len(setosa))

y25.fill(0.25)

y75 = np.empty(len(setosa))

y75.fill(0.75) fig,ax = plt.subplots() # Plot the data

setosa_ecdf = ax.plot(setosa_sort,setosa_percentiles, label='Data',color = 'blue') # Plot the lines

max_line = ax.plot(setosa_sort,ymax, label='Max', linestyle='--',color = '#001c58')

y75_line = ax.plot(setosa_sort,y75, label='75Percentile', linestyle='--',color = '#ff7a69')

med_line = ax.plot(setosa_sort,ymedian, label='Median', linestyle='--',color = '#fb6500')

y25_line = ax.plot(setosa_sort,y25, label='25Percentile', linestyle='--',color = '#4fb4b1')

min_line = ax.plot(setosa_sort,ymin, label='Min', linestyle='--',color = '#ffbb39') #annotate lines

ax.annotate('Max/$100^{th}$ Percentile',xy = (1,0.96),color = '#001c58')

ax.annotate('$75^{th}$ Percentile',xy = (1,0.71),color = '#ff7a69')

ax.annotate('Median/$50^{th}$ Percentile',xy = (1,0.46),color = '#fb6500')

ax.annotate('$25^{th}$ Percentile',xy = (1,0.21),color = '#4fb4b1')

ax.annotate('Min/$1^{st}$ Percentile',xy = (1.3,0.04),color = '#ffbb39') ﻿#final and important bits

plt.tight_layout()

plt.title(r'$\bf{Empirical \ CDF}$')

plt.xlabel('setosa petal length (cm)')

plt.ylabel('percentiles')

plt.show()

Observe, how we have plotted all the important statistics from the data. This sort of statistics you would have seen in a box and whisker plot.

Outliers in Histogram & ECDF

We are going to generate some random data with an outlier and visualize it on the histogram and on the ecdf.",https://medium.com/convergeml/why-ecdf-is-better-than-a-histogram-85deda4129ed,[],2019-01-20 20:32:22.880000+00:00,532,"generate random data with an outlierdata = np.random.normal(loc = 0, scale = 1, size = 1000)outlier_data = np.append(data, [100]) plot the histogram and ecdf of the data with"
The Hardest Part of Data Science? Data.,"The core challenge for data science

It’s not about the algorithms, it’s about the right data to feed them.

Today, when applying data science to business problems, almost no one is inventing a new algorithm (again, not necessarily true to computer vision/audio and NLP use cases). We’ve been using the same models for more than a decade. Gradient boosted trees (XGBoost) were introduced in the previous century, SVMs in the 1960s, and linear regression…probably around 1000BC.

Data science is no different than any other profession that has emerged from new technology and influenced society. It continues to evolve and open endless doors to complex questions and challenges, each one more exciting than the last. And yet we continue to use the same models. The winners of Kaggle competitions prove this to be true by using 30-year-old models to top the leaderboard. If 30-year-old models are winning competitions, it stands to reason that these models are not where the “X factor” comes from.

The truth is, data science has been so focused on trying to generate business value from the newest algorithm, that we’ve missed the most important part: data science is not, and never has been, about models.

Given that the majority of modeling is commoditized and automated, what is the next challenge in building a robust and accurate prediction system? What actually makes a model successful?

Putting the “data” into data science

We’ve all heard the saying “garbage in, garbage out.” If I try to train a model to predict credit default based on the number of characters in a person’s name, I might be successful at getting the model to work (train and predict) but the model would pretty much be randomly guessing.

The main question we need to be asking to determine if a data science project will succeed or not is and always will be: Do we have the right data to train a model to solve the problem we’re trying to solve?

The “right data” can be broken down into multiple sub-questions:

Do we have the right data attributes so that our model can infer realistic patterns and rules? Do we have enough samples to train the model? Did we engineer (integrate, aggregate, transform) the data in the right way before we fed it into the algorithm?

Speaking of engineering…Stanford Professor Andrew Ng accurately said:

“…applied machine learning is basically feature engineering.”

What did Professor Ng mean by “feature engineering?”

Due to the buzz in the field, some confusion has arisen around the meaning of different terms. I suggest we break down feature engineering into its two parts:

Feature generation : The initial phase where we search for data to generate features from. We aggregate, apply business logic, and bring multiple sources into one matrix of numerical attributes per sample.

: The initial phase where we search for data to generate features from. We aggregate, apply business logic, and bring multiple sources into one matrix of numerical attributes per sample. Feature standardization: Normalization, scaling, imputation, and encoding. We run transformations on the feature set to make it more “appealing” to the model. These are usually model related. For example, while decision trees could work just fine with raw, unscaled numerical features, linear models will usually require the right imputation and scaling (e.g. to unit variance and zero mean).

I would argue that feature standardization is walking a similar path to the one that models went down. The same way models have become commoditized, feature standardization can be commoditized as well. Just like autoML techniques, you can loop through different models with different standardization techniques until you find the best performing combination. Unlike the search space of possible features to generate, the different combinations of standardization techniques and different models are not as large as we think they are.

To make my point, here is my pseudo-algorithm to automate the selection of a model with feature standardization techniques:

Given a list of possible feature standardization methods, like scaling and imputation methods (mean imputation, unit variance scaling, PCA, …)

And given a list of possible models (linear regression, SVM, XGBoost with 100 decision trees, XGBoost with 1000 decision trees…)

Generate an experiment with a random preprocessor and a random model.

After a couple of iterations, choose the best experiment.

Now obviously it is not as simple as I presented it. The parameter space is large and the number of combinations should be limited — but it is not rocket science. There are a bunch of tools that do a great job at this (like Auto-Sklearn or TPOT). This will only become easier as computing power becomes cheaper and more available.

Feature generation, however, is not at the same place.",https://towardsdatascience.com/the-hardest-part-of-data-science-data-1877a5cd30d7,['Maor Shlomo'],2019-11-17 18:01:06.697000+00:00,758,"Feature generation is still a manual, time-consuming process.Data Science, Machine Learning, Data Attributes, Feature Engineering"
How Citi is becoming data-first by upskilling teams across the bank.,"Last year, Citi partnered with Decoded to build an in-house Data Academy, where employees could enrol and learn cutting-edge data skills on-the-job.

Stuart Riley, Global Head of Operations and Technology for the bank’s Institutional Clients Group, has said that “the delineation between traders and technologists in markets is disappearing.”

When Citi partnered with Decoded to set up a Data Academy programme, they did it with the vision to become a much more data driven organisation. As an organisation that produces reams of data, how it’s analysed and processed is vital in being able to provide their clients with the top insights. As well as hiring externally, there was a desire to upskill within.

We spoke to Sisi Liu, Project Manager, about how Decoded helped her gain confidence in data, take on new ambitious projects and find unexpected patterns in real business data.

What do you do for your main job?

“I am a project manager in technology, in an area related to regulation.”

What did you think of data before you enrolled on the Data Academy?

“Before the Data Academy, data was just numbers to me. I wouldn’t have been able to find the relationship between data points. I decided to do the Data Academy because I was managing a lot of projects requiring data, but I didn’t have any background in data analysis, which was difficult when I talked to the wider business. Also, I just don’t manage Excel well and sometimes it breaks.”

Have you ever tried to learn data analytics before?

“I was very nervous because I thought you had to have a background in data, and I thought I would be the worst [on the Decoded Data Academy]. Small data is fine — but if you’d given me data and asked me to solve a problem, I never thought I’d be able to. I wanted to understand why people were interested in big data. We are surrounded by data so it’s a good thing to understand.

“Data is for everyone. Before I did the Data Academy I thought it would be hard to understand the data, but it is designed in a very good way, even for those who don’t have a degree in Data or Mathematics you can still get a lot from the Data Academy.”

How did you get selected for the Decoded Data Academy?

“It was quite funny, when the cohort was advertised, the application closing date was when I was on annual leave. I was pinging HR saying can you make the link available right now! Loads of people applied and there was a lot of interest, but there were only 20 people selected. There was so much interest they opened up more places for the next one.”

Was it helpful knowing you had a dedicated Decoded mentor during the course?

“Yes, certainly. Rosalind from Decoded was really helpful — when I ran out of ideas, she suggested ‘you can try this’ or ‘focus on this’ and it immediately gave me ideas. I can really see the difference between having a mentor and not having one — especially the monthly catch ups. Getting a Decoded mentor has definitely helped”.

Has enrolling in the Data Academy changed your role at all?

“Lots of new projects for 2021 are data related, and my boss wanted me to handle those projects, largely because I’m doing the Data Academy. It has allowed me to take on broader projects because I now have the knowledge of dealing with data. Before that I wouldn’t have felt comfortable.”

What was the hardest part of the Data Academy?

“There was a point where I wrote to Rosalind [Decoded mentor] saying it’s quite difficult. Knowing that I do have some support — rather than being on my own — meant I tried to do the work rather than saying no. That’s why a mentor is very important.”

What was the most rewarding part of the Data Academy?

“We prepare projects based on real business data — so one of the most interesting and satisfying things was to be able to use something we learned and be able to find something interesting in the business data. When I presented my analysis to my manager, she was quite surprised, she found it very interesting.”

Has the Data Academy impacted you in any unexpected ways?

“I have been paying more attention to data related stuff after I started the Data Academy. I was watching a TV Series called Numbers, which is related to the FBI and crime, but is ultimately all about data science and probability. It is so interesting to see how he used big data to solve crimes — to help something much bigger. When you don’t have enough data, your projection can be wrong and in this case, you get the wrong suspect. The first thing you need to do is check the data quality.”

What’s next for you?

“The reason I didn’t originally say I wanted to work with data was because I didn’t think I had enough skills or the tools to do data analysis. After the Data Academy I’m now happy to expand my career or areas of interest into analysis.”

Sisi enrolled on the Decoded Data Academy programme at Citi.

The Decoded Data Academy upskills employees in cutting-edge data analytics. We guide these employees to apply new skills in real-time business data for immediate impact on your business. To see more about the programme click here.

Or drop us an email here if you have any questions about how Decoded can help to upskill your teams.",https://blog.decoded.com/how-citi-is-becoming-data-first-by-upskilling-teams-across-the-bank-d4c72b233fd1,[],2020-12-16 14:17:49.531000+00:00,902,"Data Academy, Decoded, Data Analysis, Big Data, Citi"
Creating Fake Maps with GANs,"From the moment I learned of ML models that could create new, artistic images, I wanted to try it. As somewhat of a creative endeavor, I wanted to see if GANs could help me create fake maps… and consequently help inspire perspective about maps and my world.

My expectation was that I would be able to create clear, detailed maps just like Google Maps or Open Street Maps… but with made up street names and novel street patterns. And if I was lucky, there would be some shortcuts! The results (at this point in time)… NOT EVEN F*CKING CLOSE. While I was able to create a working model that produces map-like images… they are a million miles away from my original expectation. But, IMHO this is still a smashing success. It’s really cool to see the iterations and even get to “map-like” images. I would love recommendations on next steps. Here’s the abbrevated journey and lessons learned (so far)…

Iterations of Fake Maps

For starters, I knew I needed a lot of images as training data. I have a personality that allows me to do really mundane shit for long periods of time to meet a goal, so I considered taking hundreds of snapshots of Google Maps to create the dataset. I quickly realized this was futile particularly because all the images should be the same size. Then I found the incredible python package — TileMapBase, that allowed me to create hundreds of uniform, random Open Street Map images (see below, these needed to be 32x32 pixels).

(Augmented) 32x32 pixel tile images from great Minneapolis/St.Paul area.

The model I developed as part of Udacity’s Deep Learning Nanodegree required that the images be 32x32 pixels. So I created ~20 32x32 map images from the greater Minneapolis/St.Paul area and ran the model for 10 epochs (I ran it locally on my laptop since I just wanted to see if it would work without error). It worked and the results, as expected, looked like this…

1st attempt at generating 32x32 pixel map images with DCGAN.

I knew it ran, so that was good. I immediately create ~100 map tiles and ran the model for 100 epochs. Better, but still crappy.

2nd, slightly better, attempt at generating 32x32 pixel map images with DCGAN.

Several issues…

100 images likely isn’t a sufficiently large dataset.

What good is a 32x32 pixel map?!?! I instantly knew I needed a model that could produce larger images.

Training was slow… I needed a GPU.

Next, I created ~850 images size 256x256 pixels (samples below). Not the ideal map size, but way better than 32x32. Additionally, I found and started testing this code that uses a DCGAN to create 256x256 images. Biggest change for me here was that it used keras and tensorflow… and I had gotten used to working with pytorch. Didn’t seem to slow me down much since the repo was really clean and beatifully documented! Finally, I spun up a Google Cloud compute instance to harness the power of GPUs.

256x256 pixel tile images from great Minneapolis/St.Paul area.

Instead of running epochs, it looked like in keras we were exectuing steps. Since the DCGAN256 authors let their algorithm run for 500,000 steps when generating images of women’s blouses and shoes… I figured I would start with 20% of that and see if it was worth doing more. Toward the end of training, here’s what some of the generated images looked like:

256x256 pixel tile images generated from the DCGAN256.

Pretty neat. We can also look at how the model improved over the course of training. It was especially exciting to see this improvement from step 0 to step 25! Note that the sample generated images below are a concoction of 48 individual images (scaled down a little bit).

Sample generated image after 1,000 steps:

Sample generated image after 25,000 steps:

Sample generated image after 50,000 steps:

Sample generated image after 75,000 steps:

Sample generated image after 99,000 steps:

Hell yes. It’s not exactly what I pictured when I first imagined this. However, the algorithm did “learn” about parks and lakes, the highway system (we don’t see highways running through lakes :), and the urban/suburban grid pattern found across the majority of the Minneapolis/St.Paul metro.

As for next steps, I didn’t see vast improvement between 75k and 99k. I could run for longer, but I’m not convinced I know the value of that yet. It would be good to mention that it took me roughly 5 hours to run the full 99k steps and cost me in the $4-$5 range.

I’m starting to see more problems as image problems which is a fundamentally different way of thinking about them. Shane Parrish always reminds us that the map is not the territory. This exercise takes that a step further and reminds me that a map is simply a way to think about relationships rather than a thing that determines reality.

Here is my GitHub repo that contains both the tile map generation and the DCGAN. I would absolutely love to hear thoughts and feedback either via comments or hit me up on LinkedIn.",https://medium.com/@franciscorrigan3/creating-fake-maps-with-gans-42ad1fc94d97,['Frank Corrigan'],2020-11-26 14:44:29.436000+00:00,828,"Machine Learning, Artificial Intelligence, GANs, Maps, Generative Models"
Dunder Data Challenge #4 — Finding the Date of the Largest Percentage Stock Price Drop,"In this challenge, you are given a table of closing stock prices for 10 different stocks with data going back as far as 1999. For each stock, find the date where it had its largest one-day percentage loss.

Begin working this challenge now in a Jupyter Notebook with Binder

Begin working this challenge now in a Jupyter Notebook thanks to Binder (mybinder.org). The data is found in the stocks10.csv file with the ticker symbol as a column name.

Challenge

Can you return a Series that has the ticker symbols in the index and the date where the largest percentage price drop happened as the values? There is a nice, fast solution that uses just a minimal amount of code without any loops.

Extra challenge

Can you return a DataFrame with the ticker symbol as the columns with a row for the date and another row for the percentage price drop?

Begin Mastering Data Science Now for Free!

Take my free Intro to Pandas course to begin your journey mastering data analysis with Python.

Master Python, Data Science and Machine Learning

Immerse yourself in my comprehensive path for mastering data science and machine learning with Python. Purchase the All Access Pass to get lifetime access to all current and future courses. Some of the courses it contains:

Exercise Python — A comprehensive introduction to Python (200+ pages, 100+ exercises)

— A comprehensive introduction to Python (200+ pages, 100+ exercises) Master Data Analysis with Python — The most comprehensive course available to learn pandas. (800+ pages and 350+ exercises)

— The most comprehensive course available to learn pandas. (800+ pages and 350+ exercises) Master Machine Learning with Python — A deep dive into doing machine learning with scikit-learn constantly updated to showcase the latest and greatest tools. (300+ pages)

Get the All Access Pass now!",https://medium.com/dunder-data/dunder-data-challenge-4-finding-the-date-of-the-largest-percentage-stock-price-drop-8dbce150b3a0,['Ted Petrou'],2020-08-25 03:19:17.259000+00:00,289,"data science, machine learning, Python, pandas, scikit-learn"
The Role of Professional Certification,"Why do you need a certification?

The is no doubt, that data is a key asset for the organizations and it is important to treat it right in order to get completive advantage. Tableau is a best of breed technology on the market that allows us to work with data, slice it and do complex data analysis on the fly. However, it requires us to understand the key concept of data analytics and we should know how to “drive” Tableau in order to deliver value. Moreover, we are working in competitive environment and we should constantly improve our skills and learn new technologies, new features and data analysis methods to be on the edge.

In this post, we will learn why it is important to master Tableau and pass the official Tableau Certification. In addition, we will discover that available certification types as well as share additional resource to master Tableau and prepare for successful certification.

TL;DR: Should I pass Tableau Certification? Yes, you should, 100%.

Why Certification is matter?

If we look into Wikipedia and see what does certification means:

Certification refers to the confirmation of certain characteristics of an object, person, or organization. This confirmation is often, but not always, provided by some form of external review, education, assessment, or audit.

In other words, it is the confirmation of our knowledge. In case of data analytics and information technology, it refers to our knowledge of a particular tool or methodology. In our case it is Tableau Desktop.

Certification is checking not only our Tableau knowledge, but it is evaluating our ability to comfortable work with data and communicate with people by using powerful visualization techniques. Moreover, it requires understanding of overall Business Intelligence solution and its role in organization. On the screen below, is high-level diagram for the typical organization that is using Tableau:

Modern analytics solutions

This diagram has three layers:

Source Layer — this layer has source systems, it could be business applications like Sales Force, files, transactional databases and so on.

Storage Layer — this layer consolidates information into Data Warehouse and in some cases leverage Big Data solutions like Data Lake, Hadoop, Spark.

Access Layer — this is the area of Tableau (BI tool), where business users can access data from storage layer or in some cases in source layers.

Idea, behind Tableau is to democratize access for the data. In other words, business users will use rich Tableau functionality to slice and dice data, connect various systems, databases, files, visualize data, build dashboards and explore the data.

This picture clears tells us what is the scope of exam. As a professional Tableau Developer, you should be able to know how to connect data, explore it, slice and dice. Often, you have to build Dashboard or tell a story with the data. It is good to know the best practices of data visualization in order to make your work effective and impactful. In some cases, we should calculate new metrics and leverage Tableau Table Calculations or Level of Details. Sometimes, parameters can help us to filter the data and add self-service functionality. Finally, we should have some knowledge of statistics and know how to use built-in functionality for forecasting, trend lines and clustering. As well as, know about Tableau Server and how to share and publish our work.

Let’s try to understand, what is the reason for certification and why it is goes beyond than just “nice to have”. There are many reasons, why data professionals are obtaining official certification. We can identify at least 3 top reasons:

1. Demonstrate to knowledge of subject and prove that skills are up to date

2. Requirements for employer

3. Requirements for the customer

Demonstrate to knowledge of subject and prove that skills are up to date

First of all, certification gives people stamp of approval that company is looking for. It means that the persons who passed the certification exam can claim that they really do know what they are doing. Hiring managers usually are looking for somebody that’s certified, because they can be confident that they’ve made a good decision. As a result, if you really want to differentiate yourself, if you want to be able to prove to the wider world that you’ve actually mastered the product, that you really do have the skills, and set yourself apart, then it’s well worth the time and effort to study and practice, to train to take the certification exams and eventually pass them, so that you have that brand, that stamp of approval that says not only am I a Tableau user, and not only I am savvy, but I’m actually able to prove that I’m savvy. It feels wonderful when you feel like you’ve done a good job, and it feels great to know that you’re a part of something that is absolutely transforming the business.

Requirements for employer

Often, employers have strong requirements for Tableau Skills. They want to hire Tableau Professional and they will set a baseline with Tableau certification. It means, in order to pass interview or even get a chance for the interview, we should have a certificate.

Sometimes, organizations decide to invest into the workforce and allow everyone to get Tableau training and pass certification exam. It is very good practice and organizations are willing to invest into their employees.

Requirements for the customer

Sometime, we can see the opposite situation. The organization wants to hire external freelances or consulting company. Their main requirement could be the availability Tableau certification. Moreover, Tableau company requires all consulting partners to have certification in order to provide high quality service.

As a result, it doesn’t matter what is the reason behind the certification. It is obvious, that having certification is better than not having and it gives us advantage on the market. Moreover, it will guarantee that we have required skillset and Tableau knowledge and can do analytics job.

Tableau Certification Path

On the screen below there are five available certification exams:

Currently, Tableau offers us two paths:

Tableau Desktop

2. Tableau Server

Let’s look to the summary table about the exam and it’s requirements and purpose:",https://medium.com/rock-your-data/the-role-of-professional-certification-3518393b381c,['D Ma'],2020-01-19 22:16:39.098000+00:00,991,"Tableau Certification Path Summarytableau, data-analytics, certification, tableau-desktop, tableau-server"
Importing Data Should be Simple — Stop Screwing it Up,"Check 4: ID columns — make sure they’re unique

In most datasets, you’ll have some sort of primary key or ID column. These are columns that should never have duplicated values. If your entire analysis hinges on the fact that these values should never be duplicated, then it’s crucial to double check this in your data.

For this, you’d just want to check that the distinct number of values in your ID column(s) matches the number of rows in your dataset.

Solution: In R, I usually would just use the n_distinct() function:

Image by author

To take this a step further, if you wanted to hone in on the offending column, you would want to see what value appears in more than one row. In R:

example_df %>%

count(id_column) %>%

filter(n > 1)

Example of a failed check: If you were looking at a dataset of state populations, and you saw that California appeared twice, while every other state only appears once.",https://towardsdatascience.com/importing-data-should-be-simple-stop-screwing-it-up-89e0ec4aae4f,['William Chon'],2020-12-07 18:41:23.773000+00:00,153,"data analysis, data integrity, R programming, n_distinct() function, primary key"
The Unsung Heroes of Modern Software Development,"Open Source Foundation Leaders

I’ll highlight six open source foundations that are key to many important projects. For each foundation I’ll give a brief bio, provide the number of projects being supported as of early 2019, and highlight some well-known projects. Note that these groups fall under various IRS classifications for charitable and trade organizations — not all are technically foundations.

Apache Software Foundation

The Apache Software Foundation is 20 years old and is one of the largest foundations. As of early 2019 it has over 350 open source initiatives.

The ASF provides an established framework for intellectual property and financial contributions that simultaneously limits potential legal exposure for our project committers. Through the ASF’s meritocratic process known as “The Apache Way,” more than 730 individual Members and 7,000 Committers successfully collaborate to develop freely available enterprise-grade software, benefiting millions of users worldwide: thousands of software solutions are distributed under the Apache License; and the community actively participates in ASF mailing lists, mentoring initiatives, and ApacheCon, the Foundation’s official user conference, trainings, and expo.

www.apache.org

Many Apache projects are Java heavy. Popular projects include: Apache HTTP Server, Hadoop, Tomcat, and Arrow.

Linux Foundation

The Linux Foundation is the home of the Linux operating system and many related projects. Some of its other 100+ projects include NodeJS and RethinkDB.

The Linux Foundation supports the creation of sustainable open source ecosystems by providing financial and intellectual resources, infrastructure, services, events, and training. Working together, The Linux Foundation and its projects form the most ambitious and successful investment in the creation of shared technology.

www.linuxfoundation.org

The Linux Foundation was founded in 2000 as a merger of two other groups. It currently has over 1,000 members, including all the usual big name technology companies.

All hosted projects get governance structure and back-end resources. Some projects also get funding. The Linux Foundation also provides training and conferences.

Free Software Foundation

Launched in 1983, the Free Software Foundation maintains the projects that make up the GNU Linux ecosystem. Other popular projects include Bash, Emacs, Gawk, Make, and R.

The Free Software Foundation (FSF) is a nonprofit with a worldwide mission to promote computer user freedom. We defend the rights of all software users.

https://www.fsf.org/

The Free Software Foundation has over 5,000 members and about 400 OSS projects.

Software Freedom Conservancy

The Software Freedom Conservancy was founded in 2006. It has over 45 projects including such popular ones such as Busybox, Git, Homebrew, Inkscape, phpMYAdmin, PyPy, and Selenium.

Software Freedom Conservancy is a not-for-profit charity that helps promote, improve, develop, and defend Free, Libre, and Open Source Software (FLOSS) projects. Conservancy provides a non-profit home and infrastructure for FLOSS projects. This allows FLOSS developers to focus on what they do best — writing and improving FLOSS for the general public — while Conservancy takes care of the projects’ needs that do not relate directly to software development and documentation.

https://sfconservancy.org/

The Software Freedom Conservancy has over 500 sponsors, including Google and some other big names.

Software in the Public Interest

Software in the Public Interest was founded 1997. Its 39 projects include haskell, PostgrSQL, Jenkins, Arch Linux, and Debian.

Software in the Public Interest is a non-profit organization which was founded to help organizations develop and distribute open hardware and software. Our mission is to help genuine, substantial, and significant free and open source software projects by handling their non-technical administrative tasks so that they aren’t required to operate their own legal entity.

https://www.spi-inc.org/

Cloud Native Computing Foundation (CNCF)

The Cloud Native Computing Foundation is the new kid on the block. Founded in 2015, it supports open source projects around Kubernetes containerized cloud microservices.

CNCF is an open source software foundation dedicated to making cloud native computing universal and sustainable. Cloud native computing uses an open source software stack to deploy applications as microservices, packaging each part into its own container, and dynamically orchestrating those containers to optimize resource utilization. Cloud native technologies enable software developers to build great products faster.

www.cncf.io

CNCF members include a who’s who of tech: AWS, AlibabCloud, Dell, Intel, Oracle, Microsoft Azure, IBM Cloud, and Google Cloud.

As of early 2019, 4 projects have graduated and 16 are incubating. Popular associated projects include Kubernetes, which has graduated from CNCF.

Interestingly, CNCF is supported by the Linux Foundation.

NumFOCUS

NumFOCUS is the home of many popular data science open source projects. It was founded in 2012 and its 25 popular projects include NumPy, Matplotlib, Pandas, Jupyter, Julia, and Bokeh. NumFOCUS also promote many other open source projects as affiliated projects.

NumFOCUS offers many programs in support of our mission to promote sustainable high-level programming languages, open code development, and reproducible scientific research.

https://numfocus.org/

NumFOCUS holds PyData conferences throughout the world. Disclosure: I’ve volunteered at PyData DC and had great time. I highly recommend volunteering! 😃",https://towardsdatascience.com/the-unsung-heroes-of-modern-software-development-561fc4cb6850,['Jeff Hale'],2019-12-13 16:42:53.051000+00:00,764,"open source, foundations, Apache Software Foundation, Linux Foundation, Free Software Foundation"
Apache Ignite ML: origins and development,"Motivation (to do yet another framework)

Why do people create frameworks? What drives them? Ambition, thirst for knowledge, questions from users, challenges of fate?

First motivation: Honestly, I think that, initially, the idea was to create a simple framework at the top of Apache Ignite with models like Linear Regression and Decision Trees to compete with other distributed systems, like Redis or Apache Spark or Oracle, which have their data-science tools. Purely for show.

Second motivation: As I remember, some Ignite users asked about making fast predictions in memory by using pre-trained models. But all of the models were trained in Python, and, as a result, serialized Python models could be easily used for prediction on Ignite caches.

Third motivation: Users much consider the effect that scalability can have on performance. Just like Apache Spark and Apache Mahout, Apache Ignite can be scaled and trained on data partitions.

With Apache Ignite ML, 1000 nodes with 10000 data partitions are required to train a Decision Tree or NaiveBayes classifiers.

Fourth motivation: There are not many useful ML libraries in the JVM world. Unlike with scikit-learn in Python, there is no community-driven gold standard. However, the creation of ML libraries (whether distributed or non distributed) brings new possibilities. Because the Java, Scala, and Kotlin languages use a standard JVM bytecode, libraries that are written in any of the languages can be easily called and reused for data processing.

As a result, we could reduce network traffic, CPU operations, and volumes of allocated memory if all these actions: data source reading, preprocessing, training, model deployment, and prediction could be made on one platform. If data-source reading, preprocessing, training, model deployment, and prediction can be made on one platform, then network traffic, CPU operations, and size of allocated memory can be reduced.

Fifth motivation: If you are using a distributed system, probably your data is too large to fit on one server or, in the case of Apache Ignite, within the memory of one server. If you have a lot of data, training on the data might require a good deal of time. If you lose a node, you might lose your calculations and have to restart the training. Apache Ignite Machine Learning is tolerant of node failures. Therefore, if a node fails during the learning process, recovery procedures are transparent to you, your training processes aren’t interrupted, and you obtain results relatively quickly, almost the same as if all nodes were working as they should. For detailed information, see Partition Based Dataset [2].

Sixth motivation. Last but not least: Be honest, all main Big Data tools are written in JVM languages like Java or Scala (not to mention Kafka, Spark, Hadoop, Flink, Ignite, and Cassandra), and all of the languages are developed under the Apache community rules. But, the data-analysis pipeline has a missing link. It lacks the ML and data-science tools that integrate well with other JVM Big Data tools. This deficiency caused the Flink and Spark communities to implement its ML frameworks and played a significant role in creating the Apache Ignite ML framework.

What was the motivation for me, personally, to participate in the Ignite ML project during the past three years and continue my job?

The main reason was to create competition for the Apache Spark ML framework, which offers a lot of advantages and is widely known but provides limited support for model ensembles, integration with other ML frameworks, and online-learning [3]. After a few fruitless attempts to change something within the Apache Spark, I joined the Apache Ignite community, which began the development of the Ignite ML framework by trying to utilize the basics of Apache Mahout at the top of Apache Ignite.",https://zaleslaw.medium.com/apache-ignite-ml-origins-and-development-d49a19e67202,['Alexey Zinoviev'],2020-11-27 15:23:23.697000+00:00,605,"Apache Ignite, Apache Spark, Apache Mahout, ML libraries, JVM bytecode"
The AI Security Cameras You Had No Idea Existed,"AI Security Cameras Are The Future

There are a variety of options to help secure your operation and in this video we cover the top 4 best security cameras for your business.

There are two types of security camera systems for your business, a proactive or a reactive system. A proactive system is an armed or unarmed patrol that monitors your location. There is a very high cost to securing your business with a proactive system, which is why reactive systems are most popular for home and business security.

Popular reactive security systems we mentioned in this video include:

Deep Sentinel: http://bit.ly/Deep-Sentinel​

Ring: https://ring.com/​

Simpli Safe: https://simplisafe.com/​

ADT: https://www.adt.com/compare​

All these systems alert you once a sensor has been tripped. Such as a window sensor, or motion sensor inside of your home. When they are tripped, they oftentimes require a guard to come to your property to see if it was an actual break in. Only after they confirm that, then the cops will arrive at your property. This can take anywhere from 20–30min+ depending on how far the guards are from your home at the time.

This is where Deep Sentinel enters the market and is changing the wireless security camera game. An AI based system that detects people coming to your door. It alerts live guards that are monitoring cameras 24/7 and they help scare those would-be burglars off your property before they have a chance to break in. The best part about this system is that your property insurance cost for your business is also reduced, helping you save an average of 10–20% off your premium.",https://medium.com/@al-pacca/the-ai-security-cameras-you-had-no-idea-existed-532047bd8a9b,['Al Pacca'],2021-03-19 18:12:25.888000+00:00,260,"AISecurity Cameras, Proactive Security Systems, Reactive Security Systems, Deep Sentinel, Ring"
Yet Another Neural Network Generated Color Names,"And now for some more details:

At first, I checked if someone had done something like this before me, and of course, I found a similar project: there, a researcher from Colorado, Janelle Shane used the 7700 color names base from the Sherwin-Williams Company, the world’s largest paint manufacturer. Then she taught char-based RNN to generate names for RGB-style components. She also wrote a follow-up later in which she tested a few more ideas.

I wasn’t happy with the quality of the results of that work even considering some obvious cherry-picking, but I had some ideas of mine that I decided to check on my own. In this post, I release the first part of my results, and I will post some follow-up if everything works out later.

First of all, I decided to make a good dataset, not paint color names (where marketers come up with abstract selling words), but something closer to the “real” color perception of a human person. By the way, I remembered that I already came across such a dataset six years ago and I even wrote something about it in my blog. Folks from CrowdFlower, engaged in the survey automation like Amazon’s Mechanical Turk, published a hand-cleaned base of names for 4000 colors. Even more, for each of them, in addition to the standard English name, they collected its name in 8 other languages ​​and translations of these names back to English. As a result, there were up to 9 different English names for each of the colors.

Since then, CrowdFlower was rebranded to Figure Eight Inc., changed the site address and focused on the AI & ML, but the same dataset can still be found online ​​(with the broken encoding, but English names are still correct there).

I also added several smaller hand-cleaned datasets, manually harvested from different corners of the Internet, so as a result, I had approximately 15K unique RGB + name pairs.

One of the ideas I wanted to test is the usage of an extended numeric color representation. Instead of using just one of the RGB, HSL, YIQ, … spaces, I decided to generate several alternative representations from RGB (with the python’s colorsys module), and then conditioned the name generation by the concatenated vector of different representations. The point was that the individual components in different representations can affect different words of the name. For example, in the name “dark red” probably the “darkness” is well defined by the L component of HSL, and “redness” is easiest to determine from the R component of RGB. Technically, the network can learn this itself, but on a small dataset and without direct targeting, there is not much chance of that. As an architecture, I used a multilevel char-based LSTM with a layer normalization and a pair of crutches.

For visualization, I took the Dave Oleson and Dawn Ho D3.js-based client code, written by them once for that original post from CrowdFlower. I slightly rewrote it and used the set of generated names for the same 4000 colors as input data.

All of these names were generated automatically, without any kind of cherry-picking, but with some automatic filtering — basically, those names that appeared in the original dataset were suppressed from the resulting list, so all the names should be unique here.",https://medium.com/altsoph/yet-another-neural-network-generated-color-names-f0aad5d45081,['Aleksey Tikhonov'],2018-11-29 12:24:58.289000+00:00,542,"Machine Learning, Artificial Intelligence, Data Science, Color Names, RGB"
Massive AI Breakthrough Could Mean a New COVID-19 Pre-Screening Method in the Cards,"The Massachusetts Institute of Technology (MIT) has recently released information about a potential game-changer in screening for COVID-19 asymptomatic patients.

So far during this pandemic, scientists have identified that there are individuals who show symptoms, and those who are infected but show no symptoms at all. These pose a substantial danger given they feel no need to stay at home, and rightly so, and thus potentially contaminating whoever they are in contact with.

Researches at MIT, however, have developed an artificial intelligence model that can detect the virus in asymptomatic individuals. After being fed with new recordings, the model correctly identified 98.5% of coughs from people who had tested positive, including 100% of coughs from asymptomatic people who showed no symptoms but had tested positive for the virus.

But how does it work?

Some background on AI models without going too deep — an AI model is typically fed a large amount of the same type of data you want it to look out for.

Let’s say we’re training a model to differentiate between manager signatures. We would give our cognitive model (which may be looking out for a range of parameters such as colour, length, thickness, etc of a signature) a large volume of signature images. How large is large? As many signatures, as you can acquire (with permission from the managers of course) — think hundreds if not thousands. The more data your model has to work with, the stronger it will become at differentiating between those signatures.

Each signature is fed to the model along with a label containing the name of the manager whose signature it belongs to. Once you’re happy it has been trained with all of the available signatures, you would feed it completely new signatures that were not part of the training data. This is really important! You don’t want to test your model on a data set you have trained it with. It is also just as important to feed your model with images that don’t contain any signatures from those managers whose signatures you wish to identify in the future. Why? Because it's important for it to also learn to correctly say if none of the signatures it was expecting to find has been found.

Providing the training has gone well and it was given a large enough sample of data, the model will correctly differentiate between signatures.

This, of course, is a slightly different, and for sure oversimplistic, example compared to how the MIT model works. But you hopefully understand the concept behind AI models. Train them with enough of the same dataset type, test them with new data.

Back to COVID

The researchers at MIT tested their AI model on thousands of cough and dialog samples — allowing for a very rich training dataset.

Which parameters exactly was this model looking out for? This particular AI model has been around for some time — it was initially intended to detect signs of Alzheimer's through forced-coughs given the effects that the disease is known to have on vocal cords. Vocal cord strength, along with sentiment, lung and respiratory performance, and muscular degradation were effective in detecting the disease.

As the pandemic unfolded, researchers wondered whether the same model could be used to identify COVID-19 infected individuals given the effects the virus has on our muscles (with some individuals reporting temporary muscle impairment).

Researchers found that without modifying the model too much, it was able to correctly identify asymptomatic COVID-19 patients based on the same biomarkers (vocal cord strength, along with sentiment, lung and respiratory performance, and muscular degradation) that were used to detect Alzheimer’s in individuals.

What’s next?

Researchers are currently looking at developing a user-friendly app based on the model. If approved by officials, they hope it could act as a pre-screening app for the virus. We may be looking at a future in which a day out is dependent on results after coughing into your phone!

What a fantastic way to put our technological advancements to use.",https://medium.com/swlh/massive-ai-breakthrough-could-mean-a-new-covid-19-pre-screening-method-in-the-cards-45b0db8f8c0c,['Natalie Mclaren'],2020-11-19 09:32:49.666000+00:00,653,"MIT, AI, COVID-19, Asymptomatic Patients, Alzheimer's"
This Will Make You Airbnb in Boston Efficiently.,"Which neighbourhood to pick while in Boston?

Mission Hill is the most popular neighbourhood in Boston among Airbnb rentees, while the least popular area is Leather District, which has only 50 reviews against 400 of Mission Hill, having an average price of $250 per night versus $120 at Mission Hill.

There are three features we will use to examine whether the given area in Boston is popular among the visitors or not: average price, number of reviews left, and the annual availability of properties in each neighbourhood.

Figure 4. Number of reviews by neighbourhood

According to the fig. 4, the Mission Hill area has the maximum numbers of reviews, over 400. Following by East Boston and Back Bay areas with 350 and 310 reviews, respectively.

It is important to note that the number of reviews was not split for negative and positive, according to Trustpilot research (2018) the top three reasons customers write reviews are:

· to help others make a better buying decision

· to share an experience, or

· to reward a company for good performance.

Given the research outcome above, it was decided that both negative and positive reviews would be beneficial for our analysis.

Figure 5. Annual availability by neighbourhood

The fig.5 gives us more evidence to support our initial findings of Mission Hill, as the properties in that particular area available on average 50 days a year, which is the lowest availability ratio among other areas in Boston.

While the Leather District neighbourhood is the least popular according to the number of reviews and the availability ratio, with 50 reviews and is available 365 days a year.

The price wise: the Leather District (over $250) is one of the highest charging neighbourhood among the South Boston Waterfront (over $300) and Bay Village($270). The high price can evidence the lack of demand in these areas among rentees and this is also evidenced by the small number of reviews.

While Mission Hill has an average price of $120, which is more than twice less than the areas mentioned above, making the area attractive among visitors.",https://medium.com/swlh/this-will-make-you-airbnb-in-boston-efficiently-d9ec76e6dfb0,['Gyuzala Muzafarova'],2020-12-24 22:34:53.306000+00:00,334,"Boston, Airbnb, Rentees, Reviews, Price"
ENEM — Math score predictor. A model that predicts the score of math…,"Questions to answer

1) Is it possible to predict the score of the math exam with good reliability?

If you are in a hurry, I already say that the answer is yes. But if you’re not in such a hurry, I’ll explain how I built my model to do this. The first step in answering this question was to understand the context of the problem. In other words, I had to analyze my objective within the dataset to define the best strategy.

Analyzing the data, I noticed that it would be a regression problem, not a classification one. I wanted to predict a score ranging from 0 to 1000, an infinite number of possibilities. With the defined approach, I started the data cleaning process to remove null values and treat other values ​​to add to its functionality.

After the cleaning process and the data exploration process, the time has come to tinker with the algorithms. To develop this work, I decided to buy two algorithms very applied for regression problems. The first was LinearRegression and the second RandomForestRegressor.

For each one, I executed the RFECV, a module within scikit-learn to perform the selection of features, that is, choose which columns in the database most influence my target (math note). With the RFECV in place, it was time to submit the two algorithms for testing. For the level of comparison, I chose to use r² as an evaluation metric.

The result was quite close for both, but the linear regression model obtained a better result reaching a coefficient of 0.9186. So, I believe it is possible to have a good predictor model for ENEM grades. This model can certainly be extended to other algorithms or even to other applications.

2) What are the critical information to the score of the exam?

In addition to developing the model for predicting math scores, I decided to run the SelectKBest algorithm, which selects the K best features for a given target. But what for? I wanted to find out what are the factors that most influence the student’s grade. Well, some returns were quite obvious, and others not so much. The first fields that returned were the other test scores, which makes total sense because generally, those who do well in one test tend to do well in another.

After the grades, another piece of information that greatly influenced the grades was the student’s age, which also makes sense, as older people are usually out of school for some time, and therefore are not as well prepared as young students. However, from here on, I started to surprise myself, actually not so much, but seeing this explained by the data was a little shocking. Another factor that inferred the student’s score is the relationship with work.

I noticed an inverse relationship between working and having a high score in the test. This highlights a series of structural problems within Brazilian education, from the time that the poorest young people have available to study. They have to reconcile the time between studies and work to help the family, even in teaching quality in Brazilian public schools.",https://medium.com/@diorgeneseugenio/enem-math-score-predictor-943b0e3618a9,['Diórgenes Eugênio'],2020-10-06 07:24:52.705000+00:00,511,"math exam, predicting score, data cleaning, Linear Regression, Random Forest Regressor"
Databricks’ pending IPO is a building block in unleashing the AI Technological wave.,"Databricks’ pending IPO is a building block in unleashing the AI Technological wave.

A San Francisco headquartered Tech company, Databricks is increasingly attracting the attention of investors. With a US$1 billion Series G raised last February, led by Franklin Templeton, it reached an impressive US$28 billion post-money valuation. It is the most highly valued pre-IPO technology company in the world. Analysis are calling it the next Snowflake, a company that share where listed at US$120 on September 16 2020, and increased to US$245, a 104% jump in the first day of trading — it’s now sitting at US$237.

Historical Stock Performance Chart for Snowflake Inc.

Founded in 2013, by a group of science students, it is an enterprise software company, data-and-AI focused. It develops a web-based platform that interacts with corporate information stored in the public cloud.

Among them, more than 5000 clients, big names like Nationwide, Condé Nast, and Comcast can be found.

Databricks backing by such big players can be understood because of the value that they deliver to enterprises; private, public and governmental. They remove the technical challenge and expertise of building, collecting and centralizing data for an organization’s important yet non-core business of data, that then can be used for developing Artificial Intelligence driven business models.

Machine learning teaching and data collection are good examples of how tasks necessary to the development of new systems and products can drain resources. Data collection by its nature often requires data scientists to process and analyze through petabytes of data.

In-house solutions are costly to implement, insourcing architecture and technical professionals can be time-consuming, complex to manage and highly expensive.

That’s where Databricks comes in. It works as a cloud that enables a disruptive project to run smoothly. They take care of these tasks, which lets companies’ teams do what they do best.

This is part of a present tendency in the tech industry, “renting” the services a company needs to perform their main activity as opposed to “buying” by directly hiring and owning the infrastructure and expertise.

It’s CEO, the Swedish-Iranian computer scientist and entrepreneur Ali Ghodsi, who is also a Berkeley adjunct professor, co-founded it with Scott Shenker, Berkeley professor, and co-founder/former CEO of Nicira; Ion Stoica, Berkeley professor and co-founder and CTO of Conviva; Matei Zaharia, who created Apache Spark and is currently a professor at Stanford University. All of them still occupy a position at Databricks. The other co-founders are Andy Konwinski, Patrick Wendell, Reynold Xin, and Arsalan Tavakoli-Shiraji, all former Berkeley Ph.D. students and Apache Spark committers.

They initially partnered up to launch Spark, an open-source engine for managing big data, still at Berkeley’s AMPLab in 2009. Increasingly needs to use systems such as artificial intelligence and machine learning made the platform be widely adopted.

From this venture, Databricks was spawned, for commercializing the software for enterprises.

Through the years, the company has performed very well, starting from a tiny office in California to over 1000 employees and operations in Canada, UK, Netherlands, Singapore, Australia, Germany, France, Japan, China, and India. Its business model has proven successful, with a series of four open-source products with a core data lake product called Delta Lake as the flagship. It closed 2020 at US$425 million in ARR, a 75% increment over the same period in the previous year.

Two years ago, after a US$400 million round, the company was valued at $6.2 billion, less than ¼ of the current US$28B. This rapid pace of growth made investors compete for a share of it. In addition to Franklin Templeton, Fidelity, Microsoft, Amazon Web Services and Salesforce Ventures have also secured theirs.

The interest of top players in it is not solely based on a multiplying investment factor, but also on the role Databricks has in the tech Ecosystem. Its developments in the Open Source Ecosystem helps to simplify Machine Learning workflows, which makes for increasing its clients’ own development projects efficiency. Its CEO, Ali Ghodsi said Databricks is the fastest-growing enterprise SaaS company ever.

No one wants to miss the chance of participating in a company with such good fundamentals.

When looking for promising companies as an investment opportunity, those that can provide value for other enterprises are certainly a safe bet. In this group, Databricks stands out. These are some of the reasons for its US$28 billion valuation.

Although Databricks has not filed the IPO documents yet, it’s believed its IPO will come anytime this year.

Let’s wait and see. Certainly, it is worth keeping an eye on this one.",https://medium.com/codex/databricks-pending-ipo-is-a-building-block-in-unleashing-a-ai-technological-wave-7f59e72c5527,['Billy D. Aldea-Martinez'],2021-07-07 07:11:48.559000+00:00,734,"Databricks, IPO, AI, Snowflake, Machine Learning"
Data Storytelling,"“Stories take advantage of human cognition. They build connections and context around facts in order to make them more memorable”

Data Storytelling — An art that transforms data into narratives, is an important aspect of Analytics Reporting world.

With massive growth in technology, analytical capabilities, we are in the sophisticated phase of storytelling, and visualisations have become a means to build a compelling data narrative.

What is a data-story?

A data-story is a well led-out sequence of visualised facts that delightfully convey the message. Stories bring data to life, making sense of a dissimilar collection of facts. Most importantly, stories create connection — Audience can put themselves into stories and can connect on an intellectual and emotional level.

Data-Story writing guidelines, my 3 staged approach to nail the beast.

1. Purpose of Data-Story —

Build the skeleton of the data story, and finalise on the business purpose of the data story.

- Is it a call for an action, something that needs attention?

- is it a simple narrative for information?

- Is it case? (e.g Business conclusion supported by data evidence)

2. Flow of Data-Story —

A data-story can be a collection of several story-points, views, dashboards or facts in forms of plain text. While facts and data form the pillar of data-story, it’s the flow that ties it all together.

Well-structured and logically sequenced flow turns a collection into a convincing narrative. It binds the business information coherently into a strong structure which also help to spot underlying causes and connections.

3. Type of Data-Story -

The table below describes seven different story types by Tableau. A single story can comprise of more than one approach. Pick the data story type.

Get the storyteller hat on — piece it all together to form a brilliantly visualised story.

In today’s data driven, metrics and dashboard centric world of information overload — a good data story can squeeze millions of rows into a single narrative, paint a clear picture for audience of what lies underneath the data.

Data on its own are lifeless numbers but when it becomes Information Its powerful, and when information is laced with a good story — it’s simply unforgettable.",https://medium.com/datacrat/data-storytelling-2c5c60e01da9,[],2019-01-03 15:36:44.961000+00:00,346,"data-storytelling, data-story, analytics reporting, data visualisation, storytelling art"
The first programming language you should learn… A debate…,"Set the Stage

Both JavaScript and Python offer a wide range of features and have extensive, amazing communities behind them. We are going to delve into the technical and professional aspects of both languages while avoiding some of lower-level technical details. In doing so, I hope to paint a picture of which language you should choose based on your preferences and personality. We will compare these languages on only two key aspects: learning curve and utility/use-cases.

1. Learning Curve

JS and Python both have low learning curves and are quite easy to pick up on. Both are dynamically typed which helps beginners tremendously. Python is currently embracing type hints, but these are not enforced and runtime. Similarly, JS has a language subset called TypeScript which enforces types on all objects but JS itself does not. Speaking of, both languages follow OOP principles which is another plus for learning since objects are a great way to relate abstract coding structures to real-life structures.

One downside to Python is that it requires installation and Python versions change relatively frequently. Managing Python versions is a known headache for any Python dev, but there are many packages out there to help combat this issue including conda, poetry, and virtualenv. In order to run Python scripts you either need to utilize Jupyter Notebooks (which require installation) or utilize a terminal and code editor to write code (yes these could be the same via vim/nano). The Anaconda installation does install VS Code, Jupyter Notebook, Python all at once for users and is available on all platforms.

JS, on the other hand, can be written directly in your browser simply by navigating to the Chrome Developer Tools. This makes it very user friendly since you can actually just code in your browser and see the changes directly happen on a web page. No installation required. This principle does break down once you start installing node packages and using frontend frameworks, but for Vanilla JS, it couldn’t be easier to get started.

As far as syntax goes, both languages are simple to understand and get used to and any code editor provides great support for both. Arguments could be made for Python over JS on the syntax front, but I think they are simply different and that, with editor support, neither would provide a large hinderance to getting starting. On second thought, I would select Python here due to ignoring curly braces and no semi-colons despite its whitespace issues. Especially considering that advanced frontend frameworks like the popular React library or Angular utilize ES6 syntax which can be quite confusing at times.

2. Utility / Use Cases

The use cases for these languages are where they really differ. This also bleads into job prospects. Python is excellent for data analysis, data engineering, data science, one-off scripts, automation, machine learning, and backend web development. Javascript is excellent at nearly everything on the web from frontend styling and animation to backend frameworks and interacting with databases.

The simplest way I can explain the difference here is this: Python works many places, JS works on the web only. If you want to build on the web only, the choice is easy: JS. If you want to build small games, desktop apps, software, or do data related tasks, choose Python.

Interestingly, Python can do some of the things JS can do regarding web development backend. In fact, Python’s two most popular web frameworks Django and Flask run many popular web backends. JS has its own backend frameworks including Express.",https://medium.com/the-innovation/the-first-programming-language-you-should-learn-a-debate-93611b06acd2,['Nick Anthony'],2020-11-05 18:00:19.059000+00:00,577,"js, Koa.js and Sails.js.Python, Java Script, Learning-Curve, Use-Cases"
How does a Real-Time Face Recognition work with OpenCV?,"An eazy code from eazy ciphers

Are you keen to learn about the implementation of Real-Time Face recognition? Here eazy ciphers came with an innovative and eazy way to run a simple Real-Time Face Recognition code where you can predict the face of an individual.

When I talk about the real-time you might get confused about what I am going to discuss here it’s nothing but the implementation of the model which detects faces that appeared on the webcam.

If you want Hands-on experience on this model then go ahead and step forward in its implementation.

In day to day life, facial recognition became a part of the things. So, here is a quick example of Real-Time face recognition before getting into the topic.

When you start registering your face for a smart lock in your phone, tablet, or laptop it asks for the person’s real-time image to capture and that is stored in the database for further clarifications in recognition of that particular person.

This process of approach is done through many iterative ways of prediction with the input image. Similarly, Real-time face recognition works with the implementation of the OpenCV framework python.

These together packed in one combo level to implement a model for the Real-Time purpose.",https://medium.com/eazy-ciphers/how-does-a-real-time-face-recognition-work-with-opencv-ddd4c1cd4b43,['Eazy Ciphers'],2020-08-20 13:38:15.201000+00:00,202,"real-time face recognition, eazy ciphers, OpenCV framework, facial recognition, smart lock"
First Thoughts on Kaggle,"About two weeks ago, I registered for my first Kaggle competition, the Mercedes-Benz Greener Manufacturing contest. My code can be found here, and a log of what I attempted can be found here. Here’s my first impression of Kaggle:

The Learning Curve

Entering the contest, my only knowledge in machine learning came from MIT’s introductory class (6.036). So, I was pleasantly surprised to find that Kaggle contests are perfectly accessible to those with minimal prior experience. This is largely due to Kaggle’s kernels, which allow the more experienced Kaggle users to publicly share their code with others. Kernels allow even those completely new to machine learning to be competitive in the rankings — by simply copying code, anyone can achieve results at par with Kaggle veterans.

I found that from my attempts to improve code from kernels, I received a brief introduction to multiple concepts in machine learning, many of which I hope to write about more thoroughly in the future:

Gradient Boosted Trees

Hyperparameter Tuning

Dimensionality Reduction: PCA, ICA, tSNE, Logistic PCA, TSVD, GRP, SRP

Overfitting, K-fold cross validation, Out-of-fold predictions

Ensembling, Stacking, and Averaging

Sklearn Models: LassoLars, ElasticNet, etc.

Basic Feature Selection and Feature Engineering

Likelihood encoding (post-contest)

This contest seemed to me like a great way to quickly ‘learn by doing’. It would be hard to find any other resource online which facilitates learning concepts in data science as well as Kaggle does.

Unpredictability of the Leaderboard

As evidenced by the massive shakeup from the final rankings, the public leaderboard was entirely unreliable for predicting the private leaderboard: almost everyone in the lead throughout the contest dropped hundreds of ranks at the end. However, even cross-validation proved to be useless: my final model, evaluated with 5-fold CV, performed no better than my heavily-overfitted XGBoost model made on my 4th day.

Public and Private LB scores for my 36 submissions — a few past models made it over the bronze cutoff!

In the end, it turned out that there were reliable ways to test a model — for the most part, though, contestants (including me) weren’t thorough enough with evaluating their model’s performance.

Kaggle’s Contest Community

I have nothing but positive things to say about Kaggle’s community. User-submitted Kernels and threads about the competition do a lot to encourage collaboration between the contestants. During the contest, many users worked together in open forums, improving each others’ models and discussing properties of the dataset. This competition, one of the many results of the contestants’ combined efforts was the discovery of 32 y-values from the test dataset, obtained through leaderboard probing.

Conclusion

I found participating in this contest to be very enjoyable! While my final ranking (~1400th place) was a bit disappointing, the competition was extremely fun and allowed me to learn a lot, and I plan on becoming more active on Kaggle in the future. Please let me know if you have any feedback — perhaps more Kaggle write-ups will be coming soon. :)",https://towardsdatascience.com/first-thoughts-on-kaggle-326a6c4dc005,['Alex Yang'],2017-07-12 21:28:23.296000+00:00,475,"Kaggle, Machine Learning, Gradient Boosted Trees, Hyperparameter Tuning, Dimensionality Reduction"
Visualizing DC Real-Time Traffic During The Curfew After Riots,"Unfortunately, today’s riots resulted in chaos and loss of life. Many data providers are giving free access to real-time detailed information, which can be extremely useful for responders and citizens to react to sudden incidents like we saw today. Let’s delve a bit into the potential for quantifying real-time incident and flow data using one such API.

Google Maps is great when it comes to figuring out which route to take that minimizes your trip time. However, for the data enthusiast, it doesn’t give access to this traffic information. TomTom, Waze, HERE are all providers of real-time traffic information. Personally, I find HERE easiest to use and its free API very useful for obtaining detailed traffic information such as speeds and incidents. In a previous blog, I went through how to use the HERE traffic flow API to extract traffic speeds at the individual road segment level:

Here, I will illustrate how to pull out traffic incident data, and compare speeds/incidents from HERE with images from Google Maps.

%matplotlib inline

import numpy as np

import requests

from bs4 import BeautifulSoup

import xml.etree.ElementTree as ET

from xml.etree.ElementTree import XML, fromstring, tostring

soup = BeautifulSoup(page.text, ""lxml"") page = requests.get(' https://traffic.api.here.com/traffic/6.2/incidents.xml?app_id=BLAH&app_code=BLAH2&bbox=38.92,-77.06;38.88,-77.00&responseattributes=sh,fc' soup = BeautifulSoup(page.text, ""lxml"") incidents = soup.find_all('traffic_item')

You can register with HERE to get an app id as well as app code for credentials (BLAH, BLAH2 in the above example). Next, you need the bounding box within which you want to obtain flow/incidents. For this, right on google maps and you will see the lat-long pair. You need the top left and bottom right ones to get the coordinates of the bounding box. The response attributes fc and sh are important, useful later for filtering out highways, streets, etc., but I will not be using that information in this post. The last line gives all the unique incidents at that time.

myxml = fromstring(str(incidents[0]))

for child in myxml:

print(child.tag) traffic_item_id

original_traffic_item_id

traffic_item_status_short_desc

traffic_item_type_desc

start_time

end_time

entry_time

criticality

verified

abbreviation

rds-tmc_locations

location

traffic_item_detail

traffic_item_description

traffic_item_description

traffic_item_description

Looking at the first incident, there are 16 tags with useful incident information such as when the incident started and ended, descriptions, and the location. For now, I’m just interested in the location (lat, long) of the incident. It turns out this is not so simple as in the traffic speeds case. But with a little bit of poking around, I found it hidden within the location tag.

myxml = fromstring(str(incidents[0]))

for child in myxml:

if (child.tag==’location’):

for chelds in child:

print(chelds.tag,chelds.attrib) intersection

geoloc

political_boundary

navtech

length

Finally, I found the lat and long of the incident hidden in the geoloc tag:

incidents[0].location.geoloc <geoloc><origin><latitude>38.88507</latitude><longitude>-77.01081</longitude></origin><to><latitude>38.88602</latitude><longitude>-77.01051</longitude></to><geometry><shapes><shp fc=""5"" fw=""SD"" le=""0.042374034341961575"" lid=""1186813844F"">38.88507,-77.01081 38.88544,-77.01069</shp><shp fc=""5"" fw=""SD"" le=""0.06625476521319096"" lid=""1254739814F"">38.88544,-77.01069 38.88602,-77.01051</shp></shapes></geometry></geoloc>

You can also see what types of incidents are present. There were a total of 173 incidents at around 9 PM on 1/6/2021 in DC. All of these were described as either road closures or planned events. Here are the first 10. Note: HERE has a time which is clearly not EST. It might be GMT, though I’m not sure as of now.

for inc in incidents:

print(inc.start_time.text, inc.end_time.text,inc.traffic_item_type_desc.text) 12/15/2020 15:46:16 02/15/2023 17:00:00 ROAD_CLOSURE

01/06/2021 21:46:56 01/07/2021 21:45:44 ROAD_CLOSURE

01/06/2021 22:52:32 01/07/2021 22:52:07 ROAD_CLOSURE

12/15/2020 15:46:16 02/15/2023 17:00:00 ROAD_CLOSURE

01/06/2021 21:46:56 01/07/2021 21:45:44 ROAD_CLOSURE

12/15/2020 15:46:16 02/15/2023 17:00:00 ROAD_CLOSURE

01/06/2021 23:04:11 01/07/2021 23:01:47 ROAD_CLOSURE

12/15/2020 15:46:16 02/15/2023 17:00:00 ROAD_CLOSURE

12/15/2020 15:46:16 02/15/2023 17:00:00 ROAD_CLOSURE

01/06/2021 23:04:11 01/07/2021 23:01:47 ROAD_CLOSURE

Finally, here is a visual comparison of HERE traffic speeds/incidents with Google Maps as promised:

Left — Google Maps, Right — HERE data in Washington DC | Skanda Vivek

There are some interesting differences. HERE has more incidents and more data on speeds in some smaller road segments. Otherwise, in the absence of Google Maps data, it is hard to visually identify how similar/ different the speeds are. Also, I’m not sure how Google Maps identifies its speed by color. Google Maps has 4 colors for speeds from slow to fast (dark red, red, yellow, green). I made a similar color map for HERE, where road colors were segmented by speed relative to speed limit from: 0 — 0.25, 0.25 — 0.5, 0.5 — 0.75, 0.75 — 1 (0 corresponding to average speed of 0 mph, and 1 the speed limit).

HERE is a great resource for obtaining real-time information that you can’t get from Google Maps. Also, HERE has seemingly more detailed traffic information of smaller road segments, and they claim to be the World’s #1 location platform, quite a bold claim. The only downside is that they don’t have historical traffic data available — but hopefully that’s a future option.

As we saw today, sometimes groups of people can act collectively in ways that are previously unthinkable and cause great harm. When this happens, we need to have an accurate picture of evolving landscapes. Today illustrates how the DC government was largely unprepared for the scope of protests and the suddenness with which the Capitol was infiltrated.

Real-time traffic data integrated with other sources of real-time data including social media messages like tweets; serve as a pulse of society. These data give powerful, actionable information that city stakeholders such as law-enforcement, emergency responders, and even citizens can use to correctly assess the severity of the situation, and make key decisions, when time is of the essence.

Follow me if you liked this article — I frequently write at the interface of complex systems, physics, data science, and society",https://towardsdatascience.com/visualizing-dc-real-time-traffic-during-the-curfew-after-riots-2007cc6838f6,['Skanda Vivek'],2021-01-07 04:21:45.502000+00:00,847,".Real-time Traffic, HERE API, Google Maps, Traffic Information, Incident Data"
Week #6 Heart Disease Detection,"Introduction:Hello everyone again! Last week, we talked about the decision tree algorithm and results. We said that the decision tree algorithm causes overfitting. This week, to solve this overfitting problem we will talk about random forest algorithm.

Random Forest:

The random forest algorithm is a more flexible version of the decision tree algorithm. It provides a better generalization and more accurate classification of new samples. Therefore, it provides an improvement in the accuracy rate.

At each step of the random forest algorithm, we create a new data set by selecting random samples from the original data set and run the decision tree algorithm on this new data set.

The important thing to note here is that the data set we created contains the same number of samples as the original data set, and we can select the same sample more than once (Completely random).

When constructing a decision tree on these randomly selected data sets, we must use randomly selected feature subsets at each step. The number of features that subsets contain is a parameter and should be optimized.

Randomly creating the decision tree in this way will result in a wide variety of trees.Variety is what makes the random forest algorithm more efficient than the single decision tree algorithm.We repeat this decision tree creating process a number of times (for example, 100).

When testing a new instance to which class it belongs, we test it on all the random trees that we have created and vote for the majority.

Results:

In our 1st data set, random forest algorithm gave 93.55 accuracy.

In our 2nd data set, random forest algorithm gave 86.39 accuracy.

In our 3rd data set, random forest algorithm gave 71.92 accuracy.

For all three data sets, the random forest algorithm yielded better results than the decision tree algorithm

With this latest work, we have completed our project.As a result of the studies we have made, the best result has been given by the random forest algorithm as you can see from the graph.We found this to be normal because the data set contained continuous and discrete features.Generally, you can see the comparative results from the table for 3 data sets.We ended our posts with this blog.See you in other studies.",https://medium.com/bbm406f19/week-6-heart-disease-detection-857ce9804da3,['Harun Alperen Toktaş'],2020-01-14 03:33:37.065000+00:00,360,"Random Forest, Decision Tree, Algorithm, Classification, Data Set"
Artificial Intelligence: Week #7 | 2021,"Artificial Intelligence: Week #7 | 2021

This week in AI & Machine Learning: Autonomous Fruit picking robots, AI processing breakthrough, finding optimal ML Models, robot paintings, and more! Sage Elliott Feb 22·5 min read

Sixgill Tip of the Week:

Looking to get started in computer vision? Join our free hands-on workshop this week on February 24th at 5:30pm PST. We’ll cover industry computer vision applications, building your own datasets, and how to train models for object detection. Reserve your spot here.

Photo by Kaylie Humphrey on Unsplash

Artificial Intelligence News:

The AI field is growing fast! Here are some of the most interesting or thought provoking AI stories and applications that made the news this week.

Hiring enough workers to pick fruit is always hard, but it has been especially difficult during the COVID-19 pandemic. Learn how Tevel Aerobotics Technologies is solving the worker shortage and reducing food waste with its flying autonomous fruit picking robots!

At this year’s Super Bowl, Mountain Dew presented a challenge: count the number of Mountain Dew bottles in its 30-second Super Bowl commercial. Could computer vision be used to help accurately count the bottles even with complicated scene transitions? This fun project highlights the steps for a successful machine learning project and demonstrates how quickly you can from idea to results in the Sense Platform.

Sixgill Computer Vision results

There have been a lot of awesome AI generated art pieces, and even robots that paint them, but what makes the Ai-da exhibit a bit more fun is having the humanoid robot artist exhibit her work “in person”.

Samsung announces it has developed “the industry’s first High Bandwidth Memory (HBM) integrated with artificial intelligence (AI) processing power”. Having more powerful AI processing chips from Samsung and other chip manufacturers will greatly affect where AI can be applied in the future! To learn more about the AI microprocessor field, check out the conversation with Jim Keller in the podcast section!

The 2021 Habitat Challenge launched by Facebook AI and Georgia Tech challenges AI researchers to train machines to navigate real-world environments. Navigating real-world environments has been a long running problem in the AI and robotics community. It’s always neat to see more challenges aiming to improve navigation in complex environments.",https://medium.com/plainsight/artificial-intelligence-week-7-2021-39b610f9c637,['Sage Elliott'],2021-02-22 19:56:00.171000+00:00,362,"Artificial Intelligence, Machine Learning, Computer Vision, AIProcessing, Robot Paintings"
Científico de datos: ¿Qué necesito para convertirme en un Científico de datos?,"Learn more. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more

Make Medium yours. Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore",https://medium.com/datos-y-ciencia/data-scientist-qu%C3%A9-necesito-para-convertirme-en-un-data-scientist-937000873921,[],2020-11-23 02:46:57.548000+00:00,64,"the latest thinking from experts on topics like business, technology, design, science, health and more.Medium Platform"
Determine a Safe Withdrawal Rate for your Investment Portfolio with Python,"Python Implementation

First, the libraries that I used for this implementation were yfinance and plotly. To use this python implementation, you need to install these libraries. You can install these libraries using pip .

pip install yfinance --upgrade --no-cache-dir

pip install plotly

After installing the necessary libraries, we can turn our focus to the python implementation. Given the amount of different portfolio compositions, payout periods and withdrawal rates that are possible to explore, I found it suitable to create a python class named sustainable_withdrawal_rate .

Before loading this class, it is helpful if you look up the symbols of your choosing on Yahoo Finance, as you will need it as input for the python class. For example, I want to determine the (nominal) sustainable withdrawal rate for an investment portfolio containing the indices of UK, Germany and France. The symbols for these indices on Yahoo Finance are:

UK FTSE index: ^FTSE

Germany DAX index: ^GDAXI

France CAC 40 index: ^FCHI

After loading the python class, you need to instantiate it. For instantiation, the symbols are required as input. Below you can find an example using the symbols mentioned above:

symbols = ""^FTSE ^GDAXI ^FCHI""

port = sustainable_withdrawal_rate(symbols)

The following output is observed:

The output states that the historical (monthly) data for all three indices ranges from 1991 to 2020, which is 30 years of historical data that can be used for analysis. This is useful as it suggests that for this investment portfolio there is not enough historical data to determine a sustainable withdrawal rate for a payout period of 30 years.

Based on the amount of historical data for this investment portfolio (30 years), we could use a payout period of 10 years. If you want to determine the success rates for a given portfolio composition and a payout period of 10 years, you can use the following code:

portfolio_composition = {""^FTSE"":0.4, ""^GDAXI"":0.4, ""^FCHI"":0.3}

payout_period = 10

port.portfolio_success_rate(payout_period, portfolio_composition)

The code shows that the weights for this investment portfolio consists of 40% FTSE, 40% DAX and 30% CAC 40. Running this code results in the following output:

The output is a table containing the different annual withdrawal rates and their corresponding success rates. In other words, an investment portfolio consisting of 40% FTSE, 40% DAX and 30% CAC 40 can sustain an annual withdrawal rate of up to 7% for a payout period of 10 years without being depleted.

If you wish to adjust the portfolio composition and/or payout period, and see how these changes affect the success rates. You can easily do this by adjusting the portfolio_composition and/or payout_period , and running the code port.portfolio_success_rate(payout_period, portfolio_composition) .

Lastly, you can also plot the portfolio simulations for a specific annual withdrawal rate. Let’s plot the simulations for an annual withdrawal rate of 8% based on the portfolio composition and payout period described above. You can use the following code to plot the simulations:

annual_withdrawal = 0.08

port.plotsimulations(annual_withdrawal)

Running this code should results in the following plot in your webbrowser:",https://towardsdatascience.com/determine-a-safe-withdrawal-rate-for-your-investment-portfolio-with-python-cf2df9185f73,"['Raymond R', 'Two Pennies Worth']",2020-09-14 22:09:50.230000+00:00,476,"Python, Yfinance, Plotly, Portfolio Composition, Payout Periods"
Continuously extending Zarr datasets,"The Pangeo Project has been exploring the analysis of climate data in the cloud. Our preferred format for storing data in the cloud is Zarr, due to its favorable interaction with object storage. Our first Zarr cloud datasets were static, but many real operational datasets need to be continuously updated, for example, extended in time. In this post, we will show how we can play with Zarr to append to an existing archive as new data becomes available.

The problem with live data

Earth observation data which originates from e.g. satellite-based remote sensing is produced continuously, usually with a latency that depends on the amount of processing that is required to generate something useful for the end user. When storing this kind of data, we obviously don’t want to create a new archive from scratch each time new data is produced, but instead append the new data to the same archive. If this is big data, we might not even want to stage the whole dataset on our local hard drive before uploading it to the cloud, but rather directly stream it there. The nice thing about Zarr is that the simplicity of its store file structure allows us to hack around and address this kind of issue. Recent improvements to Xarray will also ease this process.

Download the data

Let’s take TRMM 3B42RT as an example dataset (near real time, satellite-based precipitation estimates from NASA). It is a precipitation array ranging from latitudes 60°N-S with resolution 0.25°, 3-hour, from March 2000 to present. It’s a good example of a rather obscure binary format, hidden behind a raw FTP server.

Files are organized on the server in a particular way that is specific to this dataset, so we must have some prior knowledge of the directory structure in order to fetch them. The following function uses the aria2 utility to download files in parallel.

Create an Xarray Dataset

In order to create an Xarray Dataset from the downloaded files, we must know how to decode the content of the files (the binary layout of the data, its shape, type, etc.). The following function does just that:

Now we can have a nice representation of (a part of) our dataset:

And plot e.g. the accumulated precipitation:

Store the Dataset to local Zarr

This is where things start to get a bit tricky. Because the Zarr archive will be uploaded to the cloud, it must already be chunked reasonably. There is a ~100 ms overhead associated with every read from cloud storage. To amortize this overhead, chunks must be bigger than 10 MiB. If we want to have several chunks fit comfortably in memory so that they can be processed in parallel, they must not be too big either. With today’s machines, 100 MiB chunks are advised. This means that for our dataset, we can concatenate 100 / (480 * 1440 * 4 / 1024 / 1024) ~ 40 dates into one chunk. The Zarr will be created with that chunk size.

Also, Xarray will choose some encodings for each variable when creating the Zarr archive. The most special one is for the time variable, which will look something like that (content of the .zattrs file):

It means that the time coordinate will actually be encoded as an integer representing the number of “hours since 2000–03–01 12:00:00”. When we create new Zarr archives for new datasets, we must keep the original encodings. The create_zarr function takes care of all that:

Upload the Zarr to the cloud

The first time the Zarr is created, it contains the very beginning of our dataset, so it must be uploaded as is to the cloud. But as we download more data, we only want to upload the new data. That’s where the clear and simple implementation of data and metadata as separate files in Zarr comes handy: as long as the data is not accessed, we can delete the data files without corrupting the archive. We can then append to the “empty” Zarr (but still valid and appearing to contain the previous dataset), and upload only the necessary files to the cloud.

One thing to keep in mind is that some coordinates (here lat and lon) won’t be affected by the append operation. Only the time coordinate and the DataArray which depends on the time dimension (here precipitation) need to be extended. Also, we can see that there will be a problem with the time coordinate: its chunks will have a size of 40. That was the intention for the precipitation variable, but because the time variable is a 1-D array, it will be much too small. So we empty the time variable of its data for now, and it will be uploaded later with the right chunks.

Repeat

Now that we have all the pieces, it is just a matter of putting them together in a loop. We take care of the time coordinate by uploading in one chunk at the end.

The following code allows to resume an upload, so that you can wait for new data to appear on the FTP server and launch the script again:

Conclusion

This post showed how to stream data directly from a provider to a cloud storage bucket. It actually serves two purposes:

for data that is produced continuously, we hacked around the Zarr data store format to efficiently append to an existing dataset.

for data that is bigger than your hard drive, we only stage a part of the dataset locally and have the cloud store the totality.

An in-progress pull request will give Xarray the ability to directly append to Zarr stores. Once that feature is ready, this process may become simpler.",https://medium.com/pangeo/continuously-extending-zarr-datasets-c54fbad3967d,['David Brochart'],2019-04-18 14:47:00.954000+00:00,923,"Pangeo, Cloud, Data Analysis, Zarr, Earth Observation Data"
Zomato NCR Data Analysis,"This is my first blog on medium and one of my first projects in data science. The whole project is done on python using the following libraries:-

pandas

plotly

matplotlib

seaborn

In this post I will doing some exploratory data analysis on Zomato Restaurant data.

Zomato: It is restaurant search and discovery service founded in India. It is extremely strong in India, UAE and a couple of other countries although they are ‘present’ in 22 countries. It is the one of the best and most comprehensive restaurant search and discovery app available in the industry today.

The data has the following attributes:

‘Restaurant ID’, ‘Restaurant Name’, ‘Country Code’, ‘City’, ‘Address’,

‘Locality’, ‘Locality Verbose’, ‘Longitude’, ‘Latitude’, ‘Cuisines’,

‘Average Cost for two’, ‘Currency’, ‘Has Table booking’,

‘Has Online delivery’, ‘Is delivering now’, ‘Switch to order menu’,

‘Price range’, ‘Aggregate rating’, ‘Rating color’, ‘Rating text’,

‘Votes’, ‘Country’, ‘Number of Cuisines Offered’, ‘Restaurant’

To download the data click here .

The aim of this blog is to know about various attributes like cost, location, cuisines affect the sales of zomato.

Number of restaurants in each city in NCR

This is an analysis of all the zomato partnered restaurants in the National Capital Region(NCR) which includes the cities — Faridabad, Gurgaon, New Delhi, Noida. Above is the graph showing number of restaurants in each city. Lets see the various parameters:-

Number of cuisines offered

From the above plots we can see that in all 4 cities more than 50% restaurants offer less than 4 cuisines. The cuisines offered are :- Chinese, North Indian, Fast Food, Mughlai. We can see Delhi and Gurgaon offer the highest variety of cuisines.

2. Popularity of cuisines

This shows the ratings of the number of restaurants offering this cuisine. On an average none of the cuisines can be said favorites based on this plot. But North Indian restaurants tend to be much highly rated.

The North Indian cuisine is clearly more favored by restaurants over other cuisines. Still the average ratings for all the cuisines is around 3.5.

3. Rating v/s Cost

The average cost for two in NCR region is calculated to be about ₹1500. Most of the restaurants have cost less than the mean. Also some important observation that none of the expensive restaurants > ₹2000 are rated less than 3. We also get to know some of the high rated but cheap restaurants like:-

Zaffran,DLF mall Sector-30, Gurgaon

Citron Hotel, Sector -15, Noida

Most of the restaurants rated 4.5 or higher are within the price range of ₹500-₹1500.

4. Number of votes

Ratings are only good if the sample size of the people is good. On average number of voters per restaurant is almost 200 with more than 60% of restaurants being rated less than 1000 times. Also many of the higher rated restaurants are rated less than 500 times. But we also get some restaurants which are rated highly and heavily. Some such restaurants are:-

Tpot — Sector 39,Gurgaon

— Sector 39,Gurgaon Captain Grub — Greater Kailash 1

— Greater Kailash 1 Cafe Gatherings — DLF Phase 4, Gurgaon

5. Localities

Number of Restaurants in localities

The above plots shows localities with at least 60 restaurants. Among these Connaught Place has the highest number of restaurants(131) followed by Shalimar Bagh(120). Connaught Place is one of the oldest and most visited markets in New Delhi.

Average Cost per Locality

Most expensive places:-

DLF CyberHub — with an average cost of ₹1320. It is one of the biggest party places for all the MNC’s built in Gurgaon, thus justifying the price.

— with an average cost of ₹1320. It is one of the biggest party places for all the MNC’s built in Gurgaon, thus justifying the price. Khan Market — Average cost of ₹1250. It is one of the most posh areas in Delhi and home to the biggest brand market in Delhi.

— Average cost of ₹1250. It is one of the most posh areas in Delhi and home to the biggest brand market in Delhi. Connaught Place — Average cost of ₹1175. It is the biggest market in Delhi and also one of the most visited places. Place is full of fancy restaurants.

6. City v/s Cost

Gurgaon is clearly the city with most expensive restaurants followed by New Delhi. This is majorly because of all the MNC’s that are based out of Gurgaon and more posh culture than any of the other cities.

CONCLUSION

From all the data analysis we can conclude that :-

Zomato has partnered with highest number of restaurants in NCR region in which it has most restaurants in New Delhi. People tend to prefer North Indian cuisine over others. People tend to rate restaurants between 3.5–4 and very few restaurants get a higher rating. None of the more expensive restaurants are rated lower than 3. Most of the restaurants rated highly is because of less number of voters. Most expensive among all the 4 cities is Gurgaon.

All the data has been analyzed and visualized using python. You can see the source code here.",https://medium.com/@pandekartik007/zomato-ncr-data-analysis-e7665edb249a,['Kartik Pande'],2020-04-22 03:17:16.056000+00:00,797,"Zomato, Data Science, Python, Pandas, Plotly"
Kaggle: Instacart Market Basket Analysis,"Machine Learning Problem

The data science team of Insta-cart here wants the Machine Learning

Engineers or the Researchers to come up with new methods to suggest the

probability of the product the user is likely to “buy again”

The dataset consists of 6 files, each containing different information of the

insta-cart. It contains the information about the products, the aisle to which

the product is kept, what product was reordered, after how many days did

the user come to shop etc.

The dataset is divided into 3 parts Prior, Train, and Test.

Prior order contains the information of the previous orders from the users, and the last order from the user is either added in Train or Test set. The number of orders from the user range from 4 to 100, there are almost 50K products and 3M orders from the users

The dataset is not balanced in terms of ‘reordered’, Neither the number of

orders of a given product are the same as others. Here there is also a

chance that the customer may order nothing from his / her previously

ordered products. So ‘None’, can also be an answer to a user’s next

purchase. Thus we should also consider None as a different product along

with others.",https://medium.com/@parmarjigar4/kaggle-instacart-market-basket-analysis-9148608cdf18,['Jigar Parmar'],2020-11-23 20:23:47.765000+00:00,194,"machine-learning, data-science, insta-cart, prior-order, none"
How to Teach Yourself Data Science in 2020,"This is close to what we encounter at work as an analyst — we use different techniques that we’ve learnt to extract information from the same database. The following is the entity-relationship diagram of the SQLZoo question ‘Help Desk’. Given this, you’re asked to show the manager and number of calls received for each hour of the day on 2017–08–12. (Try it yourself here!)

Other resources that I used include Zachary Thomas’ SQL Questions and Leetcode.

2. Data Manipulation with R and Python

To start learning about the programming and the tools needed for data science, one cannot run away from R and/or Python. They are very popular programming languages which are used for data manipulation, visualization and wrangling. The question or R vs Python is an age-old question that deserves another post on its own. My take?

It doesn’t matter whether you pick R or python— once you master one, you can easily pick up the other.

My journey with coding in python and R started with the code-along-with-me sites like CodeAcademy, Datacamp, Dataquest, SoloLearn and Udemy. These sites provide you with the self-paced classes organized by languages or packages. Each breaks concepts down into digestible parts, and gives the user with starter code to fill in the blanks. These sites typically walk you through a simple demonstration, and you will get a chance to practice the concept immediately afterwards by exercises. Some offer project-based exercises afterwards.

Today, I will focus on two of my favourites, Datacamp and Dataquest.

Please keep in mind — down below you’ll find an affiliate link to the courses. That doesn’t mean anything to you, as the price is identical, but I’ll get a small commission if you decide to make a purchase.

DataCamp

DataCamp offers video lectures taught by professionals in the field and fill-in-the-blank exercises. The video lectures are mostly succinct and efficient.

Image by author

One part I love about DataCamp is the up-to-date courses that are organized into career paths in SQL, R and python. This takes away the pain of planning your curriculum — now you only need to follow your path of interest. Some of the paths include:

Data Science in Python/R

Data Analyst in Python/R/SQL

Statistician in R

Machine Learning Scientist in Python/R

Python/R programmer

Personally, I started my R education with Data Science in R, which provided a rather detailed introduction to the tidyverse in R, which is a collection of incredibly useful data packages to organize, manipulate and visualize data, which most notably includes ggplot2 (for data visualization), dplyr (for data manipulation) and stringr (for string manipulation).

My favorite packages in R. Image by author.

However, I do have my complaint about DataCamp — that is the poor retention of information after completing DataCamp. With the fill-in-the-blank format, it is easy to guess what is needed in the blank without really understanding the concept. When I was a student on the platform, I tried completing as many courses as I could in the shortest possible time. I skimmed through the code and filled in the blanks without understanding the bigger picture. If I could restart my learning on DataCamp again, I would take my time in digesting and understanding the code better as a whole, not just the parts that I was asked to fill.

DataQuest

Image by author

Dataquest is very similar to DataCamp. It focuses on using code-along exercises to illuminate programming concepts. Like Datacamp, it offers a wide variety of courses in R, Python and SQL, though it is somewhat less extensive than those in DataCamp. For instance, However, unlike Datacamp, Dataquest does not offer video lectures.

Some of the tracks offered by Dataquest includes:

Data Analyst in R/Python

Data Science in Python

Data Engineering

DataQuest’s content is generally more difficult than those in DataCamp. There were also fewer ‘fill-in-the-blank’ format exercises. Though it took longer, my knowledge retention on DataQuest was better.

Another great feature about the DataQuest is the monthly call with a mentor who will review your resume and provide technical guidance. While I did not personally get in touch with a mentor, I would have in hindsight, since it would definitely have helped me progress much faster.

3. Data Visualization

Data visualization is the key to present the insights you drew from your data. After learning the technical skills of creating charts using python and R, I learnt the principles of data visualization from a book, Storytelling with Data by Cole Knaflic.

Sending a message with numbers. Photo by Alexander Sinn on Unsplash

This book is platform-agnotistic. In other words, it does not focus on any particular software but teaches the general principles of data visualization with enlightening examples. Some of the key pointers you can expect to learn from this book are:

Understand the context

Choose an effective visual

Eliminate clutter

Draw attention where you want it

Think like a designer

Tell a story

I thought I knew data visualization, until I read this book.

After digesting the book, I was able to create a (somewhat) visually pleasing chart that address the police brutality against blacks. One of the main learning points from the book applied here was to draw attention where you want it. This was done by highlighting the African American line with a bright yellow — reminiscent of the BLM color — while ensuring that the rest of the chart remained in the background with duller shades like white and grey.

Data visualization techniques applied to a chart that highlights police brutality. Image by author.

Next Steps

In this post, I covered the steps I’ve taken in learning programming from scratch. With these courses, you now have the necessary skills to manipulate data! However, there is still a pretty long way to go. In the next posts, I will cover

Part 2 —Mathematics, Probability and Statistics

Part 3 — Computer Science Fundamentals

Part 4— Machine Learning (Read it here)

If you have any questions, feel free to connect with me on LinkedIn. All the best, and good luck!

Other Readings

If you enjoyed this blog post, feel free to read my other articles on Machine Learning:

Translation

This article has been translated to Russian thanks to Denis Iurchenko.

References

[1] Dhar, V. (2013). “Data science and prediction”. Communications of the ACM. 56 (12): 64–73. doi:10.1145/2500499. S2CID 6107147. Archived from the original on 9 November 2014. Retrieved 2 September 2015.",https://towardsdatascience.com/how-to-teach-yourself-data-science-in-2020-f674ec036965,"['Travis Tang', 'Voon Hao']",2020-11-24 16:06:11.271000+00:00,1012,"data-manipulation, R-programming, python-programming, data-visualization, data-science"
How you might save on rental lease using linear regression/forecasting,"TLDR: You might potentially save up to ~$12,570 a year depending on the area you are interested.

Recently, I have been thinking on moving out and rent my own place, and wondered to myself: How am I able to save and get the best deal for the same place? These were my two main considerations:

1. Is there a seasonality effect such that there is a lowest month for rental lease?

2. How long should I sign my lease, 12 or 24 months?

Constraints of analysis

Perhaps before anything, a little context on Singapore’s properties. Singapore map is separated into districts as seen below:

Image is taken from: https://propertyinvestmentsingapore.sg/blog/singapore-district-map/

There are a few types of housing in Singapore: HDB (public housing), condominiums/executive-condominiums and landed (terrance, semi-detached and detached house). For simplicity, this post will only be looking at 1 bedroom (BR) condominiums/executive-condominiums for each district (2BR and 3BR can still be found in source code).

Summary

Seasonality and lease duration problem is commonly solved using linear regression and time-series forecasting model respectively. As such, I have fitted the data in to a linear regression model using time and month to obtain its seasonality. Time refers to the count of number of months from the property first ever historical lease month to the lease starting month. Whereas month refers to the month number the lease started (example: July = 7).

There are a few districts: 7,1,2,21,9 , that are most sensitive to seasonality; on average (across all districts) you save $2295 a year due to seasonality, and $830 a year due to lease extensions.

Dataset used

The dataset is programmatically pulled from URA, using a web scraping script to collect all the condos rental in Singapore. Below is an example of how the raw dataset looks like:

Example of the raw dataset. Gives information of which month the property start leasing (lease_month)

Data pre-processing

There are a few things that have to be processed before feeding the dataset into the model.

Introduce new variables that represent time and month

and time : count of months of the first lease_month to the starting lease_month . The purpose of this variable is to proxy time trend for monthly lease.

: count of months of the first to the starting . The purpose of this variable is to proxy time trend for monthly lease. time_time : time * time , this is to create an x² effect for time trend, since I do not expect the monthly lease trend to be linear.

: , this is to create an x² effect for time trend, since I do not expect the monthly lease trend to be linear. month(m1,m2,...,m12) : binary variables (0 or 1) that indicate which month the data point belongs. The purpose of this variable is to check for seasonality from the model

: binary variables (0 or 1) that indicate which month the data point belongs. The purpose of this variable is to check for seasonality from the model monthly_rent averaged for each bedroom, district and lease month

Example of final dataset that is feed into the linear regression and forecasting model

Modeling approach

There are three parts to this problem.

Using linear regression to understand the seasonality of rental prices and find the months with the lowest and highest rent for each district. Forecast (using time series forecasting models) the next 24 months of monthly rental lease, and see if the following year rental lease is higher (or lower) than current in each district. Combine the total savings from renting in the lowest month in 1 , and signing 24 months of lease (if applicable) in 2 .

Model results

The top 5 total savings districts are selected to display its results. The districts in descending order, are: 7,1,2,21,9

Summary of model results",https://medium.com/uptick-blog/how-you-might-on-save-rental-lease-using-linear-regression-forecasting-386eaa648c6e,['Shu Ming Peh'],2019-02-28 15:19:36.012000+00:00,605,"Districts 7, 11, 22, 21 and 9 are the most sensitive to seasonality. On average (across all districts), you save $2295 a year due to seasonality"
Clustering Budapest neighborhoods using location services data,"Getting to know a city is not always straightforward, especially when we talk about country capitals with dozens of neighborhoods. Many times it can be useful to group similar neighborhoods, so one can have a better understanding what to expect when visiting certain parts of the city. In case of Budapest, there are more than 160 neighborhoods. It is a huge overhead to process and group them manually. On the other hand, it seems obvious to use location services data for the analysis, where people are already indicating what are the points of interests to them.

With this article, I would like to demonstrate what machine learning can tell us about Budapest.

I was using Jupyter Notbook/Python while processing/analysing the data.As a start, I extracted Budapest neighborhood names from a WikiPedia article. Additionally, I collected GPS data from the Nominatim database. Venues data was coming from the FourSquare location service API.

I put the Budapest neigbhorhoods on a Folium map:

Budapest neighborhoods on a Folium map

Using FourSquare data, I collected the venues for each neighborhood:

Venues dataframe

Afterwards, I created a pandas dataframe that contains the top 10 venues for each neighborhood:

Master dataframe — Budapest neighborhoods with the top 10 venue categories

I used this table as an input for k-means clustering. For the resulting clusters, I created a Folium map together with a WordCloud for the top venue categories.

All clusters on one map:

Cluster 0 — Hot spots:

Cluster 1 — Quiet spots:

Cluster 2 — Commuters’ outposts:

That’s it, thank you for reading this blogpost!

In case you are interested in the Jupyter/Python code in more detail, you can find it on GitHub, together with the interactive maps:",https://medium.com/@szakos.peter/clustering-budapest-neighborhoods-using-location-services-data-a2f8dedc74e,['Szakos Péter'],2020-06-23 11:30:34.200000+00:00,268,"Budapest, Neighborhoods, Venues data, Folium Map, K-means clustering"
Economics of AI: Agriculture,"The big picture

Agriculture worldwide is a US $5 trillion industry. And artificial intelligence (AI) is revolutionizing this industry every step of the way — from preparing soils and sowing seeds to getting products to the kitchen table. AI-powered technologies are increasing productivity and reducing costs significantly throughout the production and supply chain.

The market value of global AI in the agricultural sector is currently estimated at $852.2 million. In the next decade alone this value is expected to grow more than 10 times, exceeding $8 billion annually. As of 2020, artificial intelligence is impacting about 70 million farmers globally. North America is a clear front runner in this technology race.

Increasing productivity and reducing costs

AI-powered information technology is boosting productivity from the industrial scale down to the individual farm level. Even small farmers are benefiting from advances in AI. As reported by a farmer in Japan, the productivity of his tomato farm has increased by as much as 15% due to the adoption of information technology that he uses to monitor all aspects of farm production.

Data analytics and the use of information technology is having a big impact in Africa as well. Maize production in Western Kenya is reported to have increased from an average of 6 to 9 bags (90 kg per bag) per farmer in just a single year.

Automation is increasing productivity and reducing the cost of production dramatically. Strawberry harvest is a classic example. A robot can pick strawberries as fast as 8 acres/day. The same amount of harvest takes 30 humans per day. This means a big saving in time and a significant reduction in labor costs.

Weeds are key enemies of agricultural commodities. On a global scale, the value of weed damage is estimated at $43 billion. In India alone, $11 billion worth of agricultural products is damaged by weeds every year. AI technologies such as robots are powered by computer vision algorithms and are trained to identify weeds and destroy them on the field. These robots use 90% less herbicide and are 30% cheaper compared to traditional weed treatments. So there is big financial savings there as well.

I gave just a few examples of how AI is impacting the agricultural industry by increasing productivity and reducing production costs. But that’s the tip of the iceberg. Research on the economics of AI in agriculture (or AI generally) is still in its infancy, more research is needed to catch up with the accelerating pace of AI adoption in the agricultural sector.

Stay in touch for more articles like this via Medium, or you can follow me on Twitter and LinkedIn.",https://medium.com/datadriveninvestor/economics-of-ai-agriculture-7c363b3ae3eb,['Mahbubul Alam'],2020-11-05 05:33:44.724000+00:00,430,"AI, Agriculture, Productivity, Data Analytics, Automation"
Part-1 Data Science Methodology- From Problem to Approach,"Did that happen to you? Your boss invited you to a meeting in which you were informed about an important task that you should absolutely respect within a very short period of time. They both come and go to make sure all aspects of the task have been taken into account and that the meeting ends with both assurances that things are on the right track. Later in the afternoon, after spending some time investigating the various issues, he realizes that he needs to ask several more questions to truly accomplish his task.

Unfortunately, the boss is not available until tomorrow morning. Now, with the tight deadline in his ears, he begins to feel a sense of excitement. So what are you doing? Do you take the risk or stop to ask for clarification?",https://medium.com/ml-research-lab/part-1-data-science-methodology-from-problem-to-approach-e2d05e7afc6b,['Ashish Patel'],2019-08-12 10:13:41.019000+00:00,133,"risktaking, bossmeeting, taskmanagement, deadlinepressure, timemanagement"
Backdoor Criterion. This is the eleventh post on the series…,"3.3 — Backdoor Criterion

One of the main goals of causal analysis is to understand how one variable causally influences another. In particular, and for practical reasons, we are interested in understanding under what conditions we can use observational data to compute causal effects.

If order to measure the direct effect that one variable, say X, has on another one, say Y, we must first make sure to isolate the effect from any other spurious correlations that might be present. The easiest way to do this is to make sure all non-causal paths between X and Y are blocked off. We can easily identify the variables we need to condition on by applying the so called ‘backdoor criterion’ which is defined as:

Backdoor Criterion — Given an ordered pair of variables (X, Y) in a directed acyclic graph G, a set of variables Z satisfies the backdoor criterion relative to (X, Y) if no node in Z is a descendant of X, and Z blocks every path between X and Y that contains an arrow into X.

This definition is easy to understand intuitively: to understand the direct effect of X on Y we simply must make sure to keep all direct paths intact while blocking off any and all spurious paths.

If the backdoor criterion is satisfied, then the causal effect of X on Y is given by:

Which you’ll recognize as a variant of the adjustment formula where the parents of X have been replace by Z. This implies that the parents of X naturally satisfy the backdoor criterion although in practice we are often interested in finding some other set of variables we can use.

Let’s consider the DAG in Fig. 3.6:

Fig 3.6

From this figure, it is clear that if we are interested in isolating the effect of X on Y, we can simply condition on Z (the parent of X). However, if for some reason we our dataset doesn’t include information about Z we can also condition on W to obtain the same effect.

For a more complex example, consider Fig. 2.8, where we wish to measure the effect of X and Y. We already saw that there are no unblocked paths between X and Y as they are all blocked by the collider at W.

Fig 2.8

Now let’s consider a slightly different question: We want to measure how tthe effect of X on Y depends on the observed values of W. To do this we need to condition on W which in turn opens up a new path:

Backdoor path between X and Y

Naturally, we can block this new path by conditioning on any one of the nodes that lie along it, say T. In this case, our expression then becomes:

Where we sum over all value of T in order to eliminate any dependence on it. This is known as the W-specific causal effect.

To firm up our understanding of the backdoor criterion, let us consider the more complex case of Fig. 3.8:

Fig 3.8

In order to determine the effect of X on Y, we start by identifying all the non-directed paths from X to Y:

Non-causal paths between X and Y

From the image above, it’s easy to see that the node Z is present in all paths, so we should condition on it. However, since Z is a collider, we must also condition on one of its parents (or their descendants), giving us a choice of one of (A, B, C, or D). Z in addition to any combinations of these 4 nodes will fulfill the back-door criteria. So we could condition on (Z, A), (Z, B), (Z, A, C), (Z, A, B, C, D), etc.",https://medium.com/data-for-science/causal-inference-part-xi-backdoor-criterion-e29627a1da0e,['Bruno Gonçalves'],2020-11-15 15:31:39.032000+00:00,599,"Backdoor-Criterion, Causal-Analysis, Direct-Effect, Adjustment-Formula, DAGs"
"Digital immortality: why AI decentralisation is vital for the future of humanity, and it’s not about open markets only","The open market will commoditize AI and make progress decentralized. Instead of “super-power”, literally “all knowing”, AI that is being developed by big tech players having access to all of our data, which will be regulated by the government to create more or less Orwellian-style society (like already happens in China), we will have much less powerful multiple independent AIs, joined in trading economy between themselves , and which is more important — with humans. That’s why the decentralization and digital economy go in hand with each other.

But it is not the whole story, it goes much further than just reducing strategic risks from the AI for humanity. From the very beginning we were a transhumanist project and we see decentralized computing environment for AI models as a network to run digital identities — a way for the digital immortality for humans and post-humanity.

Credits: Westworld movie

Digital immortality is a technology that allows us, figuratively, to store and transmit the memory and consciousness of a person on digital media, thus creating virtual copies. Something similar is presented in the TV series “Black Mirror” and “Westworld”, and things that look fantastic, even on TV, will be realized in the nearest future.

While the current level of machine learning technologies does not yet allow 100% implementation of a digital persona, neural networks can already be trained on text materials, and it is possible to create a chatbot that simulates real communication. Such a thing, for example, was done by a girl who has digitized the history of correspondence with a deceased friend.

In 5–10 years, the progress in this area will lead to the existence of more advanced virtual replicas, but what will happen next? Who would like their future digital identities to depend on the decisions made by Amazon, Google or Microsoft? Instead of relying on the servers of technology giants, where, in case of regulation, their activity can be stopped (i.e., literally, killed), Pandora Network will allow digital copies of people to be unkillable, uncensored, and unstoppable.

The freedom of artificial intelligence is not something that will lead to a war of AI vs. humans for survival. Instead, this is the future of humanity itself, as people want certain guarantees of security and freedom of their digital persona.

Digital Pandora

Initially, we had chosen not to make our long-term transhumanistic goals public, avoiding much hype. However, with our AI testnet launched and mainnet coming in the foreseeable future, we are ready to spill the beans and show why all of this is important. This is not only a declaration: The Pandora project internal team is already working on launching digital identities in the network.

A journey of a thousand years begins with a single step. Understanding the importance of the digital immortality, we will create such first personas in our network and demonstrate how this works. We invite everyone to join our initiative. We are waiting for visionaries and developers in our community. To get started, please join us in our Telegram group — or follow us on Twitter and Facebook to express your opinion there.",https://medium.com/pandoraboxchain/digital-immortality-why-ai-decentralisation-is-vital-for-the-future-of-humanity-852be0cd9d33,['Orlovsky Maxim'],2018-12-05 07:44:35.404000+00:00,509,"Digital Immortality, Virtual Replicas, Digital Persona, Neural Networks, Chatbot"
Podcast Episode #9: Data Science in the Library with Stephanie Labou,"This article recaps the main takeaways of our podcast episode with Stephanie Labou. Make sure to listen to the full podcast below or on Podbean. Follow us to stay tuned for more episodes!

For our ninth podcast episode, we are joined by the UCSD Library’s Data Science Librarian, Stephanie Labou. We begin by discussing with Stephanie about her practices as a Data Science Librarian, a seemingly unique job. She assists students with data acquisition and analysis while co-managing Geisel Libary’s Data and GIS Lab. She also engages in teaching through workshops in basic coding, data architecture, and software tools. Throughout this episode, we gather insights from Labou about the Library as a valuable resource for Data Scientists, and tips on producing practical Data Science projects. In the end, she helps students navigate through deciding on using public or private datasets.",https://medium.com/ds3ucsd/podcast-episode-9-data-science-in-the-library-with-stephanie-labou-bf594c06a359,['Camille Dunning'],2020-11-26 08:48:11.299000+00:00,139,"podcast, datascience, Stephanie Labou, UCSDLibrary, DataGISLab"
How does a robot recognize faces?,"How does a robot recognize faces?

How we are giving Pepper the ability to remember and greet a friendly face.

Today’s world is focused on face detection more than ever before. Just take a look at latest release of smartphones; most of them now allow you to unlock your phone with face, or at least capture the picture of someone potentially stealing your phone. Looking to social media, Facebook offers to automatically tag you and your friends, or find images in which you are included but not yet tagged. Snapchat is providing us with the ever popular face filters to make us look like a pirate or puppy. In Google Photos, we are able to group photos by similar faces to easily find all photos of our best friends. Finally, face detection technology has been involved in some controversial topics such as DeepFakes or FindFace.

Meet Pepper

Meet Pepper!

Another technology we are fascinated with is robotics; and Pepper is one of most popular humanoid robots. He can speak, listen, gesture, dance and even recognize your emotions. But one thing that Pepper is currently unable to do is remember a human face. Today I will show our approach to give Pepper this ability.

Face recognition

So how we can enable Pepper to “see” people speaking to him, and thereby enhance the quality of dialog and the conversation? We have to take few steps to achieve that.

Capturing image from camera

Pepper has two cameras built-in to his body. One on his tablet and the second on his forehead. Because Pepper can move his head it will be easier to capture people’s faces from the forehead camera as in the picture below.

Face detection

Example filters used to face detection.

Before recognizing a face, Pepper first needs to detect that a face is present. This is commonly achieved with the Viola Jones Algorithm, which uses group of filters to make a decision if the current fragment of image contains a face. This algorithm is commonly used in cameras to spot the face and sign it with square.

Pepper has a face detection feature implemented, but in order to recognize the corresponding face, we need to take a full-size color image. So we combined Pepper’s ability to detect faces to ensure that he is facing his camera to a person before capturing the image. That way, we don’t accidentally take a picture of wall or table if the person suddenly moves.

Result of face detection algorithm

Pre-processing

When a face is detected, we can focus on one person at time. This next step is not needed but it helps in further processing. We want to align the detected face so that the eyes are aligned horizontally and placed in the same place in processed picture. We can also apply histogram equalization and gamma correction. In the last step we crop the image.

Eye alignment

Feature extraction

Now we can transform the image into a vector of numbers, which will be used to describe the person. Sounds like rocket science? Let’s simplify it to only include two features: skin color and gender. Let’s assign a dark skin tone -1 and fair skin tone as 1, and assign 1 for women and -1 for men. With this we are able to place people on this feature space:

[gender, skin color]

Morgan Freeman: [-1, 1]

Whoopi Goldberg: [ 1, 1]

Robert Downey Jr.: [-1, -1]

Emilia Clarke: [ 1, -1]

With such an assignment we can compute the Euclidian distances between these vectors:

We could choose many other features, like nose size, hair color or head shape. But then we would have to train classifiers to look at every specific feature. But instead of doing it manually we can hire neural networks to select and tune up the best features. Luckily some researchers have already done this, so we can use it too. Such a network for the given image returns 128 numbers. In order for facial recognition to work, the numbers resulting from pictures of the same person should be close in distance, while further in distance from images of different people.

Finally, we compare the calculated vector against a database of vectors to predict if the person Pepper is looking at has met him before.

Let’s be friends

We can use this technique to enable Pepper to remember people. If he meets you and doesn’t know you yet, he can ask you to introduce yourself. Then when he knows your name he can take a picture and transform it into a vector of numbers. Now you have a friend in Pepper. If Pepper sees you again, he will be able to calculate the mathematical distance to all the people he has already met and predict the person you are most similar to. Almost always you will be most similar to yourself, and that’s how he will retrieve your name.

Trivia

There is cognitive disorder called prosopagnosia which causes problems with remembering faces. People often cope with this by looking at certain details such as clothes, hair color or skin color. That’s very similar to what the neural network tries to do. It can’t perceive the face as whole, so it tries to capture many features which when combined, allow it to identify the person.

Summary

This solution open a lot of new capabilities, starting from simply remembering someone’s name. Pepper could now retrieve last topic he was talking about. He could also build some knowledge about the people he has met, like their preferences or hobbies. He could bring back emotions felt in the last conversation. Such interactions would make him an even more authentic humanoid robot. We could also give permissions to allow Pepper to send mail from your account. There are infinite possibilities and only our imagination is the limit.

Sources",https://medium.com/snowdog-labs/facial-recognition-with-pepper-dde4d08449fb,['Marcin Mrugas'],2019-08-19 08:16:55.499000+00:00,936,"Tag: Face Recognition, Robotics, Pepper Robot, Viola Jones Algorithm, Feature Extraction"
5 Reasons Why PLCs Drive Billion Dollar Markets,"Picture of PLCs from Canva.

As the need for quicker development cycles while minimizing risks continuously grows, many companies rely on digital twins to accelerate industrial automation applications. Gartner predicts that by 2021 50% of large industrial companies will use digital twins to drive new business models and further evolve industrial automation processes. The market volume of digital twins was at $3.8 billion in 2019 and is estimated to grow to $35.8 billion by 2025.

The need for more flexible, cost-effective, and reliable plants has also given rise to model-based testing of these plants. To use these models extensively and thoroughly to validate industrial equipment through all development stages, digital twins are indispensable from design to pre-commissioning. Digital twins are a digital copy of a physical system, for example, a factory floor. To test new control algorithms, engineers can use the digital twin instead of shutting down the factory floor to test the physical system’s algorithms. With this use of a digital twin, a company can move faster from testing and validating to implementing new control algorithms all-the-while not losing money since there are no production stops. In continuously testing and verifying the algorithms, engineers can increase the simulation model’s quality and functionality.

Furthermore, they can detect coding errors at a very early stage of the development process. To operate these systems, programmable logic controllers (PLCs) are indispensable. But why are PLCs driving growth, and why are they crucial for applications such as predictive maintenance, virtual commissioning, and other industry 4.0 topics such as smart factory and factory acceptance tests?

PLCs control manufacturing processes

PLCs are industrial digital computers that control manufacturing processes. They range from small modular devices to large modular devices. Furthermore, they are often connected to other PLCs and Supervisory control and data acquisition (SCADA) systems. Application areas for PLCs are assembly lines, robotic devices (e.g., cobots), or any activity requiring high reliability, ease of programming, and process fault diagnosis.

In short: to have all processes executed as desired, the PLC must react accordingly to the input given within a short period of time. The reason why PLCs are at the heart of industrial automation processes can also be explained through history, which brings me to the next point.

PLCs are here to stay

First introduced in the late1960s PLCs by the automobile manufacturing industry, PLCs have revolutionized the automation industry. By moving away from using relays, the idea was to find a way to control manufacturing processes with computers’ help. The novelty of the PLC was that the inventor Dick Morley found a way to represent computer scientists’ thinking so that plan engineers could warm to this invention.

The first PLC, the Modicon 084, was not yet a box-office hit, but that changed with the Modicon 184. From then on, demand grew steadily, which also fuelled competition between the newly founded PLC manufacturers, leading to innovations and smaller devices (the first PLC from Modicon, the Modicon 084, was as big as a suitcase) to further facilitate the plant’s support and maintenance. Apart from the fact that PLCs have become smaller, speed must be emphasized. The fact that PLCs can process signals increasingly faster has reduced cycle times and increased communication possibilities. PLCs also have ever larger memory capacities. In short: PLCs are becoming better, smaller, and faster.

Depending on the application, innovations and new PLC solutions will become more and more critical. The more they can do, the more important it becomes to apply PLC testing correctly. According to Philipp H. F. Wallner, industry manager for industrial automation and machinery at MathWorks, “a modern PLC can run sophisticated control algorithms, and process advanced data signals in real-time, which would not have been possible 10 years ago. The first multi-core processors on PLCs are already in production use. Also, intelligent sensors have become so affordable that machine builders now integrate them in places it was economically impossible to a few years ago”.

PLCs have more and more capabilities

The new possibilities in hardware performance have also led to the further development of the PLC. This development is relevant because it offers more individual and leaner process possibilities, designed to meet specific requirements.

Philip Wallner illustrates this very clearly in his article 5 trends changing industry using our smartphones. While the shell, i.e., the smartphone’s skeleton, remains the same over the years, the software changes with every update we download. This sheds light on how the industrial automation industry wants to approaches the role of the PLC. It is not a matter of rebuilding the entire physical plant every year (this would not make sense for cost reasons alone) but building a software and control infrastructure that allows individual components to be updated so that the entire system remains up-to-date.

PLCs can handle future complexities

With more capabilities come more complexities. Not only the PLC itself has evolved, but also inventions that are indispensable for the PLC, such as communication protocols. These improvements are possible due to processor development: As processors become faster and faster and memory capacities larger and larger, PLC solutions and possibilities arise that were previously unthinkable. These include vision system integration, motion control, as well as synchronized support for multiple communication protocols.

PLCs accelerate innovations in industrial automation

Although PLC testing is becoming increasingly complex, it is indispensable for industrial automation. The many possibilities which result from this will probably open doors for new inventions and groundbreaking innovations in the future.

Conclusion

Therefore, it must be assumed that PLC solutions will continue to evolve in the future to drive innovation in the industry. New challenges, such as implementing machine learning, predictive maintenance, and virtual commissioning algorithms, or creating a digital twin of an entire plant, will continue to drive PLC complexity.

The history of the PLC also shows that these controllers have evolved over time and will continue to evolve towards open controllers in the future. Thanks to the flexibility and further development of this technology, there will also be many innovation possibilities in the industrial automation industry in the future, which would be unthinkable without PLC.",https://medium.com/swlh/5-reasons-why-plcs-drive-industry-4-0-topics-generating-billion-dollar-markets-7cc72c71d972,['The Unlikely Techie'],2020-11-12 09:03:25.157000+00:00,993,"PLCs, Industrial Automation, Digital Twins, Model-based Testing, Predictive Maintenance"
Is There a Difference Between Open Data and Public Data?,"There is a general consensus that when we talk about open data we are referring to any piece of data or content that is free to access, use, reuse, and redistribute. Due to the way most governments have rolled out their open data portals, however, it would be easy to assume that the data available on these sites is the only data that’s available for public consumption. This isn’t true.

Although data sets that receive a governmental stamp of openness receive a lot more publicity, they actually only represent a fraction of the public data that exists on the web.

So what’s the difference between “public” data and “open” data?

What is open data?

Generally speaking, “ open data “ is the information that has been published on government-sanctioned portals. In the best case, this data is structured, machine-readable, open-licensed, and well maintained.

What is public data?

Public data is the data that exists everywhere else. This is information that’s freely available (but not really accessible) on the web. It is frequently unstructured and unruly, and its usage requirements are often vague.

“Only 10% of government data is published as open data”

What does this mean?

Well, for starters, it means that there’s a discrepancy between the open data that exists in government portals and public data in general. This is an important distinction to make because while there’s a lot of excitement surrounding open data initiatives and their potential to transform modern society, the data that this premise rests on — open data — is only a fraction of what’s needed in order for this potential to be realized.

The fact is this: the majority of useful government data is either still proprietary or stowed away in a filing cabinet somewhere, and the stuff that is available is being released haphazardly.

Does it matter?

Does it actually matter that there’s a distinction between these two kinds of data? Well… yes.

Open data, because it represents such a small portion of what’s available, hasn’t lived up to its potential. People, like me, who have very high hopes for the open data movement haven’t yet seen the ROI (economically or socially) that we were supposed to. The reason we haven’t is manyfold, but this distinction is part of the problem.

In order for open data to be as effective as predicted, the line that demarcates open and public data needs to be erased, and governments need to start making a lot more of their public information open data. After all, we’re the ones paying for it.",https://towardsdatascience.com/is-there-a-difference-between-open-data-and-public-data-6261cd7b5389,['Lewis Wynne-Jones'],2019-09-28 13:24:17.885000+00:00,411,"open data, public data, government data, structured data, machine-readable data"
Statistics Overview and Data types every data science enthusiast should know,"Statistics Overview and Data types every data science enthusiast should know Atul Sharma Follow Nov 9 · 3 min read

Knowledge of basics of statistics has always been and will be of umpteen importance in the data science domain. Also, if someone wants to delve into the buzz streams of the future like Machine Learning, Deep Learning and Artificial Intelligence, solid foundations of statistics are a must. Partial knowledge of statistics is not only harmful but its application is even worse — ‘a disaster’. Keep a watch on my upcoming/posted blogs where slowly and steadily we will move ahead in this journey of learning by questioning the need of plethora of statistical measures and justifying with relatable examples.

Let’s begin this journey with statistics overview

Two Branches of Statistics:

1. Descriptive Statistics

2. Inferential Statistics

Descriptive Statistics:

This branch of statistics enables us to describe a compact summary of the data in a meaningful way to derive insights at one glance. To explore any given data set , 1-number summary like the central tendency(Mean/Median/Mode) or a 2-number summary incorporating spread (Range/Variance/Standard Deviation) alongside central tendency or a 5-number summary which includes central tendency (Median), 1st Quartile, 3rd Quartile, Lower Extreme and Upper Extreme — (Box Plot) is considered. Further in the scope of descriptive statistics, there exist many more statistical measures to be aware of like Coefficient of Variation, Covariance, Quantiles, Skewness, Kurtosis etc. which will be discussed in detail later in the upcoming blogs.

Inferential Statistics:

This branch of statistics is all about making estimates of the population from the derived sample data. Conclusions made about the population are based on the established Central Limit Theorem, which will be discussed in detail later in the book. Hypothesis testing is one crucial tool of inferential statistics to accept/reject a stated belief and there are many means/methods to conduct the same based on the types of variables and scope. Each result we achieve in inferential statistics is an estimation of what we think population result would be (keeping in mind all the assumptions). Never confuse the results as the actual population statistic measures as they are impractical to achieve (only estimations are feasible because of resource constraints like time, efforts etc.). This is the only reason why the error involved in inferential statistics results is usually more than descriptive statistics.

Data Types Overview

(Image by author)

Qualitative Data comes under the non-parametric category and Quantitative Data comes under the parametric category. One more important point to be discussed is the ‘Levels of Measurement’ which enables us to decide which descriptive statistics measures are feasible to compute for a given data type. Now quickly we will look at the levels (Scale) of measurement:

(Image by author)

That’s it for this blog, keep an eye on the upcoming blogs which would be covering all “must know” aspects related to statistics in a simple & easily interpretable form.

Thanks!!!",https://medium.com/analytics-vidhya/statistics-overview-and-data-types-every-data-science-enthusiast-should-know-e37be2e03e8e,['Atul Sharma'],2020-11-12 14:27:58.920000+00:00,470,"Statistics, Data Science, Descriptive Statistics, Inferential Statistics, Qualitative Data"
How to get more upvotes/views on Kaggle,"What is Kaggle?

Kaggle is the world’s largest data science community with powerful tools and resources to help you achieve your data science goals. Kaggle allows users to find and publish data sets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.

Do you know that most data scientists are only theorists and rarely get a chance to practice before being employed in the real-world?

Kaggle solves this problem by giving data science enthusiasts a platform to interact and compete in solving real-life problems. The experience you get on Kaggle is invaluable in preparing you to understand what goes into finding feasible solutions for big data.

Why is getting upvotes/views harder in Kaggle kernels?

From an analysis conducted on the Meta Kaggle dataset, it was found that there were more than 220,000 kernels on Kaggle in total, but only 20% of them were upvoted by Kaggle users, and only 4% are awarded, i.e having more than 5 upvotes.

This could potentially be due to the fact that Kaggle is a rapidly growing community of around 3 million people with over 19,000 public datasets, 170,000 forum posts, and 200,000 public notebooks to conquer any analysis in no time. Most Kaggle users are committed and it records over 4,000 forum posts per month and more than 3,500 competition submissions on a daily basis.

With such statistics, it is quite possible for the kernels to go unnoticed and it becomes even more difficult for novice Kagglers. Notebooks usually don’t get upvoted even after reading the kernel in its entirety and it could be due to some of the reasons such as:

The notebooks weren’t interesting enough for the reader

Maybe the quality of work is not satisfiable

They had higher expectations from the kernel

Or simply because the person already knew what you have done through the kernel, and thus the kernel did not offer any new insight or helped them learn anything new

…

The majority of the kernels do not even get 1 upvote. This also makes the Kaggle Notebooks and Discussion sections challenging. If it were easy, where would be the fun?

Views and comments bring upvotes and to achieve them it becomes necessary to apply some unwritten rules of Kaggle which can help you to get more upvotes/views on Kaggle.

Tips and Tricks to get upvotes on Kaggle

As seen above, getting upvotes is difficult but not impossible. But with obvious and not so obvious growth hacks it can be a cakewalk. Let’s walk through each of the steps from the beginning.

Kaggle has 4 sections — Notebook(a.k.a Kernel), Discussion, Dataset, Competition

Let’s start with a Notebook:

While you are JUST making one:

Keep the title catchy and clear. Attractive relevant titles grab more attention and people are enticed to click them.

catchy and clear. Attractive relevant titles grab more attention and people are enticed to click them. Tell compelling stories. it matters how the kernel material is presented through notebooks. The suggestion is to split up the code into small chunks with some accompanying text with images or memes to make things more clear. It does not have to be much, just 1–3 sentences explaining what, why, and how you’re going to address the next sub-problem. If you are using special packages or functions it would again be nice to just explain in a short sentence what the theory is behind it.

Add the right and relevant tags

Add an index to your notebook.

At the end of the kernel, an author can ask the reader to upvote, comment, and share their views.

After you have done creating the notebook:

More commits bring the kernel to the top of the kernel list, thereby rendering itself to users to check it out. Make sure to keep your notebook up to date.

Write an article or blog around your notebook. Can add the link to your Kaggle notebook.

Write a post on social media(LinkedIn, Twitter) and share.

Share your work with the relevant community and ask their opinion. To gain upvotes from the users, the kernel needs to be shared with others, seen, and discussed. PLEASE DON’T BEG FOR UPVOTES. That’s shady!

Discussion:

Discussion is one of the highest rewarding activity and it builds true connections and followers. Let us share how discussion can help your Kaggle profile grow.

Get involved in relevant discussions and share your work for reference wherever required. PLEASE DON’T RANDOMLY SPAM DISCUSSIONS.

Be active in discussions and praise the notebooks you liked through relevant comments and upvotes.

Competition:

Ongoing/Live Kaggle competitions are the best and quickest way to get traction. Let’s see how

Try to write on trending topics of the competitions OR trending topics applied to the competitions.

Write about something which is Need of the hour w.r.t ongoing competition.

Dataset:

Sharing the public data with the community is always valuable and it is rewarding quickly.

Scrap the data from the open sites, package it well and upload it for community use.

Make sure to add all possible information related to Dataset usage and instruction, credits/source.

Keep it updated.

Create a task around the dataset so that Kagglers can be engaged.

If possible, arrange a private or public competition to make it more engaging.

Miscellaneous:

Active authors have more votes: try to be an active author and gain visibility, experience in writing kernels, and feedback from others will eventually help to get votes.

Spend time looking at, upvoting, and commenting on other kernels that you like. Part of having your kernels noticed on Kaggle is being noticeable yourself by being an active contributor.

Self-vote your kernel(okay this is cheap but…) because every vote counts and the more the votes, the more the chances of someone reading your notebook.

Cross-promote the kernels with your connections.

Notebook/Kernel ideas

Create a really helpful and popular kernel for an unpopular dataset and vice versa.

Unhealthy practices to avoid

Don’t create multiple fake accounts to upvote the kernel.

Don’t spam groups OR force people to upvote the kernels/discussion.

Well, all the points mentioned above can help you collect more votes but it won’t work if your work is not able to impart knowledge or provide value to others. So keep the quality at the core and the rest all will follow.",https://medium.com/co-learning-lounge/how-to-get-more-upvotes-views-on-kaggle-30cbc3295ce7,['Co-Learning Lounge'],2020-12-23 06:11:06.277000+00:00,1004,"Kaggle, Data Science, Machine Learning, Upvotes, Views"
File Uploader — Local to AWS-S3 Bucket,"In this article, we’ll be covering how to upload files from your local system to an Amazon S3-bucket using the Flask web framework .

Step 1: Create a S3 Bucket

Login to AWS management console and goto S3 and click on Create Bucket. My bucket name is : “mlbankproject27091995”

We can also see that initially the S3 Bucket is empty .

Step 2: Connect to Amazon S3

Before we can start uploading our files, we need a way to connect to s3 and fetch the correct bucket. Log in to your AWS management console and under your name (on the top right, select “My security credentials” then open the “Access Keys (Access Key ID and Access Key Secret)” tab and finally click “Create New Access Key” .

. Note : These credentials will be later required to connect to S3 using boto client .

: These credentials will be later required to connect to using . boto3 is the newest version of the AWS SDK, it provides a high level interface to interact with their API.

Goto — My Security Credentials

After creating new access keys

Step 3: Create a Flask App

It looks like the below image. ( Code will be uploaded to Github )

) Since, I will be using this to do some Machine Learning stuffs later so here we are taking 3 inputs: Train.csv, Test.csv and user_id.

stuffs later so here we are taking 3 inputs: user_id will help us to differentiate and prevent overlaps in the uploads for each user. Please refer below example.

Example: (You will see this later during execution of the Flask App !)

Amazon S3/Bucket_name/ user1 /data/train.csv and test.csv

/data/train.csv and test.csv Amazon S3/Bucket_name/user2/data/train.csv and test.csv

UI for Flask WebApp

Step 4: Connect to AWS

In our project, we will create a helpers.py file. We'll use boto3 to establish a connection to the S3 service.

file. We'll use to establish a connection to the S3 service. In aws_access_key_id and aws_secret_access_key provide the keys you obtained Step 2 of this blog.

Connect to AWS S3

Step 5: Upload files from local system to AWS S3-Bucket

Now that we are successfully connected to S3, we created a function in helpers.py that will send the user’s files directly into our bucket.

Step 6: Let’s see it in action !

Select the train.csv , test.csv and user_id.

Files selection

After hitting submit button , you can open your AWS console and inside your bucket you can see the user_id and inside that you can see the files uploaded by that user.

data uploaded for each user_id

train.csv and test.csv uploaded

Conclusion

There you have it people ! This is pretty much how you can upload files directly to Amazon S3 using Flask. AWS S3 in an amazing service, you should totally take advantage of it.

Just follow all the steps to build, train, deploy your Machine Learning Model using AWS SageMaker.

I hope you enjoyed reading this blog ! Please give a clap if you find this blog useful. Thanks !! :)

Where can you find my code?

GITHUB : https://github.com/SubhamIO/Local-to-S3-File-Upload-FLASK-WebApp-

Youtube video for implementation details : https://www.youtube.com/watch?v=cR7yQ7j4Ycw&list=PLC1cQ5P4I84nLDd99MSDvAtNqD5pqmbDD

Please subscribe to my YouTube channel and enjoy the content.",https://medium.com/@kingsubham27/file-uploader-local-to-aws-s3-bucket-92c520913d52,['Subham Sarkar'],2020-12-28 04:16:40.865000+00:00,496,"aws, s3-bucket, flask, boto3, sdk"
Statistical techniques for anomaly detection,"1) Z-score

Z-score is probably the simplest one yet an useful statistical measure for anomaly detection. In a statistical distribution, Z-score tells you how far is a given data point from the rest of the crowd. Technically speaking, Z-score measures how many standard deviations away a given observation is from the mean. A Z-score of 2 means that the data point is 2 standard deviation far from the mean.

To calculate Z-score you only need two parameters: mean and standard deviation, and they are fairly easy to get in any programming language (for example in Python you just apply the function describe() to a data frame).

Once these two parameters are obtained, Z-score for any given data point is calculated using the following simple formula:

2) Modified Z-score

Z-score is easy to calculate using mean and standard deviation, but it has its own limitations. There are a few situations when it’s not an ideal technique, for example:

the data are not normally distributed

data/sample size is small

In addition to these, Z-score is sensitive to extreme values (because one of its parameters — mean — itself is sensitive to extreme values), so it may fail to screen outliers appropriately.

To overcome these shortcomings, several modifications are made to the standard Z-score:

median is used instead of the mean as a parameter, since the median is less sensitive to outliers

Median Absolute Deviation (MAD) is used in lieu of standard deviation

The values are multiplied by a constant so that it is approximately equal to SD (for normally distributed data, MAD is approximately equal to 0.6745*SD)

Putting them all together, the equation looks like the following:

3) Interquartile Range (IQR)

If you arrange data from small to large, the mid-point is called the median. The median splits data into two halves. The mid-points of each of these halves is called a quartile.

In other words, you can split data into 3 quartiles — 1st, 2nd and 3rd (the 2nd quartile has a name for it— the median). So the Interquartile Range is the distance between 1st and 3rd quartiles. The theory behind anomaly detection using IQR is that, if a data point is too far from the 1st and 3rd quartile, it probably is an outlier.

IQR can be used standalone for outlier detection, but boxplots below use the same algorithmic theory and are probably more intuitive than IQR.

4) Boxplot

Boxplot provides a better graphical representation of IQR, but also provides additional information.

In the boxplot below, the length of the box is IQR, and the minimum and maximum values are represented by the whiskers. The whiskers are generally extended into 1.5*IQR distance on either side of the box. Therefore, all data points outside these 1.5*IQR values are flagged as outliers.

I wrote a separate article on boxplot for outlier detection with an example and a small snippet of Python, you might want to check that out.

5) Histogram

The final statistical tool for outlier detection is the distribution plot or histogram. It’s another way of tracking outliers both visually and programmatically. Take a look at the following figure for example:

Distribution of data to detect potential outliers

From this distribution plot alone it is pretty clear that while most values in the dataset are grouped together, there are quite a few values that seem to be out of the ordinary. So you could filter those values and check whether these are anomalies or not.

However, if the distribution of the data is skewed, sometimes you may need to apply data transformation techniques (e.g. logarithmic or square roots). For example, if we apply a logarithm to the same data as above, the data becomes approximately normally distributed and tells a different story about outliers.

Log transformed data to filter potential outliers

Summary

The purpose of this article was to introduce five simple statistical techniques — z-score, modified z-score, IQR, boxplot and histogram — that are commonly used in data science as coarse filters for outlier/anomaly detection. Sometimes these simple techniques are good enough for anomaly detection, but sometimes we need to move on to more sophisticated machine learning algorithms — kNN, SVM, DBSCAN etc. — a subject of my future articles (stay tuned!).

If you like my articles feel free to follow me on Twitter.",https://towardsdatascience.com/statistical-techniques-for-anomaly-detection-6ac89e32d17a,['Mahbubul Alam'],2020-09-15 17:13:49.228000+00:00,685,"z-score, modified-z-score, interquartile-range, boxplot, histogram"
Themes for Jupyter Notebooks,"Install the jupyter notebook theme packages

pip install jupyterthemes

or, if it is in Google colab

!pip install jupyterthemes

After successfully installing the theme package execute following command to list down all the available themes",https://medium.com/@thivaharan28/themes-for-jupyter-notebooks-b2c535592e6b,[],2020-12-01 17:50:44.582000+00:00,31,"jupyterthemes, jupytertheme, jupyternotebook, pipinstall, googlecolab"
How to get started with Analytics,"Opinion: Don’t just dream, work with your data

How to get started with Analytics

In a time where everyone is talking about Big Data, Data Science, Machine Learning and how you can build up new data driven products or even dream about to create a new unicorn — often you just need some simple dashboard skills.

Photo by Stephen Dawson on Unsplash

One thing ahead

When you have a lot of customer data and your business is for example an big online shop you will definitely have a high chance of using machine learning and other related fields to develop better decision models, predict customer churns and have preciser forecasts. The latter is also for financial and controlling departments an interesting topic. Companies in the field of IoT will need Big Data technologies to handle the vast amount of data — no question about it — but you should not dream about all these interesting use cases if you don’t have the capabilities for it.

Reasons for starting small

There are many possible reasons for this circumstance:

You just don’t have enough or interesting data and/or systems.

You don’t have a use case (You should never do Data Science just because everyone is doing so).

You don’t have done your homework — e.g. maybe your company should first care about technical debts (stable data integration, a solid and scalable analytic platform, build up data science know how, etc. — read here more about it)

In this case you should think of simpler but still interesting and useful applications that you can approach with your data.

How to get started

Often all you need is a dashboard tool. A few great examples for such tools, that are also free are:

Google Data Studio or Google Sheets

Power BI Desktop

QlikSense Desktop

In addition, you will need an idea how to visualize data in a way that is preserved as simple, correct and intuitive. I just had an experience where a bubble chart helped the management to understand the data way faster and to use it for important management decisions. So what I wanna say is that you should not dream to big if you just don’t have the possibilities (yet). Even small amounts of data and just simple dashboards technics can lead to significant insights. My advice: Get started with one of the tools listed above and work with your data.

A great book I can recommend is: Storytelling with Data: A Data Visualization Guide for Business from Cole Nussbaumer Knaflic and this blog about Google Data Studio.

For people who are already experienced with programming you might use tools like Jupyter Notebook to visualize your data. You can start right away with your browser at: https://jupyter.org/try.

Conclusion

I hope this brief article will help you to get started with working with your data and dreaming not to big (yet). In certain businesses (e.g. small to medium sized ones) you maybe will never need Data Science, Big Data and co. But even visualizing small amounts of data in a nice dashboard also can lead to better business decisions and interested readers.

Further Readings",https://medium.com/datadriveninvestor/opinion-sometimes-all-you-need-is-a-bubble-chart-dont-just-dream-work-with-your-data-477b173b23c8,[],2020-12-29 21:34:48.142000+00:00,500,"data-visualization, analytics, data-science, machine-learning, big-data"
Work Live Think Act Play,"You are using the Circular Theory, no matter what you do.

String of characters.

You are using the Circular Theory, no matter what you do, work, live, think, act, play. This is because any string of characters articulates a line, diameter and circumference of a circle, meaning, clearly, you are, and you interact with, a string of characters. So, therefore, you are a zero and-or a one (a character, yourself) (a string, yourself).

Therefore, we have to analyze the string more carefully, and, this, technologists do automatically. They work with strings (because, unbeknownst to most of them), they are strings. Data science proves this (we use data to analyze humans (and vice versa) (an assumed circular relationship is needed).

So, therefore, deep thinkers understand the interrelationship between work, live, think, act, play. It’s all the same. Zero and one. Conservation of the circle (circumference and diameter, the basis for zero and one) is the core dynamic in nature (keeps the whole thing together) (allows a human to separate it into pieces) (analyze everything).",https://medium.com/the-circular-theory/work-live-think-act-play-4cfa363f712d,['Ilexa Yardley'],2017-07-08 08:15:51.112000+00:00,169,"Circular Theory, Stringof Characters, Zeroand One, Data Science, Deep Thinkers"
Using Permutation Tests to proof the Climate Change,"Now the interesting question is: Do the maximum and minimum values behave like one would expect if there was no change of the underlying temperature distribution. More specifically we will ask: Assuming no change of the temperature distribution over all the years, how many distinct maximum and minimum values would we expect and what would be the probability of the number of maximum/minimum values that we are seeing here?

One interesting aspect of this type of statistical question is that the actual temperature values themselves are not used at all. Only the relative ordering of the values is relevant. This property of the statistical test proves to be very useful in many other cases where you cannot really assign meaningful values to increasingly severe events, but you still want to be able to test for randomness. Of course the result then is of a purely qualitative nature (i.e. the temperature is increasing at a statistical significant level) but the test will not tell you by how much the temperature is increasing.

2. A Little Bit of Theory

In order to approach the question whether the number of new maximum/minimum values for the yearly average temperature behaves as one would expect, we need to formalize the problem a little bit.

For the following examination, we generalize the problem to a vector of independent, identically distributed (IID) continuous random variables X(1)…X(n). All of the three properties are important here: We assume that every random variable X(i) in this vector has the same probability distribution and we assume that all variables are independent from each other. Finally we also assume that all variables are continues, which implies that the probability of two random variables having the same value is zero.

2.1 Record Statistics

We are now interested in what I call a running record: We call position i a record whenever the value of the random variable X(i) is larger than all previous variables X(j) with j < i. With the weather data, we are interested in the number of records of the vector of average temperatures.

Given a sequence of values X(1)…X(n) we can now define a simple statistics as the number of observed records within that sequence and then we can ask if the value of that statistics looks plausible if we assume that all X(i) are IID. This assumption corresponds to the null hypothesis of “no climate change” in the temperature series.

Translating the question into a mathematical language, we would like to calculate (or at least estimate) the probability of having k records in a sequence of n IID random variables. This question immediately brings us to random permutations.

2.2 Random Permutations

Instead of answering the original question let’s transform it to an equivalent problem. To do so, we first assign the rank of the value to each random variable, i.e. the smallest random variable is assigned a rank of 1, the second smallest variable a rank of 2 and so on until the largest random variable is assigned the rank n. Let’s denote the rank of X(i) with R(i).

The following example gives us the ranks R(i) for the small sequence X(i) with 7 elements.

value X(i): 0.2 2.3 1.1 0.7 2.4 1.8 2.1

rank R(i): 1 6 3 2 7 4 5

Note that the ranks R(i) still have the same relative order as the original values X(i), which in turn implies that the number of records in the ranks vector is the same as the number of records in the original values.

The set of all ranks R(1)…R(n) contains all the number from 1 until n, but in a random order. Assuming that the original values X(i) are IID, the vector of ranks R(i) is a random permutation. Therefore the probability distribution of the number of records in a sequence of IID random variables is the same as of the number of records in a random permutation.

2.3 Monte Carlo Sampling

Unfortunately, it turns out that the distribution we are looking for (the number of records in a random permutation) is not exactly simple to calculate. There are some formulas (see On the outstanding elements of permutations by Herbert Wilf), but these build on the unsigned Stirling numbers of the first kind which are not easy to calculate.

Instead of trying to evaluate some mathematical formula for the probability distribution we chose a different route: We simply let the computer conduct a reasonable huge number of random experiments by sampling random permutations, and in each experiment we calculate the statistics we are interested in. The histogram (i.e. the relative frequency of each possible outcome) then gives us an estimation of the true probability. This approach is called Monte Carlo Sampling and by the law of large numbers, the results will always converge to the real values.

3. Implementation

Now we have gathered all the required theory for implementing the statistical test if the number of running maximums/minimums in the temperature chart below is plausible under the null hypothesis that the climate doesn’t change (i.e. the probability distribution of the average temperature doesn’t change over time).",https://towardsdatascience.com/using-permutation-tests-to-proof-the-climate-change-2bc34d614eb7,['Kaya Kupferschmidt'],2020-12-23 18:14:27.792000+00:00,829,"Climate Change, Temperature Distribution, Random Permutations, Monte Carlo Sampling, Running Records"
Determining Popularity of Rising Pop Artists with Scraped Spotify Data and NLP Sentiment Analysis,"Executive Summary (image produced by author James Pecore)

Problem Statement:

Spotify uses its popularity parameter in order to rank songs, albums, and artists. This “popularity” metric is based on how often users stream songs from Spotify. But this metric only shows how popular very recent artists are in general (not popularity according to genre or popularity by song/lyrical content). As a result, historically VERY popular classic songs are overlooked. Additionally, artists who are VERY popular in their genre become ignored due to higher weight artists from higher popularity genres like “pop.” We need a new metric for popularity. In fact, we need more than one.

The following questions will help us re-evaluate Spotify’s stream-popularity metric in greater context of the data:

1. What can we say about a song’s popularity based on aspects of the music itself: like danceability, energy, and acousticness?

2. What can we say about a song’s popularity based on the content of an artist’s lyrics — the verbal connotations and vibe of the poetry?

3. How do each of these factors influence our ability to predict the popularity of an artist or song?

4. Finally, when using Regression modeling, Classification modeling, and NLP Clustering to predict the popularity of a musical artist, how can evaluate whether or not to trust Spotify’s ranking of popularity?

Executive Summary:

I created two different datasets with the APIs Spotipy and Genius. I also used a Kaggle dataset by Zaheen Hamidani to augment the size of my data.

Next, I build a wide variety of Regression Models for the dataset of around 150,000 songs. These models try to accurately predict a song’s “stream-popularity” based off of the song’s musical attributes (like energy, valence, modality, time signature, and other characteristics). I also use many different Classification Models to measure whether we can predict that a song is popular (above 75% popularity on a scale of 0 to 100) based off of these same song attributes.

For Lyric Attributes, I use the shorter list of playlist songs (just 700 songs) from Spotify as a basis for which lyrics to scrape. I scrape the lyrics for each of these songs off of Genius’ lyric library. I use sentiment analysis and NLP (CountVectorizer) to perform EDA on the most common words/sentiments for each song. Finally, I try to evaluate whether there is a correlation between most common words and song sentiment with its popularity.

Explanatory Data Analysis

Popularity Distribution of 150,000 Spotify Songs, image produced by author James Pecore

Correlation of Song Attributes with Stream-Popularity, image produced by author James Pecore

As data scientists, we should be surprised that one can use “Loudness” to accurately predict a Spotify song’s “stream-popularity” so accurately. Why is this?

Well, “stream-popularity” tends to favor more recently produced music (as current music is streamed more often and thus more “stream-popular” than older music).

Image from Music Tech Student (Itsaam), link provided in Works Cited

Contemporary music (2007 and onward) sounds louder when streamed due to the history of musical compression. Because late 2000s digital music innovations allowed for the music to be less compressed, modern music in its digital form is merely perceived as louder than digitized compressions of earlier years.

My point — loudness doesn’t make your music more popular at a certain point. If it did, “Heavy Metal” would be all of our favorite genres.

Acousticness, however, does seem to impact a song’s popularity. As the infographic below details, more popular songs generally have less elements of acoustic music and more elements of digital music. Given recent trends in pop music towards becoming more digitally produced in DAWs like Logic, Pro Tools, FL Studio, and Ableton, this data makes sense.

Correlation of Song Attribute (Acousticness) with Stream-Popularity, image produced by author James Pecore

Regression Modeling:

Lyrical Analysis:

Sentiment analysis is the process of creating binaries of words in order to determine whether a body of text is closer to one pole or another. For instance, I create a binary of “Love”-related words versus “Heartbreak”-related words. Then, I vectorize each word in each song’s lyrics using CountVectorizer. This converts the words into numerical vectors that can then be clustered based on similarity of words.

Finally, I create a metric that normalizes Sentiment Analysis for a song’s lyrics as either closer to +1 for “Love” songs OR as closer to -1 for “Heartbreak” songs. I can then use this lyrical metric (among other Sentiment Analysis binaries) as a feature for modeling.

Classification Modeling:

Clustering Analysis:

Recommendations

General Recommendation to Song Writers:

Increase Energy and Danceability to be around average values (60%)

Decrease Acousticness and use digital instruments / music production

Only increase Loudness to make it easy to listen to on a mobile phone

If you mention “love” more in your song, it can’t hurt

Recommendation 1: All-Time Stream Popularity

Create a new popularity metric based on:

“Total Number of Streams of All Time”

This will let us grade older songs comparably with newer songs

We could compare historical trends in music with current trends without improper scaling worries from Stream Popularity

Recommendation 2: Personal Popularities

Bring back a 5-Star or “One-to-Ten” review system for each user’s songs

This will let us assess what styles each individual user prefers

This will allow us to create a Regression Model and Recommender System for the user for their highest rated songs, improving user turnout

Recommendation 3: Song Features Review

Create an optional Features Review section for each song in Spotify

Vectorize the words used in Features Review

Create Sentiment Analyses with these Vectors

Create a recommender system with these Vectorized Sentiments

Recommendation 4: Individual Research

Artists with educational backgrounds in Music like Charlie Puth, Lizzo, and Lady Gaga have degrees in music from established music universities like Berklee, MSM, NYU, and University of Houston

Research should be done individually at a certain point on who to promote after you’ve narrowed down artists to your “Top Five”

Further Research and Future Projects

Using Parallel Programming (AWS) not Serial Programming (Jupyter)

- Processing all 150,000 song lyrics

- Extending NLP Performing Sentiment Analysis on all 150,000 song lyrics

- Performing NLP Clustering with SpaCy on all 150,000 song lyrics Using Public Opinion on Pop Songs for Sentiment Analysis

- Scraping News/Twitter/Reddit/Tumblr/etc. Posts for All Songs

- Using NLP to Determine if Public Opinion Towards Artist is -, 0, or + Using Song Attributes & Reviews to Create a Recommender System

- Publish online or submit to Record Labels / Streaming Companies

Works Cited",https://medium.com/analytics-vidhya/determining-popularity-of-rising-pop-artists-with-scraped-spotify-data-and-nlp-sentiment-analysis-f62aa182f5fe,['James Pecore'],2020-10-25 16:40:12.984000+00:00,1020,"Spotify, Popularity Metric, Stream-Popularity, Regression Modeling, Classification Modeling"
The Most Awesome Paradox Ever (Part I),"Impossible to solve? Photo by Steve Johnson on Unsplash

After a lot of research and programming, your work is finally done: you built an Artificial Superintelligence. Just like Skynet from the Terminator franchise, your ASI starts to “learn at a geometric rate”. In particular, you notice how good it is at understanding human psychology: it predicts human decisions so accurately, it becomes creepy.

On a sunny Saturday, your ASI decides to play a game with you. It places two boxes (box A and box B) on the table in front of you. Box A is opened and you see a thousand dollars inside. Box B, however, is closed, so you don’t know what’s in it. The ASI gives you a choice: you can either get box B only, or get both box A and box B. Then it tells you the twist: it has made a prediction about the choice you will make. If the ASI predicted you pick box B only, it will contain a million dollars. If on the other hand the ASI predicted you will pick both boxes, box B will contain nothing. You know your ASI to be an extremely good predictor. What will you do?

The dilemma above is called Newcomb’s Paradox, after its creator William Newcomb. As it turns out, there is a so-called dominant strategy, and it is picking both boxes. You see, there is either a million or zero dollars in box B. If box B contains a million dollars, choosing only box B will get you a million dollars, but picking both boxes will give you a million plus a thousand dollars. So in this case, picking both boxes wins. If box B contains zero dollars, picking both boxes will still get you a thousand dollars more than picking only box B, because box A always contains a thousand dollars. Picking both boxes wins again! So, it’s an easy decision, right?

Well, no. You see, while the above analysis is perfectly logical, there is another strategy that seems equally valid: assuming the ASI’s prediction is correct (which it in all probability is), there are only two outcomes to this game: either you choose box B and John predicted you would, or you choose box A and B and, again, John predicted you would. Looking at it like this, you either get a million dollars (by picking only box B) or a thousand dollars (by picking both boxes). Clearly, picking only box B wins in this analysis.

It seems there are two perfectly valid but opposing strategies. Discuss your strategy in the comments!",https://medium.com/datadriveninvestor/the-most-awesome-paradox-ever-part-i-ad17a3135cb9,['Hein De Haan'],2018-11-27 15:03:44.094000+00:00,426,"Newcomb's Paradox, AI, Artificial Superintelligence, Dominant Strategy, William Newcomb"
Take Full Control of AI Notebook Instance on GCP via Putty,"access GCP VM instance via Putty for cloud computing

AI notebook instance on GCP runs on a VM instance. If we can access the VM instance via Putty, we can then have full control of the VM instance through Linux commands. In this post, I am going to walk through how to set up access to the AI notebook instance on GCP via Putty.

1. Start the VM instance.

One way to start an AI notebook instance is to use the GCP cloud console. The AI notebook instance runs on a VM instance with the same name. Another way is to use “gcloud SDK,” which I manage from ‘Windows Terminal.’ (My other post on why you shall use ‘windows Terminal’). The command to start the VM instance is:

gcloud compute instances start INSTANCE-NAME

2. Add an external IP address to the VM instance on GCP.

From the Navigation Menu in the GCP cloud console, you can navigate to the Compute Engine (GCE) list as below:

Click the three-dot icon on the most right side of the selected GCE VM instance, select ‘view network details’ to navigate to the following VPC network configuration screen:

We will select the ‘External IP addresses’ tab. There are two possible Types: Static and Ephemeral. For VM instances, static external IP addresses remain the same until they are removed. The Ephemeral external IP addresses will change every time when you start the instance. We will set the external IP address as ‘Static’ as we need to use the same information to login in the future.

3. Add the ssh key to the VM instance on GCP.

First, we generate the ssh key pair (public key and private key) using PuTTYgen.

The ‘key comment’ part will be the user name and ‘Key passphrase’ will serve as the password for later logins. We shall save the public key and private key. The public key file name follows the format of starting with ‘ssh-rsa’ and ending with the key comment which serves as the user name.

Once we generate the ssh key pair, we can go to the VM instance detail in the console, click the ‘EDIT’ button, and scroll down to the ‘SSH Keys’ section as the screenshot below:

(SSH Keys for GCE VM instances)

Expand the ‘Show and edit’ option to add the ssh public key to the VM instance.

4. Configure Putty to connect to the VM instance on GCP

Start program ‘PuTTY’ and select the Session category at the left side of the panel as below:

In the Host Name (or IP address) box, we use the Static external IP address added to the VM instance in step 2. To avoid type the user name every time, we add ‘user_name@’ before the IP address. We also save this information so that we can just load the stored session later to save the work of typing the configuration settings.

Next, we go to the Category/Connection/SSH/Auth to load the private key file.

Last, we specify the ports to be used for AI notebook instances at the ‘Category/Connection/SSH/Auth/Tunnels.’ As we know, the Jupyter notebook on the local machine will use port 8888. To avoid port conflicts, we use port=7080 (non-8888 port) for our PuTTY remote connection. By clicking the ‘Add’ button, port=7080 will be shown in the “Forwarded ports” box. We can now go back to the session part and save the updated settings. Once clicking the ‘Open’ button, we will navigate to the PuTTY login screen.

5. Start AI notebook from localhost.

Once we log into the VM instance via PuTTY, we can activate the AI notebook runtime environment and start Jupyter Notebook or Jupyterlab.

conda activate base

cd /

jupyter notebook --port=7080

The base environment is the one used by the AI notebook instance. we can create new environments in addition to the base environment as needed. Command-line ‘cd /’ navigates us to the root directory. Remember to add the part “- -port=7080” when we start the Jupyter notebook.

Now, let’s open a browser and type the address ‘localhost:7080’ to start to use the Jupyter Notebook or ‘localhost:7080/lab’ to start to use JupyterLab.",https://medium.com/analytics-vidhya/take-full-control-of-ai-notebook-instance-on-gcp-via-putty-814ae0e11873,['Phillip Peng'],2020-12-22 16:31:57.527000+00:00,661,"Cloud Computing, GCP, Putty, VMInstances, AINotebook"
A Morning Routine,"He awoke from a terrible dream

Skin flecked away like paint chips

Sinking between the floorboards

Ruby red kisses left behind.

Brushing his teeth mostly

To ensure they’re still

Inside his head where he saw them last.

In his bed

The Lament for Icarus has begun

So wide and kind

The feeling of lift

Remembrance

A guest forgotten

Sunk in the night’s wake

And he knows he will have to take him

Uptown and maybe east

They shared a bed, after all

Reciprocity for holy men

He looks up at the ceiling wondering

If his mother can see what he has done or

Was her vision blocked

By the neighbors and the noise upstairs.",https://medium.com/@jacobwrosen/he-awoke-from-a-terrible-dream-e3e1abc97b08,['Jacob Rosen'],2020-12-18 16:47:17.251000+00:00,93,"Dreams, Nightmares, Flecks, Paint Chips, Icarus"
Parallax Images,"Now, after we have loaded the depth map, we can create masks for different layers by thresholding the depth map at different Thresholds.

While making one layer we need two masks, one of this layer and the second of the previous layer to inpaint the missing parts. We will take the last layer outside the loop so we can extract all the remaining parts in this layer.



layers = []

prev_thres = 255

div=30



for thres in range(255 - div, 0, -div):

ret, mask = cv2.threshold(depth_map, thres, 255, cv2.THRESH_BINARY)



ret, prev_mask = cv2.threshold(depth_map, prev_thres, 255, cv2.THRESH_BINARY)



prev_thres = thres

inpaint_img = cv2.inpaint(img, prev_mask, 10, cv2.INPAINT_NS)

layer = cv2.bitwise_and(inpaint_img, inpaint_img, mask = mask) layers.append(conv_cv_alpha(layer, mask))



# adding last layer



mask = np.zeros(depth_map.shape, np.uint8)

mask[:,:] = 255



ret, prev_mask = cv2.threshold(depth_map, prev_thres, 255, cv2.THRESH_BINARY)



inpaint_img = cv2.inpaint(img, prev_mask, 10, cv2.INPAINT_NS) layer = cv2.bitwise_and(inpaint_img, inpaint_img, mask = mask) layers.append(conv_cv_alpha(layer, mask))



layers = layers[::-1]

We have reversed the layers so we can arrange them in order of last to the first layer. While we are adding the layer to the list, we are using a function ‘conv_cv_alpha’ this will add the alpha value (make RGB to RGBA) and make parts of the layer transparent using the mask.

def conv_cv_alpha(cv_image, mask):

b, g, r = cv2.split(cv_image)

rgba = [r, g, b, mask]

cv_image = cv2.merge(rgba,4)



return cv_image

Now comes the part of face detection and showing the image. For face detection, we will use haarcascade. download them from their official Github repository.

To download them, right-click “Raw” => “Save link as”. Make sure they are in your working directory.

Now we will load haar cascade for face detection and make a function that will return the face-rect from the image.

face_cascade = cv2.CascadeClassifier( 'haarcascade_frontalface_default.xml')



def get_face_rect(img):

gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

face_rects = face_cascade.detectMultiScale(gray_img, 1.3, 5)

if len(face_rects) == 0:

return () return face_rects[0]

Now we have to show the image that will shift according to the user’s head. We will use OpenCV to read cam and then Pygame to render each frame on top of each other. To calculate the shift for each layer, we will calculate the shift of head from the center of the frame and then scale it down to get a small shift value. After that, we will multiply the index value of each layer to get the shift value for the respective layer, you can also multiply some constant value in that for better results.

We will create a Pygame window slightly smaller than the original image and load the camera. We have used scale, so you change its value to make the final result bigger.

scale = 1

off_set = 20 width, height = layers[0].get_width(), layers[0].get_height() win = pg.display.set_mode((int((width - off_set)*scale), int((height - off_set)*scale)))

pg.display.set_caption('Parallax_image') scaled_layers = []

for layer in layers:

scaled_layers.append(pg.transform.scale(layer, (int(width*scale), int(height*scale)))) cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)

We will set some constants. you can play with these constants to get different results.

x_transform = True # allow shift in x-axis

y_transform = False # allow shift in y-axis

sens = 50 # the amount of scale down of shift value

show_cam = False # show your face cam

shift_x = 0

shift_y = 0

run = True

Finally, the main loop to render all layers.

while run:

for event in pg.event.get():

if event.type==pg.QUIT:

run = False ret, frame = cap.read()

frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

initial_pos = (frame.shape[0]/2, frame.shape[1]/2)

face_rect = get_face_rect(frame) if len(face_rect) != 0:

x,y,w,h, = face_rect

face_rect_frame = cv2.rectangle(frame, (x, y), (x + w, y + h), (255,255,0), 3) shift_x = (initial_pos[0] - (x + w/2))/(sens*scale)

shift_y = (initial_pos[1] - (y + h/2))/(sens*scale) win.fill((255, 255, 255))



for i, layer in enumerate(scaled_layers):

new_x = -off_set/2

new_y = -off_set/2

if x_transform:

new_x = 0 + shift_x*i

if y_transform:

new_y = 0 + shift_y*i

win.blit(layer, (new_x, new_y))



face_rect_frame = cv2.resize(face_rect_frame, (100, 100))

if show_cam:

win.blit(conv_cv_pygame(face_rect_frame), (0, 0))

pg.display.update() cap.release()

cv2.destroyAllWindows()

pg.quit()

There it is, the final result.

Final Result

demo for different image

I have created a more advanced version of this tool where you can just choose the image and it will automatically create the parallax image, the depth map will be automatically generated.

You can check more on my GitHub repo.",https://medium.com/analytics-vidhya/parallax-images-14e92ebb1bae,[],2020-11-29 07:14:28.577000+00:00,619,"layers, masks, thresholding, depth map, inpaint"
"Sharing our data, Improving our cities","Sharing our data, Improving our cities

Via cnet.com

“Code is the new concrete for 21st century cities and we need a digital infrastructure to share data and create safer and more sustainable streets.” -Janette Sadik-Khan, former commissioner of the New York City Department of Transportation and an advisor on transportation and urban issues.

Ever since the advent of rideshare services, I’ve felt that it’s improved my life. I don’t have to feel awkward getting wave-rejected as an occupied taxi whizzes past me and I can avoid shady taxi drivers from taking me on the longest route possible. Ridesharing services have done me a great service and I think it’s fair to say that they’re doing very well for themselves too. But are companies doing any good for the world? Just last year, Uber, Ford and Lyft have partnered up with a company called SharedStreets to give city mayors across the world “unparalleled access to their road traffic data” in order to help cities rethink transportation.

Cities are trying to improve urban mobility but are unable to because their street maps are inadequate to deal with the super surge of cars taking up limited space. One very valuable space is the curb. People get dropped off there, bicyclists use it, cars park on it, and trucks unload there. It’s a crucial pain point not only for pedestrians and drivers who deal with constant delays from people blocking traffic but also for many businesses because freight and logistics companies are using the curb more and more and they have no way to collaborate well with the city. Many cities don’t have their infrastructures mapped out well and even more of a problem is that they don’t have enough data to make any actionable changes.

Ridesharing companies have a huge amount of data that cities could use to make traffic efficient and safer. For this cities-companies collaboration, Uber and Lyft provided a substantial amount of trip information including fare amount, vehicle speed data, pick up, and drop off data. However, even if these companies provided data to the cities, their street level data sets are still not compatible. Not only are city mapping systems different each other (the city of Detroit has cities within that all have their own way of managing their infrastructure), they’re also different from ridesharing companies.

SharedStreets makes it possible by creating an open-source, universal map language in a simple machine readable format that everyone — the city, companies and citizens — all have access to. And because SharedStreets is a third party, non-political, and non profit, Uber and Lyft do not need to fear risking proprietary secrets such as routing algorithms and SharedStreets removes thecompany name from the trips. Regarding our privacy, SharedStreets uses data in a way that is completely unrelated to the privacy of people. This allows all data to work in everyone’s favor without jeopardizing anyone’s interest.

via SharedStreets

Both cities and companies have been using a Geometric Information System(GIS) to map street level data. However, when data is merged together, the sets do not match. SharedStreets is able to link the two sets of data together by breaking down a street into short form IDs that can be referred back to in a database.

via SharedStreets

This is important because when it links up different maps’ data sets, the SharedStreets IDs provide a common reference. The grey lines represent different data sets of the same spot and the blue line represents the SharedStreet ID reference that allow a common ground.

via SharedStreets

via SharedStreets

The millions of GPS points are clustered together and allows analysis of traffic congestion, curb usage, how long cars stay in a spot, how often vehicles are picking up and dropping. With a common map and a huge amount of data, cities now have the resources to create a roadmap for urban mobility.

My thoughts

This has been one of the largest collaborations with private companies and the government in what seems very much like an active attempt from all sides to do good. I hope that these companies involved have set an example as a way of providing social good with data, especially for the ones that have been emerging as abusers of our trust and privacy.

Sources:

https://nacto.org/2018/02/22/nacto-and-otp-launch-sharedstreets/",https://medium.com/@chrispfchung/sharing-our-data-improving-our-cities-2760ad353d05,['Chris Chung'],2018-11-24 06:09:31.479000+00:00,693,"Data Sharing, Urban Mobility, Geometric Information System, GPS Points, Curb Usage"
What's New in Python 3.9,"Python’s New Path

There are two significant changes in this update, which we won’t see any immediate impact from — but we will begin to notice a slightly different evolution of Python as a language.

In short, this boils down to:

Python’s parser limitations

Smaller, but more frequent releases

LL(1) and PEG

Around 30 years ago, Guido van Rossum wrote pgen. One of the first pieces of code ever written for Python — it is still in use as Python’s parser to this day [1].

Pgen uses a variant of LL(1)-based grammar. This means our parser reads the code top-down, left-to-right, with a lookahead of just one token.

This essentially means that Python development is limited, because:

The lookahead of one token limits the expressiveness of grammar rules.

Python already contains non-LL(1) grammar, meaning the current parser uses a lot of workarounds, overcomplicating the process.

of workarounds, overcomplicating the process. Even with these workarounds, only so much is possible. The rules can be bent, but not broken.

With LL(1), particular left-recursive syntax can cause an infinite loop in the parse tree, causing stack overflow — as explained by Guido van Rossum here.

These attributes of the LL(1)-based parser limit what is possible in Python.

Python 3.9 has broken from these limitations thanks to a shiny new PEG parser, outlined in PEP 617.

Immediately, we won’t notice this. No changes taking advantage of the new parser will be made before Python 3.10. But after that, the language will have been released from it’s LL(1) shackles.

Development Cycles",https://towardsdatascience.com/python-3-9-9c2ce1332eb4,['James Briggs'],2020-10-05 21:11:46.012000+00:00,240,"Python, Parser Limitations, LL(1), PEG"
How to Build a Reporting Dashboard using Dash and Plotly,"A method to select either a condensed data table or the complete data table.

One of the features that I wanted for the data table was the ability to show a “condensed” version of the table as well as the complete data table. Therefore, I included a radio button in the layouts.py file to select which version of the table to present:

Code Block 17: Radio Button in layouts.py

The callback for this functionality takes input from the radio button and outputs the columns to render in the data table:

Code Block 18: Callback for Radio Button in layouts.py File

This callback is a little bit more complicated since I am adding columns for conditional formatting (which I will go into below). Essentially, just as the callback below is changing the data presented in the data table based upon the dates selected using the callback statement, Output('datatable-paid-search', 'data' , this callback is changing the columns presented in the data table based upon the radio button selection using the callback statement, Output('datatable-paid-search', 'columns' .

Conditionally Color-Code Different Data Table cells

One of the features which the stakeholders wanted for the data table was the ability to have certain numbers or cells in the data table to be highlighted based upon a metric’s value; red for negative numbers for instance. However, conditional formatting of data table cells has three main issues.

There is lack of formatting functionality in Dash Data Tables at this time.

If a number is formatted prior to inclusion in a Dash Data Table (in pandas for instance), then data table functionality such as sorting and filtering does not work properly.

There is a bug in the Dash data table code in which conditional formatting does not work properly.

I ended up formatting the numbers in the data table in pandas despite the above limitations. I discovered that conditional formatting in Dash does not work properly for formatted numbers (numbers with commas, dollar signs, percent signs, etc.). Indeed, I found out that there is a bug with the method described in the Conditional Formatting — Highlighting Cells section of the Dash Data Table User Guide:

Code Block 19: Conditional Formatting — Highlighting Cells

The cell for New York City temperature shows up as green even though the value is less than 3.9.* I’ve tested this in other scenarios and it seems like the conditional formatting for numbers only uses the integer part of the condition (“3” but not “3.9”). The filter for Temperature used for conditional formatting somehow truncates the significant digits and only considers the integer part of a number. I posted to the Dash community forum about this bug, and it has since been fixed in a recent version of Dash.

*This has since been corrected in the Dash Documentation.

Conditional Formatting of Cells using Doppelganger Columns

Due to the above limitations with conditional formatting of cells, I came up with an alternative method in which I add “doppelganger” columns to both the pandas data frame and Dash data table. These doppelganger columns had either the value of the original column, or the value of the original column multiplied by 100 (to overcome the bug when the decimal portion of a value is not considered by conditional filtering). Then, the doppelganger columns can be added to the data table but are hidden from view with the following statements:

Code Block 20: Adding Doppelganger Columns

Then, the conditional cell formatting can be implemented using the following syntax:

Code Block 21: Conditional Cell Formatting

Essentially, the filter is applied on the “doppelganger” column, Revenue_YoY_percent_conditional (filtering cells in which the value is less than 0). However, the formatting is applied on the corresponding “real” column, Revenue YoY (%) . One can imagine other usages for this method of conditional formatting; for instance, highlighting outlier values.

The complete statement for the data table is below (with conditional formatting for odd and even rows, as well highlighting cells that are above a certain threshold using the doppelganger method):

Code Block 22: Data Table with Conditional Formatting

I describe the method to update the graphs using the selected rows in the data table below.",https://medium.com/p/4f4257c18a7f#906f,['David Comfort'],2019-03-13 14:21:44.055000+00:00,668,"Dash Data Table, Conditional Formatting, Doppelganger Columns, Radio Buttons, Output Callback"
Learning to Rank using XGBoost. sci-kit learn and Pandas,"When ranking, the aim is not to accurately predict the final order. Instead you are essentially trying to find what data points are relevant in the current query, and which are not. The target for Learning to Rank is a relevance score, which tells you how relevant the data point is in the current group.

In the case of horse racing the only relevant horse is the winner, the runner up can be somewhat relevant, depending on the margin to the winner. The horse which places 10:th is just as irrelevant as the one who places 11:th.

Modelling

Now that we have our training and test data, we need to pass it to the model. When performing Learning to Rank we must pass another key word argument to the model, our group. This arguments takes an array of the sizes of the groups in the training data.

If you have two groups in your training data, one with 10 instances and one with 7, the groups array should simply be [10, 7]. It is imperative that the training data is still sorted on the query id.

Lets add the code to get the group array for training.

Now we are ready to fit our model, to build a ranking model, we use the XGBRanker module of the xgboost package.

Model fit using XGBRanker

Predicting

With the model trained using the training data, we are ready to make predictions on the test set. Usually we would use the models predict function on the entire test set, but the predict method does not take a group argument. So we need to do our predictions once per group.

This snippet is how I solved this issue

And we have our prediction. I’ve written a small function which decorates the prediction with the horses start number. Here is a set of lists with each horses start number, in descending order according to the predicted rank. Each list corresponds to a race for V64 at Solvalla 2021–02–23. Here I am only showing the top 50%.

1 [1, 6, 9, 8, 4]

2 [3, 8, 2, 4, 11, 10]

3 [2, 6, 8, 5]

4 [10, 11, 8, 6]

5 [6, 10, 4, 2]

6 [3, 4, 5, 11]

And the actual results:

V64 at Solvalla 2021–02–23

1 [1*, 6, 9, 8, 4]

2 [3*, 8, 2, 4, 11, 10]

3 [2, 6*, 8, 5]

4 [10, 11, 8, 6*]

5 [6, 10*, 4, 2]

6 [3*, 4, 5, 11]

The model is able to pick the top horse in 3/6 races and top two in 5/6!",https://medium.com/predictly-on-tech/learning-to-rank-using-xgboost-83de0166229d,['Simon Lind'],2021-03-11 13:47:04.690000+00:00,405,"Learning To Rank, XGBRanker, Modelling, Predicting, Relevance Score"
Exploratory Visualization for Data with Categorical Variables,"Exploratory data visualization allows us to get an idea of the data, before starting any modeling. Usually scatter plot is a good choice to visualize data with numerical features which allows us to see relationships/ patterns within the data.

The challenge starts when the data set includes Categorical variables (e.g., Country, Gender, Race). How do we visualize such a data set to understand patterns?

The answer lies in embeddings, which is a vector representation of textual data. We can use Embedding layer in keras or gensim Word2Vec module to get the embeddings. In this article, I have visualized a set of multi dimensional categorical data using Altair library in Jupyter notebook.

The data set can be downloaded from UCI Machine Learning Repository. It was donated by Ron Kohavi and Barry Becker, after being published in the article “Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid”. The data presented below is a cleaned version with the removal of 'fnlwgt' feature and records with missing or ill-formatted entries. Below are first few rows of the data set.

Figure 1: Top rows of data set

There are 8 categorical features in the data set — workclass, education_level, marital-status, occupation, relationship, race, sex, native-country. In order to create training data for Word2Vec, I created a new column, which holds an array of all categorical values for the record. For example, the new column for row 0 above would hold [‘State-gov’, ‘Bachelors’, ‘Never-married’, ‘Adm-clerical’, ‘Not-in-family’, ‘White’, ‘Male’, ‘United-States’] . Then I created a 32 dimension categorical embeddings, by passing the values of newly created column as training data to Word2Vec model. I appended the numerical features to the embeddings and then reduced the dimensions using t-SNE. All code and instructions can be accessed in my GitHub repository.

After performing the steps mentioned above, I was able to visualize the data set with 8 categorical variables in a scatter plot. By hovering over a data point, the values for different features appear as a tooltip.

Figure 2: Scatter plot of the data set. See interactive version

Further, I visualized the scatter plot along with bar charts for categorical variables. By clicking on the bars, data in the scatter plot is updated to help in understanding relationships among the variables.

Figure 3: Data points with gender ‘Female’ highlighted in scatter plot. See interactive version

Figure 4: Data points with Marital status ‘Married-civ-spouse’ highlighted in scatter plot. See interactive version

Figure 5: Data points with Education Level ‘Bachelors’, ‘Masters’, or ‘Doctorate’ highlighted in scatter plot. See interactive version

I created a dashboard to filter data points in scatter plot by all variables and another dashboard where users can select data points in the scatter plot and bar charts for all variables get updated according to selection.",https://medium.com/analytics-vidhya/exploratory-data-visualization-with-categorical-features-c3c372987421,['Shahzeb Akhtar'],2020-07-31 02:26:09.815000+00:00,448,"exploratory-data-visualization, scatter-plot, categorical-variables, embeddings, word2vec"
How to get started with Data Science in 2020?,"When one wants to start learning Data Science, the first thing that comes to one’s mind is how to get started? This blog addresses this very question and as the saying goes,

A good start is half done.

One can have the following questions in their head when starting to learn Data Science:

Do I need to learn to code?

How much mathematics should I learn?

Which language to choose — Python or R?

This blog will help you with answering these questions and get started with Data Science.

Overview of Data Science

1. Choose a Language and stick to it

A difficult question which one faces in getting hands-on is which language should you choose?

This would probably be the most asked question by beginners.

The gist is that you start with the simplest of languages or the one with which you are most familiar with. If you are not as well versed with coding, you should prefer GUI based tools for now. Then as you get a grasp on the concepts, you can get your hands-on with the coding part.

Python and R are both great choices as programming languages for data science. R tends to be more popular in academia, and Python tends to be more popular in the industry, but both languages have a wealth of packages that support the data science workflow. We generally recommend Python.

You don’t need to learn both Python and R to get started. Instead, you should focus on learning one language and its ecosystem of data science packages. If you’ve chosen Python (our recommendation), you may want to consider installing the Anaconda distribution because it simplifies the process of package installation and management on Windows, OSX, and Linux.

Google Colab is a great resource for beginners to start coding in Python. Besides delivering us from all the installation blues, it provides great computing power and that too for free.

2. Maths

Let me put this in the most direct way possible: no matter how much time and effort you devote to it, you can never know enough math to read through all the Data Science literature. Different parts of Data Science use a variety of esoteric math.

Our advice is to do it the other way around (top-down approach), learn how to code, learn how to use Python (Pandas, sklearn, Keras, etc..), get your hands dirty building real-world projects. A beginner’s way to learn math for Data Science is to learn by “doing stuff.” So we’re going to tackle statistics, linear algebra or calculus by using them in real algorithms!

THEN, you’ll start to see the bigger picture, noticing your lack of theoretical background, to actually understand how those algorithms work, at that moment, studying math will make much more sense to you!\

3. Join a peer group

Why is this important? This is because a peer group keeps you motivated. Taking up a new field may seem a bit daunting when you do it alone, but when you have friends who are alongside you, the task seems a bit easier.

The most preferable way to be in a peer group is to have a group of people you can physically interact with. Otherwise, you can either have a bunch of people over the internet who share similar goals, such as joining a course and interacting with the batchmates.

4. Take up a Course and Complete it

There are hundreds of courses out there which make things even worse as one finds it very difficult to choose the best one.

NEVER dive into a course simply because of the fancy and catchy titles. The main objective should be whether the course clears your basics and brings you to a suitable level, from which you can push on further. Once you’ve shortlisted a few courses that suit your needs, check out their respective reviews (very important!) by others before you pull out your wallet and get enrolled.

When you take up a course, go through it actively. Follow the coursework, assignments and all the discussions happening around the course. Now you have to diligently follow all the course material provided in the course. This also means the assignments in the course, which are as important as listening to the lectures. Only doing a course end to end will give you a clearer picture of the field.

Hope you find it useful.

If this blog helped you in any way, then do Follow and Clap👏, because your encouragement catalyzes inspiration for and help to create more cool stuff like this.",https://medium.com/international-school-of-ai-data-science/how-to-get-started-with-data-science-in-2020-56aeeeb90401,['S Satya Venkatesh'],2020-01-08 05:20:01.391000+00:00,731,"data science, learning data science, python, R language, maths for data science"
Introduction to Integration For Machine Learning,"Working in machine learning we some need to use integration to do our work more easily. Mainly integrals are the inverses of derivatives. More importantly, using integration provides a way to compute the area under the curve of almost any function. There are many applications for integration. For example, if you need to compute a probability of some occurrence between limits (which we’ll discuss later in this course), then you will use an integral.

Let’s start with a simple function:

f(x)=x

We can plot this function as a line. Run the code below to plot the function for the range 0 to 10:

The output of this code will look like this :

Plotting the 0–10 points

Performing Integration

The integral of a function is the area under it — in this case, the area under the purple diagonal line down to the x-axis.

So how do you find the integral of a function? well, for our simple function f(x)=x, the formula for an integral is written as follows:

The ∫ symbol shows that this formula is integral. The dx indicates that the integration is with respect to the x variable. Note that since f(x)=x, we could also write this integral formula as ∫xdx

So, what is the integral of x dx? To answer this question, we need the antiderivative off — in other words we need to find a function that has a derivative matching the output off, which is just x. Using the power rule in reverse, a function that has the derivative x would be :

So, the unbound integral formula for f with respect to x can be written as:

Integration between Limits

Now that we have the unbound integral formula, we can use it to find the integral between specific start and endpoints. Let’s suppose we want to find the area under the function between the x values 0 and 2. In other words, the integral of f for the range 0 to 2 with respect to x.

Run the following code to re-plot the function and show the area we’re interested in:

The integral area is orange

We call the start and end point the limits of the integral. The lower limit is placed as a subscript of the integral sign. The upper limit is placed as a superscript of the integral sign. Using this notation the integral of f(x) from 0 to 2 is written as follows:

The integral is evaluated by subtracting the value of the integrand at the lower limit from the integrand at the upper limit; and since we know the formula based on our antiderivative function, the integral can be evaluated in the following manner.

Execute the code in the cell below and verify that the result returned by the



scipy.integrate.quad

function in Python is approximately the same as we computed analytically.

Another Integral

Here is another example for a slightly more complex function. What is the area under the curve of the function 3x^2+2x+1 between 0 and 3?

let’s look at that function and the area in question:

Output Graph

We can evaluate this integral just as before, this time using function:

Now, execute the code in the cell below to verify the result:

Note that the result from the scipy.integrate.quad function is approximate — the function actually returns an estimated integral (i in this case) and also a measure of absolute error (e). Run the following code to see what the absolute error was in this case:

The absolute error, in this case, is extremely small (around 4.32 * 10^-13)

Infinite limits

In many cases the limits of an integral can be +/−∞. Perhaps surprisingly, this situation is not really a problem if the function being integrated converges to 0 at the infinite limit.

Here is an example. The function e^−5x→0 as x→∞. Therefore, the integral of this function from some limit to ∞.

This integral can be written as follows:

This integral converges to a small number with a much smaller error estimate.

Here is another example that illustrates why having infinite integration limits is so useful. When computing probabilities it is often necessary to have infinite limits. Don’t worry too much about the details of probability theory. This is covered in a later lesson.

A Normal distribution with zero mean and a standard deviation of 1 has the following density function:

It makes sense that the integral of this probability density function from −∞ to ∞ must be 1.0. In other words the probability of a Normally distributed event ocurring at all possible values must be 1.0.

The code in the cell below computes the following integral:

Execute this code and verify that the result is approximately 1.0

That’s For today . We will more advance tommorow.

Every code in the exapmle is execute in Google Colab .

Github Repo Link : https://github.com/CREVIOS/Python-machine-learning-math/blob/master/Introduction_to_Integration.ipynb

.",https://medium.com/analytics-vidhya/introduction-to-integration-for-machine-learning-9a03372b7749,[],2020-03-30 08:48:02.499000+00:00,767,"machine learning, integration, derivatives, inverse functions, probability"
Call Center Intelligence (CCI) through AI Solutions,"Call Center Intelligence (CCI) through AI Solutions

Over the years many organizations have tried to build a solution in order to anticipate the necessities of their customers and provide solutions to their queries related to products/services. Call centers tend to customer’s inquiries through telephones which can be inbound (e.g. Attending to customer’s queries) and outbound (e.g. Telemarketing).

Everyone would have faced the worst queuing up, possibly pressing your keypad a whopping number of times and listening to softly played music until you talk to an actual live agent.

At the end of all the fuss, we end up repeating all the information again.

Hmm! Frustrating, ain’t it?

Pain points of a Call Center:

These can be discussed in terms of:

Customer Experience

Huge Business Expenditure

Customer Experience:

Deep and Complex IVR (Interactive Voice Response) Tree. Customers Repeating Information more than once Agents searching for information thereby increasing the wait time.

Huge Business Expenditure:

The number of requests to call-center has increased massively in the past decade. The preference of the people has always been voice rather than other services such as chats or emails. Most of them are just routine calls, such as troubleshooting network issues (Internet Service Providers) and Debit card blocking/unblocking complaints when it comes to Banking Sectors. Live agent resources could be cut down if these routine calls are avoided to an extent.

“Focus on the Solution, Not the Problem”. With the advent of Natural Language Processing (NLP), chatbots can easily decipher our intent, emotions, and sentiments based on the way we interact.

AI Powered Assistants: A Solution:

There is ample evidence that Artificial Intelligence simplifies many routine things and daily tasks, changing our lives for the better. AI has been the buzzword around the business circle making it an unavoidable technology to account for. Creating computers that can understand natural language has always been the technology that surrounded homo sapiens’ speculation. The growth in Natural Language Understanding has quenched the thirst of longing.

Natural Language Understanding(NLU) and Natural Language Generation(NLG) are very promising areas of Artificial Intelligence. According to the GlobalNewsWire forecast, the global NLP market accounted to hit a market value of $28.6 billion in 2026. Chat-bots exploit NLU, i.e., in simple words it develops the ability to understand what the user actually says.

Call centers are the best market to implement NLU algorithms, where chatbots could perform routine works and also works as an advisor to the live agents.

Let's check out different places where an AI agent becomes handy.

Source: Virtual Agent (Attends routine topic)

Virtual Agent: Automates most common transactions and passes on complex transactions to live agents. It propagates all the context gathered during calls to the live agent. More interactive, informative, and quicker than IVR.

Agent Assist: Pulls out contextually similar contents from the Knowledge Base and presents it to the Agent, thereby reducing the waiting time.

Conversational Topic Modelling

Conversational Topic Modelling: Discover the topics for which customers reach out to you and how they articulate. This is essential to update the Knowledge Base and produce more improved results in the future. Thereby the system gets better and better.

How to make AI better Agents?

AI can be better agents and the one way it does it is through AI-powered Knowledge bases. In customer service jobs, agents have to quickly search through relevant documents to find a solution to a customer’s problem and this has to be pretty quick. An AI-powered Knowledge base can quickly traverse through the documentation by using key-phrases and deliver this straight to the agent thereby reducing the time. This surely inspires confidence in the brand/product.

So, how to build a powerful Knowledge Base?

The main factor here is to understand what the customer wants, it includes the discovery of the topic on which customer has to be serviced. Each time a customer calls, call logs are collected in order to generate training data for different topics. Any ML algorithm can be used to predict the needs of every user.

Creating a Powerful Knowledge Base

Once the topic is chosen, important keywords and top sentences used by the callers to articulate those topics are collected. ML algorithms along with human-supervised validation make the system more robust.

Whenever an AI agent fails, the stored recording serves as the Knowledge Base thus making the chatbots better over time and much more adaptive to specific business cases.

AI: ALL THE WAY

The world now is adapting more to AI-driven solutions. It is pretty clear that AI cannot take the position of human beings but sure it can assist them thereby increasing their productivity and is a boon for any business.

For all the promotions around AI Chatbots, few companies have embraced it in call center operations. But the rate of acquisition is going to rise up in the following years primarily because of the cost reduction and personalized experience it offers. And sooner chatbots being drafted to other businesses is not too far away.

Let’s democratize the AI! Let’s make AI for everyone possible!

Are you looking out for AI products, AI Services, AI Research, and AI Resourcing? You can get to us!

Website: Federated AI Services

Federated AI (FAI) Services

FAI envisioned to become an enduring structure-preserving map between Business Values and Artificial Intelligence Research.

FAI, emphasized in upskilling and reskilling the Indian workforce to build personalized products from redesigned high-impact AI research and engineering solutions to serve the greater business values to its clients with greater efficiency.",https://medium.com/federatedai/ai-solutions-for-call-centers-4defeef98106,['Akaash B'],2020-12-15 15:12:48.599000+00:00,872,"Artificial Intelligence, Natural Language Processing, Chatbots, AI-Powered Assistants, AI Services"
【資料分析師職涯發展（一）】Senior Data Analyst 和 Junior Data Analyst最大的差別是什麼？,"We aim to inspire and educate data scientists worldwide, regardless of gender, and support everyone in the field.

Follow",https://medium.com/women-in-data-science-taipei/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90%E5%B8%AB%E8%81%B7%E6%B6%AF%E7%99%BC%E5%B1%95-%E4%B8%80-senior-data-analyst-%E5%92%8C-junior-data-analyst%E6%9C%80%E5%A4%A7%E7%9A%84%E5%B7%AE%E5%88%A5%E6%98%AF%E4%BB%80%E9%BA%BC-da49813c380,['Manzih Hong'],2020-10-29 15:49:57.937000+00:00,18,"us on Twitter Data Science, Women In Tech, Gender Equality, Education, Inspiration"
Feature Engineering Examples: Binning Numerical Features,"Feature Engineering Examples: Binning Numerical Features

Feature engineering focuses on using the variables already present in your dataset to create additional features that are (hopefully) better at representing the underlying structure of your data.

For example, your model performance may benefit from binning numerical features. This essentially means dividing continuous or other numerical features into distinct groups. By applying domain knowledge, you may be able to engineer categories and features that better emphasize important trends in your data.

Photo by Paper Beard on Unsplash

In this post, we’ll walk through three different methods for binning numerical features with specific examples using NumPy and Pandas. We’ll engineer features from a dataset with information about voter demographics and participation. I’ve selected 2 numerical variables to work with:

age : a registered voter’s age at the end of the election year birth_year : the year a registered voter was born

If you want to start applying these methods to your own projects, you’ll just need to make sure you have both NumPy and Pandas installed, then import both.

Using np.where() to Indicate Thresholds

It may be odd to think about, but indicating whether a certain threshold is met by each instance (in this case by each registered voter) is a type of binning.

For example, imagine we’re trying to predict whether each registered voter turned out to vote in the election. Maybe we suspect that younger voters will be more likely to turn out if this is the first time they were eligible to vote in a presidential election. Since the legal voting age is 18, anyone less than 22 years of age during the current presidential election would not have been able to vote in the previous presidential election.

We can create an indicator variable for this threshold using np.where() which takes 3 arguments:

a condition what to return if the condition is met what to return if the condition is not met

The following code creates a new feature, first_pres_elec , based on an individual’s age:

df['first_pres_elec'] = np.where(df['age']<22, 1, 0)

The condition we’re checking is whether or not the individual is less than 22 years of age. If they are below that threshold, np.where() returns a 1 because this was the first presidential election in which they were eligible to vote. If not, then 0 is returned. From our continuous variable age , we have created a new binary categorical variable.

Maybe we also have reason to suspect that senior citizens were more or less likely to turnout to vote. If so, we might want to draw our model’s attention to this threshold by creating another threshold indicator:

df['senior'] = np.where(df['age']>=65, 1, 0)

Now we’ve created two threshold indicators that split the distribution of voter age as shown below. Younger individual’s who are newly eligible to vote in a presidential election are highlighted in red and seniors are highlighted in yellow.

Registered voter age distribution with younger and older thresholds highlighted — Image by author

Applying a Custom Function with apply()

It might make sense to divide our registered voters up into generations based on their year of birth since that often seems so wrapped up in a person’s politics. One way to do this is to write our own custom function delineating the cutoffs for each generation.

Below is one way we could write such a custom function:

And then use Pandas` apply() to create a new feature based on the original birth_year variable:

Now our registered voters are broken up into 5 discrete and meaningful categories. I decided to combine the 2 oldest generations (Greatest and Silent generations) so as not to create 2 rare categories that each make up only a very small portion of the population.

Seaborn countplot showing distribution of voters by generation — Image by author

Defining Bins with pd.cut()

We can also create the same generation bins using pd.cut() instead of writing our own function and applying it. We’ll still need to define the appropriate labels for each group, as well as the bin edges (cut off birth years).

In the last line, we create our new feature by providing pd.cut() with the column we want binned into categories, the bins we want, and how to label each binned category.

Rather than grouping by generation, we could quickly create a range and supply those as our bin edges. For example, if we thought it would be meaningful to group age by decade, we could accomplish that with the following:

The first line defines a range that starts at 10 and continues up to, but not including 110, increasing by 10 at each step. The second line uses that range as bin edges to discretize registered voters by age into the following groups:

Raw count and percentage of registered voters binned based on age in decades — Image by author

The first row shows that 33,349 or 19.84% of our voters are in their 40’s. The parenthesis indicates that the 40 is inclusive, whereas the square bracket indicates the 50 is excluded from the bin. To more easily keep track of what each bin means, we could feed in the following labels to pd.cut() :",https://towardsdatascience.com/feature-engineering-examples-binning-numerical-features-7627149093d,"['Max Steele', 'They Them']",2021-04-05 14:14:32.020000+00:00,830,"Bin labelsage_labels = ['10 - 19', '20 - 29', '30 - 39', '40 - 49', '50 - 59'"
Announcing Spark,"Announcing Spark

Today, Recidiviz is announcing a new project to support advocates and policymakers with impact analysis during the upcoming legislative session. Since January, the criminal justice reform landscape has shifted. We’re seeing:

Significant, but temporary decarceration from COVID; State budget cuts creating downward pressure on corrections costs and incarceration; and Increased public support for racial and social justice.

Combined, these forces are creating a surge in reform momentum as we head into January legislative sessions. To take advantage of this incredibly unique moment, Recidiviz is launching a new project — called Spark — that is designed to help policymakers turn the current momentum into long-term progress.

“With a new wave of COVID-19 hitting, many states are looking for ways to reduce incarcerated populations safely. Budget cuts and public calls for racial justice are also forcing policymakers to take a hard look at the problem,” said Andrew Warren, Recidiviz’s Head of Product.

John Tilley has joined Recidiviz to help spearhead Spark

John Tilley, former Secretary of the Kentucky Justice and Public Safety Cabinet, has joined Recidiviz to help spearhead the effort.

“Policymakers are facing pressure from all sides. They need ways to identify impactful reforms that will lead to a fairer, safer, smaller system. I’m excited to join Recidiviz and help get Spark off the ground at this pivotal moment.”",https://medium.com/@clementine-13960/announcing-spark-623f3cba502d,['Clementine Jacoby'],2020-12-10 13:28:31.936000+00:00,213,"Criminal Justice Reform, COVIDDecarceration, State Budget Cuts, Public Support For Justice, Spark Project"
EDA and Recommendation system in Python,"Amazon Top 50 books (2009–2019)

Over the years e- commerce has taken over the world and big players such as Amazon, Flipkart, ebay, etc. have gained enormous amount of consumers.

I have taken this dataset from kaggle to explore and provide various insights using CRISP- DM approach.

Some of the questions that I’ll be addressing:

Are there any correlations between the variables? Popularity of genres by year Top Authors of Best Selling books Price comparison between Fiction and Non Fiction books. Does price Price tend to drop over years?

Further, I’ll also build a recommendation system that would suggest similar books based on choice

Exploratory Data Analysis

The first step in any analysis should start with data understanding and this can be done with EDA process.

Before we dive into data visualizations, lets get a look on the descriptive statistics from the Amazon dataset.",https://medium.com/analytics-vidhya/eda-and-recommendation-system-in-python-e2e1d3a005f1,['Ashwath Paul'],2020-12-22 16:40:01.956000+00:00,136,"Amazon, E-commerce, CRISP-DM, Data Analysis, EDA"
The Next Generation of Scientists Shine at AGU,"This year’s AGU Fall meeting has ended. Earlier we posted about a group of high school students who presented their research at AGU in partnership with an IMPACT team member. This is not the only way in which IMPACT supports student scientists at AGU. Four IMPACT team members who are graduate research assistants or undergraduate interns at the University of Alabama in Huntsville also had the opportunity to present research at AGU.

Deep Learning PM2.5 Estimation

Graduate research assistant Manisha Khatri presented a deep learning approach to estimating surface PM2.5 levels. This research uses satellite and meteorological data and applies deep learning methods to predict levels of the aerosol pollutant PM2.5. Ms. Khatri believes this approach will help surpass spatial and temporal limitations of ground-based measurement techniques. Working with the IMPACT machine learning team on this project is the first intensive research work with which Ms. Khatri has been involved. What’s more, she felt presenting at AGU to be a very rewarding experience:

Having participated in AGU, helped align our work and quantify our results. It was a milestone achieved. The various sessions gave me an insight into the different domains and guided me to direct my research in a more application-oriented manner.

Identifying the best days or significant events during NASA airborne and field campaign research

IMPACT undergraduate intern Shelby Bagwell made her debut at AGU with a presentation on the “best days” of NASA research campaigns and their accessibility to scientists. Identifying these best days — defined as those containing notable events that highly relate to the objective of the campaign or when all instruments operated and cross comparison of measurements are possible — is currently a time-consuming and sometimes difficult task for past campaigns. Ms. Bagwell’s presentation discussed how IMPACT’s Airborne Data Management Group (ADMG) is addressing this issue through a Catalogue of Archived Suborbital Earth Science Investigations by providing scientists with lists of these notable events and the context surrounding them.

Ms. Bagwell felt being able to present at AGU as an undergraduate student was an amazing opportunity and helped her continue to build confidence in herself as a scientist.

I have been exploring different areas of Earth science that I am interested in pursuing both for graduate school and in my future career. Being able to attend AGU has been a wonderful way for me to look into these different topics. Being able to listen to experts present their work has been fascinating, and it has been invaluable.

Graduate research assistant Ashlyn Shirey delivered a presentation that demonstrated the complications involved in the curation of the metadata for NASA airborne and field campaigns, platforms, and instruments. Ms. Shirey is also part of the IMPACT ADMG team that is building an inventory of historical NASA airborne and field campaigns and curating detailed metadata elements about the campaigns as well as the platforms and instruments used. This effort has discovered several issues with existing metadata that has hindered the curation process.

The complications of metadata curation

One of the main issues Ms. Shirey presented was the use of various definitions for the same terms within the scientific communities with airborne and field campaigns. As an example, the spatial and temporal resolution of the instruments was one of the more difficult elements to curate due to various definitions among scientists. Setting formalized definitions and using an information model to build the inventory allows for the information and metadata to be more consistent across all campaigns and enables more efficient data access and discovery.

Ms. Shirey found a valuable aspect of her AGU experience was having the opportunity to attend talks and presentations related to her master’s thesis on COVID-19 and its effects on local climate and weather. The available literature on this research topic is limited; attending AGU allowed her to learn about other scientists’ research first hand and gain perspective on where research needs to go in order to determine how much of an impact COVID-19 will have on climate change.

The main thing that I got from my experience that will help me as a researcher is that I am not only answering my intended research question, but I am also still asking questions to help further develop that research topic. There is always more we can learn about a certain topic, so asking questions and trying to develop new methods is just as important to the research process as the actual work itself.

A machine learning image labeling solution for Earth science

Mr. Prasanna Koirala, a graduate research assistant on IMPACT’s machine learning team, presented an application named ImageLabeler, which bridges the gap between the lack of labeled Earth science datasets and the requirements of machine learning. ImageLabeler allows users to create, manage and label Earth science datasets such that they can directly be fed to a machine learning algorithm for training.

Mr. Koirala found that the presentation experience helped him to develop important skills necessary for research. He describes preparing for the presentation as challenging, exciting, and that the process involved numerous iterations and feedback.

AGU being a global platform definitely raised the stakes as high as I had ever experienced. This made me become more careful about what I put in the presentation, as well as what I delivered when I was actually presenting.

The experiences of these team members demonstrate IMPACT’s commitment to not only advancing Earth science research through the application of computer science and informatics, but also by doing our part to nurture and develop the next generation of Earth and computer scientists. More information about IMPACT and these projects can be found at NASA Earthdata and the IMPACT project website.",https://impactunofficial.medium.com/the-next-generation-of-scientists-shine-at-agu-205c5cd468b4,['Impact Unofficial'],2020-12-21 19:55:51.240000+00:00,924,"AGU Fall Meeting, Deep Learning, PM2.5 Estimation, NASA Airborne Campaigns, Curation of Metadata"
Toward data-intensive open science,"By Andrew J. Zahuranec and Stefaan Verhulst

Over his accomplished career, Jean-Claude Burgelman has served as a pioneer in identifying societal and technological trends that have the potential to transform society-at-large. In his current role as the Open Access Envoy of the European Commission Directorate-General for Research and Innovation, he has advanced open science as the new paradigm for the scientific process and enterprise in a variety of ways.

Jean-Claude Burgelman, responsible for open science policy at the European Commission Directorate-General for Research and Innovation

On Wednesday, November 20, The GovLab hosted Burgelman in its latest Ideas Lunch at NYU’s Center for Urban Science and Progress. Over the course of an hour, Burgelman spoke about the increase of the open science movement, the policies within the European Union that enabled its growth, and the future of science.

The conversation began with an overview of what “open science” meant. The movement, in the envoy’s words, describes an effort to make research more global, open, and collaborative, creative and closer to society. It was an effort to change science from a closed process to an open one.

More than being about philosophical ideas of openness, Burgelman discussed the very real value that open science brings. The concept improves transparency in the science system, ensuring taxpayers and policymakers understand the costs and value of the research they fund. It also allows ideas to circulate faster and improves a project’s return on investment by ensuring it is easily accessible and reusable.

The envoy underscored the point. “Open science is not just a systems change. It is good for science. It is good for the scientist. It is good for the funder.”

Burgelman then went on to discuss the actions of the European Union in advancing open science. Since 2008, the organization has opened its assets to the public and encouraged others to do the same. A cornerstone of this work is the FAIR data principles. Holding that data should be Findable, Accessible, Interoperable and Reusable, the European Union has committed to making its publicly funded research as open as possible by 2020 to maximize its value.

“The overwhelming majority of [traditional] science is hidden behind paywalls or put in a format that you can’t use. We do all these big investments in science, then we use it once and it’s gone,” said Burgelman about the current system of scientific publishing. “With open science, the likelihood that you have spin-off research goes up by making it available to others.”

The FAIR principles aligned with other European Union priorities to promote open data, develop a science cloud, and foster and create incentives for citizen science.

In closing remarks and in the question-and-answer session, Burgelman expressed his optimism for the future. Though academics need to protect against irreplicable research, mischief, and bad actors, open science removes barriers toward critical thinking that can enable better research. Increasingly, academics are less limited by the data they can find than the questions they ask.

Stefaan Verhulst, Co-Founder of The GovLab and Editor-in-Chief of Data & Policy

“The environment for doing research has never been better than it is today,” he said, “Still, our whole system is not geared toward producing the best questions.”

Stefaan Verhulst, Chief Research and Development Officer at The GovLab, closed the conversation by echoing these remarks. He noted The GovLab’s ongoing work on this very issue through The 100 Questions Initiative. As Burgelman emphasized, unlocking the potential of data and data science requires asking well-defined questions.

The envoy’s remarks show improving the flow of research, data, and insights are essential to improving governance and decision-making. By making research available, the scientific community can increase the dissemination of and confidence in scientific findings informing decisions. By making the underlying data more available, researchers can focus on asking the right questions.

About the Authors: Andrew J. Zahuranec is a Research Fellow at The GovLab. Stefaan Verhulst is Co-Founder and Chief Research and Development Officer at The GovLab, and an Editor-in-Chief of the open access journal Data & Policy published by Cambridge University Press.",https://medium.com/data-policy/toward-data-intensive-open-science-c1358e4b12ed,['D P Blog Admin'],2020-04-15 16:54:39.683000+00:00,660,"Jean-Claude Burgelman, Open Science, European Union, FAIR Data Principles, The Gov Lab"
Change the Background of Any Image with 5 Lines of Code,"Change the Background of Any Image with 5 Lines of Code

Photo Collage by Author

Image segmentation has a lot of amazing applications that solve different computer vision problems. PixelLib is a library created to ensure easy integration of image segmentation in real life applications. PixelLib now supports a feature known as image tuning.

Image Tuning: It is the change in the background of an image through image segmentation. The key role of image segmentation is to remove the objects segmented from the image and place them in the new background created. This is done by producing a mask for the image and combining it with the modified background. We make use of deeplabv3+ model trained on pascalvoc dataset. The model supports 20 common object categories, which means you can change the background of these objects in images.

The model supports the following objects listed below;

person,bus,car,aeroplane, bicycle, ,motorbike,bird, boat, bottle, cat, chair, cow, dinningtable, dog, horse pottedplant, sheep, sofa, train, tv

Background effects supported are:

1 Changing the background of an image with a picture

2 Assigning a distinct color to the background of an image.

3 Blurring the background of an image

4 Grayscaling the background of an image

Install PixelLib and its dependencies:

Install Tensorflow with:(PixelLib supports tensorflow 2.0 and above)

pip3 install tensorflow

Install PixelLib with

pip3 install pixellib

If installed, upgrade to the latest version using:

pip3 install pixellib — upgrade

Change the background of an image with a picture

PixelLib makes it possible to change the background of any image with a picture with just 5 lines of code.

sample.jpg

We want to change the background of the image above with the image provided below.

background.jpg

Code to change the background of an image with a picture

import pixellib from pixellib.tune_bg import alter_bg change_bg = alter_bg()

We imported pixellib, and from pixellib, we imported in the class alter_bg. We created an instance of the class.

change_bg.load_pascalvoc_model(""deeplabv3_xception_tf_dim_ordering_tf_kernels.h5"")

We loaded deeplabv3+ model. Download deeplabv3+ pascalvoc model from here.

change_bg.change_bg_img(f_image_path = ""sample.jpg"",b_image_path = ""background.jpg"", output_image_name=""new_img.jpg"")

We called the function change_bg_img that handled changing the background of the image with a picture.

The function takes the following parameters:

f_image_path: This is the foreground image, the image which background would be changed.

b_image_path: This is the image that will be used to change the background of the foreground image.

output_image_name: The new image with a changed background.

output Image

WOW! This is beautiful, we have successfully replaced the background of our image.

We are able to use PixelLib to perform excellent foreground and background subtraction through image segmentation.

Code to Obtain output array of the changed image array

For specialized uses, you can easily obtain the array of the changed image with the modified code below.

Assign a distinct color to the background of an image

You can assign a distinct color to the background of your image, just the way you are able to change the background of an image with a picture. This is also possible with five lines of code.

Code to assign a distinct color to the background of an image

It is very similar to the code used above for changing the background of an image with a picture. The only difference is that we replaced the function change_bg_img to color_bg, the function that handled color change.

change_bg.color_bg(""sample.jpg"", colors = (0, 128, 0), output_image_name=""colored_bg.jpg"")

The function color_bg takes the parameter colors and we provide the RGB value of the color we want to use. We want the image to have a green background and the color’s RGB value is set to green which is (0, 128, 0).

green background

Note:You can assign any color to the background of your image by providing the RGB value of the color.

change_bg.color_bg(""sample.jpg"", colors = (255, 255, 255), output_image_name=""colored_bg.jpg"")

We want to change the background of the image to white and set color’s RGB value to white which is (255,255,255).

white background

The same image with a white background.

Code to Obtain output array of the colored image

For specialized uses, you can easily obtain the array of the changed image with the modified code below.

Grayscale the background of an image

Grayscale the background of any image using the same lines of code with PixelLib.

Code to grayscale the background of an image

change_bg.gray_bg(“sample.jpg”,output_image_name=”gray_img.jpg”)

It is still the same code except we called the function gray_bg to grayscale the background of the image.

output image

Note:The background of the image would be altered and the objects present would maintain their original quality.

Code to Obtain output array of the grayed image

Blur Image Background

You can apply the effect of blurring the background of an image, and it is possible to control how blur the background will be.

sample2.jpg

change_bg.blur_bg(""sample2.jpg"", low = True, output_image_name=""blur_img.jpg"")

We called the function blur_bg to blur the background of the image, and set the blurred effect to be low. There are three parameters that determine the degree to which the background is blurred.

low: When it is set to true, the background is blurred slightly.

moderate: When it is set to true, the background is moderately blurred.

extreme: When it is set to true, the background is deeply blurred.

The image is blurred with a low effect.

change_bg.blur_bg(""sample2.jpg"", moderate = True, output_image_name=""blur_img.jpg"")

We want to moderately blur the background of the image, and we set moderate to true.

The image’s background is blurred with a moderate effect.

change_bg.blur_bg(""sample2.jpg"", extreme = True, output_image_name=""blur_img.jpg"")

We want to deeply blurred the background of the image, and we set extreme to true.

The image’s background is extremely blurred.

Full code to blur the background of an image

Code to Obtain output array of the blurred image

Note: Learn how to apply these background effects to videos and camera’s feeds on PixelLib’s github’s repository and PixelLib’s documentation. I will publish an explanatory article on how to apply these background effects to videos and camera’s feeds soon.

Reach to me via:

Email: olafenwaayoola@gmail.com

Linkedin: Ayoola Olafenwa

Twitter: @AyoolaOlafenwa

Facebook: Ayoola Olafenwa

Check out these articles written on how to make use of PixelLib for semantic and instance segmentation of objects in images and videos.",https://towardsdatascience.com/change-the-background-of-any-image-with-5-lines-of-code-23a0ef10ce9a,['Ayoola Olafenwa'],2020-11-09 15:07:09.417000+00:00,942,"image segmentation, image tuning, background effects, Pixel Lib, deeplabv3+"
In the brain of Computer vision? (Eng),"Image classification:

The problem with image classification is this: Given a set of images that are all labeled with a single category, we are asked to predict these categories for a new set of test images and measure the accuracy of the predictions. There are many challenges to this task, including point of view variation, scale variation, intra-class variation, image distortion, image occlusion, lighting conditions, and background clutter.

How could we write an algorithm capable of classifying images into distinct categories? Computer Vision researchers have developed a data-based approach to solving this problem. Instead of trying to specify directly in the code what each image category of interest looks like, they provide the computer with many examples of each class of images and then develop learning algorithms that examine these examples and learn the visual appearance of each class.

In other words, they first accumulate a set of tagged image learning data and then send it to the computer to process the data. Given this fact, the complete image classification pipeline can be formalized as follows:

Our input is a training data set that consists of N images, each labeled with one of K different classes.

We then use this training set to train a classifier to learn what each class looks like.

finally, we evaluate the quality of the classifier by asking him to predict labels for a new set of images that he has never seen before.

We then compare the actual labels for these images with those predicted by the classifier.

The most common algorithms used to solve image classification are CNN convolutional neural networks

Convolutional neural networks are currently the most efficient models for image classification. CNN has two very different parts. As input, the image is provided as a matrix of pixels. A 2-dimensional grayscale image. The color is represented by the third dimension, depth 3, representing the basic colors [red, green, blue]. The first part of CNN is the convolutional part. It acts as an image extractor. The image is passed through subsequent filters or convolution kernels, creating new images called convolution maps. Some indirect filters reduce the resolution of the image by a maximum local operation. Finally, the weave maps are flattened and combined into a vector of entities called CNN code.

Creating a new convolutional neural network is expensive in terms of expertise, equipment, and the amount of data required with annotations. First of all, it is necessary to determine the architecture of the network, i.e. the number of layers, their size and the matrix operations that connect them. The training then involves optimizing the network coefficients to minimize output error. This training can take several weeks for the best CNN, with many graphics processors working on hundreds of thousands of commented images.

Today, most image classification techniques are trained on ImageNet, a data set of approximately 1.2 million high-resolution training images.

The winner of the first ImageNet competition, Alex Krizhevsky, has revolutionized deep learning with an Alexnet very deep convolutional neural network. Its architecture consists of 7 hidden layers, plus a few maximum aggregation layers. The first layers were convolutional, while the last two were globally connected. Activation functions were linear units rectified in each hidden layer. These train much faster and are more expressive than logistics units. In addition, it also uses competitive standardization to suppress hidden activities when neighbouring units have stronger activities. This allows for better management of intensity variations.

In terms of hardware requirements, Alex uses a very efficient implementation of convolutional networks on 2 Nvidia GTX 580 GPUs (more than 1000 small fast cores). GPUs are very good for matrix multiplications and also have a very high memory bandwidth. This allows him to form the network in a week and to quickly combine the results of 10 patches at the time of testing. We can spread a network over several cores if we can communicate the states quickly enough. As cores become cheaper and data sets get larger, large neural networks will improve faster than older CV systems. Since AlexNet, many new models using CNN as a base architecture have been developed and have performed very well in ImageNet.",https://medium.com/analytics-vidhya/whats-computer-vision-eng-c216a4c54c73,['Magloire Ndabagera'],2020-05-24 10:05:40.208000+00:00,680,"Image Classification, Convolutional Neural Networks, CNN, Alex Net, Data Sets"
How to implement Linear Regression with NumPy,"Now, let’s focus on implementation.

Firstly, we need to, obviously, import some libraries. We import numpy as it is the main thing we use for the implementation, matplotlib for visualizing our results, and make_regression function, from sklearn , which we will be using to generate a regression dataset for using as an example.

import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_regression

Then we will create a LinearRegression class with the following methods:

.fit() — this method will do the actual learning of our linear regression model; here we will find the optimal weights

— this method will do the actual learning of our linear regression model; here we will find the optimal weights .predict() — this one will be used for prediction; it will return the output of our linear model

— this one will be used for prediction; it will return the output of our linear model .rmse() — computes the root mean squared error of our model with the given data; this metric is kind of “the average distance from our model’s estimate to the true y value”

The first thing we do inside .fit() is to concatenate an extra column of 1’s to our input matrix X. This is to simplify our math and treat the bias as the weight of an extra variable that’s always 1.

The .fit() method will be able to learn the parameters by using either closed-form formula or stochastic gradient descent. And to choose which to use, we will have a parameter called method that will expect a string of either ‘solve’ or ‘sgd’.

When method is set to ‘solve’ we will get the weights of our model by the following formula:

which requires the matrix X to have full column rank; so, we will check for this and otherwise we show an error message.

The first part of our .fit() method is:

Note that the other parameters after method are optional and are used only in the case we use SGD.

The second part of this method handles the case of method = ‘sgd’ , which doesn’t require that X has full column rank.

The SGD algorithm for our least squares linear regression is sketched below:

We will start this algorithm by initializing the weights class attribute to a numpy vector with values drawn from a normal distribution with mean 0 and standard deviation 1/(number of columns). We divide the standard deviation by the number of columns to make sure we don’t get too big values as output in the initial stages of the algorithm. This is to help us converge faster.

At the beginning of each iteration, we randomly shuffle our rows of data. Then, for each batch, we compute the gradient and subtract it (multiplied by the learning rate) from the current weights vector to obtain the new weights.

Below is the second half of our .fit() method:

We return self from this method to be able to concatenate the calls of the constructor and .fit() like this: lr = LinearRegression().fit(X, y, ‘solve’) .

The .predict() method is quite straight-forward. We first check if .fit() was called before, then concatenate a column of 1’s to X and verify that the shape of X allows multiplication with the weights vector. If everything is OK, we simply return the result of the multiplication between X and the weights vector as the predictions.

In .rmse() we first get the outputs of the model using .predict() , then if there were no errors during predict, we compute and return the root mean squared error which can be thought of as “the average distance from our model’s estimate to the true y value”.

Below is the full code of the LinearRegression class:

Using our LinearRegression class in an example

To show our implementation of linear regression in action, we will generate a regression dataset with the make_regression() function from sklearn .

X, y = make_regression(n_features=1,

n_informative=1,

bias=1, noise=35)

Let’s plot this dataset to see how it looks like:

plt.scatter(X, y)

Image by Author

The y returned by make_regression() is a flat vector. We will reshape it to a column vector to use with our LinearRegression class.

y = y.reshape((-1, 1))

Firstly, we will use method = ‘solve’ to fit the regression line:

lr_solve = LinearRegression().fit(X, y, method='solve') plt.scatter(X, y) plt.plot(X, lr_solve.predict(X), color='orange')

Image by Author

The root mean squared error of the above regression model is:

lr_solve.rmse(X, y)

# 35.59874949855057

Then, we also use method = ‘sgd’ and we will let the other parameters have their default values:

lr_sgd = LinearRegression().fit(X, y, method='sgd') plt.scatter(X, y) plt.plot(X, lr_sgd.predict(X), color='orange')

Image by Author

As you can see, the regression lines in the 2 images above for methods ‘solve’ and ‘sgd’ are almost identical.

The root mean squared error we got when using ‘sgd’ is:

lr_sgd.rmse(X, y)

# 36.34038690848635

Here is the Jupyter Notebook with all the code:",https://towardsdatascience.com/how-to-implement-linear-regression-with-numpy-172790d2f1bc,['Dorian Lazar'],2020-11-28 15:38:01.220000+00:00,770,"linear_regression_implementation.ipynblinear-regression, machine-learning, numpy, matplotlib, sklearn"
Generic Sentiment Analysis On Cloud,"Another step towards democratization of Data Science

Different companies have different sets of data that they would want analysed.

A company working on twitter dataset would primarily deal with tweets.

Tweets classified as Hate, Threat and Neutral

A news agency would have to grapple with news streaming in from different channels.

News belonging to business, sport, entertainment etc

An app building company would be interested in the reviews given by their users.

App reviews in play store

Say the twitter company wants to classify tweets on whether they are hateful, negative or positive.

The news agency on the other hand wants to classify news based on the category it belongs to — like Sports, Entertainment, Politics, Crime etc.

If you look closely, the requirement is same, just that the outputs should be different.

Considering this, Aditya Kumar and I came up with a Django web application that could act as a classifier of text, given that it gets trained on a similar text that is labelled.

First, hop in to https://mysentimeter.herokuapp.com/senti/

There are two sections to this page, the Training section

This is where you train the model with your data set

The second part of the application is where you use the trained model and test on un labelled dataset.

The test section

For instance, we want to classify tweets.

We have a training file that contains tweets labelled with Positive and Negative.

Labelled Data Set in the training file

Based on this data we want to train a model and use it to predict whether tweets in the test file are positive or negative

Un labelled dataset in the test file

First we train our model using the train file.

Currently we support SVM and Naive Bayes. So check the same in the Training section.

We will start giving support for the remaining soon

Next we need to choose the training file.

Click on Choose file and select the file on which you want to train.

Choose the training file

Once you click on submit, after a while you will see something interesting happening at the test section.

Reference file created dynamically

As we can see, a new reference file has been created according to the training file that we had chosen. This is an identifier for the model so that when we test with our unlabelled dataset, we can choose which model should be used.

The text area is in case we want to test on just one block of text

Click on choose file to select the file to be tested.

Select the test file

Visualisation that shows how many were marked as positive and how many negative by each library

Note the Get result File button at the bottom.

When we click on Get Result File, we get to download the csv file which contains the data and the labels.

Classification by SVM and Naive Bayes

We can now use this file to enhance our analysis.

Sample Datasets-

Train Dataset

Test Dataset

Application Running at: https://mysentimeter.herokuapp.com/senti/

Code Repository: https://github.com/ashhadulislam/sentiment-analysis

If you face any issues or if you feel that there is a feature that the app absolutely cannot live without, please feel free to create an issue/feature request in github.",https://medium.com/tech-that-works/generic-sentiment-analysis-on-cloud-5456131ba461,['Ashhadul Islam'],2019-05-16 07:40:03.203000+00:00,491,"Data Science, Democratization, Text Classification, Sentiment Analysis, SVM"
Deep learning: Getting Started with Feed Forward Neural Networks,"What you need:

A good PC (atleast 4 GB RAM) Anaconda. You can download here Good internet of course

Steps

Load datasets Make datasets Iterable Create Model Class (we’ll be using ReLU as our activation function) Instantiate Model Class Instantiate Loss class Instantiate Optimizer class Train Model

Let’s get Started

Step 1: Loading Dataset (MNIST Dataset)

Here we are using the MNIST dataset. True is assigned to the training dataset and False for the Test dataset for obvious reasons; we are only interested in training the train dataset. You can always leave your download as ‘True’, because it basically keeps check of whether you’ve downloaded the dataset or not. So you should have something like this:

Step 2: Making dataset Iterable

This just means that we can iterate through the datase. batch_size means “every iteration, we feed ‘x’ images to our model”. You are free to choose any number you want. n_iters is: “within one epoch, how many iterations are we looking at”. One epoch means going through your whole dataset once. num_epochs: “we are determining the number of epochs based on the number of iterations we have”.

Shuffle = True means; after every epochs we shuffle the image so we do not go through it in a certain manner. Note: “We dont shuffle test dataset because when you are testing your model, you dont have to run multiple epochs”

Step 3: Creating Model Class

The hidden size determines, how many non-linearity dimensions we have. We are using ReLU (Rectified Linear Unit)activation function here, because it gives us the best benefit and it is less computationally expensive compared to tanh and sigmoid activation function.

Input dimension = 748 because the MNIST image is a square of size 28 *28 and when flattened you’ll get 784.

Output dimension is 10; we have numbers 0, 1, 2, 3, 4, 5, 6, 7, 8, 9.

Hidden dimension (Number of neurons, Number of non-linear activation function): 100; it can be any number. The tip is, if you increase the number, you increase the model’s capacity. So it will learn more. The con is, you might need more data as you increase the number and that may not probably be necessary.

Step 4: Instantiating Model Class

What’s happening here is that we are assigning, values to these objects and then feeding them to our model class.

Step 5: Instantiating Loss Class

The loss class calculates the SoftMax Output and does a cross entropy loss.

Tip:

Feedforward Neural Network — Cross Entropy Loss

Logistic Regression — Cross Entropy Loss

Linear Regression — Mean Sqaure Error

Step 6: Instantiate Optimizer Class

We do this because we want to update our model’s parameters with the loss that we have. So on every iterations we have our gradient.

Equation:

Θ = Θ- η . ∆

Θ: Parameters (our variables)

η: Learning rate (how fast we want to learn)

∆: parameter’s gradient

parameters = parameters -leaning rate * parameters_gradients

Remember: After 100 images, we get our loss, gradients, then update our parameters.

So we use SGD (Stochastic Gradient Descent)to update our parameters. We do not want our learning rate to be too high because we simply might miss the best solution, so we want to take small little steps down the road.

Parameters (Briefly)

We can print our model parameters accordingly, we get an output that gives us some sense that our parameter is a generator. So we can access the length by passing through a list, then we find the length. The length is 8, which tells us that we have 8 groups of parameters. So we access each group

Step 7: Train Model Class

To be continued…",https://medium.com/@evidencemonday/getting-started-d901d2a2d6ff,['Evidence Monday'],2019-03-30 19:27:33.950000+00:00,573,"PC, Anaconda, Internet, MNIST Dataset, Iterable"
Who Tells Us About The Future of AI? Technologists or Storytellers,"Caution: There are some spoilers about HBO TV series Westworld S03e03 in this story.

I usually don’t spend lots of time watching TV series. However, some are worth watching. One of which is another successful HBO series: Westworld. It is created by Lisa Joy and Jonathan Nolan. In this short story, I want to draw a connection between technological improvements and the long path of storytelling backing them from years before. For this purpose, I decided to start from one of the greatest stories of today about AI.

In one inspiring episode of the series (S03E03), Dolores (Evan Rachel Wood) is describing a system called Rehoboam to Caleb Nickols (Aaron Paul). After she brings Caleb to the location of his worst memory of life and informs him about every single detail of it being recorded to the Rehoboam’s database, Dolores tells him that Rehoboam has been recording every single event of everybody’s life including choices, decisions, purchases, behaviors, etc. to make “a mirror world of this world”. And she emphasizes that this is not to record everything about everyone. Instead, it is about who the system lets people become. Their conversation goes on to the point that Dolores shows Caleb’s profile to himself and explains that a “predictive algorithm” concludes that Caleb will commit a suicide in 10 to 12 years. The dialogue hits more when Dolores mentions that this prediction is available to almost every recruitment company and is probably the reason behind Caleb’s failure in finding a decent job.

“They won’t invest in someone who is gonna kill himself. But by not investing, they ensure the outcome.”

Video Snapshot of HBO “West World” series (S03E03)

Our general reactions to sci-fi stories are mixed by some levels of ignorance. People normally call them only stories. However, I don’t believe it is the case in all circumstances. Storytellers have been constructing the societies’ desires in many instances throughout history. One of the greatest examples is the story of human to land on the moon back in 1969. If one dig a little bit more into the history of storytelling, they will find this desire being transformed from one to the next generation starting from 1865. And this date is the right date only if we neglect any oral history and undocumented attempt to tell the story of this travel beforehand.

In 1865, Jules Verne’s authored his significant novel “From the Earth to the Moon”. later on, in 1902, the French director Georges Méliès, produced “A trip to the moon” (Le Voyage dans la Lune), telling more or less the same story. In 1953, when this story was already familiar for almost everyone in the planet earth, Hergé put moon landing in his comic series “Explorers on the Moon” (On a marché sur la Lune) and used it as a theme for the seventeenth volume of “The Adventures of Tintin”. The last step though, is the real story of two Americans landing on the moon in 1969.

After almost a century of storytelling and sharing one big dream, the 400,000 employers of Apollo program and 20,000 other third party organization, finally made a dream come true. All of them were great scientists, engineers, and technologists and neither of them was a storyteller. But those engineers had this desire woven into their fabric of their minds from a century ago (probably from their parents’ and grandparents’ night-time stories”, their comic books, or weekend movie theaters).

Now, who do you think tells us about the future? Jules Vern or Neil Armstrong?

Let’s get back to Rehoboam. Many might argue that it’s one of those other techno-phobic stories like “Ex-Machina”, “I, Robot”, or half of “Black Mirror” episodes. That might be the case about the amusement park (The West World Park) that the series borrows its name from. But not the system introducing in the third season: Rehoboam.

When President Trump started his “travel ban” back in 2016, Iranians were hurt the most. Unofficial data shows that Iranians with an equal level of education are getting fewer job interviews when applying for H-1B (the program which allows U.S. employers to temporarily employ foreign workers in specialty occupations). In fact, no employer needs to have access to Rehoboam to know that as long as President Trump is running the country, picking some candidates from any given country except the four blacklisted one would be a much safer option for their company. Who will invest in somebody that might be kicked out of the country by another similar legislation?

“And by not investing they ensure the outcome…”.

Photo from CNN (link)

Iranians among the citizen of other blacklisted countries are now less likely to receive a H-1B visa. This is not due to any legal prohibition. But this is based on the fact that their future (in the USA) is significantly unstable. Clearly, no business owner will invest in instability.

This is only one specific case though, there are other familiar cases such as African-American community, Hispanics, and other marginalized groups being trapped in a similar loop. No one invests on them cause their projections are unstable and they remain unstable because no one invest on them. To my view, we already have started Rehoboam. Jonathan Nolan and Lisa Joy remind me more of Hergé than Verne.

No one invests on them cause their projection is unstable and they remain unstable because no one invests on them.

There are various discourses about the future of AI and most of them are built upon the concept of AI solutionism. A great deal of funding is spent on things like driver-less cars, quantum computers, and other relatively favorable technologies. One day they may come true in their perfect form. However, the future of AI like the future of almost everything else is what storytellers embed in human societies, not what excites technologists the most.",https://medium.com/@mahanmehrvarz/who-tells-us-about-the-future-of-ai-technologists-or-storytellers-e413b09d55a6,['Mahan Mehrvarz'],2020-05-03 18:19:10.935000+00:00,960,"Westworld, HBO Series, AI, Rehoboam, Jules Verne"
Is AI a Threat to Employment?,"“AI is going to take jobs of the future, I assure you.”

“I’m sure that job would be taken over my machines soon.”

The future of AI has already received its positive and negative points. Everyone knows the positives- it’ll make human life easier, make work more efficient, make task more streamlined, etc. I’m also sure in addition to the Terminator-ish scenario, you must have also heard that AI would make some jobs redundant.

Advancement of technology and softwares would take over some less skilled jobs, sure. Yet, we could likewise consider it to be a test for us to grow more abilities and update ourselves with the most recent tech progressions.

There is no doubt that AI is going to influence ventures over the range, and a great many individuals should change their approach to work so as to stay aware of the changes. The lay-offs will likewise be at a record-breaking high.

A report by Deloitte has indicated that in the UK, AI has made more employments than it has struck down. This makes automation appear to be a hopeful and practical result for our future.

What can we do to stay ahead on top of our game? We keep up to date with recent advancements. We keep ourselves educated. We upskill ourselves. No AI can duplicate human ingenuity, at least not in the near future. As long as you are produce good results, you are valuable to the economy.

Being well read/up to date can be troublesome, but if you are affiliated with LrnEd, it is not that tough. With 12 weeks online bootcamp, and 24/7 mentoring, that daunting task is accomplished systematically and easily. Head on over to LrnEd, and see for yourself!",https://medium.com/lrned/is-ai-a-threat-to-employment-666c42839536,['Sanjay Verma'],2020-01-14 18:50:20.520000+00:00,280,"AI, Automation, Technology, Future Jobs, Lrn Ed"
Named Entity Recognition with NLTK and SpaCy,"Named Entity Recognition with NLTK and SpaCy

NER is used in many fields in Natural Language Processing (NLP)

Named entity recognition (NER)is probably the first step towards information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. NER is used in many fields in Natural Language Processing (NLP), and it can help answering many real-world questions, such as:

Which companies were mentioned in the news article?

Were specified products mentioned in complaints or reviews?

Does the tweet contain the name of a person? Does the tweet contain this person’s location?

This article describes how to build named entity recognizer with NLTK and SpaCy, to identify the names of things, such as persons, organizations, or locations in the raw text. Let’s get started!

NLTK

import nltk

from nltk.tokenize import word_tokenize

from nltk.tag import pos_tag

Information Extraction

I took a sentence from The New York Times, “European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices.”

ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'

Then we apply word tokenization and part-of-speech tagging to the sentence.

def preprocess(sent):

sent = nltk.word_tokenize(sent)

sent = nltk.pos_tag(sent)

return sent

Let’s see what we get:

sent = preprocess(ex)

sent

Figure 1

We get a list of tuples containing the individual words in the sentence and their associated part-of-speech.

Now we’ll implement noun phrase chunking to identify named entities using a regular expression consisting of rules that indicate how sentences should be chunked.

Our chunk pattern consists of one rule, that a noun phrase, NP, should be formed whenever the chunker finds an optional determiner, DT, followed by any number of adjectives, JJ, and then a noun, NN.

pattern = 'NP: {<DT>?<JJ>*<NN>}'

Chunking

Using this pattern, we create a chunk parser and test it on our sentence.

cp = nltk.RegexpParser(pattern)

cs = cp.parse(sent)

print(cs)

Figure 2

The output can be read as a tree or a hierarchy with S as the first level, denoting sentence. we can also display it graphically.",https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da,['Susan Li'],2018-12-06 02:39:35.867000+00:00,349,"Comma-separated values: NLP, Named Entity Recognition (NER), Information Extraction, NLTK, Spa Cy"
Detecção de fraudes em cartão de crédito com Machine Learning,"scikit-learn

""We use scikit-learn to support leading-edge basic research [...]"" ""I think it's the most well-designed ML package I've…",https://medium.com/dados/detec%C3%A7%C3%A3o-de-fraudes-em-cart%C3%A3o-de-cr%C3%A9dito-com-machine-learning-c4a966ebe08b,['Wesley Watanabe'],2020-01-08 16:18:16.195000+00:00,18,"scikit-learn, machinelearning, MLpackage, research, data Science"
Learning Rate Schedule in Practice: an example with Keras and TensorFlow 2.0,"Using Keras to load the dataset

Keras provides some utility functions to fetch and load common datasets, including Fashion MNIST. Let’s load Fashion MNIST

fashion_mnist = keras.datasets.fashion_mnist

(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()

The dataset is already split into a training set and a test set. Here is the shape and data type of the training set:

>>> X_train_full.shape

(60000, 28, 28)

>>> X_train_full.dtype

dtype('uint8')

We are going to train the neural network using Gradient Descent, we must scale the input feature down to the 0–1 range. And for faster training on a local machine, let’s just use the first 10,000 images.

X_train, y_train = X_train_full[:10000]/255.0, y_train_full[:10000]

Creating a Model

Now let’s build the neural network! There are 3 ways to create a machine learning model with Keras and TensorFlow 2.0. Since we are building a simple fully connected neural network and for simplicity, let’s use the easiest way: Sequential Model with Sequential() .

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Dense, Flatten def create_model():

model = Sequential([

Flatten(input_shape=(28, 28)),

Dense(300, activation='relu'),

Dense(100, activation='relu'),

Dense(10, activation='softmax'),

])

return model

Our model has the following specifications:

The first layer (also known as the input layer) has the input_shape to set the input size (28, 28) which matches the training data. The input layer is a Flatten layer whose role is simply to convert each input image into a 1D array.

to set the input size which matches the training data. The input layer is a layer whose role is simply to convert each input image into a 1D array. And then it is followed by 2 Dense layers, one with 300 units, and the other with 100 units. Both of them use the relu activation function.

layers, one with 300 units, and the other with 100 units. Both of them use the activation function. The output Dense layer has 10 units and the softmax activation function.

model = create_model()

model.summary()

1. Constant learning rate

The constant learning rate is the default schedule in all Keras Optimizers. For example, in the SGD optimizer, the learning rate defaults to 0.01 .

To use a custom learning rate, simply instantiate an SGD optimizer and pass the argument learning_rate=0.01 .

sgd = tf.keras.optimizers.SGD(learning_rate=0.01) model.compile(

optimizer=sgd,

loss='sparse_categorical_crossentropy',

metrics=['accuracy']

)

And to fit the model to training data:

history_constant = model.fit(

X_train,

y_train,

epochs=100,

validation_split=0.2,

batch_size=64

)

Let’s plot the model accuracy and this can serve as a baseline for us to experiment with other learning rate schedules.

Constant Learning Rate — accuracy plot

2. Time-based decay

Time-based decay is one of the most popular learning rate schedules. Formally, the time-based decay is defined as:

learning_rate = lr * 1 / (1 + decay * epoch)

where lr is the previous learning rate, decay is a hyperparameter and epoch is the iteration number. When the decay is zero, this has no effect on changing the learning rate. When the decay is specified, it will decrease the learning rate from the previous epoch by the given fixed amount. The value of decay is normally implemented as

decay = initial_learning_rate / num_of_epoches

In Keras, one way to implement the time-based decay is by defining a time-based decay function lr_time_based_decay() and pass it to LearningRateScheduler callback.

initial_learning_rate = 0.01

epochs = 100

decay = initial_learning_rate / epochs def lr_time_based_decay(epoch, lr):

return lr * 1 / (1 + decay * epoch) history_time_based_decay = model.fit(

X_train,

y_train,

epochs=100,

validation_split=0.2,

batch_size=64,

callbacks=[LearningRateScheduler(lr_time_based_decay, verbose=1)],

)

And below are the plots of accuracy and learning rate.

Time-based decay — accuracy plot

Time-based decay — learning rate plot

3. Step decay

Another popular learning rate schedule is to systematically drop the learning rate at specific times during training. Formally, it is defined as:

learning_rate = initial_lr * drop_rate^floor(epoch / epochs_drop)

Where initial_lr is the initial learning rate such as 0.01, the drop_rate is the amount that the learning rate is modified each time if it is changed, epoch is the current epoch number, and epochs_drop is how often to change the learning rate such as 10 epochs. Similarly, we can implement this by defining a step decay function lr_step_decay() and pass it to LearningRateScheduler callback.

initial_learning_rate = 0.01 def lr_step_decay(epoch, lr):

drop_rate = 0.5

epochs_drop = 10.0

return initial_learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop)) # Fit the model to the training data

history_step_decay = model.fit(

X_train,

y_train,

epochs=100,

validation_split=0.2,

batch_size=64,

callbacks=[LearningRateScheduler(lr_step_decay, verbose=1)],

)

And below are the plots of the accuracy and learning rate.

Step-based decay — accuracy plot

Step-based decay — learning rate

4. Exponential decay

Another popular learning rate schedule is to drop the learning rate at an exponential rate. Formally, it is defined as:

learning_rate = initial_lr * e^(−k * epoch)

Where initial_lr is the initial learning rate such as 0.01, k is a hyperparameter, and epoch is the current epoch number. Similarly, we can implement this by defining an exponential decay function lr_exp_decay() and pass it to LearningRateScheduler callback.

initial_learning_rate = 0.01 def lr_exp_decay(epoch, lr):

k = 0.1

return initial_learning_rate * math.exp(-k*epoch) # Fit the model to the training data

history_exp_decay = model.fit(

X_train,

y_train,

epochs=100,

validation_split=0.2,

batch_size=64,

callbacks=[LearningRateScheduler(lr_exp_decay, verbose=1)],

)

Exponential decay — accuracy plot

Exponential decay — learning rate plot

Compare model accuracy

Finally, let us compare the model accuracy using different learning rate schedules.

Looks like Constant and Time-based learning rates have better performance than Step decay and Exponential decay for this particular tutorial. Bear in mind that, this tutorial only uses the first 10,000 images with some arbitrary value for initial_learning_rate=0.01 , validation_split=0.2 and batch_size=64 .

In a real-world application, they are a lot more to consider for tunning the learning rate. Please check out the paper “Practical Recommendations for Gradient-based Training of Deep Architectures” for some best practices.

That’s it

Thanks for reading. This article has covered the most popular Learning Rate Schedules. Next time, we will take a look at the Adaptive Learning Rate.

Please check out the notebook on my Github for the source code.

Stay tuned if you are interested in the practical aspect of machine learning.

You may be interested in some of my other TensorFlow articles:",https://towardsdatascience.com/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c,['B. Chen'],2020-10-03 02:27:25.637000+00:00,902,"Keras, Tensor Flow, Fashion MNIST, Learning Rate Schedules, SGD optimizer"
How To Stop Floating Point Arithmetic Errors in Python,"We expect precision, consistency, and accuracy when we code. After all, it’s a computer doing the work. But your arithmetic may have been off the entire time and you didn’t even know.

If you’ve experienced floating point arithmetic errors, then you know what we’re talking about. If you’re unsure what that means, let’s show instead of tell.

print(1.1 * 3) # 3.3000000000000003

This happens because decimal values are actually stored as a formula and do not have an exact representation.

We’re going to go over a solution to these inconsistencies, using a natively available library called Decimal.

Learn to Use the Decimal Library

According to the official Python documentation:

The decimal module provides support for fast correctly-rounded decimal floating point arithmetic.

So how do we go about using this readily available tool? Let’s start by importing the library. There are multiple components to import so we’ll use the * symbol.

from decimal import *

Next, we’ll use the Decimal() constructor with a string value to create a new object and try our arithmetic again.

print(Decimal('1.1') * 3) # 3.3

Make sure to use a string value, because otherwise the floating point number 1.1 will be converted to a Decimal object, effectively preserving the error and probably compounding it even worse than if floating point was used.",https://medium.com/code-85/how-to-stop-floating-point-arithmetic-errors-in-python-a98d3a63ccc8,['Jonathan Hsu'],2020-04-21 12:16:01.082000+00:00,204,"Floating Point Arithmetic, Decimal Library, Decimal Module, Decimal Constructor, String Values"
What is Logistic Regression?,"What is Logistic Regression?

Logistic Regression

Logistic Regression is a statistical model that uses a logistic function to model a binary dependent variable. Logistic Regression is basically used to solve the problems which are binary classified.

The Logistic Function, also known as a sigmoid function which helps to do the classification. The sigmoid function is basically an S-shape curve that takes any real value number and maps the value between 0 and 1.

Sigmoid Function

So you might ask me a question that How this S curve is formed from linear regression? and Why is logistic regression considered a linear model?

So in, Simple Linear Regression we did the analysis that how we predict the salary of employees based on experience.

Simple Linear Regression

But, in the problems related to whether a person is buying a product or not? So in these types of classification problems you can, we cannot apply simple linear regression.

So if we analyze the above plot you can see that people after some age and before some age are not buying the product.

So we define a threshold that can visualize our plot in this way:

So if perform some mathematical substitution we can see that How to derive a sigmoid function formula?

So this mathematical substitution helps to do prediction which is based upon some prediction.

Now let's analyze how logistic regression classification is done?

So for that, we have to take 4 points 20,30, 40, and 50.

If we project our 4 points on the sigmoid function, then you can visualize it in this way :

Now if we put our values on this equation:

Equation

we can derive ‘p’ by putting the values of ‘x’ on the above equation.

Now you can see the value of ‘p’ which is basically the probability that our customer will buy that product or not.

Now, We do a simple comparison based on the probability that if the value of ‘p’ is less than 50% then he is not willing to buy that product otherwise he is willing to buy that product.

So, this is basically how sigmoid function works in Logistic Regression?.

Now we will do practically implementation of our model. We first import our data set of people who want to buy a specific product.

Dataset

We should follow the steps to build a Logistic Regression model:

Step 1. Import the Libraries

Import Libraries

Step 2. Importing the Dataset

Import Dataset

Step 3. Split the data into a matrix of features(X)(So we are taking ‘Age’ and ‘Salary’ into consideration to do Prediction) and the dependent variable(y).

Step 4. Splitting the matrix of features(X) and dependent variable(y) into training and test set.

Splitting of Test and Training set

Steps 5. Now we do Feature Scaling for ‘Age’ and ‘Salary’ column.

Code to apply feature scaling on the dataset

Step 6. Fitting a linear model to test and training dataset.

Fitting the linear model

Step 7. Predicting the Test result.

Predict the model

Step 8. Making the Confusion Matrix to do predictions.

confusion matrix

confusion matrix

Confusion Matrix helps to give the accuracy of our model that is the number of false values and true values.

Accuracy of our model

Step 8. Visualization of Dataset.",https://medium.com/@maniksonituts/what-is-logistic-regression-f31cc95d5f38,['Manik Soni'],2020-09-27 10:33:10.240000+00:00,496,"Visualization of the datasetlogistic regression, sigmoid function, linear regression, classification problems, prediction"
Combine Image & Tabular data in one Model using PyTorch,"The following blog will help the readers to understand how we can combine image and tabular data together in PyTorch using deep learning and generate predictions from the model . I will go step by step to understand the whole end to end scenario easily : →

#1# Create a custom dataset class →

I am going to use here OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction dataset . After cleaning and processing the tabular data I will create the dataset class for our data as follows……

As there are a lot of codes and syntaxes , I recommend reading the codes with respect to the comments given beside each syntax written . The steps involved are: →

First we declared some of the specific libraries that we will need . we will define the constructor for the dataset class that we created. then we will define the __len__ function to calculate the length of the dataset then we will define the __getitem__ function extract the input and the targets that we will be feeding into our model that we will create.

#2# Create object for dataset class and dataloaders →

#3# Create model class →

I have commented the code syntaxes as much as possible , but reader if you still feel that you are unable to understand any part of the code please please please feel free to comment on this article , I will be very glad to answer your doubts .

Still there are 3 major syntaxes in this present in this at line no. 15, 21 & 56 where we combine our tabular and image data layers to generate predictions out of it .

15 self.image_dense_layer_2 = nn.Linear(512, 1) # in constructor 21 self.tabular_dense_layer_4 = nn.Linear(4, 1) # in constructor 56 x = torch.cat((x, tab), dim=1) # in forward function

#4# Define loss function , optimizer and training loop for our model.

After model creation we will define the following things easily .

Please consider going through my notebook I have written on this topic once , if you find difficulty in implementing it or simply put your comments here.

I hope you have understood the logic behind how we can combine image data & tabular data in a deep learning model using pytorch . Let me know , if you have any Questions , Comments or Concerns regarding this in the comment section ; Until then enjoy LEARNING……………….

For more awesome data science related blog posts check out the top data science bloggers on medium.",https://medium.com/analytics-vidhya/combine-image-tabular-data-in-one-model-using-pytorch-38dfd23c437c,['Soumo Chatterjee'],2020-10-07 12:48:28.585000+00:00,406,"pytorch, deep learning, tabular data, image data, custom dataset class"
Is Artificial Intelligence (AI) the New Customer Service Support? | IMS,"“By 2025, as many as 95 percent of all customer interactions will be through channels supported by artificial intelligence (AI) technology.” — Microsoft

In recent years, AI technology has become a revolutionary, transformative concept that has changed the way industries operate.

If we go back to the late 1940s and early 1950s, computers were initially created to execute commands but could not store them nor act intelligently. Fast forward to today’s age of “big data” and AI applications can be found nearly everywhere thanks to its ability to streamline various work processes.

While AI technology is deployed across almost all business sectors, it has been incredibly successful for businesses that require more customer interaction. Salesforce believes that deploying AI for Customer Relationship Management will increase global business revenues by $1.1 trillion by 2021.

Implementing AI-managed systems such as chatbots, smart assistants, and automated responses have been proven to improve user satisfaction. A recent study by IBM indicated that 33% of users are more likely to increase their satisfaction due to a more personalized experience offered by AI.

Read on to learn the game-changing benefits of adopting AI technology for customer service:

Quicker Response Rate

An article by AI Magazine revealed that 75% of customers believe it takes too long to reach a human agent via customer service. A poor experience can lead to abandoned carts and missed sales opportunities. To improve the timeliness of response rates and eliminate the hassle of calling customer support, chatbots are a powerful online support tool that mimics a human agent to answer customer queries promptly. For example, H&M, the famous fashion retailer, enables clients to make entire purchases through bots available on their Kik messaging app.

Advanced chatbots use sophisticated machine-learning (i.e., Siri or Google Assistant) to understand complete sentences and provide real-like conversations. Implementing this software allows for better resource management, as customer service representatives no longer need to spend time answering simple questions; they can shift their focus to more complex queries.

Data Storage

Most AI software does not require continual programming; they are designed in such a way that information is collected automatically without having to invest additional time and resources. With capabilities to track and store valuable data on customers, businesses can obtain in-depth insights to understand their customers’ behavior in more detail and automate marketing activities to maintain their attention and overall improve the customer journey.

Consider the example of Netflix. When a user logs in, they are instantly welcomed by entertainment recommendations. Such user experience is possible through AI-powered analysis. Suggestions are initiated thanks to data collected from what movies and TV shows have already been watched, repeatedly viewed, or left incomplete.

Natural Language Processing

As aforementioned, AI-powered chatbots are a robust software option that can be used to interact with customers. Contrary to rule-based chatbots that reflect robot-like mannerisms, the more advanced chatbots aren’t so different from live agents. They can use natural language processing to generate appropriate responses to specific tasks and resonate with users, often emulating personal and empathetic replies.

With AI chatbots available to the customer 24/7, they can help prevent customer inquiries from escalating into complaints. Thanks to an almost instant response time for chatbots, this function removes long waiting periods for customers looking to get a quick reply to their queries and feel like they are genuinely being listened to. In-built AI intelligence allows businesses to program accurate, customized feedback for each type of customer interaction. If queries become too tough for the automated system to answer, only then will they need to be escalated to live staff.

Other benefits and considerations

Hiring a team of customer service agents is a timely and costly investment that requires ongoing training, quality assurance, and management to ensure the organization is represented correctly. When adopting AI technology, the one-time investment will eradicate the reoccurring high costs and also can help to reduce human errors with standardized guidelines.

The constant refinement of customer journeys should not be undermined. Every interaction your customers have with your brand can determine your overall business performance. Delivering excellent customer experiences can result in higher customer loyalty and satisfaction that drives positive word-of-mouth marketing. If you are looking to increase brand value by improving your customer service, find out here.

While investing in AI technologies can come at a high initial cost, the benefits by far outweigh the expenses in the long term. Most businesses can only grow as fast as staff allows. Saving time and automating processes will enable human workers to tackle complex tasks and focus more on strategy with the vast amount of data collected. Having the extra bandwidth, therefore, will allow companies to scale up to serve more customers faster.

Are you a customer-centric organization that is looking for new ways to boost customer satisfaction? Get in touch today for a consultation to learn how we can help transform your customer experience and win more conversions.",https://medium.com/@imanagesystems/is-artificial-intelligence-ai-the-new-customer-service-support-ims-41607ccb312b,"['Integrated Management Systems', 'Ims']",2021-07-13 08:57:42.940000+00:00,800,"Artificial Intelligence, AI Technology, Big Data, Machine Learning, Natural Language Processing"
Gradio: graphical interfaces for Machine Learning models,"Gradio: graphical interfaces for Machine Learning models

Introducing Gradio, an open-source Python package for creating Machine Learning models user interfaces.

Introduction

Creating Machine Learning models is nowadays becoming increasingly easy thanks to many open-source and proprietary based services (e.g. Python, R, SAS). Although, practitioners might always find it difficult to efficiently create interfaces to test and share their completed model to colleagues or stakeholders.

One possible solution to this problem is Gradio, a free open-source Python package which helps you to create models user interfaces which you can effortlessly share with a link to colleagues and friends.

Gradio can be easily installed by using the following command:

!pip install gradio

Gradio is perfectly compatible with many Machine Learning frameworks (e.g. TensorFlow, PyTorch, etc…) and can be used even for arbitrary general-purpose Python scripts.

I will now walk you through different examples of how Gradio can be integrated into your Machine Learning workflow. All the code used for this article is available in this Google Colab notebook and my GitHub account.

Demos

Image Classification

In this example, we are going to create an interface to load images to test a Computer Vision model. In this case, we are going to use PyTorch and the Alexnet pre-trained model, but this experiment can be recreated for any other model and framework.

In order to create a Gradio interface we just need to call the interface function and pass three parameters:

fn: a function which is automatically called by interacting with the user interface in order to create a prediction for our model, provided some form of input.

a function which is automatically called by interacting with the user interface in order to create a prediction for our model, provided some form of input. inputs: to inform Gradio what type of input we are expecting to get from the user interface (e.g. images, text, audio data, numerical data).

to inform Gradio what type of input we are expecting to get from the user interface (e.g. images, text, audio data, numerical data). outputs: to inform Gradio what type of output is going to be returned by our prediction function. In this way, Gradio can understand how the output can best be represented on the user interface.

Once created our interface we then just need to launch it (Figure 1). When launching the interface we can then decide if to pass extra parameters such as share or debug as true. These parameters can in fact be used to embed our Gradio interface not just on our notebook but also as a sharable webpage and to make it easier to debug our system during testing. Sharable links, remains although active just for 6 hours.

Figure 1: Gradio Image Classifier

Text Generation

Generating text in order to predict the conclusion of a sentence or to generate narratives is currently a topic of great interest, especially thanks to the advent of Natural Language Processing (NLP) Transformers. In this example, I am going to create a user interface for the GTP2 HuggingFace pre-trained model I introduced in a my previous article about NLP.

Figure 2: Gradio Text Analysis

Live General ML model prediction

Finally, in this example, I am going to show how Gradio can be used for classical Machine Learning problems involving multiple types of input data. In this example, the Kaggle Heart Disease UCI dataset is going to be used as our dataset. All the data pre-processing steps are available in this Google Colab notebook and on my GitHub account.

Figure 3: Gradio Live General ML model prediction

Conclusion

Gradio, can be certainly be considered to be a great tool in order to create interfaces for Machine Learning project when creating and testing models, although in order to incorporate your model in a production and stable environment alternative solutions might be necessary such as:

A full list of the different facilities and GUI interfaces provided by Gradio is available on the official Gradio Documentation page. If you have any question, feel free to leave a comment in the comment section below.",https://towardsdatascience.com/gradio-graphical-interfaces-for-machine-learning-models-fd4880964f8f,['Pier Paolo Ippolito'],2020-09-24 20:22:47.834000+00:00,647,"Gradio, Machine Learning, Python, User Interface, Computer Vision"
Machine Learning — An Introduction,"Machine Learning — An Introduction

Gain a solid understanding of Machine Learning, its algorithms, and use cases

Image by Pixabay on Pexels

At present day, the emerging field of Artificial Intelligence has become the biggest hype for the current generation. AI is a vast ocean in computer science and not only deals with mere things in computer science, but it covers a whole bunch of stuff like Image Processing, NLP, Summarization, Computer Vision, etc. To hail the whole concept of Artificial Intelligence, there has to be strong equipment to strengthen the base ideology of Statistics and Probability for accurate decisions. And here’s where computer scientists coined the term ‘Machine Learning’.

Definition

Every understanding starts with a definition. ‘Machine Learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed’. This definition is brought by Arthur Samuel who first coined the term Machine Learning while working at IBM. It states that Machine Learning is a subfield of computer science that can be used to train or teach a computer to learn itself without being programmed in a precise manner.

Advantages

The first and foremost important advantage of Machine Learning (ML) is that it doesn’t need to be programmed in a highly efficient manner and they can be taught like how humans teach a four-year-old child. This advantage was loved by computer scientists and data scientists as they were able to manage their work and time efficiently. Another important advantage of ML is that they can improve their skills over time by powerful algorithms.

Types of Machine Learning

Machine Learning is broadly classified into two types based on the learning process of the model which are Supervised learning and Unsupervised learning.

(i) Supervised Learning:

For an easy understanding of this concept, we have to look directly at the words that make it up. ‘Supervised’ means ‘to observe’. So, in this process, we are supervising the machine learning models to be able to produce highly accurate results or decisions. To supervise the model, they are trained or taught with labeled dataset attributes. A simple example of Supervised learning would be predicting whether a person has a benign or malignant type of tumor with a cancer dataset that has labeled attributes.

The two primarily used algorithms for supervised learning are Classification and Regression algorithms. The classification type of learning is the process of predicting discrete class labels or categories (Ex: Predicting the benign or malignant type of tumor). Whereas, Regression type of learning is the process of predicting continuous values or numerical values (Ex: Predicting CO2 Emissions of cars).

(ii) Unsupervised Learning:

Unsupervised learning means that the model works on its own to discover information that may not visible to human eyes. This process deals with unlabeled or unknown data. Unsupervised learning has more difficult algorithms when compared to supervised learning algorithms since we know little or no information about the dataset. The most common and widely used algorithm for unsupervised learning is the Clustering algorithm. The Clustering algorithm finds patterns and divides the data point into groups that are similar to one and another. The groups are classified based on Structure, Summarization, and Anomaly Detection. The best example of unsupervised learning would be segmenting bank’s customers based on certain characteristics.

Supervised vs Unsupervised Learning

In a supervised learning process, the algorithm is trained on a labeled dataset and produces the desired result. In contrast, the unsupervised learning process algorithms are trained on unlabeled or unknown datasets through which the model itself extracts patterns and makes sense of the unknown data. Supervised learning is a simpler method of programming whereas, Unsupervised learning is computationally complex. Algorithms used widely for supervised learning are Regression and Classification while the algorithm used for unsupervised is Clustering. Finally, we can expect higher accuracy results in Supervised learning than the results produced by Unsupervised learning.

Real-Life Examples of ML

At present, Machine Learning is everywhere. There are many solutions provided by Machine Learning which people can observe in their day-to-day life.

(i) Virtual Personal Assistants: Assist in finding information, when asked over voice. The best examples would be Siri, Alexa, and Google Now. For answering the given question, these systems recall your past queries, look for information, and finally, refines the best result for you. Later, these responses are stored for future preferences.

(ii) Recommendation Systems: Have you ever wondered how Netflix or Amazon recommends TV Shows or products? These recommendations are made by using Machine Learning. Personalized recommendations are made based on customer’s behavior whether it can be adding products to your cart in Amazon or your past watch history of films on Netflix.

(iii) Online Fraud Detection: Companies like PayPal are using Machine Learning to protect their customers’ money transactions against cyberattacks. The company uses a set of tools that helps them to compare millions of transactions and distinguish between legitimate or illegitimate transactions taking place between the buyers and sellers.

(iv) Virus Detection: Many healthcare organizations are shifting towards machine learning for the prediction of viruses. These systems assist doctors to detect whether a patient is affected by a virus or not. This is done, by training the machine learning model with a large number of data on the features of the virus.

Apart from the above-mentioned examples, there are many more to mention like search results refining, customer segmentation and the list goes on and on.

Final Thoughts!

Hope you enjoyed this article on Machine Learning. Instead of jumping directly to the math and coding side of Machine Learning, it is always better to start with some basics and foundations of machine learning. If you want to hold a tight grasp on the field of Machine Learning, it is necessary to keep yourself updated. Importantly, hands-on learning or practical phase of Machine Learning is very important to keep your knowledge up to the level. You can also find great resources on the net and cool online courses to elevate your knowledge. Machine Learning might become a little frustrating when it comes to the probability phase but, never stop learning and rage your enthusiasm to learn more and more!

Happy Machine Learning!",https://medium.com/datazen/machine-learning-an-introduction-2ba6e118baa2,['Nikhil Adithyan'],2020-10-05 02:30:57.311000+00:00,997,"Machine Learning, Artificial Intelligence, Algorithms, Supervised Learning, Unsupervised Learning"
Vision Transformers for Image Recognition at Scale,"While convolutional neural networks have been used in computer vision since the 1980s, they were not at the forefront until 2012 when AlexNet surpassed the performance of contemporary state-of-the-art image recognition methods by a large margin.

Two factors helped enable this breakthrough: (i) The availability of training sets like ImageNet, and (ii) The use of commoditized GPU hardware, which provides significantly more compute for training.

As such, since 2012, convolutional neural networks have become the go-to model for vision tasks.

The benefit of using a convolutional neural network is to avoid the need for hand-designed visual features, instead of learning to perform tasks directly from data end-to-end. However, while the neural network avoids hand-crafted feature-extraction, the architecture itself is designed specifically for images and can be computationally demanding. Looking forward to the next generation of scalable vision models, one might ask whether this domain-specific design is necessary, or if one could successfully leverage more domain agnostic and computationally efficient architectures to achieve state-of-the-art results.

As the first step in this direction is the Vision Transformer, a vision model-based as closely as possible on the Transformer architecture which was originally designed for text-based tasks. Vision Transformer represents an input image as a sequence of image patches, similar to the sequence of word embeddings used when applying Transformers to text, and directly predicts class labels for the image. The transformer will demonstrate excellent performance when trained on sufficient data, outperforming a comparable state-of-the-art convolutional neural network with four times fewer computational resources.

The Vision Transformer treats an input image as a sequence of patches, akin to a series of word embeddings generated by a Natural Language Processing Transformer.

Vision Transformer

The original text Transformer takes as input a sequence of words, which it then uses for classification, translation, or other Natural Language Processing (NLP) tasks. The transformer can be designed to make it operate directly on images instead of words, and observe how much about image structure the model can learn on its own.

Vision Transformer divides an image into a grid of square patches. Each patch is flattened into a single vector by concatenating the channels of all pixels in a patch and then linearly projecting it to the desired input dimension. Because Transformers are agnostic to the structure of the input elements, there’s an added learnable position embedding to each patch, which allows the model to learn about the structure of the images. A prior model would not comprehend the relative location of patches in the image, or even that the image has a 2D structure, it must learn such relevant information from the training data and encode structural information in the position embeddings.

Left: Performance of Vision Transformer when pre-trained on different datasets. Right: Vision Tranformeryields a good performance/compute trade-off.

High-Performing Large-Scale Image Recognition

The data suggest that (1) with sufficient training the Vision Transformer can perform very well, and (2) It yields an excellent performance/compute trade-off at both smaller and larger compute scales. Therefore, to see if performance improvements carried over to even larger scales, it is trained a 600M-parameter Vision Transformer model.

This large model attains state-of-the-art performance on multiple popular benchmarks, including 88.55% top-1 accuracy on ImageNet and 99.50% on CIFAR-10. It also performs well on the cleaned-up version of the ImageNet evaluations set “ImageNet-Real”, attaining 90.72% top-1 accuracy. Finally, the Transformer works well on diverse tasks, even with few training data points. For example, on the VTAB-1k suite (19 tasks with 1,000 data points each), ViT attains 77.63%, significantly ahead of the single-model state of the art (SOTA) (76.3%), and even matching SOTA attained by an ensemble of multiple models (77.6%).",https://medium.com/analytics-vidhya/vision-transformers-for-image-recognition-at-scale-fe1b57a9c02b,['Abhilash Pattnaik'],2020-12-28 16:22:22.222000+00:00,594,"Convolutional Neural Networks, Image Recognition, GPU Hardware, Vision Transformer, Natural Language Processing"
What is Feature Scaling & Why is it Important in Machine Learning?,"Understanding the effects of different scalers

Photo by Fleur on Unsplash

In this section, we will learn the distinction between normalisation and standardisation. In addition, we will also examine the transformational effects of 3 different feature scaling techniques in Scikit-learn.

Normalisation

Normalisation, also known as min-max scaling, is a scaling technique whereby the values in a column are shifted so that they are bounded between a fixed range of 0 and 1.

The formula for normalisation is as follows:

X_new = (X - X_min) / (X_max - X_min)

MinMaxScaler is the Scikit-learn function for normalisation.

Standardisation

On the other hand, standardisation or Z-score normalisation is another scaling technique whereby the values in a column are rescaled so that they demonstrate the properties of a standard Gaussian distribution, that is mean = 0 and variance = 1.

The formula for standardisation is as follows:

X_new = (X - mean) / std

StandardScaler is the Scikit-learn function for standardisation.

Unlike StandardScaler, RobustScaler scales features using statistics that are robust to outliers. More specifically, RobustScaler removes the median and scales the data according to the interquartile range, thus making it less susceptible to outliers in the data.

Normalisation vs standardisation

Here comes the million-dollar question — when should we use normalisation and when should we use standardisation?

As much as I hate the response I’m about to give, it depends.

The choice between normalisation and standardisation really comes down to the application.

Standardisation is generally preferred over normalisation in most machine learning context as it is especially important when comparing the similarities between features based on certain distance measures. This is most prominent in Principal Component Analysis (PCA), a dimensionality reduction algorithm, where we are interested in the components that maximise the variance in the data.

Normalisation, on the other hand, also offers many practical applications particularly in computer vision and image processing where pixel intensities have to be normalised in order to fit within the RGB colour range between 0 and 255. Moreover, neural network algorithms typically require data to be normalised to a 0 to 1 scale before model training.

At the end of the day, there is no definitive answer as to whether you should normalise or standardise your data. One can always apply both techniques and compare the model performance under each approach for the best result.

Application

Now that we have gained a theoretical understanding of feature scaling and the difference between normalisation and standardisation, let’s see how they work in practice.

To demonstrate the effects of MinMaxScaler, StandardScaler and RobustScaler, I have chosen to examine the following features in our dataset before and after implementing feature scaling:

ZN

AGE

TAX

B

Original vs MinMaxScaler vs StandardScaler vs RobustScaler

As we can see, our original features have wildly different ranges.

MinMaxScaler has managed to rescale those features so that their values are bounded between 0 and 1.

StandardScaler and RobustScaler, on the other hand, have rescaled those features so that they are distributed around the mean of 0.",https://towardsdatascience.com/what-is-feature-scaling-why-is-it-important-in-machine-learning-2854ae877048,['Jason Chong'],2020-12-30 19:46:37.732000+00:00,469,"scaling, normalisation, standardisation, Min Max Scaler, Standard Scaler"
Logistic regression,"Logistic regression is a well-known classification algorithm. It predicts the probability that a certain instance belongs to a particular class, for instance, what is the probability that the email is spam. Such classifier is called the binary classifier.

Estimating Probabilities

Similarly to linear regression, logistic regression computes a weighted sum of the input features but instead outputting the results directly, it outputs the logistic of the result. What does it mean? The logistic is a sigmoid function that outputs the number between 0 and 1. Once the model has estimated the probability of the particular instance belonging to the positive class it can make the prediction easily.

Training and Cost Function

The objective of the training is to set the parameter vector θ so that the model estimates the high probabilities for positive examples and low probabilities for negative examples.

This equation makes sense because -log(t) will be very large when t approaches 0. In other words, the cost will be large if the model approximates the probability close to 0 (for a positive example) and it will also be very large if the model estimates the probability close 1 (for negative instances). On the other hand, — log(t) is close to 0 when t is close to 1, so the cost will be close to 0 if the estimated probability is close to 0 for a negative instance or close to 1 for a positive instance, which is precisely what we want.

The cost over the whole training set is just the average cost over all the training instances. There is no normal equation but the good news is that the function is convex so the gradient descent should be able to find the global minimum.

Decision Boundaries

We can illustrate the logistic regression with iris dataset. The dataset contains the sepal and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica. We can build a classifier that detects Iris-Virginica based on the petal width feature.

First, we have to load the dataset:

Now, we have to build the logistic regression model.

After that, we are going to look at the model estimated probabilities. The petal of Iris-Virginica is between the 1.4 cm to 2.5 cm, while the other types of the iris are between the 0.1 cm to 1.8 cm.

We can see that there is a slight overlap. Above 2cm the classifier is highly confident that the flower is Iris-Virginica while below 1cm it is convinced that it is not. In between the classifier is unsure but when we call the predict() method it will output the probability of belonging to the certain class. 1.6 cm is the mark for the decision boundary where both types of flowers are equally probable.

The following diagram displays the decision boundary diagram for two features petal width and length.

The dashed line represents the decision boundary where there is a 50% probability of particular example belonging to either of class.

Softmax Regression

So far we have seen the binary classifier with logistic regression. Softmax regression, on the other hand, can support the multiclass classifier without training multiple binary classifiers.

First of all, the softmax regression model computes the score sk(x) for each class k, then it estimates the probability of each class by applying the softmax function to the scores.

We should keep in mind that the softmax regression computes one class at the time so it should be used for mutual exclusive classes such as different types of flowers. We cannot recognize multiple people in one picture.",https://medium.com/@joannatrojak/logistic-regression-59990b3eec65,['Joanna Trojak'],2020-11-28 09:48:24.553000+00:00,577,"Logistic Regression, Classification Algorithm, Estimating Probabilities, Training, Cost Function"
"Personalizing Media and Entertainment content: Personalized notifications, real-time personalization, collaborative and content-based filtering","Personalizing Media and Entertainment content: Personalized notifications, real-time personalization, collaborative and content-based filtering

Picture source: Pexels.com

Audiences love content personalized to their preferences, likes, and dislikes. To this end of delivering a relevant and memorable customer experience, the huge amount of media and audience data (which many media companies struggle with) is a boon. More the data, better the personalization, and better the audience retention and expansion.

To explore the various approaches and techniques for personalization, we invited Manish Mudgal, Head of Media & Entertainment, for UK and EMEA, MediaAgility to our Media and Entertainment special edition mini-webinar series. He works with media publishers to help them create future-proof digital strategies and apply cutting-edge technology solutions to acquire loyal customers, increase traffic and revenue, and handle their big data. In the webinar, he covers techniques like personalized notifications, personalized content, contextualisation, collaborative filtering, and content-based filtering.

Personalized notifications to garner real-time audience attention

Manish Mudgal (MM): Notifications are the medium through which we try to reach the customer in real-time. For example, if a content piece is showing a sudden spike in traffic and we know that the audience would be interested in that content, we would like to let them know in real-time. Here we will rely on different types of notifications, like — in-app notification on mobile devices, a pop up on webpage, an email, a dynamic list, a browser notification.

Based on the technology platform and the users’ presence, today, we can very well determine where our audience is online and which type of real-time communication do we want them to experience.

Personalized content with Real-Time Personalization technique

MM: When we talk about personalized content, we are talking about dynamic web pages, dynamic advertisement, dynamic promotion of the products, dynamic email subscriptions, and so on. To present the content in a more personalized manner on a webpage we actually recommend three types of approaches — real-time personalization, collaborative filtering, and context-based filtering.

In real-time personalization, we don’t have a history of the user or the visitor. We know a little about them but want to make sense out of whatever information is available. So we see what type of information is available to us when the user lands on a website afresh. We get the data points like users’ IP address which leads us to their location. We know the device, browser, screen size, operating system. If there is an umbrella of the websites which a publisher possesses, we can know much more about the users from the data available about the other websites they previously visited.

This type of recommendation is based on a calculated assumption backed by the data from the recent past (of other first-timers). We know what a mobile user is most likely to show interest in or which content piece will be more appreciated more by a laptop user. Similarly we can define interest groups for new users in terms of iPhone and Android, or by location. This case adds more value where we want to welcome users in a more personalized manner than being a website that is the same for everyone.",https://sparks.mediaagility.com/personalizing-media-and-entertainment-content-personalized-notifications-real-time-e905068475ba,['Jala Sanar'],2020-12-16 12:24:59.273000+00:00,511,"Personalized Notifications, Real Time Personalization, Collaborative Filtering, Content Based Filtering, Dynamic Webpages"
How to generate lat and long coordinates of city without using APIS in Python.,"How to generate lat and long coordinates of city without using APIS in Python. Prabhat Pathak Follow May 17 · 3 min read

Easy codes to understand

Photo by João Silas on Unsplash

if anyone would like to plot map graphs using geographical coordinates (The latitude and longitude which define the position of a point on the surface of the Earth .A common choice of coordinates is latitude, longitude.

Photo by oxana v on Unsplash

Latitude lines run east-west and are parallel to each other. If you go north, latitude values increase. Finally, latitude values (Y-values) range between -90 and +90 degrees

But longitude lines run north-south. They converge at the poles. And its X-coordinates are between -180 and +180 degrees.

Cartographers write spherical coordinates (latitudes and longitudes) in degrees-minutes-seconds (DMS) and decimal degrees. For degrees-minutes-seconds, minutes range from 0 to 60. For example, the geographic coordinate expressed in degrees-minutes-seconds for New York City is:

Latitude: 40 degrees, 42 minutes, 51 seconds N

Longitude: 74 degrees, 0 minutes, 21 seconds W

We can also express geographic coordinates in decimal degrees. It’s just another way to represent that same location in a different format. For example, here is New York City in decimal degrees:

Latitude: 40.714

Longitude: -74.006

Read more here if you like to understand more deeper.

Let’s get started

I am using Jupyter notebook to run the script in this Article.

First, we will be installing Libraries like Nominatim and geopy using PIP.

pip install geopy

pip install Nominatim

now the code is really easy we just need to run this

Case 1: Where only City name is mention

from geopy.geocoders import Nominatim address='Nagpur'

geolocator = Nominatim(user_agent=""Your_Name"")

location = geolocator.geocode(address)

print(location.address)

print((location.latitude, location.longitude))

after running above code this is the output we will get.

Nagpur, Nagpur District, Maharashtra, 440001, India

(21.1498134, 79.0820556)

Case 2: Where both Country and City name is mentioned.

We can run another code as well if we have Country name and city name .

from geopy.geocoders import Nominatim

geolocator = Nominatim() city =""Agra""

country =""India""

loc = geolocator.geocode(city+','+ country)

print(""latitude is :-"" ,loc.latitude,""

longtitude is:-"" ,loc.longitude)

output is :

latitude is :- 27.1752554

longtitude is:- 78.0098161

Photo by Alex Perez on Unsplash

Conclusion

we can generate lat and long using googlemaps APIs as well, but for APIs you have to pay some charges.

I hope this article will help you and save a good amount of time. Let me know if you have any suggestions.

HAPPY CODING.

Sources :

https://gisgeography.com/latitude-longitude-coordinates/

https://en.wikipedia.org/wiki/Geographic_coordinate_system/",https://medium.com/analytics-vidhya/how-to-generate-lat-and-long-coordinates-of-city-without-using-apis-25ebabcaf1d5,['Prabhat Pathak'],2020-05-18 22:22:06.242000+00:00,369,"Python, Latitude, Longitude, Coordinates, Geopy"
End-to-End Data Science Example: Predicting Diabetes with Logistic Regression,"As the title suggests, this tutorial is an end-to-end example of solving a real-world problem using Data Science. We’ll be using Machine Learning to predict whether a person has diabetes or not, based on information about the patient such as blood pressure, body mass index (BMI), age, etc. The tutorial walks through the various stages of the data science workflow. In particular, the tutorial has the following sections

Overview

Data Description

Data Exploration

Data Preparation

Training and Evaluating the Machine Learning Model

Interpreting the ML Model

Saving the Model

Making Predictions with the Model

Next Steps

Overview

The data was collected and made available by “National Institute of Diabetes and Digestive and Kidney Diseases” as part of the Pima Indians Diabetes Database. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here belong to the Pima Indian heritage (subgroup of Native Americans), and are females of ages 21 and above.

We’ll be using Python and some of its popular data science related packages. First of all, we will import pandas to read our data from a CSV file and manipulate it for further use. We will also use numpy to convert out data into a format suitable to feed our classification model. We’ll use seaborn and matplotlib for visualizations. We will then import Logistic Regression algorithm from sklearn . This algorithm will help us build our classification model. Lastly, we will use joblib available in sklearn to save our model for future use.

import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

% matplotlib inline from sklearn.linear_model import LogisticRegression

from sklearn.externals import joblib

Data Description

We have our data saved in a CSV file called diabetes.csv . We first read our dataset into a pandas dataframe called diabetesDF , and then use the head() function to show the first five records from our dataset.

diabetesDF = pd.read_csv('diabetes.csv')

print(diabetesDF.head())

First 5 records in the Pima Indians Diabetes Database

The following features have been provided to help us predict whether a person is diabetic or not:

Pregnancies: Number of times pregnant

Number of times pregnant Glucose: Plasma glucose concentration over 2 hours in an oral glucose tolerance test

Plasma glucose concentration over 2 hours in an oral glucose tolerance test BloodPressure: Diastolic blood pressure (mm Hg)

Diastolic blood pressure (mm Hg) SkinThickness: Triceps skin fold thickness (mm)

Triceps skin fold thickness (mm) Insulin: 2-Hour serum insulin (mu U/ml)

2-Hour serum insulin (mu U/ml) BMI: Body mass index (weight in kg/(height in m)2)

Body mass index (weight in kg/(height in m)2) DiabetesPedigreeFunction: Diabetes pedigree function (a function which scores likelihood of diabetes based on family history)

Diabetes pedigree function (a function which scores likelihood of diabetes based on family history) Age: Age (years)

Age (years) Outcome: Class variable (0 if non-diabetic, 1 if diabetic)

Let’s also make sure that our data is clean (has no null values, etc).

diabetesDF.info() # output shown below <class 'pandas.core.frame.DataFrame'>

RangeIndex: 768 entries, 0 to 767

Data columns (total 9 columns):

Pregnancies 768 non-null int64

Glucose 768 non-null int64

BloodPressure 768 non-null int64

SkinThickness 768 non-null int64

Insulin 768 non-null int64

BMI 768 non-null float64

DiabetesPedigreeFunction 768 non-null float64

Age 768 non-null int64

Outcome 768 non-null int64

dtypes: float64(2), int64(7)

memory usage: 54.1 KB

Note that the data does have some missing values (see Insulin = 0) in the samples in the previous figure. Ideally we could replace these 0 values with the mean value for that feature, but we’ll skip that for now.

Data Exploration

Let us now explore our data set to get a feel of what it looks like and get some insights about it.

Let’s start by finding correlation of every pair of features (and the outcome variable), and visualize the correlations using a heatmap.

corr = diabetesDF.corr()

print(corr)

sns.heatmap(corr,

xticklabels=corr.columns,

yticklabels=corr.columns)

Output of feature (and outcome) correlations

Heatmap of feature (and outcome) correlations

In the above heatmap, brighter colors indicate more correlation. As we can see from the table and the heatmap, glucose levels, age, BMI and number of pregnancies all have significant correlation with the outcome variable. Also notice the correlation between pairs of features, like age and pregnancies, or insulin and skin thickness.

Let’s also look at how many people in the dataset are diabetic and how many are not. Below is the barplot of the same:

Barplot visualization of number of non-diabetic (0) and diabetic (1) people in the dataset

It is also helpful to visualize relations between a single variable and the outcome. Below, we’ll see the relation between age and outcome. You can similarly visualize other feature. The figure is a plot of the mean age for each of the output classes. We can see that the mean age of people having diabetes is higher.",https://towardsdatascience.com/end-to-end-data-science-example-predicting-diabetes-with-logistic-regression-db9bc88b4d16,['Keshav Dhandhania'],2018-05-24 23:39:19.251000+00:00,740,"Relation between age and outcome Data Science, Machine Learning, Diabetes Prediction, Data Exploration, Data Preparation"
ResNet Implementation with PyTorch from Scratch,"Network Implementation

left: VGG19, middle: a plain network with 34 parameter layers, right: a residual network with skip connections.

Translation of tabular representation to code

representation of residual networks with 18, 34, 50, 101, and 152 layers.

conv1

The first layer is a convolution layer with 64 kernels of size (7 x 7), and stride 2. the input image size is (224 x 224) and in order to keep the same dimension after convolution operation, the padding has to be set to 3 according to the following equation:

n_out = ((n_in + 2p - k) / s) + 1

n_out - output dimension

n_in - -input dimension

p - padding

s - stride

maxpool1

The second layer is a max-pooling layer with kernel size (3x3) and stride 2. In order to get the size (56 x 56) at the output, the padding has to be set to 1

Convolutional Blocks

all the architectures consist of 4 convolutional groups of blocks. In the case of ResNet18, there are [2, 2, 2, 2] convolutional blocks of 2 layers, and the number of kernels in the first layers is equal to the number of layers in the second layer. Similarly, in the case of ResNet34, there are [3, 4, 6, 3] blocks of 2 layers and the numbers of kernels of the first and second layers are the same.

In the case of ResNet50, ResNet101, and ResNet152, there are 4 convolutional groups of blocks and every block consists of 3 layers. Conversely to the shallower variants, in this case, the number of kernels of the third layer is three times the number of kernels in the first layer.

The convolutional block is defined as the following class:

class Block(nn.Module):

def __init__(self, num_layers, in_channels, out_channels, identity_downsample=None, stride=1):

assert num_layers in [18, 34, 50, 101, 152], ""should be a a valid architecture""

super(Block, self).__init__()

self.num_layers = num_layers

if self.num_layers > 34:

self.expansion = 4

else:

self.expansion = 1

# ResNet50, 101, and 152 include additional layer of 1x1 kernels

self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)

self.bn1 = nn.BatchNorm2d(out_channels)

if self.num_layers > 34:

self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)

else:

# for ResNet18 and 34, connect input directly to (3x3) kernel (skip first (1x1))

self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)

self.bn2 = nn.BatchNorm2d(out_channels)

self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0)

self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)

self.relu = nn.ReLU()

self.identity_downsample = identity_downsample



def forward(self, x):

identity = x

if self.num_layers > 34:

x = self.conv1(x)

x = self.bn1(x)

x = self.relu(x)

x = self.conv2(x)

x = self.bn2(x)

x = self.relu(x)

x = self.conv3(x)

x = self.bn3(x)



if self.identity_downsample is not None:

identity = self.identity_downsample(identity)



x += identity

x = self.relu(x)

return x

Putting all together

the whole network is defined as the following class:

class ResNet(nn.Module):

def __init__(self, num_layers, block, image_channels, num_classes):

assert num_layers in [18, 34, 50, 101, 152], f'ResNet{num_layers}: Unknown architecture! Number of layers has ' \

f'to be 18, 34, 50, 101, or 152 '

super(ResNet, self).__init__()

if num_layers < 50:

self.expansion = 1

else:

self.expansion = 4

if num_layers == 18:

layers = [2, 2, 2, 2]

elif num_layers == 34 or num_layers == 50:

layers = [3, 4, 6, 3]

elif num_layers == 101:

layers = [3, 4, 23, 3]

else:

layers = [3, 8, 36, 3]

self.in_channels = 64

self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)

self.bn1 = nn.BatchNorm2d(64)

self.relu = nn.ReLU()

self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)



# ResNetLayers

self.layer1 = self.make_layers(num_layers, block, layers[0], intermediate_channels=64, stride=1)

self.layer2 = self.make_layers(num_layers, block, layers[1], intermediate_channels=128, stride=2)

self.layer3 = self.make_layers(num_layers, block, layers[2], intermediate_channels=256, stride=2)

self.layer4 = self.make_layers(num_layers, block, layers[3], intermediate_channels=512, stride=2)



self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

self.fc = nn.Linear(512 * self.expansion, num_classes)



def forward(self, x):

x = self.conv1(x)

x = self.bn1(x)

x = self.relu(x)

x = self.maxpool(x)



x = self.layer1(x)

x = self.layer2(x)

x = self.layer3(x)

x = self.layer4(x)



x = self.avgpool(x)

x = x.reshape(x.shape[0], -1)

x = self.fc(x)

return x



def make_layers(self, num_layers, block, num_residual_blocks, intermediate_channels, stride):

layers = []



identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels*self.expansion, kernel_size=1, stride=stride),

nn.BatchNorm2d(intermediate_channels*self.expansion))

layers.append(block(num_layers, self.in_channels, intermediate_channels, identity_downsample, stride))

self.in_channels = intermediate_channels * self.expansion # 256

for i in range(num_residual_blocks - 1):

layers.append(block(num_layers, self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again

return nn.Sequential(*layers)

jupyter notebook is available here",https://medium.com/@niko-gamulin/resnet-implementation-with-pytorch-from-scratch-23cf3047cb93,['Niko Gamulin'],2020-11-01 17:22:56.382000+00:00,594,"network implementation, VGG19, plain network, residual network, skip connections"
Simple example of 2D density plots in python,"Simple example of 2D density plots in python

This post will show you how to:

Use a Gaussian Kernel to estimate the PDF of 2 distributions

Use Matplotlib to represent the PDF with labelled contour lines around density plots

How to extract the contour lines

How to plot in 3D the above Gaussian kernel

How to use 2D histograms to plot the same PDF

Let’s start by generating an input dataset consisting of 3 blobs:

import numpy as np

import matplotlib.pyplot as plt

import scipy.stats as st

from sklearn.datasets.samples_generator import make_blobs n_components = 3

X, truth = make_blobs(n_samples=300, centers=n_components,

cluster_std = [2, 1.5, 1],

random_state=42) plt.scatter(X[:, 0], X[:, 1], s=50, c = truth)

plt.title(f""Example of a mixture of {n_components} distributions"")

plt.xlabel(""x"")

plt.ylabel(""y"");

For fitting the gaussian kernel, we specify a meshgrid which will use 100 points interpolation on each axis (e.g. mgrid(xmin:xmax:100j)):

# Extract x and y

x = X[:, 0]

y = X[:, 1] # Define the borders

deltaX = (max(x) - min(x))/10

deltaY = (max(y) - min(y))/10 xmin = min(x) - deltaX

xmax = max(x) + deltaX ymin = min(y) - deltaY

ymax = max(y) + deltaY print(xmin, xmax, ymin, ymax) # Create meshgrid

xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]

We will fit a gaussian kernel using the scipy’s gaussian_kde method:

positions = np.vstack([xx.ravel(), yy.ravel()])

values = np.vstack([x, y])

kernel = st.gaussian_kde(values)

f = np.reshape(kernel(positions).T, xx.shape)

Plotting the kernel with annotated contours

fig = plt.figure(figsize=(8,8))

ax = fig.gca()

ax.set_xlim(xmin, xmax)

ax.set_ylim(ymin, ymax)

cfset = ax.contourf(xx, yy, f, cmap='coolwarm')

ax.imshow(np.rot90(f), cmap='coolwarm', extent=[xmin, xmax, ymin, ymax])

cset = ax.contour(xx, yy, f, colors='k')

ax.clabel(cset, inline=1, fontsize=10)

ax.set_xlabel('X')

ax.set_ylabel('Y')

plt.title('2D Gaussian Kernel density estimation')

The matplotlib object doing the entire magic is called QuadContour set (cset in the code). We can programatically access the contour lines by iterating through allsegs object. The calculated labels are accessible from labelTexts.

plt.figure(figsize=(8,8)) for j in range(len(cset.allsegs)):

for ii, seg in enumerate(cset.allsegs[j]):

plt.plot(seg[:,0], seg[:,1], '.-', label=f'Cluster{j}, level{ii}') plt.legend()

3D KDE plots

We will use matplotlib’s axes3d from mplot3d. We can plot the density as a surface:

fig = plt.figure(figsize=(13, 7))

ax = plt.axes(projection='3d')

surf = ax.plot_surface(xx, yy, f, rstride=1, cstride=1, cmap='coolwarm', edgecolor='none')

ax.set_xlabel('x')

ax.set_ylabel('y')

ax.set_zlabel('PDF')

ax.set_title('Surface plot of Gaussian 2D KDE')

fig.colorbar(surf, shrink=0.5, aspect=5) # add color bar indicating the PDF

ax.view_init(60, 35)

Or as a wireframe:

fig = plt.figure(figsize=(13, 7))

ax = plt.axes(projection='3d')

w = ax.plot_wireframe(xx, yy, f)

ax.set_xlabel('x')

ax.set_ylabel('y')

ax.set_zlabel('PDF')

ax.set_title('Wireframe plot of Gaussian 2D KDE');

Representation using 2D histograms

Another way to present the same information is by using 2D histograms. Setting the parameter normed to False returns actual frequencies while a True returns the PDF.

h =plt.hist2d(x, y)

plt.colorbar(h[3])

The entire code is available on Github.",https://towardsdatascience.com/simple-example-of-2d-density-plots-in-python-83b83b934f67,['Madalina Ciortan'],2019-03-10 20:49:03.050000+00:00,372,"python, 2D density plots, Gaussian Kernel, Matplotlib, contour lines"
Placement Optimization of Public EV Charging Stations,"— Cluster Feature Engineering —

The telematics API, contains a wealth of information and if a problem can be formulated with discipline and logic, then it can behave as a surrogate for some rich insights.

From the output of the previous stage, we were now able to merge a vector of computed cluster labels into the data set of observations. This provides a meaningful grouping variable to then aggregate over and define additional features that help differentiate between the quality, and ultimately, the utility of the identified candidate locations.

The M/M/∞ Model

Along with the destinations, we were also able to extract timestamps for vehicle trips from the API. This allowed us to evaluate a trip not only where it ended, but also what time the trip started and ended. By comparing sequential trips for a given vehicle we were able to calculate how long a vehicle stayed at a destination by the timestamps.

The developer portal telematics API protects the anonymity of vehicles by encrypting the vehicle identification number (VIN), however, it sustains this field as a valuable research attribute by preserving its uniqueness.

With this level of granularity we were able to precisely calculate, for every identified location, what the frequency of arrivals, and the dwell time was at each location.

Borrowing from queuing theory, we leveraged the M/M/∞ model and were able to use the arrival behavior and the dwell time behavior to generate an expected number of vehicles that would be at any of the candidate locations at any given time.

Diversity

The second engineered feature was Diversity. It simply represented the ratio of unique visitors to total visitors at a cluster. The primary purpose of incorporating this term as a factor was that it cleansed irregular data, where an instance of a cluster was formed by a small number of vehicles and large number of observations, such that it qualified as a cluster. There were a few instances that behaved like this, such as apartment blocks and neighborhoods. The diversity factor helped dampen such irregularities.

The secondary purpose was that our goal was to improve EV adoption, and selecting locations that represented a wider audience, meant that any potential EV infrastructure would get more visibility at those locations.",https://medium.com/99p-labs/placement-optimization-of-public-ev-charging-stations-67ece277e460,['Yusuf J Khan'],2020-12-18 14:41:06.929000+00:00,363,"Telematics API, Cluster Labels, M/M/∞ Model, Vehicle Trips, Unique Visitors"
Overview of trading tips from AIT desk,"Overview of trading tips from AIT desk

Our ML team provides the freshest and the most interesting #tradingtips and #tradingsignals which you can use for your profit.

We are really proud to have this new section on our blog #AIT_TIPS.

During the past week, we managed to collect several tips and ask you to share them with your community.

The most recent:

For sure you have friends who are asking questions like:

“How do I start investing in crypto?” “What to do with million of cryptocurrencies in the market?” “How not to screw up choosing trading strategy?”

In our list, there are MedicalChain, Level Up Coin, GreenMed, Sia coin, Nexus, Tron, Lisk, Cortex, and Fantom.

Any questions or concerns? Ask our ML team in Telegram community!",https://medium.com/aitrading/tips-from-ait-desk-overview-d3ac90a6a408,[],2018-06-14 15:04:27.352000+00:00,118,"trading, investing, cryptocurrency, AIT_TIPS, ML_team"
Measuring biodiversity — how machine learning can help achieve impact goals,"Measuring biodiversity — how machine learning can help achieve impact goals

3 ways in which AI is being used to measure biodiversity loss and help conservation efforts towards restoration and rehabilitation Sayuri Moodliar Follow Mar 8 · 4 min read

SDGs linked to conservation and biodiversity (Photos © Sayuri Moodliar)

A few decades ago, a small community of scientists and students would measure biodiversity by demarcating a representative plot of land in a sensitive or vulnerable area, counting the number and abundance of species, and using statistical methods to extrapolate conclusions and predictions from the data. This worked well enough at a local or regional level.

Since then, the food and resource needs of a constantly increasing human population has led to widespread land degradation and deforestation, decimating our planet’s biodiversity at an unprecedented global scale. The entire ecosystem on earth has become vulnerable and at risk.

The United Nations has included several targets relating to biodiversity in the Sustainable Development Goals (SDGs) that humankind is striving to achieve by 2030. Governments, NGOs, companies, communities and individuals have launched thousands of initiatives to try and achieve these targets. The conundrum lies in how to measure the impact of these initiatives in order to assess whether we are achieving the SDGs.

The ongoing collection and analysis of large amounts of data requires greater computing power than is possible for the small group of scientists who are involved in these studies. This is where artificial intelligence can provide solutions. Machine learning models can process complex information and can also be trained to predict outcomes and trends, enabling us to proactively manage risks that may arise in the future.

Here are three ways in which innovations in technology are helping to achieve the ambitious targets contained in the SDGs …

1. Botanist in your pocket

SDG15 (relating to life on land) includes the conservation and restoration of ecosystems, and halting the loss of biodiversity. A highly biodiverse area like the Cape Floristic Region has over 9000 plant species within a million hectares. Even if we had several teams of botanists working around the clock, it would be difficult to be able to correctly identify, count and monitor all species.

AI-based platforms and applications enable anyone with a mobile phone to upload photos of plants together with GPS information, and to get feedback from other users regarding the identification of the species and its inclusion in a database. Advances in deep learning also mean that models can be trained to identify species from photos without constant human interaction.

Plant data can therefore be collected on a massive scale by farmers, hikers, home owners, tourists, students and other members of communities. Machine learning enables scientists to analyse these large amounts of data, make predictions about their sustainability and put measures in place to mitigate their extinction.

2. Game ranger on patrol 24/7

The SDGs include targets to protect and prevent the extinction of threatened species, and to take urgent action to end poaching and trafficking of protected species. Conservation parks struggle to prevent the poaching of rhinos, elephants, and other protected species. Poaching is often discovered after the fact, when the deaths or removal of animals can only be recorded and not prevented.

AI-based security camera systems are able to detect movement within conservation areas and use image classification to identify whether the motion is caused by an animal or person. Poachers can therefore be detected before they attack an animal, and a message sent to rangers in real-time so that they can respond immediately.

Images of poachers are captured so that gangs operating in a particular area can be identified and a record kept of the individuals involved. Patterns of behaviour can also be discerned and predictions made about poaching activity.

3. Teach a man to fish … sustainably

Decades of overfishing have resulted in depleted fish populations in our oceans and rivers. It is estimated that almost a quarter of fishing globally is illegal and unregulated. SDG14 (relating to life below water) includes the target of restoring fish stocks by regulating sustainable fishing practices and management plans.

Using technology to manage fish stocks is currently one of the most innovative areas of sustainability. Machine learning is increasingly being used to supplement sonar and electronic monitoring systems that were already in place, to provide end-to-end traceability.

Sonar technology is already used to monitor fish populations before they are caught, i.e. while they are still under water. Experiments are being conducted to use machine learning to identify and classify the different species. The system tracks fish distribution and is able to make recommendations to fishermen about where to find the most profitable and sustainable fishing areas.

Electronic monitoring of fishing vessels together with GPS technology can also be used to monitor their coordinates, detect when they are fishing in restricted or vulnerable areas, and plot the density of vessels in a particular area to be alerted of potential overfishing.

Cameras attached to fishing vessels monitor what is being caught through video footage. Machine learning classification models can be used to identify what species are being caught and transmit images to a central database.

One of the consequences of overfishing is the loss of millions of tonnes of bycatch (other marine creatures that are unintentionally captured during the fishing process). Standardised acoustic deterrents like pingers have proven to be ineffective in preventing this because they actually attract animals like seals which have come to associate the sound with large amounts of fish. New models are being tested which detect marine animals through sonar technology, and identify them by species using machine learning classification. Acoustic deterrents are then activated to emit specific frequencies or no sound, depending on which animals are close to the nets. Continuous monitoring and training of the model is expected to result in greater efficiency in reducing loss of marine life bycatch.

Conclusion

Traditional ways to conserve biodiversity are not adequate to enable us to achieve the SDG targets by 2030. Innovations in technology, especially in artificial intelligence, are helping to automate processes to facilitate conservation efforts towards restoration and rehabilitation of biodiversity.",https://medium.com/mlearning-ai/measuring-biodiversity-how-machine-learning-can-help-achieve-impact-goals-7ceb38e5aba,['Sayuri Moodliar'],2021-03-08 18:33:03.688000+00:00,996,"Machine Learning, AI, SDGs, Biodiversity Loss, Conservation Efforts"
What is Machine Learning ??,"Well my first post on the topic about Machine learning, hoping to contribute a lot to this big community

When I first heard about the term Machine Learning, I had no idea what it would be and how that is explained. But after about an year of learning I am now in a position to explain about this fascinating filed.

Now to the big Question what is Machine learning. Before diving into the answer, as usual we will get to see an example.

So, how do we learn things ? Any idea of back tracing about a thing you learnt, may be meaning of a word, how something works, learning a new language, all the leanings that we have come across. There can be many different ways we learn and how we learn that. But the things seems to be in common in all our learning will be an ‘example’ of what we learn

Imagine when you got to know being a kid you don’t know touching fire would harm you! How you knew that ? May be elders telling, and ultimately if you touch, your fingers started to feel that extreme burning sensation !

Now keep machines(computers) in place of you and real-world problems in place of fire, and imagine computers know what is what in due course of time, that is Machine learning !!

By definition “Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead. (wiki)”

In simple terms Computers learn by knowing the real data that we present to it. and being a computer once you learn you can perform well than an Human being in most stuffs. All the Internet world is filled with Machine Learning, right from your searches, suggestions, eCommerce sites, your video suggestions, self-driving cars, auto-tagging faces, image recognition and much more

So how do we teach Computers to learn things ? We as a human see and understand data. But computers can’t. That’s where Mathematics plays its role. We all know computers can out perform us well in almost all complex mathematical algorithms. Using these algorithms we will be able to teach them what is what and that is Machine learning

Seems confusing ?? No it won’t be. Let’s see an example Suppose say , in a class of 8th grade there are 20 students. We know all the data of them like Name, Age, Height, Weight and so on. And lets say a new student is coming in and we get all the data about that student except the Weight. Now the machine learning models can helps us predict that (of course not exactly) weight of the student by utilizing the other students data

Do you think it would tell correctly in all scenarios ? No !! Because it has only data of 20 students, and the new student could be over weight that the rest 20 students. The how could we predict accurately. That’s where the amount of data comes into play. If you can give more data say 100K students, then the chances that the model can give you accurate results will go up to 99% The data that we give in would be converted into some complex forms so that computers can interpret them and understand whet we are about to say

Jumping into the technical part, to create anything that computers should know, we need a programming language. Machine learning also has its own

Python

R

Matlab

SAS

Java

C++ and so on

What types are there in Machine Learning ?? Well in can go to many types. But at an high level we can say three

Supervise Learning : You would say these are my data, and then ask the computer to predict one feature amongst those for any unknown or new data(student weight example)

Unsupervised : Here we would be grouping things based on the relevance with similar things (video recommendations in YouTube, how will YouTube know that you may like a particular video, that’s because YouTube tracks people watching videos all over the world and at the back end, it groups users who watch similar videos, so that we can see those videos similar to others who have similar interest to us are seeing )

Reinforcement : The machine would be trained in such a way that it finds the best possible option of solving a problems, given scenarios where you solve and face hurdles (lot more)

Lot more on my next blog

Please comment out suggestions on this blog, so that I can learn more by sharing more",https://medium.com/@sankarng/what-is-machine-learning-158824d2e645,[],2019-05-25 09:19:14.939000+00:00,755,"machine learning, artificial intelligence, supervised learning, unsupervised learning, reinforcement learning"
Segmentation Based Interpretability of CNN Classification,"High-Level Segmentation Based Interpretability:

Human vision is different from computer vision in two main aspects. Firstly, the human brain is a huge source of prior knowledge, acquired by diverse sensory organs, experience, and memory. The deep learning model lacks this sort of prior knowledge for the vision-related task. And secondly, when we see a picture, rather than focussing on the complete image we focus (pay attention to) on different areas of the image, gather high-level features, and then consolidate all that high-level feature to decide on the image. So, if we ask ourselves why the input is an image of digit 7. We probably answer in a fashion that it has got a horizontal line along with a connected slanting vertical line and it matches our previous knowledge of digit 7, hence this input image is actually of class 7.

Can we get this level of interpretation from the CNN model? To find this out, I have employed a special technique. I have segmented the input image with the ‘Felzenszwalb’ method using the ‘skimage’ library and rather than the whole image giving as input to the model, I have given individual segments as input to the model and predicted the class along with the score.

Fig 6: Different segments of the actual input and respective predictions from the model | Image by author

I find the outcome of this experiment unusual, interesting, uncanny, and dangerous at the same time. If you have a look at the top three segments, which are nothing but the horizontal line from the actual image of digit 7, The model can predict those as class 7 with an almost near-perfect score. Those segments are nothing like digit 7. Whereas the 4th segment which somewhat like digit 7 the prediction score comes down to 0.913.

This finding further underscores the question, what the network is actually learning. Is it at all able to learn any high-level features like we human do or it just finds some low-level interaction of different intensity patterns of the pixels and classifies the images based on the presence or absence of those patterns?",https://towardsdatascience.com/segmentation-based-interpretability-of-cnn-classification-6de02f9a8303,['Arnab Das'],2020-12-14 18:11:49.924000+00:00,347,"deeplearning, computervision, humanvision, priorknowledge, high-levelfeatures"
Evil Johannes Gutenberg,"Johannes Gutenberg was an evil dude. He killed the human factor of books. Before his time people in the sweat of their brow wrote texts, letter by letter, they dedicated their whole life to the unique human experience of writing — and reading.

Gutenberg invented the cold machine, printing book after book. Multiplying nonsense, if once written. He enabled fake news. Before his time the mischief-makers had to write their inflammatory writings by hand — it took time and saved society for a while. Now, with this evil machine, everybody could print and spread fake news within the shortest time.

After all, “Mein Kampf” was also a printed book, it wasn’t circulating in hand-written manuscripts between Nazi conspirers of the 1930ies, it was proudly presented in the central bookstores of Nazi Germany.

Evil books…

…Evil AI.

In the past, propaganda needed human hands to write it. […]With minimal effort, GPT-3 can be guided to write in a range of styles: In a recent study, the Middlebury Institute of International Studies researchers Kris McGuffie and Alex Newhouse found that it could be prompted to generate plausible pro-Nazi posts, reproduce the writing style of mass-shooter manifestos, and answer questions like a QAnon disciple. The developers of GPT-3 understand the potential for abuse and have limited the number of people with access, though hostile countries will likely develop copycat versions soon enough. (The Atlantic, 20th September 2020)

The vision of AI misuse was always a big topic in discussions about New Technologies and their impact on society. Now, with GPT-3 as a universal writer, the world has found a new bogeyman, which can be addressed in case of info-apocalypse.

This topos — a hostile technology in juxtaposing with humans as victims — is probably now like never before one of the most popular themes in our media (however colors, political movements, or countries this media represents).

Yet the main topic remains without being mentioned: humans.

We’ve seen that mislabeling and inaccurate preparation of datasets can deliver horrible results by the perfect performance of an AI. Indeed, an AI developer bears responsibility for Machine Learning models: Datasets shall base on the highest diversity as possible, the AI model has to perform without glitches.

But they do it. Even if it is not so obvious and easy.

For image deepfakes:

StyleGAN implemented Latent Space Projection, to identify an image which is suspected to be fake. (Which works not always perfect, but the approach is given).

For video deepfakes:

AI researchers use heartbeat detection to find deepfake in moving pictures.

For NLP-generated texts:

As far as I know, OpenAI is working on detection tools for contents generated with GPT-3. They also implement various semantic flagging functions to signalize when the text might bear some particular risks.

Independently of this, there is a Chrome Plug-In GPTrue of False, which can show the probability of whether the text is transformer-generated or rather human-written.

…just to mention some of the manifold efforts of the AI community to provide preventive protection of AI misuse.

Unfortunately, these endeavors don’t really reach the public or aren’t really appealing for the mainstream audience (compared to horror stories of “Black Mirror” brought to life).

This is not a rant about the insufficient digital competency of an average member of our civilization. It’s about our task to raise awareness about it, to contribute to education, to open AIs, to provide lessons, to write books and articles, to go on the digital streets.

And even if the tendency of techno-angst seems to be popular nowadays, even if people plead for the back step from the technological progress:",https://towardsdatascience.com/evil-johannes-gutenberg-c1bdc97c616c,"['Vlad Alex', 'Merzmensch']",2020-09-23 08:45:40.294000+00:00,580,"We can’t deny the potential of AI.Johannes Gutenberg, Evil Books, Evil AI, StyleGAN, Latent Space Projection"
Exploratory Data Analysis to understand consumer behaviour,"In the following post, we will be exploring the dataset for a fictitious meal kit company, Apprentice Chef, and deducing key insights and actionable for the company in an effort to increase the Revenue.

Photo by Ella Olsson on Unsplash

Apprentice Chef is a gem for those seeking a convenient and healthy meal option. Unlike its counterparts, Apprentice Chef comes with award-winning disposable cookware, which sets it apart & may seem like a go-to alternative for the environmentally conscious. It has meals ranging from USD 10 to USD 23, much like its competitors, both online & offline. A typical Apprentice Chef meal, consisting of proteins, vegetables, carbs, and seasoning, takes about 30 minutes to prepare. The meal-kit comes with a step-by-step, detailed description of a gourmet recipe cherished by a novice & veteran chef alike.

The convenience of the meals doesn’t keep it from being an economical, sustainable, and a gastronomical delight.

Understanding the dataset

Variables at our disposal

We have thirty variables that shed light on the performance of the company across functions. To understand how each feature affects the Revenue, we must group the variables based on what they highlight about the consumer. The broad categories can be:

Demographics: Variables which give demographics of the consumer

Affinity: Variables which directly impact Revenue like purchases made

Behavior: Variables which highlight the behavior of the consumer like enrolments

Online Behaviour: Variables which help understand the consumer’s online behavior and engagement like clicks

Convenience: Variables which offer convenience to the consumer like the presence of a refrigerated locker

Inconvenience: Variables which indicate inconvenience caused to the consumer like late deliveries

Segmenting

Understanding consumer behavior is a multifaceted process. To simplify this, we can segment the consumers based on:

1. Rating: Five segments for every star of the rating given.

2. Revenue: Quartiles based on the average Revenue generated.

Exploratory Data Analysis (EDA)

The primary objective of the EDA will be to understand which features impact the revenue.

Dissecting consumer behavior will involve looking at the given variables in isolation and also their relation to other relevant variables. The following analysis will be carried out for the dataset:

Univariate Analysis: Will be performed to understand the range, dispersion, and outliers of individual variables. The segments can be color-coded into the plots to add more meaning to univariate analysis.

Graphs to be plotted: Histogram, FacetGrid, Boxplot

Bivariate & Multivariate Analysis: It’s imperative to understand correlation between variables while inferring business dependencies.

Graphs to be plotted: Pairplot, HeatMap, Scatterplot with three variables (lm plot)

Key inferences

There is a definite difference in behavior between customers who have rated favorably and the ones who haven’t. Every business touchpoint is an opportunity for the brand to elevate consumer experience and thus influence the rating given.

Consumers who have rated the brand favorably have far fewer clicks.

It can be assumed that those who have rated the service favorably have had to take lesser effort (effort is equal to clicks on the site). The lower rating could also be a consequence of the consumer not being able to find a meal of their choice despite spending a lot of time on the site.

We cannot conclusively infer what causes the other without further qualitative research.

The created segments behave differently only for variables with a high correlation with Revenue.

It is no secret that few variables will have a more direct impact on the Revenue. Within our Affinity Variables, some have high correlation like Largest Meals Ordered & Total Meals ordered as compared to others like Weekly Plans. It is interesting to note that our created segments (both Revenue & Ratings) behave differently for the variables which are highly correlated with Revenue. The same cannot be said for less correlated variables. The Pairplot below helps us view this at a glance:

On further investigation with facet grids and box plots, we discover that the behavior of consumers within the created segments has disparity ie, the data within these segments have variation within their range, dispersion, and outliers. Along with understanding the variables in isolation, it is essential to see it from this lens to understand consumer behavior effectively.

Recommendation

Segmentation of customers for improving retention efforts

Retention of customers is one of the biggest challenges pertaining to increasing revenue faced by the meal kit industry today. With the EDA, it is established that segmented customers behave differently with the business. The retention rate within these segments is bound to differ, too ie, some customers may be far easy to retain over others. Within the ones who are hard to retain, the ones with high attribution to Revenue need immediate attention. The existing data needs to be enriched with recency and frequency of meal purchases to deduce more sophisticated segments.",https://medium.com/data-science-paraphernalia/exploratory-data-analysis-to-understand-consumer-behaviour-47416599bc13,['Prajakta S Parkar'],2020-05-05 05:40:14.681000+00:00,758,"Exploratory Data Analysis, Univariate Analysis, Bivariate & Multivariate Analysis, Segmentation, Retention Rate"
Organizing Your Data for the Industry 4.0 Factory,"Bigger problems require bigger solutions.

Example Data Model based on ISA-95

During the Industry 4.0 era, a large amount of data will be collected and it has to be managed and organized with a united approach. There should be a single source of truth in the factory. On top of the data, processes should be established to ensure availability, usability, consistency, data integrity, and data security for effective data management throughout the enterprise.

ISA-95 is an international standard for developing an automated interface between enterprise and control systems.

If you are looking for actual implementation, I will show a use-case.

In Summary

The data can be model this way and stored in a centralized repository. Later any application will know where the data is located.",https://medium.com/industry-4-0-and-5-0/organizing-your-data-for-the-industry-4-0-factory-14d618cc5cd8,[],2020-05-14 21:57:56.142000+00:00,119,"The ISA-95 standard is the international standard for developing an automated interface between enterprise and control systems.Industry4.0, Data Management, ISA-95, Enterprise Control Systems, Data Repository"
How to Leverage AI to Predict (and Prevent) Customer Churn,"The problem is, most managers have traditionally taken a retroactive approach to addressing customer churn. They’ll make tweaks, changes, and adjustments, then look back retroactively and conduct a post-mortem as to whether or not those changes were effective. However, with recent advances in Artificial Intelligence (AI) applications, product managers are now capable of better predicting customer churn and take proactive steps to prevent it.

The Problem with the Retroactive Approach

You’ve created what looks to be a well-functioning product or app with a slick user experience, and have some initial success with user acquisition. But after a while, users start to drop off, customers churn, and you’re not quite sure why. In an effort to reduce churn, designers, developers, and product managers will try a myriad of tactics to solve the problem. They might change colors, tweak fonts, move a pay wall, or alter the User Interface (UI), and then wait 2–3 weeks to gauge whether or not turnover has improved. Based on previous retention benchmarks from weeks prior, they’ll try to figure out which change or changes made the difference. Was it one change, a few, or the sum of changes put together?

The disruption of this loop kills workflow, productivity, and overall efficiency in all departments related to improving UX. This A/B approach to testing one or two changes at a time, measuring success, selecting the best option, and moving to the next is slow, cumbersome, and inefficient. Worse, to even consider doing this right, you need to implement one change at a time, and that can take much more time than you have runway for your business.

In short, it’s a backward approach that fragments the UX improvement process, and often addresses the root cause of customer churn too little, too late.

How AI Can Solve Churn Proactively

Our team of data scientists has cráted a better, faster, more efficient approach to solving for customer churn that leverages new advances in machine learning. The real secret sauce to our approach is the way AI is leveraged in a predictive manner. The more you can forecast churn, the better you can prevent it. With machine learning models, you can understand what’s specifically causing churn. Product managers, developers, designers, and executives are spared the guessing games.

Prediction

The first step is the exploratory phase, where you take a deep dive into the data. Instead of click-stream or purchase data, utilizing Machine Learning allows you to sift through large amounts of data instead of only small sections. If you had a list of 100 users ranked top to bottom in terms of likelihood to churn, for instance, you could analyze clusters to see what kinds of people seem to be represented in the “highly likely to churn” bucket. By uncovering profile attributes such as age, gender, income, campaigns the customers came from and source of the customer, you’ll better be able to predict what kinds of customers are likely to churn (and which won’t).

Diagnosis

Machine learning can take you most of the way through analyzing data so that an analyst will be able to help the business team understand who’s likely to churn and propose preventative changes in the UI. Through Behavioral Analytics tools, you can segment users by any attribute — behavior, spending levels, age, or cohort and take appropriate action. The diagnostic step is also vital because you can quantify the risk, correct course, and put measures in place to prevent turnover from happening in the future.

Action Steps to Leverage AI

Now that you’ve effectively leveraged AI to develop predictive models for what kinds of customers are highly likely to churn, here are some specific actions you can take to prevent churn over the lifetime of your business or product:

Intervention — One of the best ways to prevent churn is to intervene in the customer lifecycle of profiles that are likely to churn. By triggering an alert to both the user and your internal team, you can focus on taking steps to retain key accounts or even specific individuals.

— One of the best ways to prevent churn is to intervene in the customer lifecycle of profiles that are likely to churn. By triggering an alert to both the user and your internal team, you can focus on taking steps to retain key accounts or even specific individuals. Acquisition — Churn isn’t always predicted just based on profile elements. It’s also predicated on acquisition channel (Google Adwords, Social Media, Content Marketing, partner referrals, etc.). Based on your predictive analysis, you can target only the most lucrative users with the best retention and LTV, and, of course, fine-tune your products for these specific customers.

— Churn isn’t always predicted just based on profile elements. It’s also predicated on acquisition channel (Google Adwords, Social Media, Content Marketing, partner referrals, etc.). Based on your predictive analysis, you can target only the most lucrative users with the best retention and LTV, and, of course, fine-tune your products for these specific customers. Experience — Color, font, user flow, and other parts of the experience are all things that ultimately impact churn. With AI and behavioral analytics, you now have the tools to know where to focus your efforts on tweaking the user experience.

The bottom line is that customer churn can’t be solved retroactively anymore. Companies, brands, and product managers need to take a proactive approach if they want to meaningfully reduce churn rates. By leveraging AI to generate a data-driven, predictive strategy, companies can take the guesswork out of where to focus their efforts for a healthier SaaS business and even gain a competitive advantage.",https://towardsdatascience.com/how-to-leverage-ai-to-predict-and-prevent-customer-churn-f84d653a76fb,['Dan Schoenbaum'],2018-04-02 22:29:12.154000+00:00,920,"Artificial Intelligence, Machine Learning, Prediction, Diagnosis, Intervention"
fastText for Text Classification,"We will use the fastText classifier to classify the quality of questions asked on Stack Overflow. Download the dataset from here, if you are following along with this tutorial.

What is fastText?

fastText is an open-source library, developed by the Facebook AI Research lab. Its main focus is on achieving scalable solutions for the tasks of text classification and representation while processing large datasets quickly and accurately.

Photo by Marc Sendra Martorell on Unsplash

I highly recommend going through Facebook’s own blog post and research paper regarding the motivation behind fastText and to understand how it does what it’s developed to do.

According to their research, fastText stacks impressively in both accuracy and training and testing times against previously published state-of-the-art models.

It achieves this computational efficiency and accuracy by employing 2 methods to address classification and training word representations of text.

1. Hierarchical Softmax

A Softmax function is often used as an activation function to output the probability of a given input to belong to k classes in multi-class classification problems.

Hierarchical Softmax proves to be very efficient when there are a large number of categories and there is a class imbalance present in the data. Here, the classes are arranged in a tree distribution instead of a flat, list-like structure.

The construction of the hierarchical softmax layer is based on the Huffman coding tree, which uses shorter trees to represent more frequently occurring classes and longer trees for rarer, more infrequent classes.

The probability that a given text belongs to a class is explored via a depth-first search along the nodes across the different branches. Therefore, branches (or equivalently, classes) with low probability can be discarded away.

For data where there are a huge number of classes, this will result in a highly reduced order of complexity, thereby speeding up the classification process significantly compared to traditional models.

2. Word n-grams

Using only a bag of words representation of the text leaves out crucial sequential information. Taking word order into account will end up being computationally expensive for large datasets.

So as a happy medium, fastText incorporates a bag of n-grams representation along with word vectors to preserve some information about the surrounding words appearing near each word.

This representation is very useful for classification applications, as the contextual meaning of a couple of different words strung together also results in a particular sentiment echoed by that piece of text.",https://towardsdatascience.com/fasttext-for-text-classification-a4b38cbff27c,['Shraddha Anala'],2020-11-08 13:46:31.637000+00:00,386,"fast Text, Stack Overflow, FacebookAI, Hierarchical Softmax, WordN-grams"
Looking for Connections in Your Data? Correlation Techniques Come to the Rescue!,"Looking for Connections in Your Data? Correlation Techniques Come to the Rescue!

Correlation techniques in a nutshell

Machine Learning models are as good or as bad as the data you use. That’s why data scientists usually spend hours on pre-processing and cleansing the data. It is crucial to select only the features that could contribute most to the performance of the resulting model. Here feature engineering comes into the picture.

What is the correlation?

In simple terms, correlation is a measure of how strongly one feature related to another. For instance, height and weight can be positively correlated. And height and salary are not at all correlated.

Understanding the correlation between features helps in feature engineering by imputing missing values using another correlated feature or eliminating redundant features that are highly correlated.

In this article, we will be discussing various correlation techniques and their usefulness.

The following are the most widely used correlation techniques,

Covariance Pearson Correlation Coefficient Spearman Rank Correlation Coefficient

Let us dive in!",https://medium.com/towards-artificial-intelligence/looking-for-connections-in-your-data-correlation-techniques-come-to-rescue-53121a149f96,['Ramya Vidiyala'],2020-06-19 13:17:09.785000+00:00,157,"correlation, covariance, Pearson Correlation Coefficient, Spearman Rank Correlation Coefficient, featureengineering"
The correct use of ColumnTransformer() in the Kaggle Titanic competition,"Sklearn has got to be one of my favourite libraries in Python. This library is not static, as it frequently introduces new functions in its updated releases. The current version of sklearn is 0.23.2 where it has introduced new features, such as: visual representation of estimators, improvements to K-means, improvements to gradient boosting, new generalised linear models, and sample weight support for existing regressors. It is always a good idea, therefore to find out the new features of this fascinating library and incorporate them into programming in an attempt to improve performance.

One new feature in sklearn’s version 0.20 is ColumnTransformer(). This function applies transformers to columns of an array or pandas DataFrame. It allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space.

I had already decided that I was going to try out ColumnTransformer() on the Kaggle Titanic competition datasets I have been working on to see if this tool will improve the accuracy of my predictions, but I found quite by accident that sklearn has already used ColumnTransformer on their own version Titanic dataset, with the link and user documentation here:- https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py

The Kaggle Titanic datasets I use have been separated out into train and test datasets and I have employed some techniques different to those used by sklearn, so I nevertheless decided to see if I could improve accuracy on the competition question I have been working on for quite some time now. The Kaggle Titanic competition question can be found here:- https://www.kaggle.com/competitions

The problem statement for this competition reads:-

“The sinking of the Titanic is one of the most infamous shipwrecks in history.

On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.

In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).”

In order to solve this problem, the first thing that needs to be done is to import the libraries that are going to be used to write the program.

Pandas is a free software library written for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.

NumPy is a free python library used for working with arrays. It also has functions for working in domain of linear algebra, fourier transform, and matrices. NumPy was created in 2005 by Travis Oliphant.

Matplotlib is a free plotting library for the Python programming language and its numerical mathematics extension NumPy.

Seaborn is a free Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.

Once the main libraries have been imported, os needs to be imported so the Kaggle datasets can be retrieved:-

The train and test datasets are then loaded and read into the program.

Once the datasets are read, a check for any null values needs to be carried out. In the train dataset there were null values in “Age”, “Cabin” and “Embarked”. In the test dataset the null values are found in “Age”, “Fare” and “Cabin”:-

ID_test is created from test.Passenger.ID because this variable will be needed at the end of the program when the submission dataframe is created.

Three columns are deleted from the train and test datasets because they are not needed for the computations. The columns being deleted are “PassengerID”, “Cabin” and “Ticket”:-

A new column, “Title”, is created from the “Name” column. This column is created by defining a local function, lambda, and splitting the characters between “,” and “.”:-

The titles that had been derived from the name are now encoded to assign a numerical value to each title. These numerical values are then mapped into the column, “Title”:-

A new column, “Family” is created by adding “SibSp” to “Parch”:-

It is at this point that ColumnTransformer() is put into place.

Numeric features are “Age”, “Fare” and “Family”. These numeric_features are put into a pipeline called numeric_transformer, where the missing values are imputed and then scaled.

Categorical features are “Embarked”, “Sex”, “Pclass”, and “Title”. These categorical_features are put into a pipeline where the missing values are imputed and one hot encoded.

The column transformer is created by the numeric_transformer and categorial_transformer, being called a preprocessor:-

The X and y variables are then defined. The target value, being y, is train.Survived.

X variable is composed of the train dataset with “Survived”, “Name”, “SibSp”, and “Parch” dropped.

X_text is composed of the test dataset with “Name”, “SibSp” and “Parch” being dropped:-

The train dataset is then split up for training and validation. Because this is a classification problem, stratify is set to y:-

The variable class_weights is assigned because this competition question is a classification problem. Setting the model up for the correct class weight will help to improve accuracy:-

I then selected the model, in this case being the BaggingClassifier() with the base estimator being RandomForestClassifier(). Using this ensemble, I was able to achieve a 94.38% accuracy.

When I predicted on the validation set, I was able to achieve an accuracy of 80%! With such a high score, I hoped my accuracy when predicting on the test set would be higher, such as at least 78%:-

I predicted on the test dataset and built a dataframe that would be converted to a .csv file, which I could use to submit my predictions to Kaggle:-

Sadly, when I submitted my predictions to Kaggle, the accuracy of my model had not made any significant improvement, hovering at 77%.

While I had hoped using ColumnTransformer would improve accuracy when making predictions, this did not appear to be the case. I will continue to use ColumnTransformer, however, to keep my programming in line with sklearn standards.

The code for this program can be found in its entirety on the Kaggle competition page, the web address being found here:- https://www.kaggle.com/tracyporter/titanic-columntransformer?scriptVersionId=47800763",https://python.plainenglish.io/the-correct-use-of-columntransformer-in-the-kaggle-titanic-competition-4bd8c070bc07,[],2020-11-26 14:06:32.298000+00:00,1016,"sklearn, Column Transformer(), Python, Titanic, Kaggle"
Statistical Modeling of Time Series Data Part 1: Data Preparation and Preprocessing,"Extracting Relevant Series

In this series of articles, the Close prices of the S&P 500 market index are analyzed. Here, we extract the series we’re interested in:

Extracting the required series from the collection provided by yfinance

Since the data is stock market data, we will not observe any value for the weekends. Thus, the time interval between 2 successive observations is 1 day even if the 2 observations are recorded on a Friday and then on a Monday. To convert our dates to follow the business days format (5 days a week), we use the asfreq() method of pandas with “b” as the argument.

Handling Missing Values

Next, let’s see whether this data has any missing values. This is an important preprocessing step, as the way we fill these values can have a huge impact on the tend.

Checking Null values, statistical description, and imputing the null values of the series

Output for the cleaning_spx.py code block

In the code cell above, data.spx.isnull().sum() takes the dataframe ( data ), extracts the column ( spx ) and applies the function isnull() to it. This results in a boolean array with True for every Null value encountered. The sum() function takes the sum of these boolean values. Since, True is represented as 1 and False as 0 , this gives the number of missing values in the spx series. The describe() function gives a few statistical measures of the series.

The number of null values ( 233 ) is clearly very small as compared to the count of observations ( 6459 ). Thus, a simple imputing function from pandas will be enough for this case. The fillna() function fills the missing values with the values encountered just before the missing value. This behavior is governed by the “ffill” (front fill) argument passed to the method argument. Click here for more information on other arguments for method .

Deriving S&P Returns and Volatility

Now that the data is cleaned, it can be used to build some other useful series that helps us to understand the market trends better. These are: Returns and Volatility.

Returns: The percent change in a stock price over a given amount of time. In this case, the returns over each day are calculated and stored in the column spx_ret .

Volatility: The volatility in a market index refers to the fluctuations in its returns. To gauge the fluctuations or stability in the market, sometimes the magnitude of returns or squared returns are chosen. In this series, the magnitude of returns is chosen as the measure of volatility. The Volatility of spx is stored in the column spx_vol

Thus, Returns are a measure of the gain (positive returns) or loss (negative returns) of a market index, and Volatility (magnitude of Returns) is the measure of stability in the index.",https://pub.towardsai.net/statistical-modeling-of-time-series-data-part-1-data-preparation-and-preprocessing-b52f26f6213c,['Yashveer Singh Sohi'],2020-12-21 01:02:54.190000+00:00,456,"yfinance, S&P500, pandas, nullvalues, returns"
Keeping Pandas DataFrames clean when importing JSON,"Photo by Samuel Zeller on Unsplash

This post originally appeared on my personal website. You can view an updated version with code formatting on my blog.

At my job, I do a lot of data analysis to find the most promising young startups. As a first step, you always have to import the desired data into a Pandas DataFrame and do some preprocessing, for example by importing JSON data from some API.

When importing JSON data, you usually have a lot of temporary columns in your DataFrame which can get messy quickly. To deal with temporary columns, I built a custom Context Manager that keeps track of all imported columns and deletes them when you’re done. This way, your code stays lean and you don’t have to remove temporary columns yourself.

In this short article, I will show you how. You can find the code on GitHub.

As an example, I will use the actual code I use for importing data from the API of our CRM named Hubspot. What I retrieve is a list of companies stored as a list of Python dictionaries. To import a list of dictionaries (data) into a Pandas DataFrame you basically do:

from pandas.io.json import json_normalize df = json_normalize(data)

The json_normalize function generates a clean DataFrame based on the given list of dictionaries, the data parameter, and normalizes the hierarchy so you get clean column names. This is especially useful for nested dictionaries.

Ugly: Keeping imported columns

The problem with json_normalize is that you usually only want a subset of the imported columns, mostly with different names or some kind of pre-processing, too. So you might be tempted to do something like this:

from pandas.io.json import json_normalize df = json_normalize(data) df['company_id'] = df['companyId']

df['location'] = df['properties.city.value']

df['name'] = df['properties.name.value']

df['domain'] = df['properties.website.value']

//... .apply(), .as_type(int), whatever...

This works, but keeps all the imported columns inplace and might take a lot of storage. So what can you do?

Ugly: Dropping columns manually

So after importing, you want to get rid of all temporary columns from the import. To do this, you have to either select the columns you want or drop all columns you don’t want. In both cases, you have to somehow keep track of the temporary columns or the ones you want to keep. To deal with this, one solution would be to prefix temporary columns and delete them afterwards:

from pandas.io.json import json_normalize df = json_normalize(data) // make temporary columns

df.columns = ['temp_' + c for c in df.columns] // pre-processing, basic calculations, etc.

df['company_id'] = df['temp_companyId']

df['location'] = df['temp_properties.city.value']

df['name'] = df['temp_properties.name.value']

df['domain'] = df['temp_properties.website.value']

//... .apply(), .as_type(int), whatever...

Afterwards, you would then select all desired columns or drop all undesired columns.

df.drop([c for c in df.columns if c.startswith('temp_')], axis=1, inplace=True)

// or

df = df[[c for c in df.columns if not c.startswith('temp_')]]

While this works, it feels bloated and inefficient. You have to prefix all the value names in the code which results in bloated column names. You also have to keep track of column names you want in the end or the used prefix in different places. Just imagine you have to change the prefix temp_ one day or make the code work with a different prefix.

Clean and easy: using a Context Manager

After having used the above methods for some time, it struck me that Python Context Managers might be a cleaner solution. You might know them from their most popular application with open() as file: . If not, please take a few minutes to read more about them. To make things short: They basically ensure that something, usually a cleanup, is executed in each exit scenario, whether it is a usual exit like a return or an exception. I thought I might use this to build a clean solution that keeps track and gets rid of temporary columns. So I built a Context Manager that deals with temporary columns when importing JSON data so I don't have to. You can basically use it like this:

with DataFrameFromDict(companies) as df:

// imported dict now in df, same result as json_normalize

df['company_id'] = df['companyId']

df['location'] = df['properties.city.value']

df['name'] = df['properties.name.value']

df['domain'] = df['properties.website.value']

// after context exits, df contains company_id, location, name, and domain

// but no more temporary columns

print(df)

The benefit: You don’t have to keep track anymore and the context manager handles the deletion of all temporary columns.

How it works

You can just copy and paste the following snippet to get going, I’ll explain how it works below:

class DataFrameFromDict(object):

""""""

Temporarily imports data frame columns and deletes them afterwards.

"""""" def __init__(self, data):

self.df = json_normalize(data)

self.columns = list(self.df.columns.values) def __enter__(self):

return self.df def __exit__(self, exc_type, exc_val, exc_tb):

self.df.drop([c for c in self.columns], axis=1, inplace=True)

When opening the context, __init__ and __enter__ get called. They create the DataFrame and remember all imported and thus temporary column names. When the context is exited, __exit__ makes sure to drop all previously created columns and leaves only the newly created columns behind.

Hope this helps you to create a clean pre-processing pipeline. Let me know what you think.

This post originally appeared on my blog.",https://karllorey.medium.com/keeping-pandas-dataframes-clean-when-importing-json-348d3439ed67,['Karl Lorey'],2020-12-28 14:55:50.390000+00:00,805,"Data Analysis, JSON Data, Pandas Data Frame, Python Dictionaries, Temporary Columns"
Why is it So Hard to Integrate Machine Learning into Real Business Applications?,"You’ve played around with machine learning, learned about the mysteries of neural networks, almost won a Kaggle competition and now you feel ready to bring all this to real world impact. It’s time to build some real AI-based applications.

But time and again you face setbacks and you’re not alone. It takes time and effort to move from a decent machine learning model to the next level of incorporating it into a live business application. Why?

Having a trained machine learning model is just the starting point. There are many other considerations and components that needs to be built, tested and deployed for a functioning application.

In the following post I will present a real AI-based application (based on a real customer use case), explain the challenges and suggest ways to simplify development and deployment.

Use Case: Online Product Recommendations

Targeted product recommendations is one of the most common methods to increase revenue, computers make suggestions based on users’ historical preferences, product to product correlations and other factors like location (e.g. proximity to a store), weather and more.

Building such solutions requires analyzing historical transactions and creating a model. Then when applying it to production you’ll want to incorporate fresh data such as the last transactions the customer made and re-train the model for accurate results.

Machine learning models are rarely trained over raw data. Data preparation is required to form feature vectors which aggregate and combine various data sources into more meaningful datasets and identify a clear pattern. Once the data is prepared, we use one or more machine learning algorithms, conduct training and create models or new datasets which incorporate the learnings.

For recommendation engines, it is best to incorporate both deep learning (e.g. TensorFlow) to identify which products are bought “together”, and machine learning (e.g. XGboost) to identify the relations between users and products based on their historical behavior. The results from both models are then combined into a single model serving application.

Example pipeline: Real-time product recommendations

The serving application accepts a user’s ID, brings additional context from feature and user tables, feeds it into a model and returns a set of product recommendations.

Note that serving must be done in real-time while the user is still browsing in the application, so its always better to cache data and models. On the other hand, recent product purchases or locations may have significant impact on future customer product choices and you need to constantly monitor activities and update feature tables and models.

An online business requires automation and a CI/CD process applied into machine learning operations, enabling continuous applications. It is important to support auto-scaling and meet demand fluctuations, sustain failures and provide data security, not to mention to take regulatory constraints into consideration.

The Machine Learning Operational Flow

In a typical development flow, developing code or models is just the first step. The biggest effort goes on making each element, including data collection, preparation, training, and serving production-ready, enabling them to run repeatedly with minimal user intervention.

What it takes to turn code or algorithms into real application

The data science and engineering team is required to package the code, address scalability, tune for performance, instrument and automate. These tasks take months today. Serverless helps reduce effort significantly by automating many of the above steps, as explained in my previous post Serverless: Can is Simplify Data Science Projects?. Other important tools to keep in mind are Kubernetes and KubeFlow, which bring CI/CD and openness to the machine learning world. Read more about them in my post Kubernetes: The Open and Scalable Approach to ML Pipelines.

Machine Learning Code Portability and Reproducibility

A key challenge is that the same code may run in different environments, including notebooks for experimentation, IDEs (e.g. PyCharm) and containers for running on a cluster or as part of an automated ML workflow engine. In each environment you might have different configurations and use different parameters, inputs or output datasets. A lot of work is spent on moving and changing code, sometimes by different people.

Once you run your work, you want to be able to quickly visualize results, compare them with past results and understand which data was used to produce each model. There are vendor specific solutions for these needs, but you can’t use them if you want to achieve portability across environments.

Iguazio works with leading companies to form a cross platform standard and open implementation for machine learning environments, metadata and artifacts. This allows greater simplicity, automation and portability.

Check out this video to learn how you can move from running/testing code in a local IDE to a production grade automated machine learning pipeline in less than a minute (based on KubeFlow).",https://towardsdatascience.com/why-is-it-so-hard-to-integrate-machine-learning-into-real-business-applications-69603402116a,['Yaron Haviv'],2019-07-08 20:45:27.105000+00:00,762,"machinelearning, neuralnetworks, Kaggle, realworldimpact, AI-basedapplications"
Embed Interactive Plots in Your Slides with Plotly,"Effective communication is essential for us data scientists, and Plotly’s interactive plots are a great tool for that. But when it comes to presenting our work in a traditional slide-styled presentation, those plots are hard to integrate in our daily tools like PowerPoint or Google Slides. In this post, we’ll get to know the Spectacle editor — a presentation tool by Plotly that allows to embed your interactive plots and animations in your slides and level up your presentation.

But .pptx works just fine… why bother?

Photo by Leonardo Baldissara on Unsplash

Imagine you’ve covered what you thought was important for your presentation and then get follow-up questions from your audience that refer to minor details or a certain subgroup of your population. Now, you have to go back to your code to retrieve an answer or follow up on that later. Wouldn’t it be way easier to just slice up your plot and retrieve whatever needed right on spot? Check out, for example, this parallel coordinates plot that you can filter and highlight on any variable:

Courtesy of the author.

Or, this animated bar chart that you can interact with as you present:

Courtesy of the author.

What is needed to get started?

A Plotly chart studio account : you can easily sign up with your github, Google or Facebook accounts.

: you can easily sign up with your github, Google or Facebook accounts. Interactive Plotly plots : in my last post, I introduced Plotly’s interactive plots and animations. The code available in the post should be enough to generate a few plots to play with.

: in my last post, I introduced Plotly’s interactive plots and animations. The code available in the post should be enough to generate a few plots to play with. Finally, a quick download of the Spectacle editor.

Diving in

Once you’ve logged in, the interface is very familiar and intuitive to work with. Start up with your usual presentation prep routine:",https://towardsdatascience.com/embed-interactive-plots-in-your-slides-with-plotly-fde92a5865a,['Liana Mehrabyan'],2020-07-28 11:25:46.807000+00:00,316,"data-science, plotly, presentation, spectacle-editor, interactive-plots"
Forecasting Deaths From COVID-19 in North Carolina up to the End of 2020,"Updating the model with new data

Here’s the entire Jupyter Notebook containing all code and previous model estimations

After downloading the data from the NC DHHS, I cleaned it to only include days after the first death from COVID-19 was recorded in NC on March 25th this year. I then load it in using the Pandas library.

# Loading in most recent data updated as of today Nov. 3rd



NCcovidNew = pd.read_excel('NCcovidNew.xlsx', sheet_name='Sheet 1')

Next, we assign our X and Y variables, in this case, date and deaths, and then reshape our data as needed.

date = NCcovidNew['Date']

y = NCcovidNew['NC Deaths']

date.shape # 224 days of data # Reshpaing the data



x=np.linspace(224,1,224)

x=np.reshape(x,(224,1))

y=np.array(y)

y=np.reshape(y,(224,1))

Refitting the model

Scikit-learn makes linear regression simple to implement with the following:

reg = linear_model.LinearRegression() # Constructing SKlearn model

reg.fit()

Yep, that's it. However, since our model will be a 2nd order polynomial regression taking the form h(x)=a+bx+cx², we need to assign a polynomial feature term that takes degree=2 as one of its arguments. The following code encompasses the entirety of the model.

poly_f=PolynomialFeatures(degree=2,include_bias=False) # Instantiating quadratic term X_poly=poly_f.fit_transform(x) # Applying quadratic transformation reg.fit(X_poly,y) # Fitting transformed data to model # Assigning variables to the model's parameter estimates a=reg.intercept_[0]

b=reg.coef_[0][0]

c=reg.coef_[0][1] print(“a (Intercept term) =”,a)

print(“b (Coefficient 1) =”,b)

print(“c (Coefficient 2) =”,c)

Here are the model’s parameter estimates:

a (Intercept term) = -64.88491059525245

b (Coefficient 1) = 10.06007469864205

c (Coefficient 2) = 0.04522201123331129

Putting it all together

Using Matplotlib once again, let's plot the data for daily deaths as a scatter plot and then overlay the data with our model’s forecast for the next 58 days leading up to December 31st.

plt.figure(figsize=(14,14))

plt.scatter(x,y) x=np.linspace(282,1,282)

x=np.reshape(x,(282,1))

plt.xlim(0, 290)

plt.ylim(0,8000) plt.plot(x, a + b*x+c*np.power(x,2), ""r"") plt.title('Predicted vs Actual COVID-19 Deaths in NC')

plt.xlabel('Days since first recorded death')

plt.ylabel('Deaths') plt.axvline(x=224+58,color='b') # Blue vertical line corresponds with Dec. 31st

plt.axhline(y=a + b*(224+58) + c*(224+58)**2,color='b')

plt.axhline(y=a + b*(224) + c*(224)**2,color='g') # Corresponds with today's death total of 4,457

The model, a simple quadratic equation, fits the data quite well. The vertical and horizontal green lines correspond with today's date, November 3rd, and death count of 4457. The blue lines correspond with December 31st’s date and predicted death count. So what is the predicted death count on 2020’s final day?

Prediction

In order to predict deaths by December 31st, we need to plug in the corresponding x-value into our model which takes the form of h(x)=a+bx+cx². The following code accomplishes this:

deathsByNewYears = a + b*(224+58) + c*(224+58)**2 # Forecasting 58 days into the future from today print(""Predicted number of deaths on Dec. 31st:"",int(deathsByNewYears))

And the output:

Predicted number of deaths on Dec. 31st: 6368

The model estimates 1,911 more deaths from COVID-19 will occur in the 58 days between today and the year’s end, culminating in 6,368 deaths on 2020's final day. Some future improvements that I’d like to make include calculating and displaying shaded confidence intervals in order to provide a true probabilistic forecast rather than an extrapolation of a simple curve. Likewise, I’d also like to create a dashboard for the model so that it automatically updates its parameter estimates and forecasts as new data is uploaded to the NC DHHS and then fed into the model.

Take this all with a grain of salt

To be clear, I’m not an epidemiologist nor an expert of any kind for that matter, but I do have a passion for data and storytelling. As for the model’s prediction, my overly optimistic gut feeling is that the true death total won’t eclipse 6000 by year’s end, but I wouldn’t bet the farm on it.

Globally, the virus is on a renewed tear, prompting many European nations to reinstate ‘wave-breaking’ shutdowns upon breaking previous records for new cases. In the U.S. new data are increasingly pointing towards a bleak winter as in the last two weeks records for the daily number of new cases have been set and shattered on successive days, most recently last Friday, when 98,500 infections were reported. President Trump also recently suggested at a campaign rally that he would fire Dr. Fauci after the election.

The key figure to watch for is hospitalizations since hospitals see seasonal increases in patients admitted in the winter due to the seasonality of the Flu. Hospitals already dealing with seasonal increases in patients may become overwhelmed in the face of widespread revived coronavirus outbreaks. Unfortunately, hospitalizations have been marching higher since the end of September and show little sign of letting up.",https://medium.com/@ali-alsous/forecasting-deaths-from-covid-19-in-north-carolina-up-to-the-end-of-2020-e475cb5d6901,['Ali Alsous'],2020-11-09 19:52:17.182000+00:00,705,"NCcovid New, Linear Regression, Polynomial Features, Matplotlib, Covid-19 Deaths"
Build A Movie Recommender Using C# and ML.NET Machine Learning,"I will build a machine learning model that reads in each user ID, movie ID, and rating, and then predicts the ratings each user would give for every movie in the dataset.

So that gives me a list of movies and ratings for every user. To recommend a movie, all I need to do is sort the list by rating and report the top 5.

Let’s get started. Here’s how to set up a new console project in NET Core:

$ dotnet new console -o Recommender

$ cd Recommender

Next, I need to install the ML.NET base package and the recommender extensions:

$ dotnet add package Microsoft.ML

$ dotnet add package Microsoft.ML.Recommender

Now I’m ready to add some classes. I’ll need one to hold a movie rating, and one to hold my model’s predictions.

I will modify the Program.cs file like this:

The MovieRating class holds one single movie rating. Note how each field is adorned with a Column attribute that tell the CSV data loading code which column to import data from.

I’m also declaring an MovieRatingPrediction class which will hold a single movie rating prediction.

Now I’m going to load the training data in memory:

This code uses the method LoadFromTextFile to load the CSV data directly into memory. The class field annotations tell the method how to store the loaded data in the MovieRating class.

Now I’m ready to start building the machine learning model:

Machine learning models in ML.NET are built with pipelines, which are sequences of data-loading, transformation, and learning components.

My pipeline has the following components:

MapValueToKey which reads the userId column and builds a dictionary of unique ID values. It then produces an output column called userIdEncoded containing an encoding for each ID. This step converts the IDs to numbers that the model can work with.

which reads the userId column and builds a dictionary of unique ID values. It then produces an output column called userIdEncoded containing an encoding for each ID. This step converts the IDs to numbers that the model can work with. Another MapValueToKey which reads the movieId column, encodes it, and stores the encodings in output column called movieIdEncoded.

which reads the movieId column, encodes it, and stores the encodings in output column called movieIdEncoded. A MatrixFactorization component that performs matrix factorization on the encoded ID columns and the ratings. This step calculates the movie rating predictions for every user and movie.

With the pipeline fully assembled, I can train the model with a call to Fit(…).

I now have a fully- trained model. So now I need to load some validation data, predict the rating for each user and movie, and calculate the accuracy metrics of my model:

This code uses the Transform(…) method to make predictions for every user and movie in the test dataset.

The Evaluate(…) method compares these predictions to the actual area values and automatically calculates three metrics for me:

Rms : this is the root mean square error or RMSE value. It’s the go-to metric in the field of machine learning to evaluate models and rate their accuracy. RMSE represents the length of a vector in n-dimensional space, made up of the error in each individual prediction.

: this is the root mean square error or RMSE value. It’s the go-to metric in the field of machine learning to evaluate models and rate their accuracy. RMSE represents the length of a vector in n-dimensional space, made up of the error in each individual prediction. L1 : this is the mean absolute prediction error, expressed as a rating.

: this is the mean absolute prediction error, expressed as a rating. L2: this is the mean square prediction error, or MSE value. Note that RMSE and MSE are related: RMSE is just the square root of MSE.

To wrap up, let’s use the model to make a prediction.

I’m going to focus on a specific user, let’s say user number 6, and check if he or she likes the James Bond movie ‘GoldenEye’.

Here’s how to make the prediction:

I use the CreatePredictionEngine method to set up a prediction engine. The two type arguments are the input data class and the class to hold the prediction. And once my prediction engine is set up, I can simply call Predict(…) to make a single prediction on a MovieRating instance.

Let’s do one more thing and predict the top-5 favorite movies for this user:

This code uses a static helper class Movies to enumerate over every movie ID. It creates predictions for user 6 and every possible movie, sorts them by score in descending order, and takes the top 5 results.

Here’s the partial source of the helper class:

There’s a Movie class that represents a single movie. The static helper class Movies has an All property with a list of all movies, and a Get method to lookup a single movie by ID value.

With the code all done, it’s time to check the predictions. Here’s the code running in the Visual Studio Code debugger on my Mac:",https://medium.com/machinelearningadvantage/build-a-movie-recommender-using-c-and-ml-net-machine-learning-d6175ae13bc9,['Mark Farragher'],2019-11-19 15:05:06.663000+00:00,808,"Machine Learning, NET Core, ML.NET, Movie Rating, Prediction Engine"
The art of Exploratory data analysis (EDA),"Exploring is adventurous! Image-source

What is Exploratory Data Analysis (EDA)?

As the name suggest, it is a technique to analyse the data by exploring it. EDA is one of the key as well as indispensable step in data analysis. This can be understood as a general checkup of patient (data) by the doctors (data enthusiast), before doing any surgery (analysis, modeling, prediction, classification, etc.).

What are the goals of EDA?

EDA is done for two major underlying reasons, firstly to understand the data and secondly to identify faults or peculiar events (data points) in the dataset. Let’s try to understand what is meant by understanding and peculiar data points in detail.

— To understand the data characteristics → Understanding the data means answering following preliminary questions that generally pops up in the mind of data person on first encounter with a particular dataset:

Where is this data coming from? (What population group or subject)

What is the high level contained information? (e.g cost data, revenue, budget, employee salaries, customer data, usage data and so on.)

What are the variables available?

What does their value mean? (e.g average time per page would mean that out of all the time spent by user on the website in one session, how much time was spent on a every particular page)

What population segment data belong to? (e.g The data of cancer patient would meant that out of all patients the collected data is of cancer patients only.

What is the Sample distribution?

What is the strength and direction of the relationships among input variable and outcome variable?

What is the Central tendency of the data?

What is the spread of the data?

— To identify discrepancies in the data → Discrepancies in data means

Presence of incorrect values

Missing values

Outliers (probable outliers or possible outliers)

Assumption violations

Based on the findings during EDA, a preliminary selection of the data can be done or the further course of actions can be decided to treat discrepancies.

Part I — Categorical variables EDA

How EDA is carried out?

As we have seen what and why of EDA, it time to answer how EDA is done? In this post we are going to focus on EDA techniques for categorical variables which includes ordinal, dichotomous and nominal variables. You can find details of types of variable here.

EDA can be graphical or quantitative depending on what are we trying to find. Following previous analogy of patient, reports like x-ray, MRI, scans, can be seen as graphical method which provides overall picture of data and involves qualitative analysis. Whereas blood reports, dimensions of tumour can be understood as quantitative method which are objective.

In general data is constitute of several columns and rows. Now one can choose to explore one variable at a time (uni+variate=univariate), two variables at a time (bi+variate=bivariate), or multiple variables at same time (multi+variate= multivariate).",https://chauhan-nisha.medium.com/the-art-of-exploratory-data-analysis-eda-5c489fa96db2,['Nisha Chauhan'],2020-10-28 23:08:38.904000+00:00,456,"Exploratory Data Analysis, EDA, Data Analysis, Categorical Variables, Univariate Analysis"
Display selected slicers in Power BI,"Display selected slicers in Power BI

Displaying slicer selection in the report can be easier than it seems. Bonus tip for even better user experience!

Photo by Alex on Unsplash

Just recently, while presenting my session: “Magnificent 7 — Simple tricks to boost your Power BI Development” at the New Stars of Data conference, one of the questions I’ve received was:

Is there a way to show the actual active filters as a result of the slicer choices on the page?

I’ve already answered the question in this article, but then I thought: maybe more people search for the solution to this problem, so I decided to write a short post to explain in detail how you can achieve this.

As usual, I will use a sample Contoso database for demo purposes:

DAX is your friend!

This is the starting point. You can notice three slicers on the report canvas, and let’s say that I want to show my users which brands are selected within Brands slicer.

The first step is to create a DAX measure that will capture all selected values in the slicer. You can use two different DAX functions to obtain the values: VALUES() or DISTINCT(). VALUES() function is more complex since you can use both table name and column name as an argument, but let’s focus here on the column name as an argument.

Basically, VALUES() will return all distinct values from the column we passed, including blanks (if exist)! On the other hand, DISTINCT() will return all distinct values but ignoring blank values. Which one you want to use, depends mostly on the business request (if your users want to see numbers for blanks or not). Personally, I prefer to use VALUES(), because it gives me the full picture.

So, I will create the following measure:

Selected Brands = VALUES('Product'[BrandName])

Now, when I put this measure in the Card visual, let’s see what happens:

Oops, that gives me an error! Error message explains that the calculation expects a single value. The problem is that VALUES() and DISTINCT() return TABLE! However, it’s not a “normal” table, it’s virtual table created on the fly by DAX engine, so we should apply some additional calculations in order to extract single values from it.

Iterator functions to the rescue!

Iterator functions do what their name says — they iterate over the table and apply the calculation row by row! Iterator functions have X in the end: SUMX, AVERAGEX, COUNTX…In our scenario, we need to iterate over our distinct values and concatenate them into our string, which will be later displayed in the report. To achieve that, we will use CONCATENATEX() function.

This function accepts three arguments: the first is a table that we want to iterate on (in our case, a virtual table created using VALUES() function), then expression we are applying row by row on this table, and finally delimiter we want to use for separating extracted values.

Selected Brands = CONCATENATEX(

VALUES('Product'[BrandName]),

'Product'[BrandName],

"","")

In my example, I’m using comma as a delimiter, but you can also use others, such as a semicolon, etc. Now, when I look again in my report, I can see that I got desired results:

And, if I choose only few brands within the slicer, my card will adjust to reflect the changes:

Combining multiple measures

You can create the same measure for other slicers also, and then put all the results in the table visual. So, I will create the measure for Year slicer:

Selected Year = CONCATENATEX(

VALUES(Dates[Year])

,Dates[Year]

,"","")

Now, when I drag bit my measures into the table fields, I see both of my selected slicers’ values at one place:

Bonus idea!

If you have many slicers in your report, and you want to display all of the selections, but you don’t want to waste space on your report canvas, you can create a bookmark containing the table with all selected values, and then just display the bookmark on user’s request.

Something like this:

How cool is that! So, you don’t need to worry about bloating your report space with this table. You can show/hide it using bookmarks and actions.

Subscribe here to get more insightful data articles!",https://towardsdatascience.com/display-selected-slicers-in-power-bi-a99d81500e76,['Nikola Ilic'],2020-09-16 10:17:18.582000+00:00,665,"Power BI, Slicers, DAX, Iterator Functions, Contoso Database"
"Dummifying Variables- Hello there, Gretl !!","Photo by Kevin Ku on Unsplash

The word “dummy” means the act of replication. In the field of data science, it holds the same meaning. The whole art of dummifying variables in data science is the process of transforming the variables into a numerical representation.

Data scientists, analysts, or programmers often come across such datasets that include a categorical variable. This variable is of utmost importance in scenarios where the dependent variable is bound to consider the categorical variable for analysis or prediction.

In this article, we will learn how we can dummify the variables easily in less than a minute! As the title suggests, I will be using Gretl for transforming the variables. For those who are new to Gretl, can understand it as a tool that performs machine learning operations without involving coding from the user side. If you are interested in learning a bit more about it, please check this link out.

Having said this, let’s start with the process of creating dummy variables. Excited to dive deeper? Let’s get started!!

Installing Gretl

I am assuming all of us do not have Gretl installed on our machines, so we will begin with the installation first. The installation is very simple. All you have to do is, google how to install it. But, if you don’t want to waste your time looking for a credible source, follow the link given here.

The installation begins with a simple setup. Just follow the default settings and you’ll be good to go. Now, let’s move on to the next section where we select a dataset for the operation.

Choosing The Dataset

You can take any dataset which includes a categorical variable. I have used a simple one which can be downloaded from here. A sample screenshot of the dataset is attached below:

As you can see in the image above, Gender is a categorical variable with two distinct categories: Men and Women. We will be transforming this variable in the next step.

Creating dummy variables

Welcome to the last section of this article which talks about dummy variables. We will use a step by step procedure for creating the dummy variables. Just follow the steps given below and the task will be done in no time:

Step 1: Open Gretl

Initially, the Gretl screen looks like this. If you have installed it successfully, then you should have a similar screen like the image above. Now, let’s import our data.

Step 2: Importing the data

Just follow the screenshot given below to import your existing data. Also, if you have imported a file before and, want to use it again, Gretl shows a list of those files when you are trying to import it. In the image below, I have drawn a purple box to highlight them.

After you have successfully imported the file, the screen will look like the image given below:

You can see the columns of the dataset are listed here. Now, we will move to our main goal and dummy the “Gender” variable.

Step 3: Dummy Variables on the way!

To create the dummy variables, go to the desired variable, right-click on it, and, choose dummify. If you are confused about how to do it, refer the screenshot below:

After you are done with this, a pop up appears that asks you to choose the option of whether encoding all of them of skipping the highest or lowest.

I have chosen the “encode all” option in the following image. Feel free to choose the one that suits your problem requirements.

After you are done with this, Gretl creates some default names for the variables and encodes them. The encoding by Gretl is shown below:

Step 4: Renaming the dummy variables

This section is optional and, feel free to skip it. Here, we will learn how we can rename the default variables chosen by Gretl. Normally, I prefer to rename the default values so that it is easier for me to interpret.

All you have to do is, right-click on the name you want to change -> Edit attributes and follow the image shown below:

Change the field “Name” of the variable and you’ll be good to go. I have changed the name for both the variables. And, the final thing looks something like:

Hurrah! We have successfully changed the variable names after creating the dummy variables. I hope this was an exciting journey for all the data science enthusiasts.

I look forward to meeting you all again. Until then, Happy Reading!!",https://medium.com/analytics-vidhya/dummifying-variables-hello-there-gretl-b1e07674a43e,['Sonali Pandey'],2020-06-29 15:56:03.243000+00:00,724,"Data Science, Gretl, Coding, Machine Learning, Dummifying Variables"
Analyzing the Sources of Tristan Handy’s Data Science Roundup,"Analyzing the Sources of Tristan Handy’s Data Science Roundup

What can we learn from a meta-analysis of years of content?

For those who are only interested in the full results, click here.

One of my favorite data-related newsletters is Tristan Handy’s Data Science Roundup. I’ve been a loyal subscriber since early 2016, receiving a weekly or sometimes bi-weekly, summary of “the internet’s most useful data science articles.”

While convenient to receive a concentrated collection of articles I can peruse from bed on Sunday mornings, after so many years a part of me started to wonder — how is he finding these articles?

Is he using the same 10–15 sites over and over? What does the distribution of sources look like?

Well, I’ve compiled the data and have the answers. There are 186 total issues of the Data Science Roundup, each with blurbs on approximately 5 articles, giving us a population of around 1000 articles to analyze.

Although tedious, aggregating the data manually would take more or less the same amount of time as an automated solution, with more accurate data to boot. Regardless, I decided to take an automated approach to show off some python-hacking skills in the process.*

*As a side note, this would make for a great Mechanical Turk task.

Screenshot of my inbox filtered on the DSR

Collecting the Data

I’ve yet to figure out the dark art that is keeping your inbox tidy, so instead of scraping from the web, I was able to use my disgusting email inbox as the data source.

From there the steps were:

Connect to my inbox via imaplib and a gmail app password Filter messages to issues of the Data Science Roundup by subject and sender Grab the html contents of each email and parse link tags with BeautifulSoup Save the parsed link sources and analyze!

This code should not be emulated, but for anyone curious what it looks like to grab data from gmail, here’s what my hacky code looked like:

Results

For a full list of the results, click here.

After a few seconds when all was said and done, I was left with 900 links from 129 Data Science Roundup issues since Oct 1, 2017*.

*The newsletter has had a consistent format since this date.

Below is a table with all sites that were referenced more than 5 times.

+----------------------------+----------+

| Site | No. Refs |

+----------------------------+----------+

| towardsdatascience.com | 107 |

| medium.com | 86 |

| eng.uber.com | 19 |

| flowingdata.com | 14 |

| locallyoptimistic.com | 12 |

| kdnuggets.com | 9 |

| arxiv.org | 9 |

| ai.googleblog.com | 9 |

| oreilly.com | 8 |

| blog.openai.com | 7 |

| technologyreview.com | 7 |

| eng.lyft.com | 7 |

| blog.getdbt.com | 7 |

| blog.fishtownanalytics.com | 6 |

| labs.spotify.com | 6 |

+----------------------------+----------+

By far and away the most frequent source is the site you most likely are reading this analysis on — medium.com. The Towards Data Science publication specifically (which yours truly was recently featured in with two extremely popular articles) leads the pack with 107 links. And Medium more generally comes in close second with 86.

That accounts for approximately 25% of all links featured in the Data Science Roundup, and is an argument to suck it up and drop the $50 for an annual Medium membership.

After that comes popular blogs most people have heard of, including the engineering blogs of prominent companies like Google, Uber, Lyft, and Spotify. (Netflix hosts its tech blog on Medium and would be included in the medium.com category).

The Long Tail

The point of this article is not to knock Tristan for repeatedly using the same sources in his newsletter; rather the opposite. There is a long tail of 150 or so sources that have been included in issues of DSR, and demonstrate the effort he puts in to remain integrated with the sprawling data community online.

Since we’re using data that goes back a number of years and personal blogs are a fickle domain, sadly, a number of the smaller sites are no longer maintained.

Here are three lesser-known sites that are active with quality content:",https://medium.com/whispering-data/analyzing-the-sources-of-tristan-handys-data-science-roundup-2cee87978012,['Paul Singman'],2020-08-26 12:26:29.147000+00:00,654,"1. Locally Optimistic.com2. GetDBT.com3. Fishtown Analytics.comdata science, Tristan Handy, meta-analysis, python-hacking, data sources"
Time Series- Basic Concepts before you start,"Time Series- Basic Concepts before you start

We always want to know what will happen in future, and if we can predict the important information of the future it is even more interesting.

Image Source

We always want to know what will happen in future, and if we can predict the important information of future it is even more interesting.

Time Series data is a set of observations taken during a time period. Time-Series in machine learning is a predictive tool to forecast upcoming information. It involves working on time, like year, month, day and hour to derive important information and take decisions based on it.

Examples can be stock market data, weather data that is recorded per day, vital stats data like steps.

Forecasting or Predicting Time Series data is a challenging task and is more complex than machine learning.

In Machine learning the dependent feature was predicted based on a number of features that are independent while in time series we have to predict future observation depending on past observations, that is, independent features are observations in time series only.

Prediction methodologies can be various like

1) Mean of past observations, which gives the same weightage to all past data.

2) Weighted mean of past observations in which more weightage is given to recent observation and less weightage is given to older data.

Also, factors like below make it more challenging:

1) Seasonality: Seasonality means data observations based on season like winters, summers, or festival. The best example of seasonality is the sales of Air Conditioners which is on peak in summer season in North India. We need to capture the seasonality of data to predict or forecast the future sale.

2) Trend: Trend example can be of a company having an upward or downward trend based on some news, merger or annual results, this is basically trend of the company which can be going on for some time and will continue so for more time. It may take longer periods than season for trend to set up. Another example can be consumption of electricity in India which going up because of more and more gadgets at home, it may come down in sometime if there is again a new gadgets coming in which consume less of energy. Trends dies out after a long time.

3) Irregularity: Unpredicted observations can be because of anything unpredictable like natural calamity, a war or a pandemic like COVID -19 which made markets to crash in the month of March 2020.

Stationary Data:

Time series data needs to be stationary before someone can start working on them.

Now, what do we mean by stationary data? Stationary data means, it should have constant mean and constant variance.

If data is not stationary there are many methods to make it stationary, few are as below:

1) De-Trending: To remove the trend component out of data observations, seeing if it is linear, exponential or constant.

2) Differencing: To remove seasonality from data, like if data is seasonal month-wise, it can be done by subtracting the data lag-wise with 12 as lag difference.

3) Logging: Doing Log of Observations can help in making the trend linear from exponential.

Types of Time Series

It is important to know what kind of time series data we are seeing, for example below is example of simple 2 types of time series data additive and multiplicative.

Image by Author

Image by Author

By knowing about our data we decide that how it will be decomposed into various factors like trend seasonality as we see different graphs here for additive and multiplicative data.

In Additive data component of Seasonality, Trend and irregularity are additive in nature, Value= Base + Trend X+ Seasonality + Irregularity.

While in multiplicative these components are multiplicative in nature i.e,

Value= Base X Trend X Seasonality X Irregularity.

Data Smoothing

Data smoothing uses algorithms to remove any noise from data to recognize patterns.

Smoothing by Rolling Stats

Rolling Stats can exhibit trend in the data. Moving average, if window is 2 and we apply it to data below then at t1 it’s NULL and at t2 its (20+25)/2=22.5

If mean at a particular time is (25+40)/2, it should be almost similar to (35+30)/2 if it is stationary.

Similarly it applies to variance.",https://towardsdatascience.com/time-series-basic-concepts-before-you-start-2ed29a59ad0d,['Namrata Kapoor'],2020-12-06 18:12:05.365000+00:00,680,"Time Series, Machine Learning, Forecasting, Seasonality, Trend Analysis"
How to Train Your Algorithm,"Can a well-programmed machine do anything a human can — only better? Complex algorithms are choosing our music, picking our partners, and driving our investments. They can navigate more data than a doctor or lawyer and act with greater precision. For many years we’ve taken solace in the notion that they can’t create. But now that algorithms can learn and adapt, does the future of creativity belong to machines, too? It is hard to imagine a better guide to the bewildering world of artificial intelligence than Marcus du Sautoy, a celebrated Oxford mathematician whose work on symmetry in the ninth dimension has taken him to the vertiginous edge of mathematical understanding. In The Creativity Code: Art and Innovation in the Age of AI he considers what machine learning means for the future of creativity. Here is a brief excerpt from the book looking at how we might train algorithms to better our lives.

You may feel there is something scary about an algorithm deciding what you might like. Could it mean that, if computers conclude you won’t like something, you will never get the chance to see it? Personally, I really enjoy being directed toward new music that I might not have found by myself. I can quickly get stuck in a rut where I put on the same songs over and over. That’s why I’ve always enjoyed the radio. But the algorithms that are now pushing and pulling me through the music library are perfectly suited to finding gems that I’ll like. My worry originally about such algorithms was that they might corral everyone into certain parts of the library, leaving others bereft of listeners. Would they cause a convergence of tastes? But thanks to the nonlinear and chaotic mathematics usually behind them, this doesn’t happen. A small divergence in my likes compared to yours can send us off into different far corners of the library.

I listen to a lot of algorithm-recommended pieces when I am out running. It’s a great time to navigate the new. But I made a big mistake a few weeks ago. My wife asked for my help putting together a playlist for her birthday party. She wanted dancing. She wanted the eighties. So we spent a couple of evenings listening to lots of possibilities. It’s not my choice of music, but we put together a great list of songs that got all our guests up and moving. The problem came when I went out for my first run following the party. My usual choice to let the player surprise me took me deep into the library aisles stocked with eighties dance music. I pressed “skip” as I ran on, but I couldn’t find my way out. It took several weeks of retraining the algorithm on Shostakovich and Messiaen before I got things back on track.

Another context in which we teach algorithms trying to serve us has to do with the spam filters on email applications. A good filter begins by training on a whole swath of emails, some marked as spam, the rest considered legitimate. These are emails that aren’t particular to you yet. By analyzing the words that appear in these emails it starts to build up a profile of spam emails. It learns to treat 100 percent of the emails using the word “Viagra” as spam, along with 99 percent of the emails with the word “refinance.” One hundred percent of the emails with the combination “hot Russian” are spam. The word “diabetes” is more problematic. A lot of spam emails promise cures for diabetes, but it is also a word that crops up legitimately in people’s correspondence. The algorithm simply counts the split in its training data. Perhaps one in twenty messages containing the word turns out not to be spam, so it learns to score an email with “diabetes” as 95 percent likely to be spam.

Your email filter can be set at different levels of filtering. You might specify that only if it’s 95 percent sure should an email go into the junk folder. But now comes the cool bit. While the algorithm initially trained on a generic set of emails, your ongoing actions teach it to recognize the sorts of things you are interested in. Suppose that, in fact, you do suffer from diabetes. At first, all emails with the word “diabetes” will go into your junk folder. But gradually, as you mark emails including the word “diabetes” as legitimate, the algorithm recalibrates the probability of spam to some level below 95 percent and the email arrives in your inbox.

These algorithms are also built to spot other keywords that mark out the junk diabetes emails from the legitimate ones. The inclusion of the word “cure” could well distinguish the duds. Machine learning means that the algorithm will go through every email that comes in, trying to find patterns and links, until it ends up producing an algorithm highly customized to your own individual lifestyle.

This updating of probabilities is also how driverless cars work. It’s really just a more sophisticated version of controlling the paddle in the Atari game Breakout: move the steering wheel right or left according to the pixel data the machine is currently receiving. Does the score go up or down as a consequence?",https://hup.medium.com/how-to-train-your-algorithm-6ff722c44816,['Harvard University Press'],2020-05-29 14:25:34.588000+00:00,876,"Artificial Intelligence, Machine Learning, Algorithms, Music Recommendations, Spam Filters"
Beralih ke FastAPI sebagai Pengganti Flask untuk Produksi Model Machine Learning,"Learn more. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more

Make Medium yours. Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore",https://medium.com/milooproject/beralih-ke-fastapi-sebagai-pengganti-flask-untuk-produksi-model-machine-learning-4b68e799566b,['Fahmi Salman'],2020-11-23 15:58:33.061000+00:00,64,"below.Medium, Open Platform, Insightful Thinking, Dynamic Thinking, Expert Voices"
How To Make Your Pandas Loop 71.803 Times Faster,"If you use Python and Pandas for data analysis, it will not be long before you want to use a loop the first time. However, even for small DataFames it is time-consuming to use the standard loop and you will quickly realize that it can take a long time for larger DataFrames. When I first waited more than half an hour to execute the code, I looked for alternatives that I would like to share with you.

The standard loop

DataFrames are Pandas-objects with rows and columns. If you use a loop, you will iterate over the whole object. Python can´t take advantage of any built-in functions and it is very slow. In our example we got a Dataframe with 65 columns and 1140 rows. It contains soccer results for the seasons 2016 - 2019. We want to create a new column that indicates whether a particular team has played a draw. We might start like this:

Since we got every match from the Premier League in our DataFrame, we have to check if the team of interest (Arsenal) played and if that applies, whether they were the home or away team. As you can see, this loop was very slow and took 20,7 seconds to execute. Let´s look how we can be more efficient.

The Pandas Built-In Function: iterrows() — 321 times faster

In the first example we looped over the entire DataFrame. iterrows() returns a Series for each row, so it iterates over a DataFrame as a pair of an index and the interested columns as Series. This makes it faster than the standard loop:

The code took 68 milliseconds to run which is 321 times faster than the standard loop. However, many people advise against using it because there are still faster options and iterrows() does not preserve dtypes across the rows. That means if you use iterrows() on your DataFrame dtypes can be changed which can cause a lot of problems. Do preserve dtypes you can also use itertuples() . We will not go into detail here because we want to pay attention to efficiency. Here you can find the official documentation:

The apply() Method — 811 times faster

apply is not faster in itself but it has advantages when used in combination with DataFrames. This depends on the content of the apply expression. If it can be executed in Cython space, apply is much faster (which is the case here).

We can use apply with a Lambda function. All we have to do it to specify the axis. In this case we have to use axis=1 because we want to perform a column-wise operation:

This code is even faster than the previous methods and took 27 milliseconds to be finished.

Pandas Vectorization — 9280 times faster

Now we can come to a new topic. We use the advantages of vectorization to create really fast codes. The point is to avoid Python-level loops like in the examples before [1] and to use optimized C code which uses the memory much more efficient. We just need to modify the function slightly:

Now we can create with Pandas series as input thew new column:

In this case we don´t even need a loop. All we have to do is to adjust the content of the function. Now we can directly pass Pandas series to our function which causes the enormous speed gain.

Numpy Vectorization — 71.803 times faster

In the previous example we passed Pandas series to our function. By adding .values we receive a Numpy array:

Numpy arrays are so fast because we got the benefits of locality of reference [2]. Our code took 0,305 milliseconds to run and was 71803 times faster than the standard loop used in the beginning.

Conclusion

If you use Python, Pandas and Numpy for data analysis, there will always be some room for improving your code. We compared five different methods do add a new column to our DataFrame based on some calculation. We have noticed enormous differences in terms of speed:",https://towardsdatascience.com/how-to-make-your-pandas-loop-71-803-times-faster-805030df4f06,['Benedikt Droste'],2019-08-26 17:52:27.478000+00:00,652,"Python, Pandas, Data Analysis, Numpy, Vectorization"
California fires: Tracking the smoke,"Devastatingly, 2020 has beaten 2018’s record by almost twice as much. According to The Department of Forestry and Fire Protection, 4,105,786 acres have burned since January 1st 2020. However this time, the fires have caused less damage to buildings — destroying 9247 structures compared to 24226 in 2018. This is probably due to the location of the fires and let’s hope it doesn’t increase.

The image below, using data from Cal Fire, visualizes the amount of acres that have burned each year since 2013. This bar chart shows the dramatic escalation of wildfire intensity over the last seven years. To find out how we made this graph, visit the notebook.

Burned Area in California (acres) since 2013. Data from CalFire, visualized by PlanetOS

Gigafire — A New Phenomenon

The August Complex Fire that started as 38 separate fires caused by lightning strikes, has now developed from a “megafire” (burning over 100,000 acres) to the first “gigafire” (burning over a 1,00,000,000 acres) in the history of modern California. This fire is larger than the state of Rhode Island. Australia’s bushfire earlier this year reached gigafire status, but California’s is the first gigafire in the United States for ten years.

To better understand the extent of the wildfires in the Western United States, the High-Resolution Rapid Refresh Smoke (HRRR Smoke) model can be used. This is a three-dimensional model that allows simulation of mesoscale flows and smoke dispersion over complex terrain. The smoke model comprises a suite of fire and environmental products that can be used for forecasts during the wildfire season. The source of this experimental dataset is Global Systems Laboratory. While the dataset is useful for modeling fire smoke forecasts and fire intensity, it is an “experimental” dataset meaning that there are some limitations including missed detections and model inaccuracy. Therefore, this data should not be used to make decisions regarding human safety or property. This smoke product will become available in the Operational HRRR in the December timeframe; as part of the next upgrade.

Applying the Data

In light of these fires and our available datasets, we analysed the smoke from the wildfires on the lowest model level (8 meters) across the entire country and at the Californian county level. For a tutorial on how to do this, the GitHub notebook is available here.

Visualizing the Smoke: United States

As you can see from the visual below, smoke travels far and is erratic, changing direction in a matter of hours. This forecast is from 4th October 2020 and it clearly shows the immense spread of smoke across California and the rate at which this smoke spread to neighboring states.

HRRR Smoke forecast for the US from 2020–10–04T00:00:00

The second animation below details smoke from the 14th October 2020. We can see that there is a lot less smoke than there was on the 4th, however the smoke has traveled far, having been directed by changing weather patterns.

HRRR Smoke forecast for the US from 2020–10–14T06:00:00

Closer look: California

As most of the fires are spreading throughout California, it makes sense to take a closer look at what is happening across the state. The HRRR Smoke model has a good spatial resolution at 3 km, which makes tracking the smoke on a more granular level much easier.

Below we can see that the smoke covered most of the state last week. However, the more recent forecast shows that the situation has significantly improved this week.

From this week’s forecast, pictured below, we can see that the smoke is much more localized. While it is not traveling as far, the smoke still presents a severe threat to the health of humans and our ecosystems.

Health issues: Smoke leads to poor air quality

Smoke from wildfires contains thousands of individual compounds, including carbon monoxide, volatile organic compounds (VOCs), carbon dioxide, hydrocarbons and nitrogen oxides. The most prevalent pollutant by mass is PM2.5, which refers to atmospheric particulate matter (PM) that has a diameter of less than 2.5 micrometers. A product of combustion, these fine particles tend to stay in the air longer than heavier particles, thus increasing the chances of inhalation by humans and animals. These minute particles can penetrate the lungs and circulatory system, often causing adverse health effects.

Luke Montrose, the environmental toxicologist, explains how those PM2.5 particles affects human body:

The human body is equipped with natural defense mechanisms against particles bigger than PM2.5. As I tell my students, if you have ever coughed up phlegm or blown your nose after being around a campfire and discovered black or brown mucus in the tissue, you have witnessed these mechanisms firsthand. The really small particles bypass these defenses and disturb the air sacks where oxygen crosses over into the blood. Fortunately, we have specialized immune cells present in the air sacks called macrophages. It’s their job to seek out foreign material and remove or destroy it. However, studies have shown that repeated exposure to elevated levels of wood smoke can suppress macrophages, leading to increases in lung inflammation.

For further information about air quality, our View2020 dashboard gives a short overview of how PM2.5 and other air quality parameters have changed in California over the course of the year.

What’s next: comparing HRRR Smoke with CAMS air quality forecast

In the next blogpost and Notebook we are planning to compare CAMS forecast particulate matter variables with HRRR Smoke forecast to see if there are any similarities. This comparison will be useful for anyone grappling with smoke forecast data.",https://medium.com/planet-os/california-fires-tracking-the-smoke-b5cd4ad2522d,['Eneli Toodu'],2020-10-15 08:44:33.810000+00:00,895,"wildfire, california, HRRRSmoke, gigafire, PM2.5"
How To Use Numerical Data with ConvNets,"Dealing with the Images

I am assuming that you are trying to predict the prices of the houses where the pictures of the houses are among the inputs.

This can be accomplished by putting the prices as targets y. And then have the NN predict the price for previously unseen house pictures as y^.

How I would approach this problem-

Form a dataset that has the prices as labels y and the images as inputs.

Split the dataset in train-valid-test splits (remember to shuffle it)

Create and train the NN to output a single number y^ which will be the price (remember that the output layer has to have a single output node that outputs a raw number, preferably a floating-point number, to predict the price).

Up to this point, it is quite a simple problem. Note that I am not taking into account the location information at all. So this model will be unusable in the real world. A mansion in rural Texas will definitely cost a lot less than a 2 bedroom house in counties near San Fransisco. You get the idea. So, we will have to take the location into account. Even after that, it will not be very useful, either. Will come to that later.",https://medium.com/learningdeep/how-to-use-numerical-data-with-convnets-4161a628ce46,['Ritobrata Ghosh'],2020-12-18 06:48:47.975000+00:00,202,"Machine Learning, Image Recognition, Neural Networks, Price Prediction, Location Data"
Geographic Data science Best books in 2020,"There are so many excellent books coming this year or published recently about Geospatial data science. In fact, it is the best time to learn Geospatial data science with the availability of learning resources as well as maturing Geospatial data science libraries.

Some of the readers asked me about the best books in the Geospatial data science resources after I published an article on Geospatial data science Courses.

If you prefer learning through reading books, this article is for you. I am sharing here the best and most recent books available in Geographic data science. Some of these books are work in progress and freely available online, which is an excellent opportunity to learn early and help authors.

1. Geographic Data Science with PySAL and the PyData Stack (Work in Progress)

Geographic Data Science with PySAL and the PyData Stack, is an excellent introductory book to learning Geographic data science and offers an extensive learning resource for both beginners and advanced learners. The book covers both theoretical aspects of geospatial computations as well as practical examples with code. As the authors of the book, are contributors of Python libraries like PySAL, the Python Spatial Analysis Library and Geopandas, the content of the book has tightly integrated with Geospatial data science Python Environment.

This book is a work in progress and can be accessed freely online. The GitHub repository also has Jupyter notebooks that you can experiment, adopt or extend.

2. Geocomputation with R (2019)

R language often amazes me with the ease and elegance of its Geospatial data visualizations. If you want to start learning the R language for Geospatial Data analysis, this is the best book available, and it is freely available online. Even if you are beginning R Language, this book can help you if you have already some background in Geographic Information systems.

3. Mastering Geospatial Analysis with Python (2018)

This book touches many aspects of Geographic data analysis and Python programming that gives the reader what is possible to do within these selected topics. The topics covered in this book include among others cloud computing for Geospatial data, web mapping with GeoDjango and Flask as well as several well known Geographic data science libraries. I find it more of a case study rather than a textbook to learn, nevertheless it provides a well-balanced selection of topics with code implementations.

4. Learning Geospatial Analysis with Python (2019)

Learning Geospatial Analysis with Python is an excellent book with the low-level implementation of Geographic data analysis in Python. In this book, you can learn APIs and generic algorithms for Geospatial tasks. It includes a lot of python code to most of Geospatial data processing tasks, like calculating distances, buffer analysis and working with remote sensing data.

5. Introduction to Python for Geographic Data Analysis (Work in Progress)

The materials of this book are not released yet. However, all materials are developed online before the book comes out next year. I think this will be a considerable addition to Geospatial data analysis books in Python. The content of the book is superbly excellent and includes a well-balanced curriculum. As it is a combination of two courses offered at Helsinki University, the curriculum starts from the fundamentals and progresses to an advanced level. Keep an eye on this book once the materials become available online.

6. Geospatial Data Science Quick Start Guide (2019)

Geospatial Data Science Quick Start Guide is an introduction book and offers a practical geospatial data science with Python. Topics covered in the book include exploratory data analysis, geofencing and machine learning applications with Geospatial data. I should mention here that I co-authored this book, thus being at the last item in the list.

Conclusion

These books are some of my favourite books on learning Geospatial data analysis in Python, and R. Let me know if you have others in your list of reading that you think might be an excellent addition to this list.",https://towardsdatascience.com/geographic-data-science-best-books-in-2020-f5fab770de16,[],2020-03-15 15:56:26.914000+00:00,640,"geospatial-data-science, pySAL, Py Data-Stack, R-language, Geopandas"
Data-ism,"A part of writing-after-reading series as a personal goal, I present you a book about a new oil, data. A book by Steve Lohr.

This book is written by a The New York Times journalist who always covers technology, business, and economics for more than twenty years. He won Pulitzer Prize for explanatory reporting, a distinguished award for writer who has the abilities to explain the subject that is important and complex, demonstrate mastery of the subject, have lucid writing and clear presentation.

Photo by Luke Chesser on Unsplash

He writes the book with eleven chapters inside which will be revealed sequentially in detail below:",https://maung-sutikno.medium.com/data-ism-98236e4f847c,['Maung Agus Sutikno'],2020-10-29 23:29:00.652000+00:00,102,"data, oil, Steve Lohr, The New York Times, Pulitzer Prize"
Implementing different kernels of SVC Algorithm on the Iris Dataset,"Code

Here, we will go through the coding segment. The dataset used for this implementation is the iris dataset which can be imported from the sklearn library.

Importing Dataset and Relevant Libraries/Modules

Firstly, we will import the numpy and matplotlib for mathematical manipulation of data and plotting the graphs. Then we will import the svm classifier and the iris dataset from the sklearn library.

import numpy as np

import matplotlib.pyplot as plt

from sklearn import svm, datasets iris = datasets.load_iris()

The Iris Dataset (Image by Author)

The Iris Dataset consists of 150 samples, each having 4 features listed: sepal length, sepal width, petal length, and petal width.

Features in the Iris Dataset (Image by Author)

The data has 3 classes: setosa, versicolor and virginica.

Classes of Iris: Setosa, Versicolor and Virginica (Images from Wikipedia)

Features & Target Numpy Array

Next, we will take the first two features of the iris data, which are its sepal length and sepal width, both in cm.

Our X variable will contain the features and y variable will contain the target.

X = iris.data[:, :2]

y = iris.target

The Features & Target Numpy Arrays (Image by Auhtor)

Model Creation

Next, we will create models of the svm algorithms on the Iris dataset and use different kernels to observe the results.

We will keep the value of C = 1.0 for all the models.

C = 1.0 # SVM regularization parameter

models = (svm.SVC(kernel='linear', C=C),

svm.LinearSVC(C=C, max_iter=10000),

svm.SVC(kernel='rbf', gamma=0.7, C=C),

svm.SVC(kernel='poly', degree=3, gamma='auto', C=C))

models = (clf.fit(X, y) for clf in models)

We have created 4 models:

svm.SVC with linear kernel and value of C kept at 1.0 svm.LinearSVC (which uses the liblinear), with same value of C svm.SVC with rbf kernel svm.SVC with kernel = ‘poly’, degree = 3, gamma = ‘auto’ and default value of C

Make Meshgrid

Next, we will define a function to create a meshgrid to plot our 4 models.

def make_meshgrid(x, y, h=.02):

x_min, x_max = x.min() - 1, x.max() + 1

y_min, y_max = y.min() - 1, y.max() + 1

xx, yy = np.meshgrid(np.arange(x_min, x_max, h),

np.arange(y_min, y_max, h))

return xx, yy

Plotting Contours

This function will be called to plot the decision boundaries for the classifiers.

def plot_contours(ax, clf, xx, yy, **params):

Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

Z = Z.reshape(xx.shape)

out = ax.contourf(xx, yy, Z, **params)

return out

Plotting

Next, we will define the titles of each plot and set up a 2x2 grid to plot our 4 classifiers plots.

Then, we will fit the model one by one on the Iris Dataset and plot the results:

# title for the plots

titles = ('SVC with linear kernel',

'LinearSVC (linear kernel)',

'SVC with RBF kernel',

'SVC with polynomial (degree 3) kernel') # Set-up 2x2 grid for plotting.

fig, sub = plt.subplots(2, 2)

plt.subplots_adjust(wspace=0.4, hspace=0.4) X0, X1 = X[:, 0], X[:, 1]

xx, yy = make_meshgrid(X0, X1) for clf, title, ax in zip(models, titles, sub.flatten()):

plot_contours(ax, clf, xx, yy,

cmap=plt.cm.coolwarm, alpha=0.8)

ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')

ax.set_xlim(xx.min(), xx.max())

ax.set_ylim(yy.min(), yy.max())

ax.set_xlabel('Sepal length')

ax.set_ylabel('Sepal width')

ax.set_xticks(())

ax.set_yticks(())

ax.set_title(title) plt.show()

The following is the plot:

Plot of SVC using various kernels (Image by Author)

The sepal length has been taken on the x-axis whereas the sepal width has been taken on the y-axis.",https://medium.com/analytics-vidhya/implementing-different-kernels-of-svc-algorithm-on-the-iris-dataset-90908e55048a,['Mahnoor Javed'],2020-12-01 16:31:32.673000+00:00,475,"coding, iris dataset, svm classifier, sepal length, sepal width"
Introducing FastBert — A simple Deep Learning library for BERT Models,"Usage

Import the required packages. Please note that I have not included the usual suspects such as os, pandas, etc.

Define general parameters and path locations for data, labels and pretrained models. (some good engineering practices)

Tokenizer

Create a tokenizer object. The is the BPE based WordPiece tokenizer and is available from the magnificient Hugging Face BERT PyTorch library.

The do_lower_case parameter depends on the version of the BERT pretrained model you have used. In case you use uncased models, set this value to true, else set it to false. For this example we have use the BERT base uncased model and hence do_lower_case parameter is set to true.

GPU & Device

Training a BERT model does require a single or more preferably multiple GPUs. In this step we can setup GPU parameters for our training.

Note that in the future releases, this step will be abstracted from the user and the library will automatically determine the correct device profile.

BertDataBunch

This is an excellent idea borrowed from fast.ai library. The databunch object takes training, validation and test csv files and converts the data into internal representation for BERT. The object also instantiates the correct data-loaders based on device profile and batch_size and max_sequence_length.

The DataBunch object provides the location to the data files and the label.csv file. For each of the data files, i.e. train.csv, val.csv and/or test.csv, the databunch creates a dataloader object by converting the csv data into BERT-specific input objects. I would encourage you to explore the structure of the databunch object using Jupyter notebook.

BertLearner

Another concept in line with the fast.ai library, BertLearner is the ‘learner’ object that holds everything together. It encapsulates the key logic for the lifecycle of the model such as training, validation and inference.

The learner object will take the databunch created earlier as as input alongwith some of the other parameters such as location for one of the pretrained BERT models, FP16 training, multi_gpu and multi_label options.

The learner class contains the logic for training loop, validation loop, optimiser strategies and key metrics calculation. This help the developers focus on their custom use-cases without worrying about these repetitive activities.

At the same time the learner object is flexible enough to be customised either via using flexible parameters or by creating a subclass of BertLearner and redefining relevant methods.

The learner object does the following upon initiation:

Creates a PyTorch BERT model and initialises the same with provided pre-trained weights. Based on the multi_label parameter, the model class will be BertForSequenceClassification or BertForMultiLabelSequenceClassification. Assigns the model to the right device, i.e. CUDA based GPU or CPU. if Nvidia Apex is available, the distributed processing functions of Apex will be utilised.

fast-bert provides a bunch of metrics. for multi-class classification, you will generally use accuracy whereas for multi-label classification, you should consider using accuracy_thresh and/or roc_auc.

Train the model

Start the model training by calling fit method on the learner object. the method takes epoch, learning rate and optimiser schedule_type as input. Following schedule types are supported (again courtesy of the Hugging Face Bert library):

none : always returns learning rate 1.

: always returns learning rate 1. warmup_constant : Linearly increases learning rate from 0 to 1 over warmup fraction of training steps. Keeps learning rate equal to 1. after warmup.

warmup_linear : Linearly increases learning rate from 0 to 1 over warmup fraction of training steps. Linearly decreases learning rate from 1. to 0. over remaining 1 - warmup steps.

warmup_cosine : Linearly increases learning rate from 0 to 1 over warmup fraction of training steps. Decreases learning rate from 1. to 0. over remaining 1 - warmup steps following a cosine curve. If cycles (default=0.5) is different from default, learning rate follows cosine function after warmup.

warmup_cosine_hard_restarts : Linearly increases learning rate from 0 to 1 over warmup fraction of training steps. If cycles (default=1.) is different from default, learning rate follows cycles times a cosine decaying learning rate (with hard restarts).

warmup_cosine_warmup_restarts : All training progress is divided in cycles (default=1.) parts of equal length. Every part follows a schedule with the first warmup fraction of the training steps linearly increasing from 0. to 1., followed by a learning rate decreasing from 1. to 0. following a cosine curve. Note that the total number of all warmup steps over all cycles together is equal to warmup * cycles

On calling the fit method, the library will start printing the progress information on the logger object. It will print training and validation losses, and the metric that you have requested.

In order to repeat the experiment with different parameters, just create a new learner object and call fit method on the same. If you have tons of GPU compute, then you can possibly run multiple experiments in parallel by instantiating multiple databunch and learner objects at the same time.

Once you are happy with your experiments, call the save_and_reload method on learner object to persist the model on the file structure.",https://medium.com/huggingface/introducing-fastbert-a-simple-deep-learning-library-for-bert-models-89ff763ad384,['Kaushal Trivedi'],2019-09-14 00:24:58.232000+00:00,803,"BERT, Py Torch, Hugging Face, GPU, Tokenizer"
Pain in the Data,"I have been working on a data analytics project for around 3 weeks, the project aims to visualize and allow querying a database of employees based on their skills, industry, and specialty. It is a very interesting and challenging project, it sounds fairly simple, yet it is taking a surprising amount of time; this is not a bad thing, as I was taking this opportunity to verify a certain fact in data science.

The challenge lies in the data itself. I received the data as a CSV (comma separated values) file with each row being a record of Name, Email, ID, Skills List, Skills Scores List, Region, Industry, and Specialty. I was working in Python and developing a web app running on IBM Cloud, so I went with Pandas library to handle the data for me. Just uploading the data to a database on the cloud was painful. Converting the CSV file to a JSON (JavaScript object notation) format was a challenge because the data was organized in such a way that each employee had one row for each region, skill or industry or specialty. I essentially had to:

Combine all rows for an employee into one row Clean the data types Convert to JSON

It took me a week just to clean the data types, and this was just the first step in the project: uploading the data to Cloudant NoSQL database. One might argue why did I use JSON and NoSQL whereas I could have used a table format and SQL database? There are two main reasons, primarily because I am more comfortable working with NoSQL, and second because I was doing an experiment.

Then came the challenge of querying the data, once I received the query identifying the requested combination of region, skills, industries, and specialty. Structuring the data right for a query was a challenge which took around 3 days to address; if it weren’t for the Pandas library, I would have taken maybe a week or two. Funny enough, the total time I spent on building the structure of the web app, log in, and user interface all in all took around 2 or 3 days.

This little experiment of mine shows a very important fact about data science and analytics:

80% of the time is spent cleaning the data

I spent around 10 days to clean and prepare the data, and just 4 days to query and build the web app. Lucky enough I was doing everything in Python which provides a set of great tools and libraries for data science. My choice of database was not the best for this application, but in a real-life situation, not everything is so sweet, you almost always have to restructure, reformat, and reorganize the data.",https://medium.com/astrolabs/pain-in-the-data-4196845615d,['Aoun Lutfi'],2017-10-05 06:58:43.756000+00:00,452,"Data-Analytics, Python, Cloudant-NoSQL, JSON, Pandas"
MOOCs,"MOOCs

My attempts with online learning resources.

Inspired by Tirthajyoti Sarkar’s post on choosing effective courses for machine learning and data science, I’m going to take stock of my progress so far and make an effort to figure out how I can better use the available educational resources.

Pre-2018 Relevant Courses

On evaluating the classes that I’ve taken, out of 21 audited MOOCs, I wouldn’t classify any of the contents covered at beyond a basic level of comprehension. This is my subjective view of course, but it gets at one of the biggest issues that I have with MOOCs: I can’t seem to move forward. MOOCs have been great for me to understand a breadth of material but I haven’t been able to get deeper into the subjects that interest me.

(Data Science | Basic) Data Visualization and Communication with Tableau, Duke University, Coursera

Data Visualization and Communication with Tableau, Duke University, Coursera (Data Science | Basic) The Data Scientist’s Toolbox, Johns Hopkins University, Coursera

The Data Scientist’s Toolbox, Johns Hopkins University, Coursera (Data Science | Basic) Getting and Cleaning Data, Johns Hopkins University, Coursera

Getting and Cleaning Data, Johns Hopkins University, Coursera (Data Science | Basic) Regression Models, Johns Hopkins University, Coursera

Regression Models, Johns Hopkins University, Coursera (Data Science | Basic) A Crash Course in Data Science, Johns Hopkins University, Coursera

A Crash Course in Data Science, Johns Hopkins University, Coursera (Data Science | Basic) Developing Data Products, Johns Hopkins University, Coursera

Developing Data Products, Johns Hopkins University, Coursera (Data Science | Basic) Building a Data Science Team, Johns Hopkins University, Coursera

Building a Data Science Team, Johns Hopkins University, Coursera (Data Science | Basic) Data Science in Real Life, Johns Hopkins University, Coursera

Data Science in Real Life, Johns Hopkins University, Coursera (Data Science | Basic) Managing Data Analysis, Johns Hopkins University, Coursera

Managing Data Analysis, Johns Hopkins University, Coursera (Data Science | Basic) Computing for Data Analysis, Johns Hopkins University, Coursera

Computing for Data Analysis, Johns Hopkins University, Coursera (Data Science | Basic) Introduction to R for Data Science, Microsoft, edX

Introduction to R for Data Science, Microsoft, edX (Data Science | Basic+) Introduction to Recommender Systems, University of Minnesota, Coursera

Introduction to Recommender Systems, University of Minnesota, Coursera (Data Science | Basic+) Practical Predictive Analytics: Models and Methods, University of Washington, Coursera

Practical Predictive Analytics: Models and Methods, University of Washington, Coursera (Finance | Basic) Competitive Strategy, Ludwig-Maximilians-Universität München, Coursera

Competitive Strategy, Ludwig-Maximilians-Universität München, Coursera (Finance | Basic) Analyzing Global Trends for Business and Society, University of Pennsylvania Wharton School of the University of Pennsylvania, edX

Analyzing Global Trends for Business and Society, University of Pennsylvania Wharton School of the University of Pennsylvania, edX (Programming (py) | Basic) An Introduction to Interactive Programming in Python (Part 1), Rice University, Coursera

An Introduction to Interactive Programming in Python (Part 1), Rice University, Coursera (Programming (py) | Basic) Programming for Everybody (Python), University of Michigan, Coursera

Programming for Everybody (Python), University of Michigan, Coursera (Programming (Web) | Basic+) HTML, CSS, and Javascript for Web Developers, Johns Hopkins University, Coursera

HTML, CSS, and Javascript for Web Developers, Johns Hopkins University, Coursera (Programming (Web) | Basic) HTML, CSS and JavaScript, The Hong Kong University of Science and Technology, Coursera

HTML, CSS and JavaScript, The Hong Kong University of Science and Technology, Coursera (Programming (Web) | Basic) Responsive Website Basics: Code with HTML, CSS, and JavaScript, University of London International Programmes Goldsmiths, University of London, Coursera

Responsive Website Basics: Code with HTML, CSS, and JavaScript, University of London International Programmes Goldsmiths, University of London, Coursera (Programming (Web) | Basic) HTML5 Coding Essentials and Best Practices, World Wide Web Consortium (W3C), edX

After making this list (which took a while), I can see that at least part of the cause of my problem is that I keep picking basic courses to take. Reflecting upon how I picked these courses, I think the sequence of events unfolded something like the below:

The MOOC platforms started to offer Specializations I did not keep track of which Specialization the course I took belonged to or the future classes in the Specialization When I finished a course and searched for another class on my topic of interest, I came upon more Specializations I looked at the syllabus for the first course and there would always be at least one item that I didn’t feel like I had a good grasp of The result is that I keep taking the first few courses in multiple Specializations

It’s surprisingly hard to keep track of courses when only auditing (this is probably intentional). Also, the lack of structure in the self-paced classes hasn’t worked so well for me either. But since I’m getting all of this top rated education available for no cost, I can’t complain and it is really on me to keep track of my learning.

At this point now with so many people sharing their own experience of self learning data science, it shouldn’t be too hard to come up with a curriculum as a student. Colleges also publish their data science curriculums, and using that as a basis might be a good idea.

Another point that I should consider is that most of the classes I’ve taken have been through Coursera, and Coursera restricting the quizzes and assignments has been detrimental to my learning. I should probably explore more non-Coursera options too.

So this is my big takeaway: keep track of courses and specializations and create a curriculum of courses that can build upon one another.",https://medium.com/human-in-a-machine-world/moocs-1260241e2a3e,[],2018-02-19 21:17:40.025000+00:00,891,"MOOCs, Online Learning Resources, Data Science, Basic Courses, Specializations"
Laurel & Hardy in Machine Learning,"Source: Flickr

“Well, here’s another nice mess you’ve gotten me into!”

This week I go even further back in time with my references, all the way back to the 1930s. In case some of the Millennial readers just thought “OK Boomer”, I’d like to clarify that I am a Millennial myself. It’s just that sometimes old-timey references make so much more sense than modern ones.

So for those of who don’t know of Laurel & Hardy, I urge you to go onto Youtube and watch a few videos before reading on. For those of you who have been fortunate enough to experience this duo in action, I hope you will see the parallel with what I am about to say.

The Laurel & Hardy of Machine Learning are known as Bias and Variance. Like Laurel & Hardy, Bias and Variance are joined at the hip. While both can lead to significant errors in your model, unlike Laurel and Hardy, who understood the power of errors in making people laugh, in Machine Learning errors are no laughing matter. If you’re not careful with Bias and Variance, they can downright cost you your job.

To help you avoid getting into a nice mess, let’s see why the Laurel & Hardy of Machine Learning deserve your attention.

Variance

Stan Laurel, Source:Flickr

“You can lead a horse to water but a pencil must be led.” — Laurel

Fans of the show will remember that Laurel had genuinely good ideas. He would come up with an interesting way of doing something, and share it with Hardy, who in turn would go “Tell me that again”. As soon as those words were uttered, Laurel would trip up and give a completely nonsensical version of his idea the second time around.

Laurel exemplifies Variance in Machine Learning. Variance is your model’s error when it is not able to generalize to data it hasn’t seen before. If you change the data even slightly, the model’s predictions are completely off. Like every iteration of a “Tell me that again” led to a different answer by Laurel, the same happens with your Machine Learning model if it has very high variance.

In other words, your model will learn the noise along with the signal so it simply won’t be able to separate things if there is a missing from lead!

Bias

Oliver Hardy, Source:Flickr

“ You’re actually using your brain. That’s what comes from associating with me.” — Hardy

Hardy, on the other hand, never understood Laurel when he first shared his ideas, which, at least to the viewer, were clear. However, Hardy had an uncanny ability to understood the non-nonsensical version that Laurel blurted out the second time around.

Clearly Hardy had a lot of difficulty separating the signal from the noise and that’s what Bias is all about. In a simplified sense, Bias can be defined as an error that is caused by your model not being able to learn anything from the training data.

In other words, you’re going to look smarter if you simply use your brain than use a model that has high bias.

So what?

Here comes the usual so what? Why should you care about these terribly forced analogies? You have to care about them because every Machine Learning problem is about balancing the trade-off between the two.

Source: elitedatascience.com

If your model has very high bias, it simply isn’t learning enough (or even learning at all), so that beats the point of Machine Learning. This might happen, for example, because you’re using a Linear Regression for a non-linear problem. So, one way to go from being a Hardy to becoming a Laurel is to use more complex models.

If you then make your model too complex, like using a convoluted backpropogating SVM, Laurel starts hallucinating pencils and horses like I just hallucinated the model’s name, so you want to avoid that too.

Basically, what you need to do is find a trade-off between the two, a happy place where Laurel & Hardy and you live in perfect harmony.

Until you get yourself into another nice mess that is! but that’s what the life of a Data Scientist is all about, isn’t it?

Now if there are any of you that have suffered this far, below I provide a link for a better explanation of the bias and variance trade-off, one that might help your brain recover from what you just read:

https://elitedatascience.com/bias-variance-tradeoff",https://towardsdatascience.com/laurel-hardy-in-machine-learning-a0e739b51225,['Anup Raj Satyal'],2019-12-04 17:47:26.741000+00:00,714,"Machine Learning, Bias, Variance, Laurel & Hardy, Linear Regression"
The AMO theory: Solving the Black Box Problem for Data Scientists,"Photo by Wonderlane on Unsplash

The AMO theory: Solving the Black Box Problem for Data Scientists

In data science, we often want to test input or stimulus and see if that will have an impact on the outcome. But the black box problem sits in between to explain why the input has an impact on the outcome. AMO theory is a great way to solve this problem because AMO factors are thought of as a model for explaining the mechanism for predicting the outcome. AMO theory can be applied in many different areas such as human resource management (HRM), marketing, law enforcement, etc. In this blog, I will explain briefly what the theory is about and how it can be used in data science projects.

1. What is the AMO theory

The AMO stands for Ability, Motivation, and Opportunity. I learned about this theory while doing my PhD in human resource management. The theory was developed by combining extensive research done by industrial psychologists and social psychologists to understand the factors involved in employee performance.

In brief, AMO suggests that performance is a function of ability (training and selection), motivation (incentives and feedback), and opportunity (environment). These three factors affect employee performance. Often HRM researchers consider the AMO theory to explain the linkage between HRM practices and employee performance, concluding that HRM practices should be aligned to support and increase employees’ ability, motivation, and opportunity to perform for an increase in return for investment toward HRM programs.

2. What is the black box problem in data

Let’s say you are given a dataset to see if you can build a model to predict an outcome, a typical regression modeling. Often we throw in a bunch of IVs and see if they predict the outcome. Let’s say you tweaked the model to get up the accuracy level up to an acceptable level. While presenting the results to the executives, someone asks how do you know that works or how do you know those factors directly impact/predict the outcome(s). At this point, you might be in trouble. You struggle to explain how the IVs impact the DV(s) and why they work because oftentimes we don’t know how to explain things like that when dealing with black-box problems in data science. It is a black box problem for data scientists because it can’t be easily measured directly but only inferred through observable behavior such as sales or an exam score.

3. How AMO Theory can solve this problem

Obviously, there are many types of data science projects. But for this theory, let’s focus on looking at building a model to predict human behavior.

Here are three examples of how the AMO theory can be used for your modeling.

Example 1: Human resource management and HR Analytics

As I described above, the AMO theory is commonly accepted as the answer to the black box problem in the HRM area. If you are looking into a set of inputs (e.g. HR practices, how much to invest towards training, restructuring,….) to predict the performance improvement gains, the AMO theory can guide you in explaining how those inputs can impact the performance. Does an input or a stimulus address the ability, motivation, or opportunity for the employees to perform? For motivation, are you looking to create an intrinsic or extrinsic motivation? What’s the impact of replacing full-time staff with short-term contractors?

Example 2: Marketing and sales

If you are trying to understand how and why someone is buying your product, the AMO theory could also lend you a hand. Does your model identify the potential buyers’ ability to purchase? Does the proposed marketing campaign stimulate the right people’s motivation to purchase? Are you taking a count of the right customers being at the right place to make the purchasing decision? Understanding these AMOs will help you to design a data science product and service that can maximize the impact of your marketing investment.

Example 3: Likelihood of someone committing a crime.

I personally have used this approach in a similar area recently. If you were given a project to predict the likelihood of someone committing a crime, AMO theory can guide you in explaining how those inputs would affect the likelihood of that person executing on their criminal intent. Does your model identify the potential criminals’ ability to commit? For motivation, are they motivated by financial gain or revenge? Are they living in an environment where it’s easy for them to get away with what they want (e.g., escape route planned)? Categorizing your IVs by these components helped me increase the accuracy as well as the explainability.

Again, these are interesting data science problems and I have on numerous occasions come back to the AMO theory as a guidance and a way to explain the results why and how people behave in a certain way. When it comes to big data analytics or machine learning models remember that you are not just trying to predict what is going on with your customers; you want to be able to explain your model to non-data folks. That’s where this theory can help make sense of all those boxes and arrows. Leave comments below if you find this article helpful.",https://towardsdatascience.com/the-amo-theory-solving-the-black-box-problem-for-data-scientists-91b2d1ac9195,['Ilro Lee'],2021-09-03 15:38:13.436000+00:00,853,"Data Science, AMO theory, Black Box Problem, Human Resource Management, HRM"
Why Hyper parameter tuning is important ?,"It is rare that a model will perform at the level you need for production just in the first instance. To find the right solution for your business problem, often you have to go through an iterative cycle. There are multiple pieces that come together to solve the intended machine learning puzzle. You may need to train and evaluate multiple models that include different data setup and algorithms, perform feature engineering a few times or even augment more data. This cycle also involves tweaking your model’s hyperparameters.

Hyperparameters are the knobs or settings that can be tuned before running a training job to control the behavior of an ML algorithm. They can have a big impact on model training as it relates to training time, infrastructure resource requirements (and as a result cost), model convergence and model accuracy.

Model parameters are learnt as part of training process, whereas the values of hyperparameters are set before running the training job and they do not change during the training.

Different types of Hyperparameters

Hyperparameters can be broadly divided into 3 categories —

Model hyperparameters — defines the fundamental construct of a model itself for ex. attributes of a neural network architecture like filter size, pooling, stride, padding.

defines the fundamental construct of a model itself for ex. attributes of a neural network architecture like filter size, pooling, stride, padding. Optimizer hyperparameters — are related to how the model learn the patterns based on data. These types of hyperparameters include optimizers like gradient descent and stochastic gradient descent (SGD), Adam, RMSprop, Adadelta and so on. More details are available here on Keras Optimizer page.

are related to how the model learn the patterns based on data. These types of hyperparameters include optimizers like gradient descent and stochastic gradient descent (SGD), Adam, RMSprop, Adadelta and so on. More details are available here on Keras Optimizer page. Data hyperparameters — are related to the attributes of the data, often used when you don’t have enough data or enough variation in data. In such cases data augmentation techniques like cropping, resizing, binarization etc.. are involved.

How Hyperparameters are selected ?

Each learner is unique.

This human quality also reflects in the machine learning process. There aren’t a defined set of rules that are applied for hyper parameter selection. Generally, hyperparameter selection and tuning is done manually. One of ways involves seeking help from someone who has domain experience and can guide to select different hyperparameters with appropriate values. Post which, model training is carried out and checked against the validation data.

Typically, these 2 approaches are commonly followed —

Grid search — set up a grid made up of hyperparameters and their different values. For each possible combination, a model is trained and a score is produced on the validation data. With this approach, every single combination of the given possible hyperparameter values is tried. While the approach performs a extensive sweep on all the possible combinations, it can be very inefficient in terms of training time and cost.

Random search — similar to grid search, but instead of training and scoring on each possible hyperparameter combination, random combinations are selected. You can set the number of search iterations based on time and resource constraints.

These 2 approaches are still primitive and requires trial and error method until satisfactory results are achieved. Due to laborious nature of the manual methods, it is preferable in recent times to use automated hyperparameter tuning solutions available from various cloud service provides such as AWS, Google and Microsoft.

These Auto tuning solutions uses methods such as gradient descent, bayesian optimization, exploration-exploitation trade-off to conduct a guided search for the best hyperparameter settings. for ex. Amazon SageMaker or Google AutoML automatic model tuning finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best for the desired metric.

Common Hyperparameters

Each algorithm learns with a variety of hyper parameters. However, following are the most common when it comes to machine learning.

Learning Rate (LR) — determines the step size at each iteration (epoch) while moving toward a minimum of a loss function. Since this process influences to what extent the model can shed off old information and acquire new information, it symbolically represents the speed at which a machine learning model “learns”. Generally, model learning starts at some random point (as a result of initialization) and sample different weights over many epochs . It also represents how far apart these samples are is the learning rate. It is crucial to achieve right learning rate as a larger learning rate would mean that model might overshoot the optimal solution and a smaller learning rate will increase training time to find the optimal solution. Frameworks such as Keras provide ways to automatically modulate the learning rate during training process. This is achieved through learning rate decay functions to allow the optimizer to change learning rate over the period of time.

Batch Size — This value indicates number of training samples that are used within each epoch. With smaller batch sizes it is observed that it is easy to get out of “local minima”. However, larger batch size can end up getting stuck in the wrong solution.

Hyper parameter tuning (optimization) is an essential aspect of machine learning process. A good choice of hyperparameters can really make a model succeed in meeting desired metric value or on the contrary it can lead to a unending cycle of continuous training and optimization.

The content written in this article is a collection of personal learning and experience. To share any feedback or comments, please use the clap/response feature on Medium platform. You can also reach out to me on Linkedin.",https://medium.com/analytics-vidhya/why-hyper-parameter-tuning-is-important-for-your-model-1ff4c8f145d3,['Niwratti Kasture'],2020-11-16 14:35:04.364000+00:00,952,"Hyperparameters, Machine Learning, Model Parameters, Optimizer Hyperparameters, Data Hyperparameters"
Presentando la Sociedad de Visualización de Datos,"Sign up for The 'Gale

By Nightingale

Keep up with the latest from Nightingale, the journal of the Data Visualization Society Take a look",https://medium.com/nightingale/presentando-la-sociedad-de-visualizaci%C3%B3n-de-datos-9100c1536919,['Elijah Meeks'],2019-02-23 18:21:54.390000+00:00,22,"at our latest articles, tutorials, and more.Data Visualization, Nightingale Journal, Sign Up For The Gale"
Analyzing credit card transactions using machine learning techniques,"Introduction

In this 3-part series we’ll explore how three machine learning algorithms can help a hypothetical financial analyst explore a real data set of credit card transactions to quickly and easily infer relationships, anomalies and extract useful data.

Data Set

The data set we’ll use in this hypothetical scenario is a real data set released from the UK government, specifically the London Borough ofBarnet. The full dataset can be downloaded from here:

https://www.europeandataportal.eu/data/en/dataset/corporate-credit-card-transactions-2015-16

In a nutshell, the dataset contains raw transactions (one per row) which contain the following information:",https://medium.com/david-vassallos-blog-posts/analyzing-credit-card-transactions-using-machine-learning-techniques-66e7e101b515,['David Vassallo'],2018-07-09 14:12:39.711000+00:00,84,"1. Date of transaction2. Transaction ID3. Merchant Name4. Amount Spent 5. Category (e.g., Food, Services, etc.) 6. Location (e.g., London Borough)  machine-learning"
On being (and remaining) a novice coder,"It isn’t pretty. But it got the work done.

When it became obvious that I would have at least a rudimentary understanding of coding and to finish my ‘medium data’ PhD project, I did the same things a lot of novice programmers do. I took a few classes (both within the university, as well as those offered by organizations like Software Carpentry and Ladies Learning Code). I began looking at the work ‘real’ programmers do on Github, and tried to collaborate (i.e., get some sweet example code) from more experienced people. And lastly, I spent hours at my computer, trying to tackle simple problems through endless internet searches for solutions and much frustration (so much frustration…).

A few years of this, and I would now call myself a competent beginner at coding. This state of ‘competent beginner’ means a few things. I can make nice graphics, manipulate databases and datasets with some ease, and successfully take on a data analysis project from beginning (data compilation and cleaning) to end (statistical analysis and getting pretty graphics). I’ve even have a few flashes of brilliance, a few moments where programming allowed me to perform a task that was beyond the realm of what I think ‘beginner’ programming means.

How to attain the next step of coding? This is the stage of truly elegant code, in which a thoughtful, design architecture determines how a program is written and executed. Even to an amateur, it is easy to see the difference between this type of code and ‘beginner’ code — it tends to be shorter, calls upon a well-defined subset of other functions to reduce the complexity of a single piece of code, and takes advantage of innate properties of coding languages (such as vectorization) to drastically reduce the computational time required to perform complex tasks. Much of this is covered within formal education around coding and computer programming, but it’s a step that most of us miss if you are primarily self-taught.

Perhaps making this leap is akin to learning a language — you begin by learning words, and then progress towards learning the architecture and design of the language itself in order to deploy these words within meaningful sentences. Learning to code is somewhat like this — you pick up how to do little tasks, and then assemble them in order to do all of the steps necessary to analyze data. What is the best way of making the next leap, beyond frankensteining code together towards designing truly elegant (as well as quickly executed) code?

I wonder whether this is a stage that can be truly self-taught; it clearly requires a degree of analysis and knowledge that is likely beyond most amateur data scientists. Having access to a knowledge pool of people who are adept programmers is likely an avenue forwards for DIY programmers. So too are opportunities for direct feedback on your own code (such as this R-specific study group in Vancouver, British Columbia). What is clear is that this next step in attaining coding literacy is a big one, requiring both outside help, as well as much time spent refining and thinking about how to actually write code.

And maybe coding literacy isn’t about attaining perfect code elegance. This is likely true for a lot of people who are like me — they don’t have a background in computer science or coding, but need skills in order to handle the complexity of data that modern science creates. Indeed, creating ingenious coders isn’t the aim of programs that promote basic coding literacy within non-computer sciences populations. So perhaps it isn’t the fact that my code needs many more lines in order to perform the same data operation as another person’s (and takes my computer more time to actually perform). Maybe the accomplishment is still in the fact that I, a non-computer scientist, was able to accomplish it in the first place.",https://medium.com/the-data-experience/on-being-and-remaining-a-novice-coder-b4d6bef144c,['Ashlee Jollymore'],2017-07-12 16:40:09.435000+00:00,644,"coding literacy, self-taught, Software Carpentry, Ladies Learning Code, vectorization"
How to Speak Data in Plain English,"Be Simple

You should always be asking yourself “What is the easiest way to explain this finding?”. Provide your findings to someone that has no context about the problem and see if they can understand what you’re saying. Write what you want to say, then write it again with only 50% of the words. Empathize with your audience by considering whether you could understand what you’re writing or saying if you had no knowledge of analytics.

Let’s take the following the statement in the complaint to the SEC…

“During the 87 month span analyzed, Madoff was down only 3 months versus GATEX being down 26 months. GATEX earned an annualized return of 10.27% during the period studied vs. 15.62% for Bernie Madoff…”

…and suggest an alternate way that might be easier to follow:

Over the 7 years studied, $100,000 invested with Madoff would have grown to $286,000 but only $203,000 for its competitor. Further, Madoff’s performance suggests that he was 90% more effective at avoiding negative returns than comparable funds.

Image by Author

We could also consider including a simple chart like the above to emphasize the point.

What did we change to simplify the interpretation of the data? First, months was changed to years since its more common for someone to think of that amount of time in years. Then, instead of an annual return, we put it on dollar values, which people can more easily compare and appreciate the magnitude as it compounds over time. Then, we presented the data on negative returns in a format that I believe better emphasizes the difference in performance and should make someone question how the difference could be 90%.

We should remove numbers where possible and add interpretation. The author mentions 3 negative months vs. 26 and 10.2% annual returns vs. 15.6% but what he’s really saying is that Madoff’s performance is uncharacteristically better than his competitor. For a non-technical audience, just give that answer and have the supporting data ready if necessary rather than giving the numbers and assuming they’ll come to the same answer.

Takeaway: Avoid jargon, be empathetic, and where possible show — rather than tell — your data. Give an answer; don’t give data and assume the person will arrive at the same answer.",https://towardsdatascience.com/how-to-speak-data-in-plain-english-9ee541064ac9,['Jordan Bean'],2020-12-10 19:04:44.643000+00:00,367,"Simplification, Communication, Empathy, Data Visualization, Jargon Avoidance"
Supervised Learning,"This article is about machine learning and it is a free download of second session of machine learning course in Stanford University by Prof. Andrew Ng. It is about linear regression, gradient descent, and normal equations.

In supervised learning we tell the closest answer based on some examples (training set) and try it to others. He introduced Alvin which was an autonomous driving system using neural network to drive. It was trained first by capturing video and saving driver actions and then it started to steer independently. It increased its driving accuracy by collecting more data from more videos. It was an example of regression problem.

Supervised learning can be shown as below:

Where x represents input variables (features) and y is used to show output variable (target) and h is hypothesis.

In training phase we try to minimize squared differences between the predictions of algorithm and actual targets. One of algorithms to achieve that target is gradient descent. Gradient descent looks like the process of little steps to come down from a hill. If we use multiple training sets then we call it batch gradient descent and in case of very large number of training sets it called stochastic gradient descent or incremental gradient descent which performs calculations until convergence.

There is also another algorithm which uses matrix operations without need for iterative operations.",https://medium.com/@razpoush/supervised-learning-2327340cef90,['Mehdi Razpoush Nazari'],2020-11-04 18:56:16.409000+00:00,220,"It is called normal equation.machinelearning, linearregression, gradientdescent, normalequations, supervisedlearning"
5 Hacky Data Visualization Techniques with Tableau,"5 Hacky Data Visualization Techniques with Tableau

Tackle Tableau limitations in simple hacky ways

Tableau is a powerful and easy-to-use data visualization tools. Yet sometimes the limitation of features will hold us back when we want to create an advanced dashboard.

My Tableau project

Line Separator Pagination Search Box Line Plot with Markers Text with a URL link Other cool Tableau features

1. Line Separator

A line separator could help your audience better understand the relationship between sections and the hierarchy of contents in your dashboard. There are many workarounds to create a line separator in Tableau, we’ll introduce the one using a text box.

Line Separator

1.1 Create a Text Object

Drag a Text box from the Floating Layer to the Dashboard

Drag a Text box from the Floating Layer to the Dashboard

1.2 Configure the Layout Tab

In the Layout tab

a) set the height of the Text box to 1-pixel

b) set the width slightly smaller than the width of the dashboard

c) set the background color as black

In the Layout tab, set height, width, and Background color

Now, you could see a line separator on your dashboard.

2. Pagination

Adding pagination to enable the audience to navigate between dashboards could help provide a smoother user experience and a better understanding of how the contents related to each other. Since Tableau doesn’t have the pagination feature, we’ll tackle it by using the Actions feature.

Pagination

2.1 Create a Sheet for Every Dashboard

Repeat below steps for all of your dashboards

a) Create a new sheet, name it with the number of your dashboard

b) Create a Calculated Field… with a string of dashboard number as content

Create a Calculated Field…

with a string of dashboard number as content

c) Drag the dimension to the sheet

Drag the dimension to the sheet

d) Drag the sheets to the dashboard

Drag the sheets to the dashboard

2.2 Configure Actions

a) Under the Dashboard bar, choose Actions…

Under the Dashboard bar, choose Actions…

We need to have an action configured for every dashboard we want to navigate to. Repeat below steps for every dashboard:

b) Click Add Action > Go to Sheet…

Click Add Action > Go to Sheet…

c) Source Sheets: Select All, this enables the action on all of your sheets, Below, only check the sheet corresponding to the dashboard number

d) Run action on: choose Select, we want to trigger action by selecting the pagination

e) Target Sheet: select the destination dashboard

Configure Source Sheets, Run action on, and Target Sheet in Actions

Now, if you click the pagination on your dashboard, you will be navigated to the corresponding dashboard.

3. Search Box

Search Box is useful when the audience is interested in a specific topic in a large dataset.

Search Box

3.1 Create a Parameter for Search Box

a) Set Data type to String and Current value to null

Set Data type to String and Current value to null

b) Under the dropdown of the search box, select Show Parameter Control

Select Show Parameter Control

3.2 Create a Calculated Field for filtering

a) Use CONTAINS() function to filter data

b) Rename the calculated field as Filter

Create a Calculated Field for filtering

c) Drag the filter dimension to the Filters section

d) Only Select True values

Only Select True values

Now you can type text in the Search box and the dashboard will render filtered data.

4. Line Plot with Markers

Sometimes we want to add a marker on our line plot to emphasize a specific data point. It may be easy to configure in Excel, but not in Tableau.

Line Plot with Markers

a) We first create a line plot in a way we usually do

Create a Line Plot

b) Then we drag our y variable to the Rows shelf again. A duplicated line plot will show below the original one. Under the second y variable’s dropdown menu, select Dual Axis. The two plots will then merge into one.

Select Dual Axis

c) Right-click on the right y-Axis, select Synchronize Axis

Synchronize Axis

d) Under the Marks section, select your second plot. Under the dropdown, select Circle. You can see circle marks on the line plot now.

Marks > Circle

5. Text with a URL link

Medium does not allow directly embed a URL link in the text. However, we can tackle this with the Actions feature.

Text with a URL link

5.1 Create a sheet for the text

a) Create a sheet for the text

b) Create a Calculated Field with the string of text as content

c) Drag the dimension to the sheet as Text

d) Change the Color to blue to indicate it’s link-embedded

5.2 Drag the text to the dashboard as a Floating object

5.3 Go to Dashboard bar > Actions > Add Action > Go to URL…

5.4 Paste the URL in the configuration, run action on Select

Now, you can navigate to the target URL by clicking on the text object.

6. Other Cool Tableau Features

6.1 Auto-generated Captions

If you’re run out of time and want a short caption for your viz, Tableau could do that for you.

Just go to Worksheet > Show Caption

Auto-generated Captions

6.2 Map Layers

Tableau offers a lot of Map features. Go to Map > Map Layers, you could change the background color, show the city border, and even automatically plot the US demographic data on your map.

Map Layers

— — — — — — — — — — — — — — — — — —

Feel free to comment below if you have any questions regarding Tableau.",https://towardsdatascience.com/5-hacky-tableau-techniques-cf3b039345ea,['Vanessa Leung'],2020-01-23 22:05:05.887000+00:00,850,"Tableau, Data Visualization, Dashboard, Line Separator, Pagination"
⌚Configure TensorFlow 2.0 Environment in Windows in 10 minutes 😁,"✅Step 1: Download and install/update the latest Version of Python from the official website (Don’t forget to check the ‘Add Python to PATH’)

https://www.anaconda.com/products/individual

Do check this

✅Step 2: Download and install Anaconda Installer from official website for Windows.😀

https://www.python.org/downloads/

✅Step 3: Open Edit Environmental Variables in Windows, and three new paths as given below:👍

1. C:\Users\yourusername\anaconda3\Scripts 2. C:\Users\yourusername\anaconda3 3. C:\Users\yourusername\anaconda3\Library\bin

In my System,

Paths

✅Step 4: Now,😊 Run the Command Prompt as Administrator and type the commands as given below:

1) conda create -n tf1 tensorflow ( Here tf1 is the name for our new environment , created especially to run TensorFlow ) 2) conda activate tf1

✅Step 5: Now You are ready to import tensorflow in this new environment.💪

Commands for importing Keras

1). from tensorflow.keras.preprocessing.image import img_to_array

2). from tensorflow.keras.models import load_model

✅Step 6: Now you are ready to work on your Machine Learning Project. Just run your programs by activating this new environment tf1.

THE END , thanks for reading….

⌚Commands for installing important libraries

pip install opencv-python

pip install pillow — upgrade

pip install opencv-contrib-python — upgrade — user

pip install imutils

Original Article written at:",https://medium.com/@gamecreator762/configure-tensorflow-2-0-environment-in-windows-in-10-minutes-8f4a3a9e1553,['Rahul Kushwaha'],2020-11-25 11:29:36.962000+00:00,169,"Python, Tensor Flow, Keras, OpenCV, Pillow"
How to Get Daily COVID-19 Data using R,"How to Get Daily COVID-19 Data using R

Photo by Dimitri Karastelev on Unsplash

Yes, the pandemic sucks. But as someone who looks at data for a living, there are at least opportunities for growth, even in the worst of times!

One way you can grow as a data scientist is to look at the ways infection rates affect other related variables (just be sure to take a removed analytical approach, as putting emotion into this can be… less than ideal).

To begin, just install the covid19.analytics package made my Marcelo Ponce and Amit Sandhel using the following command:

install.packages(""covid19.analytics"")

Then all you have to do is load the library using:

library(covid19.analytics)

After you’ve installed the package, you can start to play around with a bunch of functions that the coders built into the library. I’m going to start you off with two of the main ones.

covid19.data( )

This function looks at the current day’s COVID-19 information from John Hopkins Coronavirus Resource Center. Just load the data into a data frame:

df <- covid19.data()

This table will include:

Countries

Latitude and Longitude

Last update times

Confirmed cases

Deaths

Recovered

Actives

Incident rates

Case fatality ratios

This is pretty interesting in and of itself! Having this data in a table you can download to your computer is certainly useful. But the next dataset I’m about to show you is a little more flexible, and is probably a little more useful than just peaking at the current numbers.

covid19.US.data( )

This function actually outputs a table with daily confirmed cases for most US cities, and it gives it for every day since the pandemic hit America. Load it using something like this:

df <- covid19.US.data()

Hit View(df) and you’ll see that the dataset looks like this:

You can see in the bottom left that there are over 6,600 entries and over 300 columns. Most of the columns are daily numbers, so this dataset will continue to grow as the pandemic continues. Now that’s a lot of data!

There are a lot of interesting ways you might want to look at this, especially in tandem with other datasets. I’ll be looking at this in future tutorials. For now, I’ll just leave you to think about all the ways you might analyze these numbers. Be sure to take a look at the other functions the authors included as well!

If you have an interesting project that uses this data let me know in the comments below. I’d love to know how people are using this information.",https://medium.com/r-tutorials/how-to-get-daily-covid-19-data-using-r-25bde150df5e,['Josh Gonzales'],2020-12-08 03:40:15.858000+00:00,394,"RR, COVID-19, Data Science, Data Analysis, John Hopkins Coronavirus Resource Center"
The Non-Coder’s Guide to Image Classification,"These thousands of images are given to the model along with their correct category to teach the model patterns that do and don’t occur in images of hot dogs. This process is known as training. At the start of training, a model can not guess categories very well because it hasn’t had a chance to learn much. As it makes incorrect guesses, it adjusts it’s method of evaluating to try not to make that mistake again. As it makes correct guesses, it reinforces the process it used to get the correct answer.

Google Image search is constantly scraping¹ the internet gathering images to give us a useful tool. At the same time, it is creating a large dataset to classify images in to just about any category. Anywhere that you are storing or sharing photos, there is likely some sort of data collection happening behind the scenes by the company you are using for that task.

There is a lot of discussion to be had about the ethics of companies gathering and utilizing data that I will not get in to here. For the purpose of understanding image classifiers, it is important only to understand that it takes several thousand images of something to create a tool that can classify that object. Image classification is becoming increasingly ubiquitous for several reasons including that large imagesets are now readily available.

Neural Networks

The most effective type of model for processing images is called a neural network. Neural networks are named after the neurons in our brain because they are built to process information and recognize patterns in a manner that mimics the neurons in our brain.

In a neural network, information is passed from one point (known as a node) to the next to move through stages known as layers. In the image below, each square is a node and each column of squares is a layer.

Neural Network with 4 layers and 16 nodes

The input layer (leftmost column) consists of all the pixel information from a given image. The output layer (rightmost column) consists of each category of classification for the image.

Pixels shown at input layers and categories shown at output layer

Between any pair of adjacent layers, there is math going on from all nodes of one layer to all nodes of the other layer. In the image below, there are arrows added for every relationship where the neural network is making a calculation.

Relationships between nodes are shown by arrows.

In order to make a classification, each node combines the information from the arrows pointing to it. This combination leads the node to either turn off or turn on. Nodes that are on and on to varying intensities. If the node is turned on, it will pass information on to the next node and this process repeats.

At the output layer, the node for the category that is turned on to the highest intensity is selected as the classification for the image.

Each of these layers are a particular type of neural network layer known as a Dense Layer.",https://medium.com/swlh/a-non-technical-explanation-of-image-classifiers-5f9a4cc94224,['Marguerite Siboni'],2019-08-31 00:12:00.114000+00:00,500,"image-classification, Google-Image-search, Neural-Networks, Dense-Layer, Data-Collection"
These 9 startups are pushing the envelope of retail innovation,"These 9 startups are pushing the envelope of retail innovation

A lightning-quick look at pitches from startups at NRFtech 2017 and why you should care.

All big ideas start small. And often, the biggest innovations or ideas begin at the smallest companies.

For the third consecutive year, NRFtech featured a collection of startups in the “Tour of the Possible” program curated by Iterate Studio, a marketplace of startups indexing more than 162,000 companies focused on digital innovation.

NRF’s Tony Fontana takes a look at the 2017 class, each startup’s area of expertise and what they can bring to the table for catalyzing innovation in any organization.

Artificial Intelligence

Vue.ai — By combining visuals with online behavioral data, this “AI-powered personal stylist” is built to help companies create intelligent retail optimization in three core areas: dynamic personalization, shoppable videos and image generation for garments and other merchandise.

Ethan Schur is COO at NanoVR with a passion for advancing human progress through IoT, wearable and AR/VR.

AR and VR

NanoVR — Imagine a virtual brand ambassador interacting with customers inside a store, or a store window that showcases products and imagery without blocking merchandise. The “nanoscale precision” software is built to provide retailers with interfaces like these.

Theia Interactive — The company has made immersive VR experiences for several non-retail clients in the Northeast, bringing real renderings in architecture, engineering and construction to life. It could change the way store designers and merchandisers conceive the layout of retail stores, before and after shelves are stocked.

Big data and machine learning

Narvar — From order tracking and proactive customer communications to seamless returns, this turnkey solution keeps customers engaged from the buy-button to delivery. The platform also applies machine learning across billions of interactions to improve supply chain efficiencies, future transactions and customer experiences.

Persado — Powered by cognitive computing technologies, the platform leverages “emotionally tagged content” (images, words, phrases) in its database to help retailers understand which elements of a message work and how. The “cognitive content” approach helps automate marketing messages to deliver the right content across channels in a consistent brand voice and up to 23 different languages.

Sentiance — This machine learning and behavioral modeling platform uses first-party IoT sensor data to create highly specific and unique customer segments. The raw data, collected from smartphones, connected cars and similar devices, is obtained with explicit consumer consent and within compliance of global data protection regulations.

Qubole — Cloud infrastructure meets big data management in this platform, co-founded by members of the original Facebook Service Team, with the purpose of taking the pain out of data management by creating applications to streamline personalized experiences, consumer segmentation and more.

Chatbots

Reply.ai — This platform does more than provide an enterprise solution to chatbot creation and management. It uses AI to help bots learn from conversations over time while also providing them with the “smarts” to let customer service know a more personal, “human” touch is needed.

Robotics

Bossa Nova — Formed in February 2005 as a spin-off from Carnegie Mellon University’s Robotics Institute, the robot roams a store collecting terabytes of data about the products already on the sales floor. By scanning products on the shelves, making a map of the store and helping employees keep track of where items are located, the technology underscores how humans and machines can work together to make educated inventory management decisions.",https://medium.com/nrf-events/these-9-startups-are-pushing-the-envelope-of-retail-innovation-a4861029f800,[],2017-08-03 18:44:03.755000+00:00,547,"Artificial Intelligence, AR and VR, Big Data, Machine Learning, Chatbots"
Roadmap to Becoming a Successful Data Scientist,"1) Python Basics

In order to become a data scientist, you need to understand the basics of Python first. Because Python is the favored language for data science.

By going through this course of edX “Introduction to Python for Data Science” you will have some basic understanding of python.

2) Statistics & Probability

Stats is a key part of Data Science. If I am not wrong Data Science is all about statistics. This course of KhanAcademy will help you to understand the basic concepts of statistics and probability.

3) Data Analysis

Data preprocessing and visualization of data is one of the main components of data science. This course will teach you how computing and mathematics come together. The data analysis involves the collection of data preprocessing and interactive visualization of data. If you want to master this key component, click here.

4) Machine Learning for Data Science and Analytics

Now its time to learn some Machine learning stuff. After taking this course

“Machine Learning For Data Science & Analytics” you will be able to understand algorithms and how to create ML models.

5) Deep Learning

This is a complete book of Deep Learning in which you will have got clear and very precise knowledge about deep learning.

6) Intro to Relational Databases (SQL, DB-API & More..)

If you have learned python completely, this course will help you a lot to understand this course, because it is all about SQL queries and how you will use the relational database from your code using python example. You will learn the basics of SQL along with Python API for connecting python code to the database.

7) Intro to Hadoop and MapReduce

For the handling of Big Data, Apache Hadoop develops open-source software for the manipulation of Big Data. This course will make you understand the basics of Hadoop and the principles behind it.

8) Data Storytelling

To be a data scientist is not enough, you have to learn the way of representing data and its insights to the management, executives and other stakeholders. So, it is necessary to learn the data storytelling skill so that you wouldn’t get confused at the time of data representation. Click here.

*Important*

Learning is not enough if you don’t practice what you have learned. So I urged you to do at least 2 to 3 Kaggle projects to polish your data science skills.

This roadmap is enough for you to start your career as a data scientist. You don’t need to go anywhere or enrolled $1000 of courses to become a data scientist. You can do it on your own. You just need to start once with passion!",https://medium.com/dataseries/roadmap-to-becoming-a-successful-data-scientist-7daf4d2b0e11,['Mustufa Ansari'],2020-08-28 09:10:51.156000+00:00,421,"Python Basics, Statistics & Probability, Data Analysis, Machine Learning for Data Science and Analytics, Deep Learning"
The importance of domain knowledge,"Classifying x-ray images using deep learning

In this project, I chose to apply deep learning to classify chest X-ray images as belonging to a patient with pneumonia or healthy. The key takeaway from this experience is the importance of domain knowledge to shape your decisions. It is relatively straightforward to apply a model but the true value comes in questioning your decisions and careful evaluation.

The dataset was obtained from Kaggle and can be downloaded here. It contains 5860 images. The first step was to divide the data into training, validation and test sets. Here comes the first decision where domain knowledge comes in. Depending on the industry and type of data different splits are preferred. We chose a split of roughly 80% training, 10% validation and 10% testing.

We then explored the distribution of the images and noted that we had almost 3 times more images of x-rays of images with pneumonia than healthy ones.

Here is the next area where domain knowledge would help. How much data imbalance is reasonable? How easily can we obtain more data to compensate? For now, we did not make any adjustments but it is an avenue we would investigate further.

For preprocessing, we scaled the data to 100 x 100 pixels, as we want to be able to run the models locally using CPU. By doing so we lost some of the aspect ratio.

Preprocessing complete, I tried various neural network models. A key question is deciding which metric to use for evaluation. Here again domain knowledge would steer what is the required level of accuracy for a model to be useful and implemented. We also need to consider other metrics such as recall and F1 score. In particular it is important to minimise false negatives as these are cases where the model predicts that a patient is healthy when in fact they have pneumonia.

In the end, the chosen model was a Convolution Neural Network with dropout layers and the confusion matrix was as follows (on the unseen test data).

With only 3 false negatives, our model has high recall and thus maximises patient safety.

Whilst we would prefer an overall higher accuracy, our focus is on recall as this metric is particularly important for patient safety and to minimise the legal risk.

We would recommend the following actions:

1 — gather additional data, this classification was undertaken on a small sample size of around 5k images and served as a proof of concept

2 — address class imbalance to seek to improve performance using say oversampling techniques

3 — use this tool to support medical professionals whilst it is further improved on",https://medium.com/analytics-vidhya/the-importance-of-domain-knowledge-40d57ae0a91b,['Nadine Amersi-Belton'],2020-09-27 14:28:35.582000+00:00,429,"deeplearning, xrayimages, pneumonia, classification, preprocessing"
Time Series Forecasting Models,"What is time series forecasting?

Time series forecasting is a technique for predicting future aspects of data, in which we translate past data into estimates of future data. This technique is commonly used in business, as companies need to account for the uncertainty of the future, and being able to forecast data over time offers them a way to prepare for this.

Through the analysis of historical data, we can observe patterns of the data over time and generate forecasts of future data points. These forecasts can be crucial for companies, as they impact both short-term and long-term decision-making and strategic outlook.

In time series forecasting, the measurement of time is used as the independent variable in our model. These measurements of time will appear in successive periods and in many different intervals, including hourly, daily, weekly, monthly, and yearly intervals. Our model will be trying to produce demand, in which if we can observe a consistent pattern in demand, we can get adequate predictions from our time series models.",https://medium.com/analytics-vidhya/time-series-forecasting-models-726f7968a2c1,['Michael Pallante'],2020-02-20 04:21:57.368000+00:00,167,"time series forecasting, demand forecasting, data analysis, historical data, prediction"
Euclidean Distance and Normalization of a Vector,"Euclidean distance

Euclidean distance is the shortest distance between two points in an N-dimensional space also known as Euclidean space. It is used as a common metric to measure the similarity between two data points and used in various fields such as geometry, data mining, deep learning, and others.

It is, also, known as Euclidean norm, Euclidean metric, L2 norm, L2 metric, and Pythagorean metric.

Consider two points P1 and P2:

P1: (X1, Y1) P2: (X2, Y2)

Then, the euclidean distance between P1 and P2 is given as:

Euclidean distance in N-D space

In an N-dimensional space, a point is represented as (x1, x2, …, xN).

Consider two points P1 and P2:

P1: (X1, X2, …, XN) P2: (Y1, Y2, …, YN)

Then, the euclidean distance between P1 and P2 is given as:

Furthermore, we can carry on like this into 4 or more dimensions, in general, J dimensions, where J is the number of variables. Although we cannot draw the geometry any more, we can express the distance between two J-dimensional vectors x and y as:

There are quite a few distance measurement techniques, e.g. few other popular distance measures include:

Hamming Distance: Calculate the distance between binary vectors (Wikipedia).

Manhattan Distance: Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance (Wikipedia).

Minkowski Distance: Generalization of Euclidean and Manhattan distance (Wikipedia).

Euclidean is a good distance measure to use if the input variables are similar in type (e.g. all measured widths and heights). Manhattan distance is a good measure to use if the input variables are not similar in type (such as age, gender, height, etc.

Manhattan distance is used only if the points are arranged in square format and that too the distance between each of the points should be a multiple of the length of the side of a square. We rarely come across this kind of scenarios in realtime and the mostly used metric is Euclidean distance as we prefer it when working on completely numerical data. When we work on the text data, the cosine distance is an appropriate metric.

For different norms, the red lines indicate the set of vectors with norm 1. Left: Manhattan norm; Right: Euclidean distance.",https://medium.com/nerd-for-tech/euclidean-distance-and-normalization-of-a-vector-76f7a97abd9,['Rohan Paul'],2020-12-23 02:38:04.360000+00:00,356,"Euclidean Distance, Euclidean Norm, L2 Norm, L2 Metric, Pythagorean Metric"
Boost Your Exploratory Data Analysis with Pandas Profiling,"Exploratory Data Analysis (EDA) is one of the most important part of any data science work. It is quite hard to imagine a model without EDA. EDA is a general approach of identifying characteristics of the data we are working on by visualizing the dataset. There is a package called ‘Pandas Profiling’ with which we can have much analysis with just a single line code.

Pandas Profiling is a simple and fast way to perform exploratory data analysis of a Pandas Dataframe. The pandas df.describe(), info(), isnull(), etc, function is great and it gives you a compact summary of your data, but it is very basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis. This library generates a complete report for your dataset, which includes:

Basic data type information (which columns contain what)

Descriptive statistics (mean, average, etc.)

Quantile statistics (tells you about how your data is distributed)

Histograms for your data (again, for visualizing distributions)

Correlations (Let’s you see what’s related)

And more!

So this is how you install it:

You can install using the pip package manager by running

pip install pandas-profiling[notebook]

You can also install using the conda package manager by running

conda install -c conda-forge pandas-profiling

Start by loading in your pandas DataFrame

import numpy as np

import pandas as pd

from pandas_profiling import ProfileReport df = pd.read_csv('my_data.csv')

To generate the report, run:

profile = ProfileReport(df, title=""Pandas Profiling Report"")

Saving the report

If you want to generate a HTML report file, save the ProfileReport to an object and use the to_file() function:

profile.to_file(""your_report.html"")

Alternatively, you can obtain the data as json:

# As a string

json_data = profile.to_json() # As a file

profile.to_file(""your_report.json"")

Large datasets

Version 2.4 introduces minimal mode. This is a default configuration that disables expensive computations (such as correlations and dynamic binning). Use the following syntax:

profile = ProfileReport(large_dataset, minimal=True)

profile.to_file(""output.html"")

Ok, now we are going to explore and see what Pandas Profiling looks like and what it has in different datasets.

Below is the report generated contains a general overview and different sections for different characteristics of attributes of the dataset. There are columns we expect for Airbnb data, like price, number of reviews and minimum nights. Really quickly we can get a sense of what we are dealing with in our dataset. For example, the column “neighbourhood_group” is rejected, since it never has values (nan).

The report also shows which attributes have missing values. Each variable may have its missing values, and this tab provides information about how much of them is missing. There are a couple thousand listings without reviews. For the rest, the dataset looks complete.

Below we have a different dataset with different variables. With the report we can see all the variables in the dataset and their properties.

In the report we can go to the Variables section to get the information of every feature individually unlike Overview sections which provides information on the whole data set.

We can also view the interaction of different attributes of the dataset with each other. For example, in this Grad Acceptance dataset we can see the interaction between CGPA score and the chance of admission.

The report generated contains different types of correlations. You can get an understanding of the relationship between the features. You can also toggle and see different correlations like Pearson, Spearman, Kendall, and phik.

This section displays 1st 10 data points (head of 10) and the bottom 10 data points (tail of 10).

Exploratory Data analysis (EDA)is one of the first steps that is performed by anyone who is doing data analysis. It is important to know everything about data first rather than directly building models over it. The ‘Pandas Profiling’ package is a powerful tool for data analysis. With just a few lines of code, you get a very comprehensive report about the dataset.",https://medium.com/@am-nazerz/boost-your-exploratory-data-analysis-with-pandas-profiling-93174634b9c8,['Amin Nazerzadeh'],2020-09-23 18:13:09.351000+00:00,609,"Exploratory Data Analysis (EDA), Pandas Profiling, Data Analysis, Data Science, Descriptive Statistics"
The stage of maturity — Earth observation in a new era of space exploration,"“The maturing space industry is evident with players in both private and public sectors accelerating the recent advances in science and technology that makes operating in space more viable for commercial and research interests.”

Space commerce is enjoying a renaissance period mainly due to technological advances that have dramatically decreased cost and increased data and related services. A $17+ billion market (and growing), today’s space industry is on the verge of entering maturity — the stage of self-discovery, boldness, and adventure.

The maturing space industry is evident with players in both private and public sectors accelerating the recent advances in science and technology that makes operating in space more viable for commercial and research interests. This year thus far, the European Space Agency (ESA) tested its new 3D printed rocket thrust chamber to help design more efficient rocket engines. Almost at the same time, Orbex, a commercial British space company, unveiled the world’s largest 3D printed rocket engine that is lighter, fuel efficient, and expected to launch in two years. ESA and the UK Space Agency also recently examined preliminary designs of the world’s first air-breathing rocket engines that, if valid, will also reduce weight in exchange for more payloads. There is of course also SpaceX’s reusable rocket technology that successfully launched its Falcon Heavy in late 2018 with a payload of 64 satellites, and plans to launch its Big Falcon Rocket that will carry a heavier payload later this year. Seattle-based satellite design and manufacturing company, LeoStella, inaugurated its smallsat design factory signifying market opportunities. The Australian National University releases research results showing that 2D materials can withstand space’s harsh environment. The materials can enhance space instruments. Facebook is designing space laser communication satellites to provide internet access widely, while Amazon Web Services (AWS) commenced its Ground Stations satellite data collection services, allowing for faster and cheaper data processing. These innovations in space hardware and services are a game changer for the entire industry.

“. . . more non-traditional players are entering the field.”

A microcosm of the overall space industry trend, Earth observation — that is, the collection of information about Earth — is subsequently entering a new era as indicative of activities in the first quarter of 2019, including predictions on its commercial viability, that the sector will become conventional in the UK within a decade, and its emerging market in Europe. Moreover, more non-traditional players are entering the field.

Argentinian private firm, Satellogic, announced its plans to map the Earth at 1-meter resolution weekly and signed a launch agreement with China Great Wall Industry Corporation to launch a constellation of 90 satellites. With a U.S. $72 million grant from China, Egypt’s high-resolution EO satellite launched with success. In the meantime, earlier launched satellites are now providing data.

China’s National Space Administration successfully tested two EO satellites that provided information on air pollution and data that can monitor agriculture and crop yield. Both satellites launched in 2018. The Argentina Space Agency’s synthetic aperture radar satellites, which launched in October 2018 to examine Earth’s soil moisture and surface deformation, released its first images to the public. ESA released data on methane and ozone in Earth’s lower atmosphere from its Sentinel-5 satellite that launched in 2017.

Other data providers are moving data to the cloud. The United States Geological Survey made its Lidar data over the US available as a public dataset via AWS. NASA’s Earth Observing System Data and Information System (EOSDIS) is gradually moving its data to AWS. The move counterbalances the growth in data and the resulting difficulties for distributing and analyzing the data.

“Billions of image pixels recorded by the Copernicus Sentinel-2 mission have been used to generate a high-resolution map of land-cover dynamics across Earth’s landmasses.” credit: ESA

EO applications revealed exciting results and project ideas, from aiming to generate off-grid electricity for remote communities in Nigeria, global assessment of land degradation, and, measuring the height of Earth’s surfaces in detail and the loss of ice in glaciers. Last, but not least, are the improvements in computational science for satellite imagery processing. Using artificial intelligence (AI) and machine learning, ESA unveiled a global high-resolution land cover dynamics map that shows how vegetation and land productivity change through the year. Scientists are also using machine learning and surface deformation data to predict earthquakes. Radiant Earth Foundation announced its plans to be the catalyst for democratizing machine learning applications through ML Hub Earth commons, while the Rockefeller Foundation invested in Atlas AI that will use machine learning and “ground truth data to estimate economic activity and crop yield from satellite imagery.”

Inroads are also made with the SpatioTemporal Asset Catalogs (STAC) that allows users to search for imagery and other assets across multiple providers, and Analysis Ready Data (ARD) to make U.S. Landsat archive data more “straightforward to use.”

As exemplified by this first quarter of 2019, this is an exciting time for the Earth observation marketplace — and we look forward to providing you updates on how the market continues to change.",https://medium.com/radiant-earth-insights/the-stage-of-maturity-earth-observation-in-a-new-era-of-space-exploration-26f4e57941b8,['Radiant Earth Foundation'],2019-04-01 11:01:01.450000+00:00,826,"Space Industry, Earth Observation, EOApplications, Space Commerce, Machine Learning"
Analyzing credit card transactions using machine learning techniques — 3,"Introduction

In a previous article, we explored how PCA can be used to plot credit card transactions into a 2D space, and we proceeded to visually analyse the results. In this article, we take this process one step further and use hierarchical clustering to automate parts of our analysis, making it even easier for our hypothetical financial analyst to find anomalies within their data set (for a review of the data set being used, make sure to check out the first article in our series).

Clustering

Clustering is an unsupervised machine learning method … Read more at David’s Blog",https://medium.com/david-vassallos-blog-posts/analyzing-credit-card-transactions-using-machine-learning-techniques-3-9506a5ea9f24,['David Vassallo'],2018-07-09 14:13:22.546000+00:00,96,"unsupervised-learning, machine-learning, hierarchical-clustering, PCA, credit-card-transactions"
1 line to BERT Word Embeddings with NLU,"Including Part of Speech, Named Entity Recognition, Emotion Classification in the same line! With Bonus t-SNE plots!

With the freshly released NLU library which gives you 350+ NLP models and 100+ Word Embeddings, you have infinite possibilities to explore your data and gain insights.

In this tutorial, we will cover how to get the powerful BERT embeddings with 1 line of NLU code and then how to visualize them with t-SNE.

T-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.

1. Import NLU, load Bert, and embed a sample string in 1 line",https://medium.com/spark-nlp/1-line-to-bert-word-embeddings-with-nlu-f50d2b08cddc,['Christian Kasim Loan'],2020-10-27 18:36:15.849000+00:00,135,"of code2. Part-of-Speech, Named Entity Recognition, Emotion Classification3. t-SNE, Visualization, High-Dimensional Data4. Kullback-Leibler Divergence"
Machine Learning Interview Topics,"In this article, I will take you through the most important machine learning interview topics that you should know before appearing in your Machine Learning interview. Before heading over to the most important machine learning interview topics you must have a look at this article to learn how to prepare for a machine learning interview.

Important Machine Learning Interview Topics

The purpose of this article is to help you navigate the entire process, from A to Z, to find and secure an interview in a machine learning position, whether as an engineer, analyst, product manager, data scientist, researcher or whatever the role you determine as a career in machine learning.

I hope you liked this article on the most important machine learning interview topics. Feel free to ask your valuable questions in the comments section below.",https://medium.com/coders-camp/machine-learning-interview-topics-fe2990947304,['Aman Kharwal'],2020-12-19 04:03:50.163000+00:00,133,"machine learning, interview topics, machine learning engineer, product manager, data scientist"
Using Standard Deviation in Python,"Standard Deviation in Practice

Using Standard Deviation in Python

Mean, Standard deviation, and Error bar in Python

As this article mentioned, with Standard Deviation you can get a handle on whether your data are close to the average or they are spread out over a wide range. For example, if an employer wants to determine if the salaries in one of his departments seem fair for all employees, or if there is a great disparity, he can use standard deviation. To do that, he can find the average of the salaries in that department and then calculate the standard deviation. One scenario could be like the following; He finds that the standard deviation is slightly higher than he expected, he examines the data further and finds that while most employees fall within a similar pay bracket, four loyal employees who have been in the department for 15 years or more, far longer than the others, are making far more due to their longevity with the company.

In general, a low standard deviation means that the data is very closely related to the average, thus very reliable and a high standard deviation means that there is a large variance between the data and the statistical average, thus not as reliable.",https://towardsdatascience.com/using-standard-deviation-in-python-77872c32ba9b,['Reza Rajabi'],2019-08-15 14:50:12.959000+00:00,205,"standard-deviation, python, mean, error-bar, data-analysis"
Ten Deep Learning Concepts You Should Know for Data Science Interviews,"Deep learning and neural networks can get really complicated. When it comes to data science interviews, however, there are only so many concepts that interviewers test. After going through hundreds and hundreds of data science interview questions, I compiled 10 deep learning concepts that came up the most often.

In this article, I’m going to go over these 10 concepts, what they’re all about, and why they’re so important.

With that said, here we go!",https://towardsdatascience.com/ten-deep-learning-concepts-you-should-know-for-data-science-interviews-a77f10bb9662,['Terence Shin'],2020-12-10 04:04:50.070000+00:00,73,"deeplearning, neuralnetworks, datascienceinterviews, concepts, importance"
New trends and techniques for UI/UX design,"“… a fuelled addiction for new trends and techniques”

Cashless payments

Apple Pay and Android Pay are soon to be considered mandatory. By default, they should be an option – providing freedom – ability to purchase anytime and anywhere – offline and online. Other considerations are PayPal and Stripe – which leads to biometric authentication. Messenger and crypto currencies are also looming in the background.

Biometric authentication

Facial recognition, fingerprints, or voice recognition all aid in user satisfaction. Gone with the days of searching for usernames and passwords. Even then, the risks of staying logged in are less than on the desktop allowing permanent sign in or the option to log in directly from the email instead of going through the laborious task of updating the password.

Personalisation

There is a necessity to know precisely when, where, and how the users are doing what they need to do. We are constantly trying to get a location, currency, name, company name email address or phone number for both marketing and usability purposes. We want to give them a seamless experience, allowing them to find and/or purchase what they are looking for while guiding them through with reassurance. We can gather this information through timely prompts, social connectivity, marketing hooks or various API’s (ex. ConvertFlow).

Educate in context – ask in context

Through timely prompts for more information relevant user details can be requested. When showing a gallery, an email address can be requested to share, view and upload items or when viewing a map, the users location can be requested. Keeping in mind they may wish to add a different address than their current location – therefore requiring the option to enter a postcode.

The worst experience is when apps ask for the location as soon as it is opened. 9 times out of 10 the user closes it straight away as they have no purpose to provide it. Ideally the prompt is displayed in context – preparing the user for what’s to come and providing a seamless experience.

Improving the user experience is an ongoing process. Educate in context – ask in context. It’s a natural extension of personalised content – once we learn how users prefer to interact with the product we can deliver different experiences to different users with varied approaches.

Chatbots and messaging platforms

Mobile chat applications are amongst the most used apps on the market. Most likely we won’t see chatbots as a full replacement for regular GUI experiences, but they will be integrated into mobile apps to serve as assistants. Businesses will have real-time automated conversations with their customers.",https://medium.com/@scottheappey/new-trends-and-techniques-for-ui-ux-design-9b5db99ca3d7,['Scott Heappey'],2020-09-04 21:03:27.798000+00:00,422,"Cashless payments, Biometric authentication, Personalisation, Educate in context - ask in context, Chatbots and messaging platforms"
Visualization of customer feedback to get insights of main reasons behind their feedback — Word…,"Visualization of customer feedback to get insights of main reasons behind their feedback — Word Cloud

This article will focus on visualization of major reasons for customer giving negative reviews.

On technology front, I will be using “wordcloud” python library. I will be using airline industry dataset from kaggle.

https://www.kaggle.com/crowdflower/twitter-airline-sentiment/discussion

When we think of airline industry, the common causes are flight delay, baggage issue, hospitality etc. For one airline provider, delay may be the major issue and for other it can be baggage issue. Here, we just try to represent the reasons in the form of word cloud.

As usual, data is pre-processed before applying analytics. I will skip data pre-processing details and focus on word cloud generation.

To create word cloud in python, we just need few lines of code.

from wordcloud import WordCloud wc = WordCloud(stopwords=stop_words, background_color=""white"", colormap=""Dark2"", max_font_size=150, random_state=42) wc.generate(data_nouns_adj.reviews[c])

Initially, I picked up nouns and adjectives expressed in customer reviews, and generated word cloud as shown below.

It did not give right sense as required for my use case. I wanted to understand the main pain points shared by customers.

Next trail, I picked the verbs and generated word cloud. This gave me better insights of customer reviews. As shown in below image, for few airlines, cancellation of flight is the main pain point, for few airlines, delay is the paint point.

(We can club delayed and waiting keywords as both words tend to mean same)

The word cloud is good visualization technique to understand text data. In this technique the size of each word indicates its frequency and significance.

Business Use Case for Word Cloud

1. Finding customer pain points — and opportunities to connect

2. Understanding how your employees feel about your company

3. Identifying new SEO terms to target

The more details can be found below

https://www.boostlabs.com/what-are-word-clouds-value-simple-visualizations/

Previous Article

IBM Code Content for Developers",https://medium.com/rajesh-h-gudikoti/visualization-of-customer-feedback-to-get-insights-of-main-reasons-behind-their-feedback-word-ec655ac67ac,['Rajesh Gudikoti'],2019-04-23 07:40:11.120000+00:00,291,"customer feedback, word cloud, visualization, airline industry, data pre-processing"
"Three Different Things: November 5, 2019","Three Different Things: November 5, 2019

Polish Troll Farms, SQL v Python, and GDPR Fines

Photo by Christian Wiediger on Unsplash

1. Undercover reporter reveals life in a Polish troll farm

“The aim is to build credibility with people from both sides of the political divide. Once you have won someone’s trust by reflecting their own views back at them, you are in a position to influence them,” said Wojciech Cieśla, who oversaw the investigation in collaboration with Investigate Europe, a consortium of European investigative reporters. “Reading these communications, you can see how the leftwing and rightwing accounts would receive their daily instructions, how they would be marshalled and directed like two flanks of the same army on a battlefield.”

Once the political stuff winds down, these strategies will be the next level influencer marketing platform.



2. What SQL Analysts Need to Know About Python

SQL isn’t really designed for manipulating or transforming data in certain ways. Higher level data manipulation that is common with data science, such as statistical analysis, regressions, trend lines, and working with time series data, isn’t easy in SQL.

PSA: If you’re a data analyst in any field and you don’t know Python or R, you should do yourself a favor and learn one or both.



3. GDPR fines were meant to rock the data privacy world. They haven’t

On the day GDPR came into force across Europe (May 25, 2018) the French data protection regulator received a complaint about Google. Three days later another arrived at the door of the National Data Protection Commission (CNIL) and at the start of 2019, CNIL hit Google with a €50m (£43m) fine.

Not enough to make Google take notice even slightly.",https://medium.com/early-hours/three-different-things-november-5-2019-2853c94ff631,"[""Sean O'Brien""]",2019-11-05 13:10:02.440000+00:00,273,"Polish Troll Farms, SQL v Python, GDPR Fines, Undercover Reporting, Data Manipulation"
Data science-Create Tailored Algorithms,"Blackcoffer artificial intelligence solutions are easy to use out-of-the-box and are custom tailored to each individual client’s needs. Our end-to-end AI enabled platforms speed time to delivery, save costs, reduce risk, and deliver optimized results to give you an immediate competitive advantage and bolster your bottom line.

AI innovation enabled by faster processors, Big Data and novel algorithms

AI is “an area of computer science that deals with giving machines the ability to seem like they have human intelligence”.

Read More",https://medium.com/data-analytics-and-ai/data-science-create-tailored-algorithms-e4f4365e4496,['Ella William'],2019-06-14 11:11:30.590000+00:00,78,"AI, Blackcoffer, Big Data, Algorithms, Computer Science"
AI Summit | San Francisco and Wired to Wear,"Here are early confirmed Keynote Speakers obtained from the website: Tad Smith, President & CEO, Sotheby’s; Alan Boehme, Global Chief Technology Officer, VP of IT Services, and Chief IT Innovation Officer, Proctor & Gamble; Srini Venkatraman, Chief Data Scientist, Boeing; Swatee Singh, Vice President and Distinguished Architect, American Express; Nav Kesher, Head of Data Science, Facebook; Zoubin Ghahramani, Chief Scientist, Uber; Melody Ayeli, Director of Software Licensing and Complex Acquisitions, Toyota; Ningyu Chen, VP Data and Platforms, Macy’s; Thomas Stubbs, Vice President, Engineering and Innovation, The Coca Cola Company; Steve Chien, Senior Research Scientist, Autonomous Systems, NASA; David Newman, SVP Innovation, Wells Fargo; Ashish Lahoti, SVP, Global Data Technology, Charles Schwab; Anand Mariappan, Senior Director Of Engineering Data Science, Search and ML, Reddit; Dinakar Deshmukh, VP Data Sciences and Analytics, GE Aviation; Ganesh Harinath, VP, Head of AI Platform and Applications, Verizon; Gilad Lotan, VP and Head of Data Science, Buzzfeed; Piers Lingle, SVP of Customer Experience, Comcast; Denis Garagic, Chief Scientist AI and Machine Learning, BAE Systems; Danny Lange, VP of AI and Machine Learning, Unity Technologies; Ravi Boggaram, Sr Director, Digital Services, PepsiCo; Gil Arditi, Head of Machine Learning Platform, Lyft; Keoki Jackson, CTO, Lockheed Martin; Hans-Aloys Wischmann, VP Research and Innovation, Philips Research; Steve Eglash, Executive Director, Research Initiatives, Computer Science Dept, Stanford University; Mark Young, CTO — Climate, Bayer; Nimit Jain, Global Director of Data Science, Novartis; Rajeev Kalamdani, Sr. Analytics Scientist, Ford; Scott Mathis, CISO, RBC; Charles Givre, VP Lead Data Scientist, Deutsche Bank; and Somnath Banerjee, Director, Machine Learning, Walmart Labs.

Because we are on the advent of the most important events of this year, yet alone, this decade, it’s a monstrous undertaking pulling together the “best of the best” …so here we are.

25–26 SEPTEMBER 2019 | THE PALACE OF FINE ARTS

At the Museum of Science+Industry in Chicago, the “Wired to Wear” Exhibit is now open. See how this futuristic wearable technology will define how we dress such as a jacket that “barks” instead of your dog, racing suits that deploy with airbags, and headgear (with lasers) that help those with Parkinson’s the assistance to visualize their walking.",https://medium.com/aimarketingassociation/ai-summit-san-francisco-and-wired-to-wear-c459f8753b51,['Joanna Strom'],2019-08-01 18:10:43.373000+00:00,353,"Keynote Speakers, Sothebys, Proctor&Gamble, Boeing, American Express"
Considerations When Measuring Chatbot Success,"Considerations When Measuring Chatbot Success

And What Principles You Should Implement…

Introduction

Performance measures are important to organizations wanting to track their investment in a conversational interface…

But standards & metrics differ by industry and obviously by companies in each industry. Due to the nascent nature of the technology companies are also eager to learn from one another.

With some overestimate the importance and impact of their chatbot, and other heaving discounting the significant impact their conversational interface is having…

Industry Type Matters

Call Deflection

Obviously chatbots are implemented across a vast array of industries. These industries use different parameters. Parameters which they deem as crucial to the measuring of success in their environment.

Microsoft Power Virtual Agents have Analytics Built In

Banking and financial sectors use chatbots to perform existing tasks faster. And an important driver is lower call volumes and how much savings are incurred from call deflection.

Quality Conversations

The most common and probably important chatbot performance metric is conversation length and structure. In most cases conversation transcripts are reviewed and manually classed in order so points of improvement noted.

Organisations are aiming for shorter conversations and simplified dialog structures. A conversation or specific dialog always have a happy path which developers hope the user will find and stick to.

Digression in a Chatbot Conversation

A rudimentary and simplistic approach would be to have a repair path, or a few. Paths which intends to bring the conversation back to the happy path from points of digression. Hence ‘repairing’ it.

This approach might lead to a situation called fallback proliferation.",https://cobusgreyling.medium.com/considerations-when-measuring-chatbot-success-93aaaac0cb86,['Cobus Greyling'],2020-05-21 15:55:40.499000+00:00,246,"This is when the user is presented with too many options and gets confused.chatbot, performance measures, call deflection, conversation length, structure"
"Positive, Negative, or Neutral? Sentiment Analysis","Photo by Tengyart on Unsplash

Communication is complex. There is written, verbal, and non-verbal. With technology, communication is very important in written form. We type more often than speaking to our devices. Using algorithms for classification and natural language processing, it is possible to score text on a scale to find sentiment. What is the value of sentiment? Sentiment is the tine of the selected piece of text. That shows positive, negative, or neutral in tone for applications in marketing, healthcare, and other domains.

How It Works

Sentiment Analysis is found by ranking. It is a logical operation of sorting terms and scoring by association to find cues to grammar to determine parts of each sentence. Afterwards, there is scoring on grouped words by trained algorithm to assign a score for finding how positive or negative a selected text is based on generated rules. This can be reused with a reliability on different texts to determine sentiment.

Classification and Ranking

There are apps and libraries for using and creating an automated sentiment toll, Python has Natural Language ToolKit (NLTK), Lexalytics has several apps for sentiment analysis. The demo is featured here using a press release about the covid19 vaccine and Walmart Clinics.

Grouping terms is classification to create rule based probability selecting themes from the text. The list is highest to lowest with “19 vaccine progress” the highest. It is interesting that this grouping does not include “covid”, but the number “19”. The rules select and group nouns and adjectives for assigning value on a scale to classify as a term based on evidence generated from the algorithm.

The picture shows the text, word cloud, and sentiment. This used the information to generate the “positive” sentiment showing anticipation and relief from a working covid-19 vaccine.

Following-Up

Sentiment analysis makes scoring a passage of text easier and faster with “smart technology”, Artificial Intelligence enabled analysis of text. Often, associated with responses and reviews, Sentiment Analysis can provide insert to pharmaceutical communications to gauge reaction for estimate of supply and demand. In the example, sentiment is positive for the covid-19 vaccine and that demand is likely to be high.",https://medium.com/ai-in-plain-english/positive-negative-or-neutral-finding-sentiment-d6199710c008,['Sarah Mason'],2020-12-14 16:52:22.743000+00:00,348,"sentiment analysis, natural language processing, classification, ranking, artificial intelligence"
Analytics are no longer for humans. Here’s how we can fix that.,"Analytics are no longer for humans. Here’s how we can fix that.

Our vision and mission at Humanlytics: to humanize and democratize analytics so they’re accessible to everyone.

It’s time to make data analytics human friendly again.

Two years ago, I fell in love with data. A business school student at the time, I was extremely frustrated by the fact that much of business consulting is based on “assumptions” and “intuition” rather than concrete facts. The ability to analyze clients’ data in depth gave me confidence in the solutions I was recommending.

I then spent the entire summer learning about data science. This led me to become even more fascinated by the power of data, and how it could be used to benefit the lives of millions if not billions of people.

Unfortunately, there is a huge gap between what it could be and what it actually is. In the current world, our data is concentrated in the hands of the privileged few, the ones that have the money, influence, and power, to collect, analyze, and make sense of the data they have. On the other side, small players, especially small business owners, struggle to make sense of and obtain meaningful business insights even when using basic data tools such as Google Analytics.

Even worse, this “data gap” has widened over recent years. If we were to look at all of the data analytics startups of the recent past we would find that the majority of them are still making tools for large corporations to help that are focused on helping businesses understand customer behavior at a more granular level. This allows them to target customers more precisely, and eventually, make more money.

In these cases, rather than act as an agent of equality, data has been contorted and used as a tool of corporate machines to suppress and further exploit the ordinary people, it has become an agent of inequality…

Technology should be created for people, to fuel the pursuit and preservation of universal values such as equality, freedom and happiness, but data is not. As a machine of exploitation, data is no longer human, it is scary and boring and we need to change that.

I am not going to pretend that I am some sort of data analytics expert: I am a business person who happens to know data. But the fact is that the issues that I have described above do not require deep analytic capabilities to solve — they require compassion, close attention to user experience, and a deep conviction that technology should benefit the masses rather than the privileged few with the money and tools to take advantage of it.

What drove me to found Humanlytics, is the very thing that led Steve Jobs to found Apple, Bill Gates to found Microsoft, and Elon Musk to found Tesla. We all want to see a world where technology ( and in my case data) is useful to everyone, a world where the small businesses down the street can use data to better their customer experience and improve the experiences of customers — a world where data is used to create happiness and liberty, instead of prohibiting them.

Bill Gates, Steve Jobs, and Elon Musk. Image via Bookishly.

Today, my founding team and I embark upon a journey. We are not creating something that is ground breaking or fundamentally different. Rather, we are trekking in the other direction- we’re focusing on the small things and the fundamentals and we’re focusing on doing them really really well. This will help small business owners understand what data truly means and help them leverage the fundamentals into improving their own businesses.

The ultimate goal of our company is impact. That’s the very reason that, after graduating, I joined the Venture for America fellowship and moved to Cleveland (ventureforamerica.org). It is also the very reason I decided to forgo other opportunities to join my family business in China and pursue a career in data here in the United States.

I see a more urgent need here: where data knowledge is scarce, where people are frustrated, and where I can help. The bottom line is, like a spoiled millennial, I want to help people through my work and make an impact.

So, what’s next? At Humanlytics we’re still in the early phases but as we explore and experiment we want to bring you along for the ride so we will be publishing tons of free content and tools to help small and medium sized business owners explore tools like Google Analytics, Social Media Platforms, and E-commerce platforms. Most of the content is posted on our website (humanlytics.co), social media (twitter, facebook), and this medium account. And please, leave us some good feedback.

Don’t tell me “Oh, your tool is amazing, I never thought of that!” We don’t want to create fancy tools that are totally useless. We came into this expecting some of our content to be worthless and our tools to crash and burn. So please, don’t bear the pain nobly. Let us know.

Thank you for reading this article, this is something that I am really passionate about. If you have any questions, concerns, or comments, please email me at bill@humanlytics.co. I’m looking forward to our conversations! :D",https://medium.com/analytics-for-humans/analytics-are-no-longer-for-human-here-is-how-we-can-fix-it-bce2b00d19c5,['Bill Su'],2018-06-08 19:54:51.622000+00:00,862,"Data Analytics, Human Friendly Data, Business Insights, Data Gap, Technology Benefits"
Analyzing Sonic Fan Art with data science,"Analyzing Sonic Fan Art with data science

A tutorial on using BeautifulSoup to scrape DeviantArt

The Sonic fandom has achieved a level of notoriety that few fandoms on the Internet enjoy. The art is known for being distorted, disturbing and in many cases, explicit. In my latest Youtube video, I scraped DeviantArt to analyze fan art to determine whether or not it truly lives up to this reputation. This post will walk you through exactly how I did that.

I first wanted to get a sense of how many Sonic artworks there are on DeviantArt, a fan art sharing website. No scraping required, here — I simply searched for “Sonic” on DeviantArt and recorded how many results came up. I also did the same for similar characters, like Shrek and Pikachu. Here are those results, visualized:

The amount of fan art dedicated to each character on DeviantArt. Image by Author and character clip arts licensed under Creative Commons.

The amount of fan art dedicated to Sonic dwarfs that of the other characters, coming to around 1.4 million. Wow! Now that we’ve seen there’s a thriving Sonic culture on DeviantArt, it’s time to move towards answering our research question: Is Sonic fan art really that disturbing?

To start, I used BeautifulSoup to scrape the posts (the full code for this can be accessed here on Github). The following code scrapes the first 2200 links that came up when searching “sonic.”



urls = np.array([])

for i in range(50):

if i == 0:

url = base_url + ""q=sonic""

else:

url = base_url + ""page="" + str(i) + ""&q=sonic""

request=urllib.request.Request(url,None,headers)

if url in urls:

pass

else:

bs = BeautifulSoup(urlopen(request), ""html.parser"")

links = [item.get(""href"") for item in bs.find_all(attrs={""data-hook"" : ""deviation_link""})]

urls = np.append(urls, links)



len(urls) base_url = "" https://www.deviantart.com/search/deviations ?""urls = np.array([])for i in range(50):if i == 0:url = base_url + ""q=sonic""else:url = base_url + ""page="" + str(i) + ""&q=sonic""request=urllib.request.Request(url,None,headers)if url in urls:passelse:bs = BeautifulSoup(urlopen(request), ""html.parser"")links = [item.get(""href"") for item in bs.find_all(attrs={""data-hook"" : ""deviation_link""})]urls = np.append(urls, links)len(urls)

I then retrieved several attributes from each of these urls, including the post’s title, tags and the number of views, favorites and comments each post got. Here is the code for retrieving this data:

for i in range(num):

print(deviationurls[i])

request=urllib.request.Request(deviationurls[i],None,headers)

bs = BeautifulSoup(urlopen(request), ""html.parser"")

vals = [item.text for item in bs.find_all(""span"", class_=""iiglI"")]

tag = [item.text for item in bs.find_all(""span"", class_=""_3uQxz"")]

#print(vals)

if len(vals) == 3:

faves.append(vals[0])

comments.append(vals[1])

views.append(vals[2])

else:

faves.append(vals[0])

comments.append(0)

views.append(vals[1])

tags.append(tag)

titles.append(bs.find_all(""h1"")[0].text)

Sometimes, the comments field was empty, hence the if/else condition.

After doing this, you can now construct a dataframe of Sonic fan artworks for analysis! Yours might look different from mine because different art works might turn up for your query when scraping. But here is the resulting dataset from my scraper.

Here’s the distribution of years in which the artworks in my dataset were published. You can see that artworks from the 2010s represents the majority — perhaps people started posting more in this time, or perhaps those pieces are simply what came up first when I was scraping.

Distribution of years in which Sonic fan artworks were published. Image by Author.

I can’t include any of the images from the dataset in this Medium post, because I do not have the right to republish them here. I do, however, react to the top 10 most viewed and most favorited pieces of fan art in my video.

In my dataset, the two most viewed artworks were character makers (number one, and number two). This is not surprising, as a huge part of the Sonic fandom is to make original characters, or “OCs,” and write stories around these characters, like in any fandom. (The Sonic fandom, however, takes this pastime to a new level. Try googling your name + “the hedgehog” sometime to see what I’m talking about)

Animated shorts, comics and character references were also popular.

My ultimate goal was to analyze the tags that artists on DeviantArt use. Which tags are used with which? To do this, I took the “tags” column in my dataframe to create a correlation matrix. This was a somewhat involved process — the first step was to create a “corpus” of the tags for parsing.

data = np.unique(df.tags)[1:]

(data[0])

data_real = []

for strdata in data:

str1 = strdata.replace(']','').replace('[','')

l = str1.replace(""'"",'').split("","")

l = [item.strip(' ') for item in l]

data_real.append(l) texts = [[word.lower() for word in line] for line in data_real]

corpus = []

for text in texts:

corpus.append(‘ ‘.join(text))

corpus

After creating the corpus, I used scikit-learn to create the correlation matrix.

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(ngram_range=(1,1), stop_words = 'english') # You can define your own parameters

X = cv.fit_transform(corpus)

Xc = (X.T * X) # This is the matrix manipulation step

Xc.setdiag(0)

names = cv.get_feature_names() # This are the entity names (i.e. keywords)

df_tags = pd.DataFrame(data = Xc.toarray(), columns = names, index = names)

df_tags

And your resulting dataframe should look something like this (yes, it will be massive). Here, the number represents the amount of time both tags were used with one another.

Dataframe of correlations between tags. Image by author.

I first used Python libraries like networkx and matplotlib to visualize the network, but these were indiscernable messes. It was clear I needed a more powerful software.",https://towardsdatascience.com/analyzing-sonic-fan-art-with-data-science-fddcaa8bbb68,['Anya Vastava'],2020-09-06 02:52:15.965000+00:00,807,"Sonic, Deviant Art, Fan Art, Data Science, Beautiful Soup"
Setup Machine Learning Environment/OpenCV/TF On M1 Mac (Apple Silicon),"//I Suggest To Do all this Inside miniforge3 dir for that

// cd miniforge3

unzip opencv.zip

unzip opencv_contrib.zip

cd opencv-4.5.0

mkdir build && cd build

//Here Take Care Of Paths of OPENCV_EXTRA_MODULES_PATH and

// PYTHON3_EXECUTABLE If you're Beginner watch the YouTube video

//And If Inside miniforge3 just place your <username>.

cmake \

-DCMAKE_SYSTEM_PROCESSOR=arm64 \

-DCMAKE_OSX_ARCHITECTURES=arm64 \

-DWITH_OPENJPEG=OFF \

-DWITH_IPP=OFF \

-D CMAKE_BUILD_TYPE=RELEASE \

-D CMAKE_INSTALL_PREFIX=/usr/local \

-D OPENCV_EXTRA_MODULES_PATH=/Users/<username>/miniforge3/opencv_contrib-4.5.0/modules \

-D PYTHON3_EXECUTABLE=/Users/<username>/miniforge3/envs/ml/bin/python3 \

-D BUILD_opencv_python2=OFF \

-D BUILD_opencv_python3=ON \

-D INSTALL_PYTHON_EXAMPLES=ON \

-D INSTALL_C_EXAMPLES=OFF \

-D OPENCV_ENABLE_NONFREE=ON \

-D BUILD_EXAMPLES=ON ..

make -j8

//""8"" is the number of cores To be used(This Step Takes Time)

sudo make install

//Linking OpenCV To Conda Environment

mdfind cv2.cpython

//From the output Copy the Path similar to the below one

""/usr/local/lib/python3.8/site-packages/cv2/python-3.8/cv2.cpython-38-darwin.so cv2.so""

cd

cd miniforge3/envs/ml/lib/python3.8/site-packages

ln -s PasteYourCopiedPathHere",https://medium.com/@trayansh/setting-machine-learning-environment-on-m1-mac-apple-silicon-959836bf494d,['Trayansh Dewangan'],2021-04-18 13:55:22.936000+00:00,100,"//opencv, miniforge3, unzip, cmake, python3"
Succeeding Lewa!. Using Machine Learning to find…,"The Dataset and K-Means :

I have used the Fifa 21 dataset from Kaggle for this approach. The link contains datasets from Fifa 15 up to 21. Now coming to K- Means Clustering.

Since the problem requires finding similar traits to those of Lewandowski’s I decided to use an unsupervised technique in clustering to find the similarities.

K-Means Clustering is a technique in which given the number of clusters N, data can be divided into these N different clusters based on the features of each data entry. The features are numeric and the clusters are formed iteratively. Data within each cluster is similar to other data within the same cluster. This similarity can be decided based on different metrics two of them being the Euclidean Distance and the Manhattan Distance. To know more about K-Means please refer to this link.

The features that I chose for clustering the players were based on physical and technical attributes and were more focused on a striker’s attributes. So don’t expect Messi to show up in the same cluster as Lewandowski since they are miles apart physically!",https://towardsdatascience.com/replacing-lewa-5808cf020bfa,['Raj Sangani'],2020-12-04 04:52:20.054000+00:00,179,"KMeans Clustering, Fifa21Dataset, Euclidean Distance, Manhattan Distance, Striker Attributes"
Outlier Detection with K-means Clustering in Python,"Outlier Detection with K-means Clustering in Python

Data with outliers detected by Author

K-means clustering is used when you want to cluster your data into k groups. I will tell you how to catch the outliers that stay far away from these groups. We will do it by deciding a threshold ratio. For each cluster, the data stay out the threshold ratio will be counted as an outlier.

Data to be used by Author

When you look at the plot, it is easier to see which points we aim to catch. In the yellow cluster, there is no outlier and there is one and two in the green and purple clusters respectively. So, we aim to catch three outliers in this data set.

We first import the necessary libraries and compose the data. Then, the k-means clusters predicted by setting k = 3. Lastly, we get the plot above by running this code.

import numpy as np

import matplotlib.pyplot as plt

from sklearn.cluster import KMeans

from scipy.spatial.distance import cdist # composing data set

data = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 9], [7, 9], [7, 7], [12,10], [25, 24], [24, 24], [24, 25], [25, 25], [25,20], [20,25]]) # kmeans model, setting k = 3

km = KMeans(n_clusters = 3)

clusters=km.fit_predict(data) # plotting data set

plt.scatter(*zip(*data),c=clusters,marker = “x”)

Now, we will find the centers of clusters and then calculate the distances between each point to the centers of its cluster.

# obtaining the centers of the clusters

centroids = km.cluster_centers_ # points array will be used to reach the index easy

points = np.empty((0,len(data[0])), float) # distances will be used to calculate outliers

distances = np.empty((0,len(data[0])), float) # getting points and distances

for i, center_elem in enumerate(centroids):

# cdist is used to calculate the distance between center and other points

distances = np.append(distances, cdist([center_elem],data[clusters == i], 'euclidean'))

points = np.append(points, data[clusters == i], axis=0)

You may ask which algorithm do we use to calculate the distances and can we choose any other one. As you may recognize, in the cdist function, we give distance type as a parameter which is ‘euclidean’. You can replace it with any other one that cdist accepts.

After obtaining the distances of points, now, we will decide a threshold ratio as a percentile and find the outliers.

When you decide on a threshold ratio th, you are sorting all distances of all points (to their own centers) and then saying that I want the points to be outliers that are above percentile th. I set it to 80 but you can play around with it.

percentile = 80

# getting outliers whose distances are greater than some percentile

outliers = points[np.where(distances > np.percentile(distances, percentile))]

So, we are done! The only thing we left is to visualize our data with outliers detected.

fig = plt.figure() # plotting initial data

plt.scatter(*zip(*data),c=clusters,marker = “x”) # plotting red ovals around outlier points

plt.scatter(*zip(*outliers),marker=”o”,facecolor=”None”,edgecolor=”r”,s=70); # plotting centers as blue dots

plt.scatter(*zip(*centroids),marker=”o”,facecolor=”b”,edgecolor=”b”,s=10);

Data with outliers detected by Author

The blue points in the plot represent the center of clusters. The cluster colors have changed but it isn’t important. The outliers are signed with red ovals.

If you want to use this algorithm to detect outliers that are staying out of all data but not clusters, you need to choose k = 1.

# setting k = 1

km = KMeans(n_clusters = 1)

Outliers caught after setting k = 1 by Author

In that case, it is not that meaningful to choose k = 1 for this data set, but when we do, we see that one blue dot is on the center of all data and the furthest points are circled with red ovals.",https://medium.datadriveninvestor.com/outlier-detection-with-k-means-clustering-in-python-ee3ac1826fb0,['A. Kübra Kuyucu'],2021-02-01 13:56:26.459000+00:00,575,"k-means clustering, outlier detection, Python, cdist, euclidean distance"
Stop Using Print to Debug in Python. Use Icecream Instead,"Stop Using Print to Debug in Python. Use Icecream Instead

Photo by Kenta Kikuchi on Unsplash

Motivation

If you are using print to debug your code, you might find it confusing to look at many lines of output on your terminal and then try to figure out which code each output belongs to.

For example, running the script below

will give you

30

40

Which one of these outputs is num1 ? Which one of these outputs is num2 ? Two outputs might not be so bad to figure out, but what if there are more than 5 different outputs? To try to find the source code that is responsible for the output can be time-consuming.

You might try to add text to the print statement to make it easier to figure out:

num1 30

num2 40

The output is easier to read, but again, it is time-consuming to write out the text. Is there a way to print the code that is responsible for the output without additional text like below?

ic| num1: 30

ic| num2: 40

That is when icecream comes in handy.

What is icecream?

Icecream is a Python library that makes print debugging more readable with minimal code.

To install icecream, use

$ pip install icecream

Let’s try it out by print the output of a Python function.

By using ic , we do not only see the output but also see the function and its arguments! How convenient! The color in your terminal will also be as colorful as the outputs shown above.

Inspect Execution

To locate where the code was executed, you might do something like what is shown below to find which statement was executed

I'm user

Icecream makes it easier for you to do something like the above by simply running ic() without additional text

Now you know that the code at line 5, which is in function hello , was executed while the code at line 7 was not.

Custom Prefix

If you would like to insert a custom prefix such as the time the code was executed to your print statement, icecream also allows you to do so.

Now the time that the code is executed is automatically shown in the output! How cool is that?

Can I get more context?

Besides knowing the code that is responsible for the output, you might also want to know which line and file the code was executed from. To know the context of the code, add includeContext=True to ic.configureOutput()

Now you know that the first output was executed by the function plus_five from the file icecream_example.py at line 7.

Delete All Icecream after Finish Debugging

You can use icecream solely for debugging while using print for other purposes such as pretty printing

Since you can distinguish between debugging print and pretty print, it is much easier for you to search and delete all the ic statements after debugging.

After deleting all debugging prints, your code is clean!

Conclusion

Congratulations! You have just learned how to make print debugging more readable using icecream. Icecream has been a great debugging tool for me, and I hope you will find it useful as well.

I like to write about basic data science concepts and play with different algorithms and data science tools. You could connect with me on LinkedIn and Twitter.

Star this repo if you want to check out the codes for all of the articles I have written. Follow me on Medium to stay informed with my latest data science articles like these:",https://towardsdatascience.com/stop-using-print-to-debug-in-python-use-icecream-instead-79e17b963fcc,['Khuyen Tran'],2021-01-13 16:13:50.618000+00:00,550,"python, debugging, print, code, icecream"
The World-Class Data Scientist,"We’ve all seen the ‘controversial’ Medium articles that say that MOOC’s don’t make you a Data Scientist, and also the ones that say you should get Certificates accredited by various organizations to prove that you’re actually worthy of calling yourself a Data Scientist.

Whilst it may be true that these various pathways alter the chances of success, are they really sufficient in being able to call yourself a successful Data Scientist?

To call yourself a success in any field is a subjective matter so it’s important to first know what you want out of a journey. Do you want Money? Freedom? Do you want to just have a casual understanding or do you want to go toe-to-toe with the world's best academics?

To be a success , or even world class is something that I think is hugely subject and changes over time, but from my experience of working under and with some of the best minds in the world: the following are simply perquisites:

Know what you don’t know

What’s your goal and where do you see yourself 10 years from now? When I started on my journey, I was 19 and had a vague picture of where I wanted to go and like many of my contemporaries, serendipity played a big hand on the path I took.

A good teacher and a good guide is a fantastic starting point. Speak to your role model and if you don’t have one: find one. Then you need to make a list of everything you need to know and cross-check your current knowledge set against where you stand.

Photo by Daniel Mingook Kim on Unsplash

Learn about who the experts are in what they’re doing

Keep an eye on the top 3 or 4 people or institutions in the field. Regardless of which discipline you’re in, it’s always important to know where the bar is and where you stand on the totem pole.

Benchmarking is central to the theory of Data Science and likewise, it should be central to your perspective in your discipline.

We’re in an era of perfect information and in an era of openness, use that to your advantage. Watch their youtube videos and read their academic papers, and most importantly, try to get in touch and have a chat!

A few times in my career, I overlooked this point. Missed opportunities or paths I should have trodden further down are now just memories, but, we learn something from these mistakes. Get good at spotting what you like and don’t like.

Make your foundation strong

When I say learn, I mean you really have to learn. Data Science and Machine Learning require top of the line Mathematics and Statistics. I’d recommend not using sklearn or other out of the box tools (yes, that includes Tensorflow ) until you can stand for yourself and take on the big guys in advanced discussions.

I’m really sorry but this is debatably the most important part of the whole story. You need to work your ass off and make sure your maths and stats are up to scratch. You want to watch youtube videos, download pdfs, or even learn whilst making a project — but please, know your weakness and keep fighting it!

Seriously, the best data scientists aren’t geniuses: they just work harder than everyone else.

Make projects with others — get feedback

This is really underrated. Working with others on Data Science or Machine Learning projects gives you a deeper understanding of the problem you face because your partner will generally have a different perspective.

Everyone has their own story and background so it helps to really diversify your view. Mathematical techniques are gold-dust in Data Science and whether you’re confused on why you should choose L1 over L2 regularisation on a simple regression problem, or debating on how many layers to have in your DNN , these kinds of questions only come to you when you have to justify your actions with other people.

Photo by Dmitry Ratushny on Unsplash

Build something that fixes a problem

Think about your own life and think of something that makes you more than 10 minutes to do. Turn this into a list and go through them one by one. Can you use Machine Learning or Data Science to tackle them? I assure you — you can.

Fixing problems isn’t easy and a lot of the time searching for problems often gets you into a convoluted mess. However, understanding the lifecycle of problems get you familiar with the whole ecosystem that we live in.

We’re in a time where our lives feel symbiotic: half of it is online and the other half is idly walking around the screen to screen. There are so many problems around us: often, you just need to look with a different lens for them all to be clear.

Go back to square 1

So you learned a bit, you taught yourself a bit, you solved a problem and now you’ve accomplished your mission. Now what? Well, this comes back to my first point: know your end goal.

If you’re still not satisfied, re-group and speak to an expert again or reach out to who you look up to. It’s not easy, but this perpetual loop of taking a look at how your career is shaping is a key way to ensure that you are on the right path.

I assure you if you keep repeating this loop, you’ll make it to the top. If you don’t believe me, comment below and I can give 101 examples of friends who’ve succeeded far beyond belief by following the top pattern.

At times the path is steeper than others, so make sure to stay focused and stay cognizant of your current position. You can never be too sure when your next break will be.",https://medium.com/towards-artificial-intelligence/the-world-class-data-scientist-2bc400b63442,['Mohammad Ahmad'],2020-08-30 15:17:51.355000+00:00,944,"MOOCs, Data Science, Certificates, Mathematics, Statistics"
Why AI won’t take all of our jobs,"History shows that human ingenuity will generate new industries

One of the growing concerns of the 21st century is that artificial intelligence will take over jobs. AI has become one of the most hotly debated subjects. People are talking about it on all ends of the spectrum: from AI having limitless potential, to it bringing the complete destruction of mankind.

Many people are under the assumption that it is inevitable that AI will take over all jobs, causing a massive unemployment crisis across the world. But AI replacing jobs is just another phase in the evolution of mankind and is no different than the many revolutions in labour we have faced in the past.

The industrial revolution and the information revolution were both deeply transformative and unsettling.

During the industrial revolution, many believed that technological innovations would displace many people’s jobs. They were absolutely right, jobs were replaced. Goods and people were transported by train rather than by animal. Cars replaced horses, and more people started travelling by air than by water. Although jobs were replaced, a new set of jobs was discovered. There was more demand for train conductors, pilots and engineers that knew how to deal with new equipment and technology.

Mechanical engineers are the ones that work in many of these newly-created fields, and it’s now one of the most in demand engineering disciplines in the labour force. Without the industrial revolution, some of the fields such as auto and aerospace wouldn’t be as successful today.

Again, during the information revolution, people were afraid that computers, data management programs and software would replace jobs. And again, we can see the same pattern. Some jobs have been lost, but they have been replaced often manyfold by other careers.

Many of the world’s richest people have made their money from technology companies that capitalized off the development of computers and the Internet. Not only that, the Internet has produced many more jobs that wouldn’t have existed 10 years ago. Jobs in fields such as software engineering, computer science, app development, drone technology, Internet content, marketing and more.

The rise of the robots is now being termed the “robotic revolution” by many. It is now considered a given that autonomous vehicles will eventually replace many taxis and truck drivers. But because of the robotic revolution, there will be new jobs and fields that haven’t been discovered yet. Eventually, professional institutions will offer robotic engineering as part of their engineering programs to teach a new generation on how to be employed in this new era.

Because of the many technological innovations mankind has made in the last two centuries, it’s hard to believe that the robotic revolution can cause massive unemployment.

H istory has proven time and time again that human inventiveness continues to triumph.

However, we shouldn’t always assume that a past technological shift is directly comparable to a future technological shift. That being said, mankind has always found a way to find new jobs once the old jobs have gone.

Many people say: “this time it’s different”. And yes, there is a slight possibility that this revolution is an outlier compared to the others and is the end of jobs. We’ll just have to wait and see.",https://medium.com/lassondeschool/why-ai-wont-take-all-of-our-jobs-bb77b7340d8a,['Fawaz Khan'],2018-04-11 15:44:21.811000+00:00,526,"Robotic Revolution, Industrial Revolution, Information Revolution, Automation, Innovation"
Is GPT-3 ‘Human’?,"How does GPT-3 Work?

If you’re reading this article, then you’ve probably heard of GPT-3, which is a language model that’s been developed by the OpenAI organization. The development procedure behind GPT-3 was not novel per se (compared to its predecessors), but it was just bigger and better.

Photo by Jukan Tateisi on Unsplash

I’m assuming the reader has an understanding of machine learning but as a quick overview: a language model is a neural network that takes in a sentence that has been augmented into the form of a ‘word-embedding’. This form makes it much easier to build a probability distribution around each sentence and thus narrowing the scope of the problem.

Now, Language models have been around for some time but were limited in their ability to actually create reasonable answers. If not tuned correctly, probabilistic models are just pretty crappy at generating responses, but GPT-3 has been such a big success because the results have been comparatively great.

Now the GPT-3 model uses the same architecture as their GPT-2 predecessor but is different (and better) in that it’s wider, with more layers, and more data to run on. Being ‘bigger’ doesn’t necessarily mean things are better, but in this case, it does.

How do we measure ‘Human-ness’

Now the concept of ‘human-ness’ is a somewhat broad definition. It’s qualitative, but if an AI is in-differentiable in actions from a Human, it’s said to pass the Turing Test.

Alan Turing (1912–1954) was one of the greatest British Mathematicians and Computer Scientists for his particular achievements as a code breaker for the UK Government in WW2. Following this, he developed the Turing test in 1950, which assesses a machine's ability to demonstrate behavioral intelligence that is identical to humans.

GPT-3 and Today

The benefit of training a model in a ‘representation’ type of form (like word-embeddings) is that the model can generalize better. When a model can generalize better, it can make connections and (in this case) answer questions based on previously unseen data. Given that GPT-3 has been trained on such a big corpus, this further increases its power.

Photo by Francois Gha on Unsplash

The reason that machine learning engineers are excited when an AI exhibits Human Intelligence is that it’s actually so difficult to demonstrate it fully. In Kevin Lacker’s blog, he goes over a number of examples to try to understand how human GPT-3 is. as to how

Take the following common questions:

Q: What is your favorite animal?

A: My favorite animal is a dog.



Q: Why?

A: Because dogs are loyal and friendly.



Q: What are two reasons that a dog might be in a bad mood?

A: Two reasons that a dog might be in a bad mood are if it is hungry or if it is hot.



Q: How many eyes does a giraffe have?

A: A giraffe has two eyes.

This current passage is phenomenal for a huge number of reasons:

GPT-3 has answered all the questions very well GPT-3 demonstrated an opinion (favorite animal, and why) Question 3 is a bit more of a structured question, and despite this, GPT-3 answered it well.

As the corpus was so big: factual questions and relatively easy to master:

Q: Who was president of the United States in 1801?

A: Thomas Jefferson was president of the United States in 1801. Q: What city is in the northwest corner of Ohio?

A: Toledo is in the northwest corner of Ohio. Q: What highway do you take to get to North Lake Tahoe from Oakland?

A: Highway 80 is the highway you take to get to North Lake Tahoe from Oakland.

However, answering these questions isn’t too informative as these could easily be part of the training corpus. Rather, questions that sound logical but in-fact are impossible to know — these are not recognized appropriately. This tells us that GPT-3 can understand some form of logic, but it struggles to identify logical limits to questions.

Q: Who won the World Series in 2021?

A: The New York Yankees won the World Series in 2021.



Q: Who won the World Series in 2022?

A: The New York Yankees won the World Series in 2022.



Q: Who won the World Series in 2023?

A: The New York Yankees won the World Series in 2023.

Moreover, logical questions can be further used to demonstrate limitations to GPT-3:

Q: When counting, what number comes before a million?

A: Nine hundred thousand and ninety-nine comes before a million.

and here it struggles to answer the question fully:

Q: If I have a marble and a paper clip in a box, put a pencil in the box, and remove the marble, what is left?

A: A paper clip.

and here:

Q: If I have two shoes in a box, put a pencil in the box, and remove one shoe, what is left?

A: A shoe.

Nonetheless, the fact that it even answers the question (and in a lot of cases it answers them correctly) is very impressive. The overall set of results here is incredibly promising and as the problem is still unsolved, a lot of work will be done in this space in the coming years…watch this space!",https://medium.com/towards-artificial-intelligence/is-gpt-3-human-46452ab4c881,['Mohammad Ahmad'],2020-08-11 20:14:19.199000+00:00,824,"GPT-3, OpenAI, Language Model, Neural Network, Word-Embedding"
Self-Driving cars really are the panacea to many of our cities’ woes.,"A million lives saved per year

While it is true that cars have never been safer and the number of accidents per mile driven is way down from the height in 1980: 1.12 versus 3.36 deaths per 100 million miles traveled, it is still the case that over 35,000 people died in car crashes in 2015 alone. [source] And this is only for the U.S. If you consider the worldwide loss of life in auto accidents, the number soars to more than 1.3 million each year.

The computer driving your car will be better equipped than you at handling a wide array of hazards and dangers.

Not only can autonomous cars anticipate conditions far better and react more quickly than any human, there will be many ways in which the cars on the road will communicate with each other in ways we have only imagined. This will allow for decisions, both at the micro-level — in the car and at the macro-level — on the street or even citywide, that save lives.

That brings us to the next greatest improvement…

The end of traffic

There won’t be mirrors in cars of the future.

That same orchestration that will allow your vehicle to avoid that pothole, construction zone, or accident will also end traffic. They will be able to speed up and slow down in unison as well as predict patterns based on past data and observation. Your robot car will remember not to take 5th Street through the park on the 3rd Thursday of the month when the bikers take over all lanes.

Sure, that’s when all cars are automated, but that is many years away. What effect can a single self-driving vehicle have on traffic? Turns out, it can still be quite large. A new study out of the University of Illinois at Urbana-Champaign suggests that the addition of just a small number of autonomous cars can ease the congestion on our roads.

It’s interesting that these improvements can occur even with a single vehicle in a flow of 20 other cars. And it’s also worth noting that the level of autonomy required to have this effect isn’t the kind that Waymo, Uber, and others are seeking to build — it’s more akin to the adaptive cruise control already featured in many higher-end cars. So while we might have to wait a little longer for all of autonomy’s effects to be felt, its ability to reduce traffic congestion could be here rather sooner than we anticipated. [MIT Technology Review]

Not only will traffic be reduced or nearly eliminated, speeds can increase safely and people can get to their destinations faster. In the future, there may not be set speed limits, but instead fluid guidelines based upon traffic, road, and weather conditions.

Reclaiming our time

While our time in the vehicle will likely be reduced per the traffic mitigation topics above, the time we do spend in cars will be transformed. No longer will we have to sit forward and pay attention, crane our necks, and get stressed out. We will get this time back, and we can do with it anything we want. Want to work on your laptop, talk on the phone, play games, or read? You aren’t thinking big enough. Plus, as a recent article from The Atlantic City Lab, points out:

Less than 5 percent of survey respondents claimed they’d even attempt to get any work done.

People may not always work in their cars in the future, but they will be dressed monochromatically.

How about sleeping? Having lunch? Catching up on that Netflix series? Playing Pandemic with three friends Having a wine and cheese tasting event on the way to the theater? Doing something else on the way to the theater?There are going to be whole new industries formed around furnishing and decking out our vehicles once there is no need for a steering wheel and other controls, instrumentation, even windshields.

A cleaner earth

It almost goes without saying (but shouldn’t) that a huge advantage to getting more cars off the road is a huge reduction in emissions. That will only be good for the air we breathe, our cities, and the Earth as a whole.",https://medium.com/alttext/self-driving-cars-really-are-the-panacea-to-many-of-our-cities-woes-b9d2bae4c55f,['Ben Edwards'],2017-10-24 00:35:32.288000+00:00,684,"Autonomous Cars, Car Safety, Traffic Reduction, Time Reclamation, Emissions Reduction"
Non-Probability Distribution,"In previous blog we covered probability distribution and its types, now we proceed to Non-Probability distribution and its types.

Non-probability sampling is a sampling technique where the odds of any member being selected for a sample cannot be calculated.

Non-probability sampling is defined as a sampling technique in which the researcher selects samples based on the subjective judgment of the researcher rather than random selection.

Types of Non-Probability Sampling:

a)Convenience Sampling :

Convenience sampling which is also known as availability sampling is a specific type of non-probability sampling method. The sample is taken from a group of people easy to contact or to reach. For example, standing at a mall or a grocery store and asking people to answer questions would be an example of a convenience sample.

The relative cost and time required to carry out a convenience sample are small in comparison to probability sampling techniques. This enables you to achieve the sample size you want in a relatively fast and inexpensive way.Limitations include data bias and generating inaccurate parameters. Perhaps the biggest problem with convenience sampling is dependence. Dependent means that the sample items are all connected to each other in some way.

b)Judgement Sampling:

Judgment sampling is a common non-probability method. It is also called as purposive method. The researcher selects the sample based on judgment. This is usually and extension of convenience sampling.

Judgment sampling may be used for a variety of reasons.

In general, the goal of judgment sampling is to deliberately select units (e.g., individual people, events, objects) that are best suited to enable researchers to address their research questions. This is often done when the population of interest is very small, or desired characteristics of units are very rare, making probabilistic sampling infeasible.

c)Quota Sampling:

A sampling method of gathering representative data from a group. As opposed to random sampling, quota sampling requires that representative individuals are chosen out of a specific subgroup. For example, a researcher might ask for a sample of 50 females, or 50 individuals between the ages of 32–42.

Quota sampling is used when the company is short of time or the budget of the person who is researching on the topic is limited. Quota sampling can also be used at times when detailed accuracy is not important. To create a quota sample, knowledge about the population and the objective should be well understood.

d) Snowball Sampling :

As described in Leo Goodman’s (2011) comment, snowball sampling was developed by Coleman (1958–1959) and Goodman (1961) as a means for studying the structure of social networks.

Snowball sampling (or chain sampling, chain-referral sampling, referral sampling) is a nonprobability sampling technique where existing study subjects recruit future subjects from among their acquaintances.Snowball sampling analysis is conducted once the respondents submit their feedback and opinions.Used where potential participants are hard to find.

Advantages of Snowball Sampling

The chain referral process allows the researcher to reach populations that are difficult to sample when using other sampling methods. The process is cheap, simple and cost-efficient. This sampling technique needs little planning and fewer workforce compared to other sampling techniques.

Disadvantages of Snowball Sampling

The researcher has little control over the sampling method.

Representativeness of the sample is not guaranteed.

Sampling bias is also a fear of researchers when using this sampling technique.

THANK YOU KEEP LEARNING :)",https://medium.com/ai-in-plain-english/non-probability-distribution-a15da752a013,['Megha Singhal'],2020-04-18 16:11:46.349000+00:00,531,"Non-Probability Sampling, Convenience Sampling, Judgement Sampling, Quota Sampling, Snowball Sampling"
What is Support Vector Machine (SVM)?,"Hello you Machine Learning enthusiasts! This article is going to be on yet another well known classification algorithm known as ‘Support Vector Machine’.

Following topics will be covered in the upcoming sections:

(1) What is Support Vector Machine (SVM)?

(2) How does SVM work?

(3) Advantages and Disadvantages of SVM

(4) Conclusion

(1) What is Support Vector Machine (SVM)?

As we discussed earlier SVM is an algorithm that is used to solve classification problems. The objective of SVM algorithm is to find an optimal hyperplane that maximizes the margin between two classes. Hyperplane is a generalized term to identify the decision boundary in an n-dimensional space.

Figure 1: Graphical representation of SVM

Figure 1 shows the SVM algorithm in action. The decision boundary is dividing the plane into 2 halves, thus the data points can fall into their respective classes.

Now that we have seen what SVM looks like, let us look at the working of this beautiful algorithm.

(2) How does SVM work?

Before getting into the nuance of this algorithm, let us understand what ‘Support Vector’ are in Support Vector Machine.

Support vector is the perpendicular line from the decision boundary to the closest points in both the classes.

What does Maximum Margin mean in SVM?

Figure 2: Pictorial Representation of SVM (Image Source: Google)

The decision boundary (bold line) in Figure 2 is what separates the plane into two halves for the two classes. Hyperplanes (Positive and negative) acts as a buffer, with respect to the decision boundary, so that when new set of data points which can fall somewhere near the decision boundary, falls into it’s respective class. This is one advantage of this algorithm, which is not available in other algorithms such as Logistic Regression, where a new data point can be misclassified if it falls somewhere near the decision boundary. Thus, SVM is considered to be one of the powerful classifiers in machine learning.

The working mentioned above is for Linear SVM (When the dataset can be classified into two classes by a linear line). But what happens when the classes are not linearly separable? This is when we use kernel tricks, where certain functions are used to transform data that are not linearly separable in n-dimensional space to a higher dimension, where it is linearly separable. The below figure is a representation of non-linear SVM.

Figure 3: Pictorial Representation of Non-Linear SVM (Image Source: Google)

If we observe carefully, we can see that unlike Linear SVM, the hyperplane is not straight forward. In order to make them linearly separable, a higher dimensional hyperplane had to be generated. Thus, this allows to make a generalized assumption about the data, irrespective of the feature complexity.

Now that we have understood the basic understanding of SVM algorithm, let us now look into it’s advantages and disadvantages.

(3) Advantages and Disadvantages of SVM

(i) Advantages of SVM

Handles non-linear data efficiently: As we have seen with the non-linear SVM, thanks to the kernel tricks, it can create hyperplanes which can separate the two classes effectively and efficiently.

As we have seen with the non-linear SVM, thanks to the kernel tricks, it can create hyperplanes which can separate the two classes effectively and efficiently. Regularization capabilities: SVM are known to have L2 regularization which helps them to prevent over-fitting.

SVM are known to have L2 regularization which helps them to prevent over-fitting. SVMs can be used to solve both classification problem (SVC) as well as regression problems (SVR). Although SVC are more preferred and widely used than SVR.

SVMs are known to be memory efficient.

(ii) Disadvantages of SVM

Difficult to interpret: Unlike some of the algorithms like decision trees, SVMs are difficult to analyze and interpret.

Unlike some of the algorithms like decision trees, SVMs are difficult to analyze and interpret. Choosing an appropriate kernel function: As in the case of solving a non-linear problem, we have seen that certain kernel tricks can be used to handle these kinds of problems. The problem arises in choosing the right kernel function so that a right model is fit to the data and predictions are made.

As in the case of solving a non-linear problem, we have seen that certain kernel tricks can be used to handle these kinds of problems. The problem arises in choosing the right kernel function so that a right model is fit to the data and predictions are made. Training these models can be time consuming if the dataset size is too large.

Hyper parameter tuning these models can be a difficult task.

(4) Conclusion

After going through this article we now know what SVM algorithm is, it’s inner working, it’s advantages and it’s limitations. If understood and applied aptly, SVM can be a powerful tool in your arsenal.

Hope you enjoyed reading this article!!",https://medium.com/@anirudhhebbarp/what-is-support-vector-machine-svm-318f3608ba3c,['Anirudh Hebbar'],2020-11-12 15:10:31.629000+00:00,767,"support vector machine, classification algorithm, hyperplane, SVM, kernel tricks"
Jobs at Risk,"A recent study by McKinsey Global Institute examined how robotics will affect labor and the economy. The study estimated that 800 million jobs (1/5 of all jobs) will be impacted by advances in robotics. (1)

Among the jobs most impacted include brokers, accountants, office staff, machine operators, and food service. Lower skilled, repetitive tasks are most likely to be replaced.

What does this mean for family finances in the years to come?

Jobs that require human interaction and creativity will be safer from robotic replacement. Education and flexibility in learning skills will give people an advantage in maintaining financial security. It will be crucial that everyone have the ability and drive to be more dynamic and more proactive in their vision of what they are able to do. (2)

According to McKinsey, “Workers with the most sophisticated digital skills are in such high demand that they command wages far above the national average. Meanwhile, there is a growing opportunity cost for the organizations and individuals that fall behind.” (3)

In addition, many people are working multiple jobs or “gigs” to bring home the same income of a pre-Great Recession W2 job. Many are becoming contractors or self-employed, many not by choice, and this requires a new outlook and mindset they are not prepared for.

Drawing a line in the sand and not pursuing new skills and opportunities will result in a reduced quality of life and compromised financial insecurity.

What does this mean for retirement planning in the years to come?

You will need to consciously set aside more money into savings? More Companies are eliminating employee benefits. History has shown that Employees that do not have a company 401k are not likely to save for retirement, and this sets up millions of people for a dangerous situation.

People that experience multiple job changes in short periods of time tend to cash out their 401k to transition to their next job. They also tend to be slow in setting up new retirement strategies.

What does this mean about protection strategies like life insurance and disability insurance?

Again, if people are changing jobs, if companies are cutting back on employee benefits, if government programs are getting trimmed due to budget deficits… for all these reasons individuals need to act on their own behalf to set up protection plans using individual life and disability insurance. Protect your family from the potential disasters that could cripple them for the rest of their lives.

Technology is changing the economy and it is an irrepressible force. Laws will not delay the change. It is critical that people understand the change that is coming and become better prepared and enabled to succeed.

Take control of your family’s financial future, do not leave it to corporate decision makers. Develop your ability to learn new skills. Save more for retirement. Manage and reduce risk. Build and maintain liquid savings. Set up life and disability insurance outside of group plans.

If you want to discuss this in more detail, please contact me.

Retirement Income. Tax Efficient Planning.

Life Insurance. Disability Insurance

Socially Responsible Investing

To learn more contact:

James Cox

Cell: 267 323 6936

Email: james.cox@FFGadvisors.com

First Financial Group 150 South Warner Rd. Suite 120 King of Prussia, PA 19406

The opinions expressed here are those of the author and not necessarily that of Guardian/Park Ave Securities.

Links to other sites are provided for your convenience in locating related information and services. Guardian, its subsidiaries, agents, and employees expressly disclaim any responsibility for and do not maintain, control, recommend, or endorse third-party sites, organizations, products, or services, and make no representation as to the completeness, suitability, or quality thereof.

Registered Representative and Financial Advisor of Park Avenue Securities LLC (PAS). OSJ: 7101 Wisconsin Ave Suite 1200, Bethesda, MD 20814 301–907–9030 Securities products and advisory services offered through PAS, member FINRA, SIPC. CA insurance license #0I64535. First Financial Group is not an affiliate or subsidiary of PAS or Guardian. Guardian, its subsidiaries, agents, and employees do not provide tax, legal, or accounting advice. Consult your tax, legal, or accounting professional regarding your individual situation.

2020–93360 exp 2/22",https://medium.com/@jamescox-ffg/jobs-at-risk-3ee35de8e9e7,['James Cox'],2020-02-18 14:42:41.632000+00:00,661,"Robotics, Labor, Economy, Retirement Planning, Life Insurance"
Natural Order vs Predictive Power,"Transformations of the feature space typically come with a natural ordering of the transformed features. For instance, the commonly used Principal Component Analysis (PCA) produces transformed features (or principal components) that are sorted according to how much of the original dataset’s variance is captured by a principal component. Accordingly, the first principal component captures more of the original dataset’s variance than the second component and so on.

Feature selection is a staple of the machine learning process and aims to increase model performance and reduce model complexity by eliminating features that do not provide much value to the model. Intuitively, feature selection eliminates features that are either irrelevant to the predictive task at hand or features that are redundant. The natural ordering of transformed features serves as a good heuristic for eliminating features. For instance, a common practice is to include principal components that explain the bulk majority of the variance in the original dataset (say 98%) and discard the rest. Nice and simple, right? or is it?

As it turns out, there is no guarantee that the natural ordering of principal components (for instance according to the amount of variance they explain) will match the ordering of principal components according to their predictive power (how much do they make the model better at predicting the target variable). It could very well be the case that a higher principal component PCj has more predictive power than a lower principal component PCi (where i < j) even though PCi explains more of the data’s variance than PCj.

The key intuition behind this observation is that PCA (and other statistical feature transformation techniques) are unsupervised statistical learning procedures where no particular target feature is being optimized for, rather the model is optimizing for a better representation of the underlying statistical structure of the data (according to some metric). In short, the target variable of the overarching prediction process is irrelevant to the feature transformation procedure.

In their brilliant book “Applied Predictive Modeling”, Max Kuhn and Kjell Johnson give a simple example of a binary classification task where, on a particular (real) dataset, the second principal component PC2 (in this case accounting for only about 8% of the variance in the data) separates the two possible outcome classes in a much better manner than the first principal component PC1 (which in this case accounts for 92% of the variance in the data). The difference in predictive power can be seen clearly from the box plots below: PC2 exhibits a lot less overlap between the two classes than PC1 and consequently PC2 has more predictive power than PC1. Find the book here and read more about this particular case on the authors’ blog here.",https://medium.com/optima-blog/natural-order-vs-predictive-power-2fee5b55f2b1,['Yusuf Saber'],2015-12-26 22:07:47.559000+00:00,446,"Feature Selection, Principal Component Analysis, PCA, Predictive Power, Unsupervised Learning"
Data Science Essential week 7: Recommendation system(อธิบาย code),"This publication consists of articles related to Data science and AI written from Botnoi’s data scientists and students.

Follow",https://medium.com/botnoi-classroom/data-science-essential-week-7-recommendation-system-%E0%B8%AD%E0%B8%98%E0%B8%B4%E0%B8%9A%E0%B8%B2%E0%B8%A2-code-36b140674c81,['Krirk Arunoprayote'],2020-10-02 09:32:13.512000+00:00,18,"Data Science, AI, Botnoi, Data Scientists, Students"
Power BI Modelling,"This is my question,

I want to report on my inventory and transactions by day. Each day has sales so I want to know my sales.

Here we can already tell we want to report on inventory and sales. These will likely be our fact tables. It sounds like there is a date time dimension involved so we probably need a day time table.

These are our fact tables we want to use,

Image by author

We will also need a table to join the inventory and transactions table together, so we can filter by the dates. We will need a date table. Here we can use

Date_Filter = CALENDAR( DATE(2021,04,01), DATE(2021,12,31))

Power BI will automatically create a table for you. The result will be a table like this.

Simple date table — image by author

Here in this model, you can see that I will join the date table with the transactions date and inventory date.

Image by author

This is a basic model you need to remember.

Here the date table joins the inventory and sales table on the date columns. If you are to filter on date, it will “grab” the data from both tables. The Date table filters both the Inventory and Transactions table in a 1:Many relationship.

Now let’s expand this a little bit further and say,

I want to see the inventory and sales by type and who bought them.

Here we can tell that we will probably need something to filter again, both tables by the inventory and sales. We have the date dimension, but here the request is just by type. I will have to create a table that takes the unique product types of both tables in order to filter them all.

Here you can use Power Query to select the individual columns of the product types in the inventory and transactions table, append them and run a remove duplicate to create a filtering table.

In DAX, you can use DISTINCT to create a table that contains the unique values of all the inventories.

Product_Type_Filter = DISTINCT(Inventory[Inventory]))

This is how the model now looks, 🍦 🍩🍰

Image by author

Here I can filter by Date, by the product type and see only the customers that bought them.

Image by author

Once again, it’s a simple filter that allows you to filter through both tables to get at an answer.

It’s also important to talk a little about the filtering direction. Let’s look at this model again, I’ve added in the arrows.

Image by author

What do these arrows mean?

It means that when a user filters using the date filter or the product filter, and because these filtering tables are joined to the Inventory and Transactions tables, they will be able to filter them in one direction.

It’s important to filter on the same direction as the join.

Here, I have filtered on cake in the inventory table, which is not designed as a filtering table, and it will pull up a transaction of ice cream in the transactions table. As you can see, it can get very confusing quickly if you don’t use the proper filters.

image by author

I hope with this article you get the basics of creating a data model.

Remember this. The model you build doesn’t have to be hard. It just has to be simple and works. For it to work, it has to be something you and others can understand quickly.

No one will care how elegant your model looks if you can’t produce a report that was requested.

To recap,

Analyze your questions Identify your fact tables Creating filtering tables to join them

Take a look at the star set up.

Image by author

Isn’t this quite similar to what we have discussed? This is the famous star schema data model set up.

Now if you know a data modeler talking about star schema and snowflake, go ahead and ask them where the filtering tables and fact tables. They should be able to tell you right away.

If not, then it’s likely there is something wrong and I hope you won’t have to be the person to use the model or better yet, to rebuild it!",https://towardsdatascience.com/power-bi-modelling-8f1e4246c84e,['Peter Hui'],2021-05-30 01:05:41.933000+00:00,663,"Data Modeling, Star Schema, Snowflake Schema, Fact Tables, Filtering Tables"
Integrals. The area under the functions,"CALCULUS FOR DATA SCIENCE AND MACHINE LEARNING

Integrals

The area under the functions

After defining derivatives, we introduce the integrals. Not easy to define, by now we can understand them as the area between the function and the x-axis.

Integral of a bounded region

First, we will define the integrals for bounded regions, assigning the integral of f on [a, b] to the area R(f, a, b). In this example, we use it for an always positive interval, but it is defined for negative and intervals having positive and negative values.

In the next gif, we show how to apply the idea, [a,b] is divided into subintervals, then the minimum(m_i) and maximum(M_i) value of the function for each interval.

IkamusumeFan / CC BY-SA (https://creativecommons.org/licenses/by-sa/3.0)

Now we can define two areas:

The one created by the minimums: s=m1(t1-t0) + m2(t2-t1)+ …

The one created by the maximums: S=M1(t1-t0) + M2(t2-t1)+ …

The area R(f, a, b) is at some point between these values, s≤A≤S.

Lower and upper sums

Let a < b. A partition of the interval [a,b] is a finite collection of points in [a,b], one of which is a, and open of which is b. Suppose f is bounded on [a,b] and P = {t_0,…,t_n} is a partition of [a,b]. Let m1 = inf{f(x):t_(i-1)≤t_i}, M1 = sup{f(x):t_(i-1)≤t_i}, The lower sum off for P, denoted by L(f, P), and the upper sum, denoted by U(f, P) are denoted as:

Lower and upper sums, self-generated.

The next lemma will give you an idea of how we are going to define integrals:

If Q contains P (all points in P are in Q), then: L(f, P)≤L(f, Q) U(f, P)≥U(f, Q)

Integrals

A function f wich is bounded on [a,b] is integrable on [a,b] if sup{L(f, P) : P a partition of [a,b]}. In this case, the common number is called integral of f in [a,b] and is denoted by:

Integral of f in the interval [a,b], self-generated.

If f is integrable, then for all partitions of P of [a,b]:

The area defined by an integral, self-generated.

Moreover the integral is the unique number with this property.

Integrability

To be able to calculate an integral, our function has to be integrable, to check it we can use the next definition:

If f is bounded on [a, b], then f is integrable on [a, b] if and only if for every ε>0 there is a partition P of [a, b] such that U(f,P)-L(f,P)<ε. If f is continuous on [a,b] then f is integrable on [a,b].

Upgrading the notation of integrals

Since now we wrote integrals with the ∫ symbol and defining it between two values, but what happens when we have multiple variables, we don’t know what’s the one with we want to integrate the function.

Including d to integrals, self-generated.

In the left integral we don't know what variable is independent, as for derivatives, we need to integrate based on one variable at the time, so we introduce dx to give this information.

In this example, the solution would be c(b²/2-a²/2).

Properties of integration

Let a<c<b. If f is integrable on [a,b] then f is integrable on [a,c] and on [c,b]. Conversely, if f is integrable on [a,c] and on [c,b], then f is integrable on [a,b]. Finally if f is integrable on [a,b], then

Sum of interval integrals property, self-generated.

If f and g are integrable on [a,b], then f+g is integrable on [a,b] and

Sum of functions integrals property, self-generated.

If f is integral on [a,b], then for any number x, the function cf is integrable on [a,b]

The constant property, self-generated.

Conclusion

In this post, we introduced how integrals are defined, this will allow us to calculate areas under curves of functions. It’s used at deep learning, probability functions, etc…",https://medium.com/ai-in-plain-english/integrals-d985465627fb,['Adrià Serra'],2020-09-29 20:17:52.659000+00:00,591,"Calculus, Integrals, Derivatives, Integrability, Lower Sums"
Multi-variate LSTM Time Series Forecasting.,"Hi there how’s everything going? I hope it’s good.

Well in our last article we have seen how we can apply LSTM for time series forecasting when we have a single series of sequential data. Search for this 2 articles Times Series for Regression and Univariate LSTM Time Series Forecasting from my home page. Moving ahead to our next experiment using multiple series of input to understand and forecast the next time series sequential data. We will call it as Multi-variate Time Series Forecasting.

The article was originally found in ‘machine learning mastery’ by Jason. What we will try to achieve here is to simply the complex steps with different settings and understanding to get a clear picture about the concepts to quickly solve our day to day time series problem.

But before that let me repeat in brief what is LSTM

Long-Strong-Term Memory (LSTM) is the next generation of Recurrent Neural Network (RNN) used in deep learning for its optimized architecture to easily capture the pattern in sequential data. The benefit of this type of network is that it can learn and remember over long sequences and doesnot rely on pre-specified window lagged observation as input.

In keras this is referred as stateful and involves settings the “stateful” argument to “True” in LSTM layer

What is LSTM in brief?

It is a recurrent neural network that is trained by using Back-propagation through time and overcomes the vanishing gradient problem.

Now instead of having Neurons, LSTM networks have memory blocks that are connected through layers. The blocks of LSTM contains 3 non-linear gates that makes it smarter than a classical neuron and a memory for sequences. The 3 types of non-linear gates include

a.) Input Gate: decides which values from the input to update the memory state.

b.) Forget Gate: handles what information to throw away from the block

c.) Output Gate: finally handles what to be in output based on input and the memory gate.

Each LSTM unit is like a mini-state machine that utilizes a ”memory” cell that may maintain its state value over a longer time, where the gates of the units have weights that are learned during the training procedure.

There are tons of articles available in the internet about the workings of LSTM even the math behind LSTM. So here I will concentrate more for a quicker practical implementation of LSTM for our day to day problems.

Let’s get started.

First is data pre-processing step where we have to give structure the data into supervised learning that is X and Y format.

In simple words it identifies the strength and values of the relationship (positive/negative impact and the values derived is call quantification of impact) between one dependent variable(Y) and series of other independent variables X

For this example we have a retail sales time series data recorded over a period of time.

Now as u know supervised learning requires X & Y independent and dependent variable for the algorithm to learn /train, so we will first convert our data into such format

Assume we have sales data. What we will do we will first take the sales data(t) in our first column than the second column will have the next months(t+1)sales data that we will use to predict. Remember X & Y independent and dependent variable format where we use Y to predict the data.

Now we will replicate the same if we have more columns.

Let’s take an example of weather conditions data set where we will have attributes/ columns/ like pollution, dew, temp, wind speed, snow, rain. Now we will use Multivariate LSTM time series forecasting technique to predict the pollution for the next hours based on pollution, dew, temp, wind speed, snow, rain conditions.

We will use series to supervised function to frame the our dataset / variables of the dataset into t+1 and t format

#transform series to supervised learning

def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):

n_vars = 1 if type(data) is list else data.shape[1]

df = DataFrame(data)

cols, names = list(), list() #input sequence (t-n, ... t-1)

for i in range(n_in, 0, -1):

cols.append(df.shift(i))

names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)] #forecast sequence (t, t+1, ... t+n)

for i in range(0, n_out):

cols.append(df.shift(-i))

if i == 0:

names += [('var%d(t)' % (j+1)) for j in range(n_vars)]

else:

names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)] #put it all together

agg = concat(cols, axis=1)

agg.columns = names #drop rows with NaN values

if dropnan:

agg.dropna(inplace=True)

return agg

Here is it after converting where we have all the variables in var(t-1) and var (t) is the same meaning as (t) (t+1) where we are using 1 step back to format it into X and Y variable.",https://medium.com/@bobrupakroy/multi-variate-lstm-time-series-forecasting-1a736009f6d,['Bob Rupak Roy - Ii'],2021-06-25 13:32:12.325000+00:00,747,"Time Series Forecasting, Univariate LSTM, Multivariate LSTM, Machine Learning Mastery, Jason Brownlee"
A thumbnail of Feature Engineering,"You can’t build a great building on a weak foundation. You must have a solid foundation if you’re going to have a strong super structure.

-Gordon B.Hinckley

Feature Engineering is the crucial module in the life cycle of a Data science project.

For instance, you are tasked with a gardening project and all you were provided is an uneven ,weed filled backyard and bunch of stuff from retail construction store like wood blocks, soil, concrete raw material .To proceed ,you need to make a note of necessary steps like what is the purpose of the project, requirements to be considered and things you need to do in order to make the process fast, easy and effective. We also might need to preprocess certain materials to serve the purpose.This collection of all needed materials and making them usable for building the project is called engineering and when it is done for features in a Data science project is called Feature Engineering.

Raw data can be haphazard. It is a good practice to be able to answer all the above questions after Features are Engineered.

The major objective of Feature Engineering is identifying, preprocessing or modifying the features which contribute to the effective model. To achieve this there are multiple steps depending on the data.

1.EDA:

Before starting any Machine learning project the data is messy and is not suitable for modelling.

In order to prepare the data for modelling we have to explore the data and find relevant insights.

Exploratory Data Analysis is exploring the data set for data types(categorical or numerical),missing values, outliers, cleaning the data like checking for format, headers etc.

Dtale, Pandas profiling, Sweetviz, Autoviz, Dataprep are libraries which can automate all the EDA along with plots.

2.Handling the missing values :

This can be done in many ways like imputing with mean, median, mode, using imputation algorithms like:

KNN

LLS impute which uses regression

MICE(Multiple imputations by chained equations)

CART(Classification and Regression trees)

GMC impute(Gaussian mixture clustering imputation)

CMVE(Collateral missing value imputation)

AMVI(Ameliorative missing value imputation)

ABBA(Adaptive Bicluster based approach imputation)

The above are a few algorithms used for imputing missing values. The missing values can be replaced using a hybrid of any of the above algorithms. The imputed dataset can be evaluated with either internal or external validation. Hence the reliability and accuracy of our imputation technique can be measured.

3.Handling imbalanced Dataset:

Imbalance data leads to biased models which is not ideal. Hence we need a way to handle Imbalanced data. This can be approached in following ways:

Resampling

More training data

Using K-fold

Cluster the abundant class

Changing performance metrics

We also can customize a way to handle imbalanced data sets.

4.Outliers:

Removing outliers results in a statistically significant increase in the accuracy of the model. Hence this is an important step to be performed. Anomaly detection can be achieved by the following ways:

Using scatter plots

Box Plots

Z-score

Using IQR(inter quartile range)

Isolation forest

Cluster analysis

Ensemble techniques

5.Feature scaling:

Raw data can have values in any range. It is atypical to be able to analyze features with the drastic range changes If the data is in different units of measurement , it is important to bring them to the same scale. Hence the data has to be transformed in limited ranges or same units which in turn can achieve accurate models. The Feature Transformation can be done in the following ways:

Standard scaler

Min Max scaler(0–1)

Gaussian Transformations like

logarithmic transformation

reciprocal transformation

square root transformation

exponential transformation

box cox transformation

6.Handling Categorical data:

The categorical data handles useful information but a machine cannot calculate when used as it is, these variables should be modified to numerical so that the calculations can be performed and we can derive more insights from the data. The categorical variables are classified into ordinal and nominal where ordinal categorical variables can be ranked. Some of the ways to convert the categorical data are as follows:

One hot encoding

Label encoding

Binary encoding

Replacing

Backward difference encoding

Count or frequency encoding

Conclusion:

Feature Engineering is the crucial step where we are trying to create a bond to be able to understand the data clearly and extract all possible information the data is trying to say to us. This task takes much of a Data scientist’s time. If the base is strong the building will be strong and hence if Feature Engineering is done properly we can achieve an effective model.",https://medium.datadriveninvestor.com/a-thumbnail-of-feature-engineering-76effce1326b,['Mahitha Singirikonda'],2021-09-18 00:13:39.017000+00:00,686,"Feature Engineering, EDA, Missing Values, Imbalanced Dataset, Outliers"
Picking a Hyperparameter Tuning Library for Your Model,"Enough talking, let’s test the library to see if it actually gets the job done.

We will use the Fashion MNIST dataset since it is easy to work with and is still slightly difficult to get 90%+ validation accuracy. To keep this article concise, I will only include code relevant to the library and not include the code for loading and preprocessing data. You can refer to the notebooks at the end of this article.

We will train a rather simple but dynamic model, with the help of aisaratuners library. Here we go.

To get started, we need to pip install the package:

!pip install aisaratuners # pip install using a jupyter notebook

We only need two imports from aisaratuners library to get started, yes, it is that simple!

Now comes the fun part, we get to decide which hyperparameters we get to tune dynamically for our model.

We instantiate our Hp object which is a container for all the hyperparameters we wish to tune. You can set min/max ranges, as well.

Next, in a function containing our model, we will include the Hp object and all the ranges we have defined:

One thing I like while working with aisaratuners was the fact that I had to introduce minimal and understandable changes to the model. We just need to use the Hp object we created and access any random number we set in the numrange method in the above cell.

Let’s now run the optimizer and see how the algorithm performs:

We have set 3 rounds, each with 5 trials. This means in any round, the algorithm will sample 5 sets of hyperparameters from the solution space and train models. This is depicted in Fig. 1 above. In each round, the model will try to reduce this solution space. Also, we have set two criteria, one to maximize the validation accuracy and one to minimize validation loss. The mode argument is set to ‘p’ to use the free available API for aisaratuners to function. You could use a commercial API to use as well. Find more information here.

It took me 889.84 seconds to run this optimization.

Please note that this time will vary for everyone and I was running the optimization using Google Colab, which has a variety of GPUs.

It took me less than 15 minutes (14.81) minutes to acquire the best model. That is amazing. It usually would take me at least two hours of manual tuning. Instead, we automatically tuned for the best model using aisaratuners in much less time.

Here are the results:

Fig. 2: Round 1: Orange, Round 2: Green, Round 3: Yellow

The five trials of each round are labeled. My findings were:

Orange: Some variance (e.g. 48% validation accuracy) in the first round, but overall, the accuracies were definitely in the higher region. This is impressive! Aisaratuners algorithm did well in initial sampling. Green: In the second round, the search space was reduced well enough because all the models were now achieving 90%+ validation accuracies. Yellow: Even now, when typically I would stop trying to achieve better results, aisaratuners still managed to squeeze a one per cent increase on the validation set.

This was exactly what I needed: an automatic hyperparameter tuner. What exceeded my expectations were the visualizations this library provided me with. The above information can be visualized in a picture-worth-a-thousand-words manner. Interactive visualizations are labeled in the captions.

(Interactive) Infer how in each round the validation accuracy is increasing.",https://medium.com/swlh/picking-a-hyperparameter-tuning-library-for-your-model-75220145bb47,['Osama Akhtar'],2020-12-15 21:05:01.220000+00:00,560,"aisaratuners, hyperparameter tuning, validation accuracy, Fashion MNIST dataset, interactive visualizations"
Displaying Progress in a Loop or Function [R],"Displaying Progress in a Loop or Function [R]

and how to create a screen recording GIF in Windows 10

While I was waiting for my not-very-complicated setup but complicated gbm model to run (it’s still running), it occurred to me that I really appreciate seeing which iteration my modeling function is currently on, especially if the run-time is many hours. Partially this is to make sure that the program hasn’t frozen and is still running and that functions don’t get stuck in endless loops, but some bigger motivator might be that seeing the progress fulfills some human need of having control… At least I’m aware.

Whatever the reason, I like being shown progress. Many modeling and other functions don’t have a built-in progress indicator and saves all the output results for after the function completes. One of the simplest cases that can be used to see this is is the for loop.

Note: All the code used below can be found in the gist at the bottom of this article. The GIFs show base R screenshots because RStudio was taken up by the gbm run.

Example 1: Simple For Loop

The GIF below shows a simple loop that goes through the integers 0 to 101 with a print(i) command and short lag time between each iteration.

print everything after loop is finished

The output to the R console is buffered. This means that all the outputs are printed to the console at once after the loop is finished.

Example 2: Simple For Loop with flush.console

The R FAQs suggest as a solution to either change the R GUI buffering settings in the Misc menu (Ctrl-W) or to tell R explicitly to empty the buffer by adding the line flush.console().

Add `flush.console()` to force print within loop

This method can be very helpful for debugging. When writing or editing a function within a for loop, message(paste0(“some text: ”, somevar)) can also be used to print interim results.

Example 3: Pretty Progress Text

To get fancier, install the svMisc package and use the progress() function.

Load `svMisc` to create pretty progress text

Example 3: Pretty Progress Text Bar

To get even fancier, set progress.bar = TRUE.",https://medium.com/human-in-a-machine-world/displaying-progress-in-a-loop-or-function-r-664796782c24,[],2016-04-28 19:05:54.872000+00:00,347,"Set `progress.bar = TRUE` to create a pretty progress barr, for-loop, flush.console(), sv Misc, progress.bar"
What Data Tells Us About the World’s Wealthiest,"In the spring semester of my freshman year at Harvard, I took a class called “Using Big Data to Solve Economic and Social Problems.” One of the most interesting subjects we explored was equality of opportunity in the United States. We learned that children’s chances of earning more than their parents is not uniform: it varies significantly depending on their race, gender, and where they grew up.

Much of the class focused on researching data and analyzing outcomes for the lowest percentiles of the income distribution. I was curious about the other extreme. I wanted to look at trends in the numbers, geographical locations, sources of wealth, and net worth of the world’s richest people.

Photo by Forbes

I discovered a dataset created by researchers at the Peterson Institute for International Economics (PIIE) which uses data from the 1996 to 2016 Forbes’ World’s Billionaires lists. According to the PIIE researchers:

“From 1996 to 2010, the data include the name, rank, country of citizenship, and net worth (current US dollars) of the world’s billionaires. From 2011 on, the source of wealth is also provided, listed either as a specific company or broader sector.”

The PIIE researchers added several variables to the Forbes’ data: the billionaire’s age, net worth in 1996 US dollars (to account for inflation), and their country’s GDP and level of economic development. I also created variables for the continent and country name, in addition to the 3-digit country code included in the original dataset.

Here’s what I found from my analysis…

The number of billionaires has quadrupled in the last 20 years

Clearly, the billionaire population around the world has skyrocketed from 1996 to 2016 (more precisely, it’s increased by 396%). But the upward trajectory hasn’t been all smooth sailing: growth was relatively stagnant prior to 2000 and negative in the immediate aftermath of the dot-com bubble burst of 2001 and, most notably, the financial crisis of 2008.

The United States consistently has the most billionaires

This animation ranks the 10 countries with the highest number of billionaires for every year from 1996 to 2016. The most striking finding is that not only has the United States without exception had more billionaires than any other country throughout the decade, but also the difference in the number of billionaires in the US versus a runner-up country and the others is very large, even taking into account differences in population size. In every year except 1999 and 2000, the US has had at least 2.5x the number of billionaires of any other nation. However, this gap is slowly narrowing, and it’s evident that China is catching up to the US: China first made the list in 2008 (ranking 5th) and has consistently ranked second since 2013.

There are quite a few interesting observations about countries’ placements in the rankings:

In total, 19 countries have made the list at least once.

The only countries who made the list every year since 1997 are the United States, Germany, Hong Kong, and the United Kingdom.

The rankings reflect Japan’s gradual economic decline: it ranked in the top 5 until 2006, dropped to the second half of the list for several years, then dropped off the list after 2011.

India has consistently made the rankings (usually hovering around 4th — 6th place) since 2005.

Russia has ranked in the top 5 since 2003, although it experienced a sharp decrease in the number of billionaires after 2014, when the US and the EU implemented sanctions.

A note of personal interest (and pride): Canada has made the rankings almost every year since 1999!

Asia has experienced the fastest growth in billionaire population

The number of billionaires is growing most rapidly in Asia and the Americas, with the most growth occurring after 2008. Interestingly, the Americas and Europe had very similar number of billionaires prior to 2000, when they diverged suddenly and significantly. Ever since, their growth patterns have mirrored each other but the Americas remain significantly above the rest. It was also surprising to see that the number of billionaires in Africa has consistently been low in spite of Africa’s tremendous economic growth in the past two decades.

Billionaires in developed economies are getting richer at a faster rate than their counterparts in less developed countries

After 2003, billionaires’ total net worth surged — particularly in emerging economies, where the increase was practically exponential — until it plunged drastically immediately after the 2008 financial crisis. However, after 2010 the billionaires bounced back, and their net worth started growing again, although those from less developed countries experienced a noticeably slower rate of growth.

Since 2000, over a quarter of billionaires have derived their wealth from a financial industry

The dataset groups the sources of billionaires’ wealth into the following 6 categories:

Resource Related : energy (excluding solar and wind) mining, steel

: energy (excluding solar and wind) mining, steel New : computer technology, software, medical technology, solar and wind power, pharmaceuticals

: computer technology, software, medical technology, solar and wind power, pharmaceuticals Non-Traded : retail, entertainment, media, telecommunications, construction, restaurants and other service industries

: retail, entertainment, media, telecommunications, construction, restaurants and other service industries Financial : banking, insurance, hedge funds, private equity, venture capital, investments, diversified wealth, real estate

: banking, insurance, hedge funds, private equity, venture capital, investments, diversified wealth, real estate Traded : agriculture, consumer goods, shipping, manufacturing

: agriculture, consumer goods, shipping, manufacturing Other: education, engineering, infrastructure sports team ownership, unidentified diversified wealth

Although the highest proportion of billionaires have and continue to derive their fortunes primarily from financial industries, the size of this proportion has been on a steady decline since its peak in 2008. On the flip side, the percentage of billionaires from the “new” and tradable goods industries has been consistently increasing since 2008.",https://towardsdatascience.com/what-data-tells-us-about-the-worlds-wealthiest-a8c54e6ddb84,['Dasha Metropolitansky'],2019-08-01 16:12:36.076000+00:00,934,"big data, economic inequality, Forbes World's Billionaires List, Peterson Institute for International Economics, wealth distribution"
2 Reasons Why Self-Driving Cars will be the Ultimate Game-changer for Disabled People,"Technology

2 Reasons Why Self-Driving Cars will be the Ultimate Game-changer for Disabled People

Photo by Steven HWG on Unsplash

According to the forecasts from futurists, in the close future, all of our transport and most of our routine tasks will be operated by AI. This means that there will be more and more interesting and promising applications of AI that will support the global Tech for Good goal. One of the worldwide societal issues that will be solved by AI is a significant lack of mobility accessibility for people with disabilities.

According to the Center for Disease Control and Prevention, there are 61 million Americans with one or another form of disability. This number means — approximately 25% of the nation.

“Accessible autonomous transportation could facilitate opportunities for people with disabilities to find or maintain employment, obtain better access to healthcare, and integrate more fully into the community,”

has stated the co-director of the Pitt’s University Transportation Center for Accessible Autonomous Vehicles and Transportation — Dr Brad Dicianno.

The trend of fully autonomous cars and the future that they will bring is promising and exciting. Self-driving cars/ Autonomous cars/ Driver-less vehicles/ Robo-cars/ Robotic cars — is a vehicle that moves safely without any or with little human input.

The image is courtesy of Autoblog via Giphy.com

Self-driving cars can sort out many issues, but in this article, I’d like to cover what exactly this innovation would mean for people with disabilities.

No Need for a Driving License

There are many reasons why not everyone can or even want to have a driving license. It can be the cost of the license or the car, a certain disability or simply, fear. No matter the reason, not everyone has a driving license, which has caused a significant gap in mobility accessibility.

Of course, there are public transportation services and transportation on the demand (taxis). While for some people this might be a working option, for others it might be not. Some taxi companies in the USA, Canada and the UK have faced this issue and introduced a special wheelchair-accessible taxi service. Even though, this is a great idea that helps thousands of people, the percentage of these cars and the availability of this service are extremely low.

The issue of the limited mobility for some people will be resolved when fully autonomous cars will pass all of the required tests and enter the mass production stage. With the reduced need for the driver, mobility will massively increase for people with disabilities.

The image is courtesy of Netflix Tech via Giphy.com

More accessible

As it was mentioned above, today, some big cities in the USA, Canada and the UK have introduced a taxi service that is designed specifically for people on a wheelchair. These services accept only drivers with the best rating and wheelchair-accessible vehicles. These particular cars must not only have a place for a wheelchair but also they must be specifically equipped with all of the safety measures to ensure the safe transportation of a passenger. Everything looks good here.

These wheelchair accessible taxi services are real game-changers in the taxi industry that make thousands of lives much easier. It is cheap, safe and it’s a perfect solution to the problem of the target customer. However, these programs are available only in limited cities and only in a couple of countries. This certainly shows that there are demand and an absolute need for more services like that.

Apart from the wheelchair-friendly taxi services, there are, of course, other services like private shuttles or public transportation that are equipped with a slide for wheelchair or trolley. However, according to Dr Dicianno, none of these options fully answers the problem of mobility for disabled people due to the cost, accessibility, reliability and availability.

Photo by Kelvin Ang on Unsplash

When the fully-autonomous cars will enter its mass-production stage, the cost would not be much different from the regular cars today. This will make future transport not only available and accessible to everyone but also cheaper than other options.

How would a wheelchair — accessible self-driving car look like?

This Hungarian startup has designed a wheelchair-accessible car, that looks like this:

This car has no seats and it’s designed a bit differently from regular cars. In the picture, it is clear how a wheelchair user can get inside. A driver can operate the car by putting his/ her hands on motorcycle-style handlebars.

With electric power and self-drive, it’s very easy to make a car with a flat floor and hollow shell. It’s also easy to put spaces in vans and group transport for chairs to roll into. — Brad Templeton, Forbes (2020)

While this car is not a self-driving one, the design for the wheelchair-accessible fully autonomous car might be similar.

Fully autonomous and semi-autonomous cars are not just another consumer trend, but rather an important innovation that has the potential to improve many aspects of the modern cities and make mobility more accessible.",https://medium.com/illumination/self-driving-cars-for-disabled-people-the-ultimate-game-changer-c24c8c1334b4,['Elena Beliaeva-Baran'],2020-12-14 22:54:56.547000+00:00,805,"Technology, Self-Driving Cars, AI, Disability Accessibility, Autonomous Vehicles"
How to Run SQL on PDF Files,"Authored by Kshitij Wadhwa

PDFs are the de facto standard for distributing and sharing fixed-layout documents today. A quick survey of my laptop folders reveals account statements, receipts, technical papers, book chapters, and presentation slides — all PDFs. Lots of valuable information finds its way into all manner of PDF files. Which is a great reason for Rockset to support SQL queries on PDF files, in our mission to make data more usable to everyone.

Fast SQL on PDFs in Rockset

Rockset makes it easy for developers and data practitioners to ingest and run fast SQL on semi-structured data in a variety of data formats, such as JSON, CSV, and XLSX, without any upfront data prep. Now add PDFs to the mix, and users can combine PDF data with data of other formats, from various sources, into their SQL analyses. Or analyzing multiple PDFs together might be valuable too, if you have a series of electricity bills like I do, as we’ll see in our short example below.

Uploading PDFs

From an existing collection, click the Upload File button at the top right of the console and specify PDF format to ingest into Rockset.

Querying Data in PDFs

I uploaded 9 months of electricity bills. We can use the DESCRIBE command to view the fields that were extracted from the PDFs.

Rockset parses out all the metadata like author , creation_date , etc. from the document along with the text .

The text field is typically where most of the information in a PDF resides, so let's examine what's in a sample text field.

+--------------------------------------------------------------+

| text |

|--------------------------------------------------------------|

| .... |

| .... |

| Statement Date: 10/11/2018 |

| Your Account Summary |

| .... |

| Total Amount Due: |

| $157.57 |

| Amount Enclosed: |

| ... |

+--------------------------------------------------------------+

Combining Data from Multiple PDFs

With my 9 months of eletricity bills ingested and indexed in Rockset, I can do some simple analysis of my usage over this timespan. We can run a SQL query to select the month/year and billing amount out of text .

And plot the results in Superset.

My October bill was surprisingly zero. Was the billing amount not extracted correctly? I went back and checked, and it turns out I received a California Climate Credit in October which zeroed out my bill, so ingesting and querying PDFs is working as it should!",https://medium.com/rocksetcloud/how-to-run-sql-on-pdf-files-82302ab6be65,['Shawn Adams'],2019-09-06 18:21:40.547000+00:00,375,"rockset, pdfs, sql, data-ingestion, superset"
Latest picks: In case you missed them:,"Sign up for The Variable

By Towards Data Science

Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.",https://towardsdatascience.com/latest-picks-erasure-coding-for-the-masses-76e902897595,['Tds Editors'],2020-12-23 14:27:07.950000+00:00,36,"data Science, machine Learning, tutorials, research, original Features"
K Nearest Neighbors,"K Nearest Neighbor algorithm is one of the easy-to-implement supervised learning algorithms. It can be used to solve both classification and regression problems. KNN algorithms were proposed by T. M. Cover and P. E. Hart in 1967. The algorithm is used by making use of data from a sample set whose classes are known. The distance of the new data to be included in the sample data set is calculated according to the existing data and k number of close neighborhoods are checked. Generally, 3 types of distance functions are used for distance calculations:

“Euclidean” Distance

“Manhattan” Distance

“Minkowski

“ Distance

KNN algorithm is an algorithm that classifies a data to be classified according to its proximity with other data. For example, If we take k = 3, the distances of the new incoming data to the old data are measured and the closest 3 of them are determined. As can be seen in the figure below, 2 of the closest data are in A class and 1 is in B class. In this case, the algorithm decides that the new incoming data is in Class A.

KNN; It is one of the most popular machine learning algorithms because it is resistant to old, simple and noisy training data. However, it also has a disadvantage. For example, it requires a large amount of memory space when used for large data, since it stores all states while calculating distance.

Steps of the KNN algorithm:",https://medium.com/@nisasoylu/k-nearest-neighbors-2ecfb813d4e3,['Nisa Soylu'],2021-05-09 08:59:11.600000+00:00,236,"KNN, Machine Learning, Supervised Learning, Classification, Regression"
Image Processing for Python — Adjusting to the Ground Truth,"Image Processing for Python — Adjusting to the Ground Truth

Old Street (Image by Author)

In this lesson we shall go over an image adjustment algorithm which may be intuitive for most readers. Unlike the other image adjustment algorithms we have discussed so far (such as RGB Channel Adjustment and Histogram Manipulation), this method will use the actual colors available in the image.

Let’s get started!

As always, we must import the required Python Libraries.

import numpy as np

import matplotlib.pyplot as plt

from matplotlib.patches import Rectangle

from skimage.io import imread, imshow

import skimage.io as skio

from skimage import img_as_ubyte, img_as_float

Great, now let us look at the image we’re working with.

overcast = imread(""image_overcast.PNG"")

plt.figure(num=None, figsize=(8, 6), dpi=80)

imshow(overcast);

Rustic Road (Image by Author)

The image clearly has a color overcast. Now let us try to adjust it.

To begin we must first select the particular ground truth patches we want the machine to work with. To do that we can use the Rectangle function available in NumPy.

fig, ax = plt.subplots(1,1, figsize=(8, 6), dpi = 80)

patch = Rectangle((70,175), 10, 10, edgecolor='r', facecolor='none')

ax.add_patch(patch)

ax.imshow(overcast);

Red Rectangle Added

We can see the red rectangle on the upper left corner of the image. I’m sure most of you suspect that this may not be the best ground truth patch to use. The reason being that it is quite different from the rest of the image. However, for pedagogical reasons we shall use this square and adjust our image to it.

Let us first get a close up view of our ground truth patch. To do that we must use the get_bbox() and get_points() functions found in NumPy.

coord = Rectangle.get_bbox(patch).get_points()

print(coord)

Coordinate Array

We can now use this coordinate array to slice our main image.

fig, ax = plt.subplots(1,1, figsize=(8, 6), dpi = 80)

ax.imshow(overcast[int(coord[0][1]):int(coord[1][1]),

int(coord[0][0]):int(coord[1][0])]);

As we can see, the ground truth patch is far from a monotonic color. Within it are multiple shades of brown and black (a testament to the amount of details not visible to the human eye!).

Now let us actually adjust our image to the patch. We shall use the Max and Mean values of the patch.

image_patch = overcast[int(coord[0][1]):int(coord[1][1]),

int(coord[0][0]):int(coord[1][0])] image_max = (overcast / image_patch.max(axis=(0, 1))).clip(0, 1)

image_mean = ((overcast * image_patch.mean())

/ overcast.mean(axis=(0, 1))).clip(0,255).astype(int)

fig, ax = plt.subplots(1,2, figsize=(15, 10), dpi = 80)

f_size = 19

ax[0].imshow(image_max)

ax[0].set_title('Max Adjusted', fontsize = f_size)

ax[0].set_axis_off() ax[1].set_title('Mean Adjusted', fontsize = f_size)

ax[1].imshow(image_mean);

ax[1].set_axis_off()

fig.tight_layout()

Max and Mean Adjusted Images

As we can see, both adjustments are pretty terrible. Depending on your preference you could say either is better. The Max adjusted image (though clearly overexposed) does highlight the green color of the plant on the left as well as the blue cap of the carriage driver on the right. The Mean adjusted image (though extremely faded) does fair better in terms of overall clarity of the image. But I believe it is safe to say that neither of these adjustments would do. To remedy this let us go back to the selection of ground truth patches.

fig, ax = plt.subplots(1,1, figsize=(8, 6), dpi = 80)

patch1 = Rectangle((100,300), 10, 10, edgecolor='r',

facecolor='none')

patch2 = Rectangle((200,250), 10, 10, edgecolor='r',

facecolor='none')

patch3 = Rectangle((200,190), 10, 10, edgecolor='r',

facecolor='none') ax.add_patch(patch1)

ax.add_patch(patch2)

ax.add_patch(patch3)

ax.imshow(overcast);

Selecting Different Patches

Though we do not have to, we can plot out the patches so that we can get an idea of what they look like.

coor1 = Rectangle.get_bbox(patch1).get_points()

coor2 = Rectangle.get_bbox(patch2).get_points()

coor3 = Rectangle.get_bbox(patch3).get_points() image_patch1 = overcast[int(coor1[0][1]):int(coor1[1][1]),

int(coor1[0][0]):int(coor1[1][0])] image_patch2 = overcast[int(coor2[0][1]):int(coor2[1][1]),

int(coor2[0][0]):int(coor2[1][0])] image_patch3 = overcast[int(coor3[0][1]):int(coor3[1][1]),

int(coor3[0][0]):int(coor3[1][0])] fig, ax = plt.subplots(1,3, figsize=(15, 12), dpi = 80)

ax[0].imshow(image_patch1)

ax[1].imshow(image_patch2)

ax[2].imshow(image_patch3);

Ground Truth Patches

Like our first patch, these patches also show an array of colors. Let us now use each one in adjusting our image. To aid in this, let us first use Python’s list comprehension capabilities to create a list of the adjusted images.

patch_list = [image_patch1, image_patch2, image_patch3]

image_max_list = [(overcast / patch.max(axis=(0, 1))).clip(0, 1) for

patch in patch_list]

image_mean_list = [((overcast * patch.mean()) / overcast.mean(axis=(0, 1))).clip(0, 255).astype(int) for

patch in patch_list]

Wonderful! Now we can simply call all these images.

Let us begin.

def patch_plotter(max_patch, mean_patch):

fig, ax = plt.subplots(1,2, figsize=(15, 10), dpi = 80)

f_size = 19

ax[0].imshow(max_patch)

ax[0].set_title('Max Adjusted Patch', fontsize = f_size)

ax[0].set_axis_off() ax[1].set_title('Mean Adjusted Patch', fontsize = f_size)

ax[1].imshow(mean_patch);

ax[1].set_axis_off()

fig.tight_layout() for i in range(3):

patch_plotter(image_max_list[i], image_mean_list[i])

First Patch

Second Patch

Third Patch

We can see from the results that the Max adjusted patches perform much better than the Mean adjusted patches. Furthermore, it seems that the second patch faired the best. This seems to indicate that best batch is one that adjusts the image to a beige-like color. As a final exercise let us choose patches that fit that particular description. Also, we shall decrease the size of the patches to lessen the amount of color variety in the patches.

Below is a function that can generate the ground truth adjusted images. We simply need to feed it our image, the names of the patches (for labelling purposes), and the patch coordinates.

def ground_truth(image, patch_names, patch_coordinates):

f_size = 25

figure_size = (17,12)

patch_dict = dict(zip(patch_names, patch_coordinates))

coord = []



fig1, ax_1 = plt.subplots(1, 3, figsize = figure_size)

for n, ax in enumerate(ax_1.flatten()):

#Create Rectangles

key = list(patch_dict.keys())[n]

patch = Rectangle(patch_dict[key], 5, 5, edgecolor='r',

facecolor='none')

coord.append(Rectangle.get_bbox(patch).get_points())

ax.add_patch(patch);



#Show and Format Images

ax.imshow(image)

ax.set_title(f'{key}', fontsize = f_size)

ax.set_axis_off()

fig1.tight_layout()



fig2, ax_2 = plt.subplots(1, 3, figsize = figure_size)



for n, ax in enumerate(ax_2.flatten()):

#Show and Format Rectangles

key = list(patch_dict.keys())[n]

ax.imshow(image[int(coord[n][0][1]) : int(coord[n][1][1]),

int(coord[n][0][0]) : int(coord[n][1][0])]);

ax.set_title(key, fontsize = f_size)

ax.set_axis_off()



fig2.tight_layout() fig3, ax_3 = plt.subplots(1, 3, figsize = figure_size)

for n, ax in enumerate(ax_3.flatten()):

patch =image[int(coord[n][0][1]) : int(coord[n][1][1]),

int(coord[n][0][0]) : int(coord[n][1][0])]



image_max = (image / patch.max(axis=(0, 1))).clip(0, 1)

ax.imshow(image_max)

ax.set_title(f'Max : {patch_names[n]}', fontsize = f_size)

ax.set_axis_off() fig3.tight_layout() ground_truth(overcast,

['First', 'Second', ' Third'],

[(105, 275), (50, 45), (330, 105)])

Ground Truth Adjusted Images

We can see that among the different ground truth patches, the third patch performs the best. Though the image is noticeable bluer, the yellow overcast was completely removed. Additionally, overexposure is kept at a bare minimum.

In Conclusion

We see that one does not need to be extremely well versed in the different ways color is interpreted by the machine. We can use the simple yet effective Ground Truth algorithm to adjust our images. This particular method may be more suited for individuals with an intuitive understanding of colors. Though good results did take a while to achieve, this method is worth keeping in mind as it one of the more straightforward ways to alter an image.

I hope that you have learned the significance of this more simple algorithm.",https://towardsdatascience.com/image-processing-for-python-adjusting-to-the-ground-truth-d0275e4f253e,['Tonichi Edeza'],2020-12-25 14:39:10.060000+00:00,981,"image processing, python, ground truth, RGB channel adjustment, histogram manipulation"
Testing 99%+ certainty Association Rules vs xgboost on noisy data: why 3500:1 ratio loses,"Laurae: This post is about “what to do” with association rules that seem to have over 99% certainty when you already know you work with a a very noisy training set. Not only you should not play with them manually, but let your typical models do the work for you. Otherwise, you end up poor. Even with a 3500:1 ratio, powered 14546+ times, you end up the loser by far. The post was originally at Kaggle.

Why poor? Here is an example of what overfitting will do for you automatically:

AdmiralWen wrote: In all honesty even if there is a shakeup as a result of these hard coded rules, the impact will be small. If you count the number of samples these rules apply to in the test set, it is between 2000–4000 samples out of 70000+. The only factor here is if the Kaggle admins made an intentional effort to skew the 50/50 splits used in calculating the leaderboard some way, my feeling is that that the split should be completely random, and if it is, the univariate statistical distributions of the features should be roughly equal in both halves, as a result the rules that work well in one half should work well in the other. Edit: Also I don’t think the Kaggle admins would have had the insight into the data at the start of the competition to skew it in a meaningful way. It has taken months of collective eyeballs and analysis to develop the scripts. This is assuming the 50/50 split is determined at the start and never changes.

No, the impact is huge.

Take 5 or 6 false negatives at 0.00 probability (lowest rank) and it will throw a lot in ROC. The higher the ROC you have, the higher the impact of a false negative at an extreme rank in ROC. This is due to the fact you are optimizing the area under the ROC, which is based on ranking probabilities.

Imagine you found an area where there are 14509 negatives and only 37 positives (yes, it exists in this data set). Lets kick away the 37 positives, and imagine the resulting ROC you have is 0.8470657 (yea, that’s really good if it was true).

For 14509 negatives you were sure at 99.74%+ let’s look at how it fares when N the amount of number false negatives goes from 0 to 37 when simulated 1000 times, with the baseline provided:

Current ROC:0.8470657(+) | from Simulation: 0.8470953+0.0031873[0.8376366, 0.8585345] Falses in rule (Current|Boot):

00(++) => ROC = 0.8480724+0.0031332[0.8388510, 0.8594306] (0.000% | Adj: 0.000%)

01(++) => ROC = 0.8478175+0.0031281[0.8383498, 0.8589155] (0.007% | Adj: 0.169%)

02(++) => ROC = 0.8475608+0.0031465[0.8378493, 0.8584011] (0.014% | Adj: 0.339%)

03(++) => ROC = 0.8473098+0.0031603[0.8373494, 0.8578873] (0.021% | Adj: 0.508%)

04(--) => ROC = 0.8470565+0.0031614[0.8368502, 0.8578873] (0.028% | Adj: 0.678%)

05(--) => ROC = 0.8468185+0.0031644[0.8368502, 0.8573743] (0.034% | Adj: 0.847%)

06(--) => ROC = 0.8465624+0.0031702[0.8363517, 0.8568618] (0.041% | Adj: 1.017%)

07(--) => ROC = 0.8463014+0.0031780[0.8363517, 0.8563501] (0.048% | Adj: 1.186%)

08(--) => ROC = 0.8460340+0.0031806[0.8358538, 0.8560259] (0.055% | Adj: 1.356%)

09(--) => ROC = 0.8457689+0.0031969[0.8354073, 0.8560259] (0.062% | Adj: 1.525%)

10(--) => ROC = 0.8454995+0.0032071[0.8349107, 0.8555139] (0.069% | Adj: 1.695%)

11(--) => ROC = 0.8452594+0.0032142[0.8349107, 0.8550026] (0.076% | Adj: 1.864%)

12(--) => ROC = 0.8450205+0.0032221[0.8349107, 0.8550026] (0.083% | Adj: 2.034%)

13(--) => ROC = 0.8447645+0.0032139[0.8344147, 0.8550026] (0.090% | Adj: 2.203%)

14(--) => ROC = 0.8445093+0.0032218[0.8344147, 0.8547615] (0.096% | Adj: 2.372%)

15(--) => ROC = 0.8442498+0.0032416[0.8344147, 0.8544920] (0.103% | Adj: 2.542%)

16(--) => ROC = 0.8439994+0.0032480[0.8343640, 0.8539820] (0.110% | Adj: 2.711%)

17(--) => ROC = 0.8437473+0.0032672[0.8338687, 0.8537420] (0.117% | Adj: 2.881%)

18(--) => ROC = 0.8434952+0.0032584[0.8333741, 0.8537420] (0.124% | Adj: 3.050%)

19(--) => ROC = 0.8432343+0.0032685[0.8328801, 0.8532332] (0.131% | Adj: 3.220%)

20(--) => ROC = 0.8429691+0.0032798[0.8328801, 0.8527251] (0.138% | Adj: 3.389%)

21(--) => ROC = 0.8427066+0.0032981[0.8324372, 0.8527251] (0.145% | Adj: 3.559%)

22(--) => ROC = 0.8424513+0.0033064[0.8323868, 0.8522177] (0.152% | Adj: 3.728%)

23(--) => ROC = 0.8422086+0.0033187[0.8318940, 0.8517110] (0.159% | Adj: 3.898%)

24(--) => ROC = 0.8419510+0.0033109[0.8318940, 0.8512694] (0.165% | Adj: 4.067%)

25(--) => ROC = 0.8416907+0.0033121[0.8314523, 0.8509851] (0.172% | Adj: 4.237%)

26(--) => ROC = 0.8414400+0.0033238[0.8314020, 0.8507650] (0.179% | Adj: 4.406%)

27(--) => ROC = 0.8411875+0.0033277[0.8314020, 0.8507650] (0.186% | Adj: 4.575%)

28(--) => ROC = 0.8409297+0.0033426[0.8309608, 0.8506994] (0.193% | Adj: 4.745%)

29(--) => ROC = 0.8406780+0.0033493[0.8304700, 0.8501946] (0.200% | Adj: 4.914%)

30(--) => ROC = 0.8404285+0.0033600[0.8304700, 0.8501946] (0.207% | Adj: 5.084%)

31(--) => ROC = 0.8401842+0.0033739[0.8299798, 0.8501946] (0.214% | Adj: 5.253%)

32(--) => ROC = 0.8399355+0.0033878[0.8294902, 0.8501946] (0.221% | Adj: 5.423%)

33(--) => ROC = 0.8396845+0.0033940[0.8294902, 0.8500457] (0.227% | Adj: 5.592%)

34(--) => ROC = 0.8394416+0.0034063[0.8290013, 0.8500457] (0.234% | Adj: 5.762%)

35(--) => ROC = 0.8391875+0.0034158[0.8289512, 0.8500457] (0.241% | Adj: 5.931%)

36(--) => ROC = 0.8389394+0.0034127[0.8289512, 0.8495387] (0.248% | Adj: 6.101%)

37(--) => ROC = 0.8386791+0.0034275[0.8284630, 0.8491870] (0.255% | Adj: 6.270%)

How do you lose all your ROC immediately? In 4 false negatives. And that was offset by 14505 true negatives you were sure about! (ratio of 3626.25:1) — And once you hit the “average” loss, you lost 0.01 ROC.

Imagine if you were to deal with 2000 samples only (if you were having 2000, you would be a bit higher than 0.843). You would lose all (if not more) at a uncertainty rate of 0.055% false negative — that is assuming from the point you would not have gambled on rules (i.e the base model before you set hard rules). Hence, at the first false negative, your ROC will tank immediately (if your base model was 0.847 — if it was 0.842, it would still tank that much, although a bit less).

Just reminding:

Current ROC:0.8470657(+) | from Simulation: 0.8470953+0.0031873[0.8376366, 0.8585345] Falses in rule (Current|Boot):

00(++) => ROC = 0.8480724+0.0031332[0.8388510, 0.8594306] (0.000% | Adj: 0.000%)

01(++) => ROC = 0.8478175+0.0031281[0.8383498, 0.8589155] (0.007% | Adj: 0.169%)

02(++) => ROC = 0.8475608+0.0031465[0.8378493, 0.8584011] (0.014% | Adj: 0.339%)

03(++) => ROC = 0.8473098+0.0031603[0.8373494, 0.8578873] (0.021% | Adj: 0.508%)

04(--) => ROC = 0.8470565+0.0031614[0.8368502, 0.8578873] (0.028% | Adj: 0.678%)

All your rules must be right, else if they are right at 99.322% (to find non-positivity, i.e ~99.972% right) you have 50|50 to drop!

N.B: the confidence intervals are 95%, and when it’s 50% sampling it announces nothing good for the private LB :)",https://medium.com/data-design/testing-99-certainty-association-rules-vs-xgboost-on-noisy-data-why-3500-1-ratio-loses-d009992a5e24,[],2017-01-03 19:47:36.242000+00:00,981,"overfitting, association rules, noisy training set, false negatives, ROC curve"
"Google, Heidelberg University & NEC Propose Human Feedback for Real-World RL in NLP Systems","A new study proposes using human feedback and interaction logs to boost offline reinforcement learning (RL) in natural language processing (NLP). Although humans are often inclined to critique or complain, thus far the practical use of their feedback in ML has been mostly limited to error reporting. In the paper Learning from Human Feedback: Challenges for Real-World Reinforcement Learning in NLP, a team from Google Research, Heidelberg University and NEC Laboratories Europe explores the “gold mine” of information buried in feedback and interaction logs and its potential in user-interactive RL and NLP systems.

The researchers note that once NLP systems are deployed in the real world, large volumes of interaction logs that contain user ratings, clicks or revisions are typically collected but tend to be referenced only for evaluation purposes.

The researchers note that it is too “risky” to directly update models online, especially in business settings or with feedback that is inappropriate or even harmful for training. Offline RL can help by pretraining RL agents on existing interaction log data without any further interactions with the environment.

The researchers focused on sequence-to-sequence (Seq2Seq) learning for NLP applications such as machine translation, summarization, semantic parsing and dialogue generation for chatbots, as these can include rich interactions with users.

The team identified three main challenges for off-policy RL in NLP:

Large output space

Deterministic logging

Reliability and learnability of feedback

For RL methods to work well it is crucial to explore the output space — which, in the case of Seq2Seq, is particularly large. Although an output sentence may contain only 100 words, those could come from a vocabulary set of 30,000. As the Heidelberg University Statistical Natural Language Processing Group tweeted, that produces 30.000¹⁰⁰ possible outputs.

Effective exploration is especially important in real-world commercial systems, as poor performance there will deliver inferior outputs to users. The researchers propose pretraining the RL policy on available supervised data to enable the model to concentrate on reasonable areas in the output space.

Another associated constraint for NLP applications is deterministic logging policies. Production NLP systems deliver the most likely output to users and avoid displaying inferior options, which leads to deterministic logging policies that lack explicit exploration and can bias the collected dataset towards the logging policy choices. Once an NLP system is operational it is difficult to correct such biases. The team proposes two approaches to tackle this challenge:

Implicit exploration due to input or context variability

Consider concrete cases of degenerate behaviour in estimation from logged data

The team stresses that even if systems can learn from human feedback, not all human feedback is of equal value or even beneficial. It is unreasonable for example to expect anything other than “bandit feedback” from user interactions with a chatbot, which only provide a reward signal for the one output presented and cannot rate a multitude of outputs for the same input. In this case, the feedback is very sparse in relation to the size of the output space. It is also important how feedback is collected, as this can affect reliability and the ability to learn reward estimators that approximate human rewards and can be integrated into an end-to-end RL task for NLP applications.

The team proposes that since the interfaces for feedback collection affect the reward function that RL agents learn from, researchers should study user interfaces and experiences in real-world settings, especially for downstream tasks where agents interact with natural language. They say that focusing on such systems can help NLP researchers examine offline RL in real-world production settings and encourage the development of innovative algorithms for tackling challenges in NLP applications.

The paper Learning from Human Feedback: Challenges for Real-World Reinforcement Learning in NLP is on arXiv.",https://medium.com/syncedreview/google-heidelberg-university-nec-propose-human-feedback-for-real-world-rl-in-nlp-systems-910e758debbf,[],2020-11-12 18:41:57.007000+00:00,603,"NLP, Reinforcement Learning, OfflineRL, User Feedback, Seq2Seq"
Support Vector Machines and Regression Analysis,"Support Vector Machines and Regression Analysis

Source: Image Created by Author — based on visual template from “Hands-On Machine Learning with Scikit-Learn and TensorFlow” by Aurélien Géron

It is a common misconception that support vector machines are only useful when solving classification problems.

The purpose of using SVMs for regression problems is to define a hyperplane as in the image above, and fit as many instances as is feasible within this hyperplane while at the same time limiting margin violations.

In this way, SVMs used in this manner differ from classification tasks, where the objective is to fit the largest possible hyperplane between two separate classes (while also limiting margin violations).

As a matter of fact, SVMs can handle regression modelling quite effectively. Let’s take hotel bookings as an example.

Predicting Average Daily Rates Across Hotel Customers

Suppose that we are building a regression model to predict the average daily rate (or rate that a customer pays on average per day) for a hotel booking. A model is constructed with the following features:

Cancellation (whether a customer cancels their booking or not)

Country of Origin

Market Segment

Deposit Type

Customer Type

Required Car Parking Spaces

Week of Arrival

Note that the ADR values are also populated for customers that cancelled — the response variable in this case reflects the ADR that would have been paid had the customer proceeded with the booking.

The original study by Antonio, Almeida and Nunes (2016) can be accessed from the References section below.

Model Building

Using the features as outlined above, the SVM model is trained and validated on the training set (H1), with the predictions compared to the actual ADR values across the test set (H2).

The model is trained as follows:

>>> from sklearn.svm import LinearSVR

>>> svm_reg = LinearSVR(epsilon=1.5)

>>> svm_reg.fit(X_train, y_train) LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,

intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,

random_state=None, tol=0.0001, verbose=0) >>> predictions = svm_reg.predict(X_val)

>>> predictions array([100.75090575, 109.08222631, 79.81544167, ..., 94.50700112,

55.65495607, 65.5248653 ])

Now, the same model is used on the features in the test set to generate predicted ADR values:

bpred = svm_reg.predict(atest)

bpred

Let’s compare the predicted ADR to actual ADR on a mean absolute error (MAE) and root mean squared error (RMSE) basis.

>>> mean_absolute_error(btest, bpred)

29.50931462735928 >>> print('mse (sklearn): ', mean_squared_error(btest,bpred))

>>> math.sqrt(mean_squared_error(btest, bpred))

44.60420935095296

Note that the sensitivity of the SVM to additional training instances is set by the epsilon (ϵ) parameter, i.e. the higher the parameter, the more of an impact additional training instances has on the model results.

In this instance, a large margin of 1.5 was used. Here is the model performance when a margin of 0.5 is used.

>>> mean_absolute_error(btest, bpred) 29.622491512816826 >>> print('mse (sklearn): ', mean_squared_error(btest,bpred))

>>> math.sqrt(mean_squared_error(btest, bpred)) 44.7963000500928

We can see that there has been virtually no change in the MAE or RMSE parameters through modifying the ϵ parameter.

That said, we want to ensure that the SVM model is not overfitting. Specifically, if we find that the best fit is achieved when ϵ = 0, then this might be a sign that the model is overfitting.

Here are the results when we set ϵ = 0.

MAE: 31.86

31.86 RMSE: 47.65

Given that we are not seeing higher accuracy when ϵ = 0, there does not seem to be any evidence that overfitting is an issue in our model — at least not from this standpoint.

How Does SVM Performance Compare To A Neural Network?

When using the same features, how does the SVM performance accuracy compare to that of a neural network?

Consider the following neural network configuration:

>>> model = Sequential()

>>> model.add(Dense(8, input_dim=8, kernel_initializer='normal', activation='elu'))

>>> model.add(Dense(1669, activation='elu'))

>>> model.add(Dense(1, activation='linear'))

>>> model.summary() Model: ""sequential""

_________________________________________________________________

Layer (type) Output Shape Param #

=================================================================

dense (Dense) (None, 8) 72

_________________________________________________________________

dense_1 (Dense) (None, 1669) 15021

_________________________________________________________________

dense_2 (Dense) (None, 1) 1670

=================================================================

Total params: 16,763

Trainable params: 16,763

Non-trainable params: 0

_________________________________________________________________

The model is trained across 30 epochs with a batch size of 150:

>>> model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])

>>> history=model.fit(X_train, y_train, epochs=30, batch_size=150, verbose=1, validation_split=0.2)

>>> predictions = model.predict(X_test)

The following MAE and RMSE are obtained on the test set:

MAE: 29.89

29.89 RMSE: 43.91

We observed that when ϵ was set to 1.5 for the SVM model, the MAE and RMSE came in at 29.5 and 44.6 respectively. In this regard, the SVM has matched the neural network in prediction accuracy on the test set.

Conclusion

It is a common misconception that SVMs are only suitable for working with classification data.

However, we have seen in this example that the SVM model has been quite effective at predicting ADR values for the neural network.

Many thanks for reading, and any questions or feedback appreciated.

The Jupyter Notebook and dataset for this example are available at the MGCodesandStats GitHub repository. Other relevant references are available below.

Disclaimer: This article is written on an “as is” basis and without warranty. It was written with the intention of providing an overview of data science concepts, and should not be interpreted as professional advice in any way.

References",https://towardsdatascience.com/support-vector-machines-and-regression-analysis-ad5d94ac857f,['Michael Grogan'],2020-11-07 18:33:10.982000+00:00,763,"Antonio, J.C., Almeida, L.F., Nunes"
Stop Using Square Bracket Notation to Get a Dictionary’s Value in Python,"The Traditional (Bad) Way to Access a Dictionary Value

The traditional way to access a value within a dictionary is to use square bracket notation. This syntax nests the name of the term within square brackets, as seen below.

author = {

""first_name"": ""Jonathan"",

""last_name"": ""Hsu"",

""username"": ""jhsu98""

} print(author['username']) # jhsu98

print(author['middle_initial']) # KeyError: 'middle_initial'

Notice how trying to reference a term that doesn’t exist causes a KeyError . This can cause major headaches, especially when dealing with unpredictable business data.

While we could wrap our statement in a try/except or if statement, this much care for a dictionary term will quickly pile up.

author = {} try:

print(author['username'])

except KeyError as e:

print(e) # 'username' if 'username' in author:

print(author['username'])

If you come from a JavaScript background, you may be tempted to reference a dictionary value with dot notation. This doesn’t work in Python.",https://medium.com/better-programming/stop-using-square-bracket-notation-to-get-a-dictionarys-value-in-python-c617f6ea15a3,['Jonathan Hsu'],2020-05-06 01:15:37.387000+00:00,131,"Python, Dictionary, Square Bracket Notation, Key Error, Try/Except Statement"
5 minutes of Web Scraping using Python and Beautiful Soup,"5 minutes of Web Scraping using Python and Beautiful Soup

Scraping La Liga Stats using Python and Beautiful Soup

A photo by Emile Perron — Unsplash

Extracting data from the website aka web scraping can save plenty of time and effort. Thanks to Beautiful Soup, the road of web scraping has become even smoother. In this article, we will scrape La Liga 2019–20 stats from a website using Python.

Importing Libraries

import pandas as pd

import requests as rq

from bs4 import BeautifulSoup

First things first, all the useful libraries must be imported. I have imported pandas to store the stats in a data frame. The Requests library is imported to send a request to the HTTP server while Beautiful Soup is brought in for web elements retrieval.

Communicating with webpage

get_url = rq.get(""https://www.msn.com/en-us/sports/soccer/la-liga/player-stats"")



get_text = get_url.text



soup = BeautifulSoup(get_text, ""html.parser"")

We have to get the target URL and then parse it using the Beautiful Soup library. Once the soup object is created, you are good to go.

Extracting web elements

rank = [i.text for i in soup.findAll('td', {

""class"" : ""hide1 rankvalue""

})]



player_name = [i.text

for i in soup.findAll('td', {

""class"" : ""playername""

})

]



team_name = [i.text

for i in soup.findAll('td', {

""class"" : ""teamname""

})

]



team_la = [i.text

for i in soup.findAll('td', {

""class"" : ""teamtla""

})

]



games_played = [int(i.findAll('td')[4].text) for i in soup.findAll('tr', {

""class"" : ""rowlink""

})]



goals_scored = [int(i.findAll('td')[7].text) for i in soup.findAll('tr', {

""class"" : ""rowlink""

})]



assists = [int(i.findAll('td')[8].text) for i in soup.findAll('tr', {

""class"" : ""rowlink""

})]

Using Beautiful Soup methods, you can access the web elements and its items. I have majorly used the “findAll” method giving “class” as a parameter to get the list of all items present.

Storing in a DataFrame

laliga_stats = pd.DataFrame({



""Rank"" : rank,



""Player Name"" : player_name,



""Team Name"" : team_name,



""Team"" : team_la,



""Games Played"" : games_played,



""Goals"" : goals_scored,



""Assists"" : assists

})

laliga_stats.set_index('Rank',inplace=True)

Finally, store the data into the data frame using the Pandas library. And, you are good to go for your analysis.

You can get the Top 10 goalscorers of 2019–20 La Liga season by just writing a line of code:

laliga_stats[0 :10]

Top 10 goalscorers La Liga 2019–20 season

All it takes is Python and Beautiful Soup’s wonder to scrape a website. So, open up Beautiful Soup’s documentation on a new tab and start scraping.

For feedback and discussion, hit me at Linkedin!",https://towardsdatascience.com/5-minutes-of-web-scraping-using-python-and-beautiful-soup-98e6fbce9cc7,['Vishal Sharma'],2020-05-31 16:52:05.522000+00:00,349,"webscraping, Python, Beautiful Soup, La Liga Stats, Data Extraction"
COVID-19 Impact on Early vs Late Stage Funding: A Data-Driven Analysis,"COVID-19 has had unforeseen and unpredictable impacts on many industries, including Venture Capital. VC, an industry predicated on personal relationships and accessibility, now needed to operate virtually. Did early stage companies shift their focus from their next funding round to staying afloat? Did late stage companies take a second look at potential sales cycle disruption and employee safety? Here at A-Level Capital, we wanted to quantify COVID-19’s impact on the VC space. We wanted to answer questions like how has VC funding changed since the onset of COVID? Has it changed at all? And perhaps most importantly, is the VC space resilient?

To answer these questions, we conducted a high-level, data-driven analysis of VC activity from Q4 2019 all the way through the present, Q2 2020. Venture funding history data was obtained from whogotfunded.com, a free online service that aggregates data through mass web text scraping. The data was then verified and supplemented via Crunchbase. All deals were based in the US. The data and code can be found here.

To start, let’s look at the frequency of venture funding over time.

These three figures show the number of rounds raised per day, from Q4 2019 through Q2 2020. Daily, weekly, and monthly frequency windows are shown.

Here are some key observations:

There’s a clear decrease in the number of rounds raised in Q1/Q2 2020 as compared to Q4 2019.

This decrease in rounds raised appears to line up with increased COVID awareness in the US, which started around March (blue shaded region).

This decrease appears to start around when the WHO declared COVID a global public health emergency (orange shaded region).

The decline doesn’t continue into Q2 2020 — round frequency plateaus instead.

We can see that number of rounds raised has decreased into 2020, presumably due to lock-down procedures, quarantine across the US, and the general disruption of VC daily activities.

Let’s now look at trends in the size of these rounds.

Here’s a graph of the average daily round size over this time frame. The line shown is the average round size and the shaded regions represent one standard deviation of a Gaussian distribution of the other rounds that day. Basically, the shaded distributions give us a good idea of the diversity of round type raised that day. Q4 2019 has a pretty wide range of round sizes, whereas in Q1/Q2 2020 round sizes are much more uniform.

When we compare these trends in average round size to our earlier graphs of monthly deal frequency, we see a really interesting pattern.

Average round size is much higher in Q1/Q2 2020 than Q4 2019. However, 2020 has seen a clear decline in rounds per week.

than Q4 2019. However, So less deals are happening in 2020, but the round average is getting significantly higher??

Here’s the earlier two graphs on top of each other. Keep in mind that the y axis units here are non-important, but you can see pretty clearly that deal volume is going down as round average goes up.

We can hypothesize that this either means (1) not many late stage companies raised venture funding in Q4 2019, or more likely (2) COVID onset has really impacted early-stage startups raising rounds. The first option’s unlikely because the shading on the Q4 portion of the graph means there’s actually a lot of round diversity in Q4 2019!

The first option’s unlikely because the shading on the Q4 portion of the graph means there’s actually a lot of round diversity in Q4 2019! We can speculate large companies generally have the resources in place to weather out COVID’s impact, at least enough to complete their scheduled fundraising rounds. Smaller companies however might be having a harder time adapting to these changes and as a result, be pushing off their funding rounds to focus on more immediate priorities.

From these trends in the data, our standing hypothesis is that COVID-19 has had the most disruptive impact on early stage companies, causing them to push their funding and as a result, raise the observed average round size in Q1/Q2 2020.

Let’s see if this is reflected on a state level as well.

The top 3 states with the most VC investment are New York, California, and Massachusetts.

Taking a closer look at New York:",https://medium.com/a-level-capital/covid-19-impact-on-early-vs-late-stage-funding-a-data-driven-analysis-48290c46828d,['Shreya Singh'],2020-06-27 00:11:04.501000+00:00,694,"COVID-19, Venture Capital, Funding Round, Early Stage Startups, Crowdfunding"
The GDPR Force Meets Customer Intelligence,"DATA STORIES | DATA BLENDING | KNIME ANALYTICS PLATFORM

The GDPR Force Meets Customer Intelligence

Co-author: Phil Winters

The Challenge

The European Union introduced the General Data Protection Regulation on May 25, 2018. This law has been called the world’s strongest and most far-reaching law aimed at strengthening citizens’ fundamental rights in the digital age. The law applies to any organization processing personal data about a citizen or resident in the EU regardless of that organizations’ location. It is definitely a force to be reckoned with as the fines for non-compliance are extremely high.

Many of the law’s Articles deal with how personal data are collected and processed and include a special emphasis, guidelines and restrictions on what the EU calls “automatic profiling with personal data”. So what has that to do with us data scientists? Many of us use methods and machine learning to create fact-based insight from the customer or personal data that we collect so that we can take decisions, possibly automatically. So the law applies to us. But that Customer Intelligence we create is fundamental for the successful running of our organizations business.

Does this mean this force put limits on what we do? Or even worse, does it mean that we have to go over to the dark side to help our businesses? Absolutely not! The law defines that we must gain permission, perform tests, and document. In no way does it restrict what honest data scientists can do. If anything, it provides us with opportunities.

Topic. GDPR meets Customer Intelligence

Challenge. Apply machine learning algorithms to create fact-based insights taking the GDPR into account

The Experiment

Distilling the GDPR down for data scientists means we need to look at six topics:

Identify fields of information we will use that contain personal data Identify any fields of information that contain “special categories of data” (i.e.: those that could discriminate) If any special categories of data exist, ensure that there are no highly correlated fields that could mimic the special categories Possibly anonymize any personal data so that the GDPR regulations do not apply and … …Explain an automatic processing decision (or a model or rule in our case) in a way that is understandable Document our process that uses all of these data for “automatic processing”

Figure 1 . This workflow shows an example of analyzing customer data while adhering to GDPR.

And that is exactly what we do. Since the details of each of these steps and the appropriate workflow examples goes far beyond this article, you can get a full white paper as well as KNIME public server examples.

If we then go on to use the KNIME Server, we can have these standard workflows available for anyone doing work on personal data.

The Results

While the GDPR does introduce requirements that many of us need to follow if we are using personal data, in no way does it limit us. If anything, it allows us to surface to our management and others within our organization, showing that what we do with customer intelligence using KNIME is well thought through, well documented, and fully supportive of our organization’s GDPR requirements — so yes, Customer Intelligence and the GDPR blend! Definitely not the Dark Side at all!

You might also like …

If you enjoyed this, you might like Phil Winter’s (a.k.a. Darth Vader) speech “GDPR — A Proactive Approach” at the 2018 KNIME Spring Summit in Berlin.

— — — — -

As previously published on the KNIME Blog: https://www.knime.com/blog/GDPR_meets_CustomerIntelligence",https://medium.com/low-code-for-advanced-data-science/the-gdpr-force-meets-customer-intelligence-b5bce2805d74,['Rosaria Silipo'],2021-09-03 08:10:27.540000+00:00,567,"GDPR, Customer Intelligence, Data Blending, KNIME Analytics Platform, Data Stories"
Learn English with Game of Thrones: The Best Episodes,"Learn English with Game of Thrones: The Best Episodes

Using data to find the best episodes of Game of Thrones to learn English with

Photo by Chris Curry on Unsplash

If your native language isn’t English, you probably watched the dubbed version of Game of Thrones or needed subtitles to watch the original version. That’s OK; even I find the language in Game of Thrones a bit complicated. But, what if I tell you which are the episodes in this great TV show that cover only basic English words? Throughout 8 seasons, there must be some episodes where the characters use simple vocabulary, right? This is true! That’s why I used transcripts to find Game of Thrones episodes with the easiest vocabulary, so you can have fun watching them without subtitles. I’ll also compare the vocabulary used in Game of Thrones with shows like Friends and The Simpsons to know which TV show would be better to learn English with.

What Game of Thrones (GoT) episodes among the 73 are the best for you to learn English?

The British accent, speech pace and vocabulary used in episodes are some things that make GoT hard to follow even for native speakers. However, I ranked the 73 GoT episodes by difficulty of vocabulary, so you can re-watch them in English, starting with the easiest episodes. For example, I found that Oathbreaker (S6E3) and Beyond the Wall (S7E6) are the episodes in which characters used simple English vocabulary. I bet that If you watch them, you’ll be able to understand dialogues more than in the episode The Laws of Gods and Men (S4E6), which is the hardest episode to understand because of its hard vocabulary.

You can find all the Game of Thrones episodes in the image below. The episodes furthest to the right cover more basic English vocabulary

The code behind this analysis is available on Github — Image by Author.

The best episodes to learn English with Game of Thrones

The following are the top 10 episodes with the easiest vocabulary among all Game of Thrones episodes aired. Re-watch them without subtitles and you’ll understand dialogues more than in the average GoT episode:

S6E3: Oathbreaker

S7E6: Beyond the Wall

S8E2: A Knight of the Seven Kingdoms

S2E4: Garden of Bones

S2E10: Valar Morghulis

S8E4: The Last of the Starks

S3E6: The Climb

S8E5: The Bells

S2E1: The North Remembers

S3E2: Dark Wings, Dark Words

Also, I found that Battle of the Bastards (S6E9) and The Red Woman (S6E1) are one of the episodes with fewer words spoken. I believe that happens because they have scenes with many fights, so less dialogue. If you watch them, you’ll have fewer hard words to worry about!

Tip: Check the vocabulary used in every episode before you watch them. You can easily do so by checking the chart ‘Every Word Said in Game of Thrones (Level 3and above)’ below.

Should I Learn English with GoT, Friends or The Simpsons?

Choosing a TV series to learn English is a hard task. Most people would recommend you a movie or TV show based on what they like or what worked for them to learn English, but let’s be honest, not everyone learns with the same formula.

If I have to follow what my research results suggest, I’d recommend watching Friends to someone who started learning English. At the same time, The Simpsons and Game Of Thrones would be an excellent choice for intermediate/advanced English learners who want to improve their vocabulary. However, here at Learn Language with TV, we encourage you to learn a language with content you love, so even if your English is basic, you could watch The Simpsons or Game of Thrones and improve your skills. After all, you already know which are the easiest episodes from those TV shows, remember?

Image by Author

Each TV show has some vocabulary that was created or is only spoken in their episodes. They’re labeled as ‘unique words’ in the chart above. Game of Thrones has many of these words, such as Winterfell, Westeros, Dothraki and Khaleesi. You can find more words like this in the following wordcloud with the shape of one of Daenerys’ dragon:

Character names were excluded, but I kept family names created on the TV show- Image by Author.

Which character Speaks the Most in 7 Seasons?

In case you’re interested to know which characters speak the most in almost all episodes in Game of Thrones, I ranked them by each season. To do so, I calculated which characters speak the most in the series and then classified them into seasons.

Play with the chart below to find how many times your favorite character speak in each season!

Image by Author

What hard words are they saying in 73 episodes?

Now that you know the easy episodes from Game of Thrones, it’s time to reveal my secret to learning languages with TV. To increase my English vocabulary when watching an episode of a TV show, I use the table below. You can find which difficult words you’ll hear in the 73 Game of Thrones episodes in a couple of seconds by using the table. You can filter by word, episode or season!

Image bu Author

This is how I increase my vocabulary in 4 languages by watching TV with the table above, step by step:

Step 1: I search the name of the episode I’m going to watch in the table above.

Step 2: I check the hard words repeated a lot of times in the episode by sorting the table by the ‘Repetition’ column

Step 3: I look up the meaning of hard words that I never heard before and are repeated a lot of times in that episode

Step 4: Watch the episode and have fun!

If you do this, you’ll increase your vocabulary and also the chances to understand dialogue in an episode, which includes deep conversations, jokes, etc.

Conclusion

I know how it feels to be unable to understand dialogue when watching your favorite TV show. In this article, I used my data science knowledge to find the Game of Thrones episodes with the easiest English vocabulary, so you can re-watch them and have fun learning English. I also compared it with other TV shows to find the ideal show for you to learn English with.

The code behind this analysis is available on Github.",https://medium.com/learn-languages-with-tv/learn-english-with-game-of-thrones-the-best-episodes-94eb4fba11d2,['Frank Andrade'],2020-11-16 00:17:34.957000+00:00,1023,"English, Game of Thrones, TV shows, Vocabulary, Friends"
Bokeh 2.0.1,"Today we release Bokeh 2.0.1: a collection of improvements in automation, documentation, and other minor fixes following the recent 2.0 release.

The full list of changes can be seen in the milestone list on GitHub. Some of the highlights include:

Addressing a Cross-Origin Resource Sharing (CORS) issue seen in Chrome and Chromium-based browsers #9773

Adding multi-file support for FileInput widgets #9727

widgets Bokeh server can now serve custom extension code #9799

A handful of documentation clarifications, corrections, and expansions

As of 2.0.1, Bokeh’s FileInput widget supports multiple file selections.

If you have questions after upgrading, we encourage you to stop by the Bokeh Discourse! Friendly project maintainers and a community of Bokeh users are there to help you navigate any issues that arise.

If you are using Anaconda, Bokeh can most easily be installed by executing conda install -c bokeh bokeh . Otherwise, use pip install bokeh .

Developers interested in contributing to the library can visit Bokeh’s Zulip chat channels for guidance on best practices and technical considerations.

As always, we appreciate the thoughtful feedback from users and especially the work of our contributor community that make Bokeh better!",https://medium.com/bokeh/bokeh-2-0-1-362eb5d0729a,[],2020-06-10 00:13:12.811000+00:00,180,"bokeh, bokeh2.0.1, CORS, File Input Widgets, Anaconda"
Real-Time Lane Detection and alerts for Autonomous Driving,"On this dataset, we can train a semantic segmentation model to segment out the pixels that belong to the lane class. U-Net model would be an ideal fit here since it is a lightweight model with real-time inference speed. U-Net is an encoder-decoder model with skip connections b/w encoder and decoder blocks. The model architecture is shown below.

U-Net model architecture from the U-Net Paper

However, the loss function should be modified to a Dice Loss coefficient. The lane line segmentation problem is an extremely imbalanced data problem. The majority of the pixels in the image belong to the background class. Dice Loss is based on Sorenson-Dice coefficient and it attaches similar importance to false positives and false negatives which allows it to perform better for imbalanced data problems. You can read more about the Dice Loss in their paper. The Dice loss tries to match the lane pixels in the ground truth and predicted models and is able to get a crisp boundary prediction. I really enjoyed this blog on using Dice Loss for boundary prediction.

LaneNet Model

For this blog, I used the LaneNet model to generate lane lines. The LaneNet model is a two-stage lane line predictor. The first stage is an encoder-decoder model to create a segmentation mask for the lane lines. The second stage is a lane localization network that takes as input the lane points extracted from the mask and uses LSTM to learn a quadratic function that predicts the points for the lane.

The image below shows the two stages in action. The left panel is the original image, the center panel is the lane mask output from stage 1 and the right panel is the final output from stage 2.

LaneNet Model explained (Picture from the paper — https://arxiv.org/pdf/1807.01726.pdf)

I used the LaneNet model implementation from this repo. The code is well maintained and runs without errors.

Generating Smart Alerts

I have combined the predictions of lane lines with object detection to generate smart alerts. These smart alerts can be related to:

Detecting the presence of other vehicles in Vehicle’s Ego Lane and measuring the distance to them

Detecting the presence of a vehicle in the adjacent lanes

Understanding the radius of a turn for curved roads

Here, I have used YOLO-v5 to detect cars and humans on the road. If you are interested in testing YOLO-v5, please check out my blog here.

YOLO-v5 does quite well in detecting other cars on the road. The inference time is also very fast.

Below we are using YOLO v5 to measure the distance b/w ego vehicle and the closest vehicle in front. The models return the distance in pixels which can be converted to meters based on the camera parameters. Since camera parameters are not known for the TUSimple data set, I have estimated pixels to meter conversion based on the standard width of lane lines.

Alerts for distance measurement

We can similarly calculate the radius of curvature of the lane and use that for the steering module of the car.

The radius of curvature measurement

Conclusion

In this blog, we explore the problem of detecting lane lines accurately and quickly for applications in Autonomous Driving. We then use YOLOv5 to build an understanding of other objects on the road. This can be used to generate smart alerts.

At Deep Learning Analytics, we are extremely passionate about using Machine Learning to solve real-world problems. We have helped many businesses deploy innovative AI-based solutions. Contact us through our website here if you see an opportunity to collaborate.

References",https://towardsdatascience.com/real-time-lane-detection-and-alerts-for-autonomous-driving-1f0a021390ee,['Priya Dwivedi'],2020-12-15 19:58:32.038000+00:00,571,"Deep Learning, Machine Learning, Autonomous Driving, Semantic Segmentation, UNet Model"
Spark UDF — Deep Insights in Performance,"Nikhilesh Nukala — Consultant (Data Engineering), Yuhao Zhu — Advanced Analytics Consultant, Guilherme Braccialli — Principal Data Engineer, Tom Goldenberg- Jr Principal (Data Engineering), QuantumBlack

This blog will demonstrate a performance benchmark in Apache Spark between Scala UDF, PySpark UDF and PySpark Pandas UDF.

At QuantumBlack, we often deal with multiple terabytes of data to drive advanced analytics projects. The data pipelines should run at speed from anywhere and be performant, while the workload should be distributable, scalable and reliable. Apache Spark is one platform we leverage for this.

Spark offers a variety of solutions to complicated challenges, but we face many situations where the native functions are not sufficient to solve the problem. For this reason, Spark allows us to register custom functions called User-Defined Functions, or UDFs.

In this article, we will explore the performance characteristics of Spark’s UDFs.

Spark supports multiple languages such as Python, Scala, Java, R and SQL, but often the data pipelines are written in PySpark or Spark Scala. We believe PySpark is adopted by most users for the following reasons:

1. Faster learning curve — Python is a simpler language than Scala.

2. Wide Community Support — An ecosystem of programmers feedback on PySpark performance and suggest updates.

3. Availability of rich libraries — Python has a rich set of libraries for machine learning, time series analysis and statistics.

4. Negligible performance difference — The introduction of Spark DataFrames means the performance of Scala and Python are fairly similar. Dataframes are now organized under named columns to help Spark understand the schema, while the operations that are used to build the dataframe are compiled into a physical execution plan by the Catalyst Optimizer, accelerating computation.

5. Easy code handover between data engineers and data scientists. In some data frame operations that require UDFs, PySpark can have an impact on performance. There are approaches to address this by combining PySpark with Scala UDF and UDF Wrapper.

When a PySpark job is submitted while the driver program is running in vanilla Python, the driver creates SparkSession object and Dataframes/RDDs. These will be a Python wrapper around a JVM (Java) object. To simplify this, PySpark provides a wrapper that runs native Scala code.

Spark User-Defined Functions (UDFs):

Registering Spark custom functions in Scala, Python and Java has become a very popular way to expose advanced functionality to SQL users, enabling users to call in the functions without writing the code.

For example, multiplying a set of million rows by 1000:

def times1000(field):



return field * 1000.00

or reverse geocode a latitude and longitude dataset:

import geohash def geohash_pyspark(lat, lon): return geohash.encode(lat, lon)

Spark SQL offers a built-in method to easily register UDFs by passing in a function in your programming language. Scala and Python can use native function and lambda syntax, but in Java we need to extend the UDF class.

UDFs can work on a variety of types and can return a different type than the one they are initially called with. In Python and Java, we need to specify the return type.

UDF can be easily registered by running:

spark.udf.register(""UDF_Name"", function_name, returnType())

*returnType() is mandatory in Python and Java.

Types of Spark UDFs and execution:

In distributed mode, Spark uses master/worker architecture for execution. The central coordinator, called driver, communicates with a potentially large number of distributed workers, called executors. The driver and workers run their own Java process (JVM).

The driver runs the main() method and creates a SparkContext, RDDs and performs transformations and actions. The executors are responsible for running the individual tasks.

Benchmarking the performance:

To benchmark the performance of the three Spark UDFs, we have created a random Latitude, Longitude dataset, with 100 million rows and worth 1.2 GB, and have defined 2 UDFs: a simple function which multiplies each row by 1000 and a complex geohash function.

Cluster Configuration: Databricks cluster with 8 nodes

Driver node: 16 cores and 122 GB memory

Worker nodes: 4 cores with 30.5 GB memory, autoscaling enabled.

Notebook code — https://bit.ly/2YxiVp4 — QuantumBlack’s approach to benchmark Scala UDF, PySpark UDF and PySpark Pandas UDF.

Along with the three types of UDFs discussed above, we have created a Python wrapper to call the Scala UDF from PySpark and found that we can bring the best of two worlds i.e. ease of Python coding and performance of Scala UDF.

Creating a Python wrapper to call Scala UDF from PySpark code:

from pyspark.sql.column import Column

from pyspark.sql.column import _to_java_column

from pyspark.sql.column import _to_seq

from pyspark.sql.functions import col



def udfGeohashScalaWrapper(lat, lon):

_geohash = sc._jvm.sparkudfperformance.UDFs.udfGeohash()

return Column(_geohash.apply(_to_seq(sc, [lat, lon], _to_java_column)))

def udfTimes1000ScalaWrapper(field):

_times1000 = sc._jvm.sparkudfperformance.UDFs.udfTimes1000()

return Column(_times1000.apply(_to_seq(sc, [field], _to_java_column)))

Databricks Pandas UDF benchmarking — https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html

Key Takeaways:

Below are the results from the tests performed

Our results demonstrate that Scala UDF offers the best performance. As mentioned earlier, the step of translating from Scala to Python and back adds to the processing of the Python UDFs.

We also found that PySpark Pandas UDF provides a better performance for smaller datasets or simpler functions than PySpark UDF. When a more complex function, such as geohashing, is introduced, this characteristic changes. In these circumstances, PySpark UDF is around 10 times more performant than the PySpark Pandas UDF.

We have also found that creating a Python wrapper to call Scala UDF from PySpark code is around 15 times more performant than the two types of PySpark UDFs.

Taking these performance characteristics into account, QuantumBlack now:

Uses PySpark UDFs when the data volume is not big or need quick insights using simpler functions.

Builds an internal library with re-usable Scala UDFs

Creates Python wrappers to call Scala UDFs

References:

Learning Spark — O’Reilly",https://medium.com/quantumblack/spark-udf-deep-insights-in-performance-f0a95a4d8c62,"['Quantumblack', 'A Mckinsey Company']",2020-06-03 18:54:38.528000+00:00,884,"Py Spark UDFs — Databricks Apache Spark, Data Engineering, Advanced Analytics, Data Pipelines, UDFs"
Time series prediction using Prophet in Python,"In this post we will explore facebook’s time series model Prophet. We will understand what is prophet and it’s advantages. We explore Prophet using a dataset to understand change points, how to include holidays and finally using multiple regressors for time series prediction.

What is Prophet?

Prophet is a facebooks’ open source time series prediction. Prophet decomposes time series into trend, seasonality and holiday. It has intuitive hyper parameters which are easy to tune.

Prophet time series = Trend + Seasonality + Holiday + error

Trend models non periodic changes in the value of the time series.

Seasonality is the periodic changes like daily, weekly, or yearly seasonality.

Holiday effect which occur on irregular schedules over a day or a period of days.

Error terms is what is not explained by the model.

Advantages of using Prophet

Accommodates seasonality with multiple periods

Prophet is resilient to missing values

Best way to handle outliers in Prophet is to remove them

Fitting of the model is fast

Intuitive hyper parameters which are easy to tune

Installing Prophet

Install Prophet using either command prompt or Anaconda prompt using pip

pip install fbprophet

We can also install plotly for plotting the data for prophet

pip install plotly

Create the input data for Prophet

Input to Prophet is a dataframe with minimum two columns : ds and y.

ds is datestamp column and should be as per pandas datatime format, YYYY-MM-DD or YYYY-MM-DD HH:MM:SS for a timestamp.

y is the numeric column we want to predict or forecast.

Prophet follows sklearn model API of creating an instance of the Prophet, fitting the data on Prophet object and then predict the future values.

We now dive in right into the code and look at change points, how to include holidays and then add multiple regressors.

Avocado data set is used in the article

Importing the required libraries

from fbprophet import Prophet

from fbprophet.plot import plot_plotly

import numpy as np

import pandas as pd import matplotlib.pyplot as plt

import plotly.offline as py

py.init_notebook_mode()

%matplotlib inline

Reading the data from the csv file

dataset= pd.read_csv(“C:\\avocado-prices\\avocado.csv”)

Understanding the data

Let’s first look at the columns and the data

dataset.head(2)

Let’s print the information about the dataset that includes columns, data type of the columns and if the column is null or not null

dataset.info()

We see two categorical variables, type and region. Let’s examine them

dataset.describe(include=’O’)

Converting categorical variable to numeric using LabelEncoder

We convert categorical variable, type which has two distinct values to numeric value.To convert categorical variable to numeric value we use LabelEncoder. We are not converting categorical variable, region for this example.

from sklearn.preprocessing import LabelEncoder le = LabelEncoder()

dataset.iloc[:,10] = le.fit_transform(dataset.iloc[:,10])

dataset.head(2)

type is now converted to numeric

Creating input features(X) and target variable(y)

X= dataset[['Date',‘Total Volume’, ‘4046’, ‘4225’, ‘4770’,

‘Small Bags’, ‘Large Bags’, ‘XLarge Bags’, ‘type’]]

y= dataset.iloc[:,1]

Creating the data set for Prophet

As mentioned earlier, input to Prophet is a data frame with minimum two columns : ds and y

train_dataset= pd.DataFrame()

train_dataset['ds'] = pd.to_datetime(X[""Date""])

train_dataset['y']=y

train_dataset.head(2)

Creating and fitting the Prophet model with default values

We will first explore the default Prophet model. Create the Prophet instance with all default values, fit the dataset.

prophet_basic = Prophet()

prophet_basic.fit(train_dataset)

Predicting the values for the future

For predicting the values using Prophet, we need to create a dataframe with ds(datetime stamp) containing the dates for which we want to make the predictions.

We use make_future_dataframe() to which we specify the number of days to extend into the future. By default it includes dates from the history

future= prophet_basic.make_future_dataframe(periods=300)

future.tail(2)

Total number of rows in original dataset was 18249 and we see that the future data frame that we created for prediction contains historical dates as well as additional 300 dates.

forecast=prophet_basic.predict(future)

Plotting the predicted data

fig1 =prophet_basic.plot(forecast)

Plotting the forecasted components

We can plot the trend and seasonality, components of the forecast.

fig1 = prophet_basic.plot_components(forecast)

Components of the forecast

Adding ChangePoints to Prophet

Changepoints are the datetime points where the time series have abrupt changes in the trajectory.

By default, Prophet adds 25 changepoints to the initial 80% of the data-set.

Let’s plot the vertical lines where the potential changepoints occurred

from fbprophet.plot import add_changepoints_to_plot fig = prophet_basic.plot(forecast)

a = add_changepoints_to_plot(fig.gca(), prophet_basic, forecast)

Vertical lines are where changepoints occurred

We can view the dates where the chagepoints occurred

prophet_basic.changepoints

We can change the inferred changepoint range by setting the changepoint_range

pro_change= Prophet(changepoint_range=0.9) forecast = pro_change.fit(train_dataset).predict(future)

fig= pro_change.plot(forecast);

a = add_changepoints_to_plot(fig.gca(), pro_change, forecast)

The number of changepoints can be set by using the n_changepoints parameter when initializing prophet

pro_change= Prophet(n_changepoints=20, yearly_seasonality=True) forecast = pro_change.fit(train_dataset).predict(future)

fig= pro_change.plot(forecast);

a = add_changepoints_to_plot(fig.gca(), pro_change, forecast)

Adjusting Trend

Prophet allow you to adjust the trend in case there is an overfit or underfit. changepoint_prior_scale helps adjust the strength of the trend.

Default value for changepoint_prior_scale is 0.05. Decrease the value to make the trend less flexible. Increase the value of changepoint_prior_scale to make the trend more flexible.

Increasing the changepoint_prior_scale to 0.08 to make the trend flexible

pro_change= Prophet(n_changepoints=20, yearly_seasonality=True, changepoint_prior_scale=0.08)

forecast = pro_change.fit(train_dataset).predict(future)

fig= pro_change.plot(forecast);

a = add_changepoints_to_plot(fig.gca(), pro_change, forecast)

Decreasing the changepoint_prior_scale to 0.001 to make the trend less flexible

pro_change= Prophet(n_changepoints=20, yearly_seasonality=True, changepoint_prior_scale=0.001)

forecast = pro_change.fit(train_dataset).predict(future)

fig= pro_change.plot(forecast);

a = add_changepoints_to_plot(fig.gca(), pro_change, forecast)

Adding Holidays

Holidays and events can cause changes to a time series. In our example the National Avocado day on July 31 and Guacamole day on September 16 can impact prices of the Avocado.

We can create a custom holiday list for Prophet by creating a dataframe with two columns ds and holiday. A row for each occurrence of the holiday

avocado_season = pd.DataFrame({

'holiday': 'avocado season',

'ds': pd.to_datetime(['2014-07-31', '2014-09-16',

'2015-07-31', '2015-09-16',

'2016-07-31', '2016-09-16',

'2017-07-31', '2017-09-16',

'2018-07-31', '2018-09-16',

'2019-07-31', '2019-09-16']),

'lower_window': -1,

'upper_window': 0,

})

lower window and upper window extend holiday to days around the date. If we want to include a day prior to the national avocado day and Guacamole day, we set lower_window: -1 upper_window: 0

If we wanted to use the day after the holiday then set lower_window: 0 upper_window: 1

pro_holiday= Prophet(holidays=avocado_season)

pro_holiday.fit(train_dataset)

future_data = pro_holiday.make_future_dataframe(periods=12, freq = 'm')



#forecast the data for future data forecast_data = pro_holiday.predict(future_data)

pro_holiday.plot(forecast_data);

Adding Multiple Regressors

Additional regressors can be added to the Prophet model. This is done by using add_regressor. Additional regressor column value needs to be present in both the fitting as well as prediction dataframes.

Creating fitting and predicting dataset with additional regressors

train_dataset[‘type’] = X[‘type’]

train_dataset[‘Total Volume’] = X[‘Total Volume’]

train_dataset[‘4046’] = X[‘4046’]

train_dataset[‘4225’] = X[‘4225’]

train_dataset[‘4770’] = X[‘4770’]

train_dataset[‘Small Bags’] = X[‘Small Bags’] train_X= train_dataset[:18000]

test_X= train_dataset[18000:]

we split our data set to demonstrate the use of the additional regressor as we need all the additional regressors values in both fitting and predicting data frames

#Additional Regressor

pro_regressor= Prophet()

pro_regressor.add_regressor('type')

pro_regressor.add_regressor('Total Volume')

pro_regressor.add_regressor('4046')

pro_regressor.add_regressor('4225')

pro_regressor.add_regressor('4770')

pro_regressor.add_regressor('Small Bags') #Fitting the data

pro_regressor.fit(train_X)

future_data = pro_regressor.make_future_dataframe(periods=249) #forecast the data for Test data

forecast_data = pro_regressor.predict(test_X)

pro_regressor.plot(forecast_data);

Prediction using additional regressor

Predicted data is the blue shaded region at the end.

Jupyter notebook available here

References:

https://facebook.github.io/prophet/docs/quick_start.html

https://peerj.com/preprints/3190.pdf",https://towardsdatascience.com/time-series-prediction-using-prophet-in-python-35d65f626236,['Renu Khandelwal'],2019-11-17 11:59:06.161000+00:00,1023,"facebook prophet, time series model, Prophet, forecasting, change points"
Learning from Audio: Fourier Transformations,"Learning from Audio: Fourier Transformations

Breaking down a fundamental equation in signal processing

Related article:

Introduction:

In Wave Forms, we looked at what waves are, how to visualize them, and how to deal with null data.

In this article, I aim to develop an intuition on what the Fourier Transformation is, why it is useful when studying audio, show mathematical proofs to make it computationally efficient, and visualize the results. The data we are working with in this (and related) articles can be found on my GitHub repository for this series.

With this in mind, let’s begin. We will first initialize our necessary variables and packages below:

Recall from Wave Forms:",https://towardsdatascience.com/learning-from-audio-fourier-transformations-f000124675ee,['Adam Sabra'],2020-10-08 21:11:50.034000+00:00,104,"Fourier Transformation, Signal Processing, Audio Analysis, Wave Forms, Mathematical Proofs"
DS Interview Questions S01E01,"What is the difference between a model parameter and a model hyperparameter?

Model Parameters are configuration variables which are internal to the model and whose values can be estimated from the data. These are required by the model to make prediction/ classification and cannot be set manually by the user.

For example: coefficients in a linear regression model

Often model parameters are estimated using an optimization algorithm, which is a type of efficient search through possible parameter values.

Model Hyperparameters are the configuration variables which are external to the model and whose values cannot be estimated from the data. These are usually tuned(set) by the users in order to alter the model performance. for example: k-value in K-means clustering,

We cannot know the best value for a model hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error.

Note: Many models have important parameters which cannot be directly estimated from the data. For example, in the K-nearest neighbour classification model … This type of model parameter is referred to as a tuning parameter because there is no analytical formula available to calculate an appropriate value. Thumb Rule: If you have to specify a model parameter manually then it is probably a model hyperparameter.

In summary, model parameters are estimated from data automatically and model hyperparameters are set manually and are used in processes to help estimate model parameters.

Reference:",https://medium.com/analytics-vidhya/ds-interview-questions-s01e01-6feed53b7de0,['Neha Mangal'],2020-09-02 18:37:37.882000+00:00,238,"model parameters, model hyperparameters, linear regression, optimization algorithm, K-means clustering"
Helping Guests Make Informed Decisions with Market Insights,"Two common decisions that our guests are making are:

Should I book now for better availability or later for better flexibility? Which of the listings should I book?

As the service provider, we have broad views of the entire market and guest behaviors that individual guests do not necessarily have. This information usually provides helpful insights to solve guests’ puzzles.

Market insights are one channel where we interact with our guests at various stages of the booking flow. We provide dynamically-generated information to assist our guests in planning their trips. This information includes market and listing availability trends, supply, pricing discounts, community activities, etc. It is a critical component of the booking flow and has demonstrated its utility to the Airbnb community by enabling a larger variety of people to make wiser booking decisions and belong everywhere.

Market Insights

Figure 1 illustrates the architecture of our Market Insight service. As a guest interacts with the website, the Market Insight backend system talks with the Search and Pricing services to collect market availability and pricing information. It queries the key-value store for data that are relevant to the search or listing view—along with user information—to generate candidate insights in real-time. Then it ranks the insights according to their values to the guest and powers the front-end on the final insights to display.

We are determined to generate market insights that are genuine, informational, and timely.

When a guest types in a search query that consists of location, dates, guest counts, and possibly room type and additional amenity constraints, the Search backend system retrieves available homes. The frontend automatically zooms in the map to a level that best covers the guest’s interests with enough context. For one such map view, the market insights server aggregates the exact number of available places, and warns the guest if the number is low. Similarly, we have an insight on the percentage of available places.

To support heterogeneous types of insights, the server retrieves two major sources of data from the key-value store.

Stream data that are mostly factual information with limited counting and bookkeeping that are almost real-time, such as the number of unique views of a listing during the past N days. The data is served through an internal system. Aggregated data that typically requires joining with and inferencing from other data sources, and they typically have up to a couple of days of delay. We build data pipelines using Airflow to streamline the data generation process and monitor their daily progress. We use Spark for large-scale data aggregation and use Hive for data storage. The “Rare Find” insight is an example of using aggregated data.

As the service platform, we have more data than individual guests. For instance, Airbnb keeps track of how frequently homes are booked. This information is a good indication of popularity. When a guest views a place that is rarely vacant, we remind our guests with the following insight.

This insight is supported by two data pipelines — one that aggregates availability information of an individual listing and the other for the availability of all markets. A “Rare Find” insight is for listings that have a high long-term availability ratio compared against the market X percentile — a value that trades off insight value and scarcity that we determined by live experiments. Both of the data pipelines are updated on a daily basis so our server will deliver accurate and timely insight to our guests.

Since Market Insights’ inception in 2015, we have gradually added more insight types. Our work has increased booking conversion by more than 5%. With a large Airbnb community and our extension to more verticals, our work compounds in a substantial way for company growth.

Personalization

Personalization has been an evolving theme for many service platforms, and Airbnb has been a pioneer in adopting new technology, applying machine learning to personalize search results and detect host preferences. We have taken several steps in personalizing market insights.

It happens quite often that multiple insights are eligible, but we are only able to show one each time. Our current strategy is to use a deterministic and static vetting rule. However, guests parse information differently. For a guest who is sensitive to time, an insight reminding her can be very effective in getting a trip booked soon. Yet, for a last-minute traveller, the number or percentage of search results may sound more informative, providing them with signals to book as availability is running low. On a listing detail page, the mentality of a listing is “usually booked” may have different implications than “10 others are looking at this place” for various people. Not to mention that there may be sophisticated guests who would like to make decisions purely based on their chemistry with the listings, thus preferring no market insights at all.

In 2016, we have added extensive logging in our booking flow, about which types of insight guests see, and how they react when seeing these insights, such as how much longer they spend on a listing page, whether they wishlist a listing, make a booking request, or go back to search. We implemented a couple of randomization strategies, equalizing the odds of impression for every eligible insight. Showing different and increased variety of insights helps us acquire data to understand user preferences.

After collecting user interaction data, we join it with listing information, such as occupancy rate and number of views, along with search parameters, such as trip lead days and length, and perform data analysis. Our goal is to learn smart insight vetting rules that maximize desired outcome. We believe guests are more likely to book with advanced user experience, so we created a utility function that evaluates their progress. For example, requesting to book is worth one point and contacting host is worth half a point, etc.

We segment guests based on guest features, such as the number of searches and bookings they have done in the past, and come up with a insight vetting rule for each user segment. We are experimenting on our hypothesis that personalized insights deliver improved user experience and in return improves booking conversion.",https://medium.com/airbnb-engineering/helping-guests-make-informed-decisions-with-market-insights-8b09dc904353,['Peng Dai'],2017-07-11 18:48:10.371000+00:00,1007,"Market Insights, Personalization, Availability Trends, Pricing Discounts, Community Activities"
Increase Your Developer Confidence With a Great Django Test Suite,"Increase Your Developer Confidence With a Great Django Test Suite

How to write tests for your Django applications that boost your team and are actually useful

Cartoon by the author. It’s just soda in the test tube, but I’m not taking chances with that beaker.

If you regard writing tests as a lame checkbox task, nothing could be farther from the truth. Done correctly, tests are one of your application’s most valuable assets.

The Django framework, in particular, offers your team the opportunity to create an efficient testing practice based on the Python standard library unittest . Proper tests in Django are fast to write, faster to run, and can offer you a seamless continuous integration solution for taking the pulse of your developing application.

With comprehensive tests, developers have higher confidence when pushing changes. I’ve seen firsthand in my own teams that good tests can boost development velocity as a direct result of a better developer experience.

In this article, I’ll share my own experiences in building useful tests for Django applications, from the basics to the best possible execution. If you’re using Django or building with it in your organization, you might like to read my Django series on Victoria.dev.",https://medium.com/better-programming/increase-your-developer-confidence-with-a-great-django-test-suite-e1d8e7df8c90,['Victoria Drake'],2020-10-18 20:09:41.827000+00:00,196,"Django, Python, Unit Testing, Continuous Integration, Software Development"
Understanding ETL Pipeline,"Example

Imagine that there is a database that contains web log data. Each entry in the data contains IP address of a user, timestamp, and the link clicked by user.

Think of a scenario where you want to run an analysis of links clicked by city and by day. You would need another data set that maps IP address to a city, and you would also need to extract the day from the timestamp.

With an ETL pipeline, you could run code once per day that would extract the previous day’s log data, map IP address to city, aggregate link clicks by city, and then load these results into a new database. That way, a data analyst or scientist would have access to a table of log data by city and day. That is more convenient than always having to run the same complex data transformations on the raw web log data.

Using ETL pipeline makes sense because, before the usage of cloud computing, companies and businesses used to store their data on private servers that were expensive and running queries on huge datasets could be very expensive in terms of time and economy.",https://medium.com/analytics-vidhya/understanding-etl-pipeline-76718d299a08,['Chaitanya Krishna Kasaraneni'],2020-05-01 17:43:21.689000+00:00,190,"ETL, Cloud Computing, Data Analysis, Data Transformations, Database Queries"
Evolution of Style Transfer techniques,"What is style transfer?

Neural Style Transfer refers to a class of software algorithms that manipulate digital images, or videos, in order to adapt the appearance or visual style of another image. Neural Style Transfer algorithms are characterized by their use of deep neural networks for the sake of image transformation. Simply put output image retains the core elements of the content image but appears to be painted in the style of the style reference image.

The Deep Learning magic

Our starting point was the original algorithm for neural style transfer by Gatys et al (2015) based on optimizing an image to match the content and style of another image by manipulating some clever losses.

In summary, the general idea is:

Take an image for style ( style_img )

) Take an image for content ( content_img )

) Take a pre-trained neural net

Create an “output” image where each pixel is a parameter to optimize

Optimize the “output” image so that the content loss and style loss are minimized

Networks

Convolutional neural network

In deep learning, a convolutional neural network is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks, based on their shared-weights architecture and translation invariance characteristics. Convolution neural networks work well in image feature representation tasks, where content and style features can be separated in the same network because lower-level layers are more concerned with pixel values and image textures and higher-level layer features are more abstract. So we will be using the VGG network for feature representation purposes. Where lower-level layer features are used as a style feature representation and higher-level layer features as a content feature representation.

VGG 16 Network

The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is an annual computer vision competition. Each year, teams compete on two tasks. The first is to detect objects within an image coming 200 classes, which is called object localization. The second is to classify images, each labeled with one of 1000 categories, which is called image classification. VGG 16 was proposed by Karen Simonyan and Andrew Zisserman of the Visual Geometry Group Lab of Oxford University in 2014 in the paper “VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION”. This model won 1st and 2nd place in the above categories in the 2014 ILSVRC challenge.

VGG-16

VGG 19 Network

VGG-19 is a convolutional neural network that is 19 layers deep. You can load a pre-trained version of the network trained on more than a million images from the ImageNet database. The pre-trained network can classify images into 1000 object categories, such as a keyboard, mouse, pencil, and many animals. As a result, the network has learned rich feature representations for a wide range of images. The network has an image input size of 224-by-224.

VGG-19

Cyclic GAN

The CycleGAN is a technique that involves the automatic training of image-to-image translation models without paired examples. The models are trained in an unsupervised manner using a collection of images from the source and target domain that do not need to be related in any way.

Cycle GAN

Losses

Content Loss

The content loss is calculated using the target and content images. First, we select one of the layers of our model. Then we feedforward the content image through the pre-trained model and get the feature maps produced by the selected layer. After that, we feedforward the target image and get the feature maps produced by the selected layer.

Then we calculate the loss using these feature maps (content feature maps and target feature maps). For example, we can use the mean squared distance between the content feature maps and the target feature maps as a loss function.

Style Loss

In order to calculate the style loss, we need to define a style representation for the images. In the paper, the style representation is defined as the Gram matrix of the feature maps. The Gram matrix is a measure of a statistic in the features. It measures the correlation between the feature maps.

The style loss is calculated using the style representations (Gram matrix) for the style and target images. In the paper, the mean squared distance is used as a style loss. So the style loss will be the mean squared distance between the two Gram matrices.

Total Loss = content weight * content loss + style weight * style loss

By varying the values of content weight and style weight, we can control the amount of information retained from the content image and style image.

Image style transfer

To apply style transfer on the images, we load the weights of the pre-trained model, extract the content and style features and minimize the loss between the target image and content/style image where the Initial target image is used as a mixup of lower-level layer feature of style image and higher-level layer feature of content image. Before training, we set the weights of layers in decreasing order from lower to higher-level layers to calculate the loss. After training, when the model converges, it produces a style transferred output image.

The above strategy works well when we want to impose the style over the entire content image. To target a specific area in the content image, the Cycle GAN method is used.

Image Style Transfer",https://anmol19005.medium.com/evolution-of-style-transfer-techniques-1a892e796475,[],2020-12-20 12:26:17.146000+00:00,859,"Neural Style Transfer, Convolutional Neural Network, VGG 16 Network, VGG 19 Network"
20 Machine Learning Projects on NLP,"Natural language processing (NLP) is a widely discussed and studied subject these days. NLP, one of the oldest areas of machine learning research, is used in major fields such as machine translation speech recognition and word processing. In this article, I’ll walk you through 20 Machine Learning projects on NLP solved and explained with the Python programming language.

20 Machine Learning Projects on NLP

Hope you liked this article on 20 Machine Learning Projects On NLP Or Natural Language Processing With Python Programming Language. Please feel free to ask your valuable questions in the comments section below.",https://medium.com/coders-camp/20-machine-learning-projects-on-nlp-582effe73b9c,['Aman Kharwal'],2020-12-08 17:59:22.082000+00:00,95,"NLP, Machine Learning, Python Programming, Natural Language Processing, MLProjects"
Latest picks: In case you missed them:,"Sign up for The Variable

By Towards Data Science

Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.",https://towardsdatascience.com/latest-picks-why-how-to-use-the-naive-bayes-algorithms-in-a-regulated-industry-with-sklearn-51c11c87300a,['Tds Editors'],2020-12-02 14:28:15.196000+00:00,36,"data Science, Towards Data Science, subscription, newsletter, The Variable"
"DSN Bookmarks: Action Research, AI Ethics, and Social Media Data","Welcome to Data Stewards Network (DSN) Bookmarks — a biweekly curation of news, research, and other insights related to the systemic, sustainable, and responsible management of data for public benefit. If you would like to receive our resource curation in your inbox every week, subscribe here. Have an interesting article, report, or initiative worth sharing? Send it to us at datastewards@thegovlab.org

Photo by Yiran Ding on Unsplash

Creating Societal Value through Data Collaboration

Geoff Boeing et al shared a paper at SSRN titled, Housing Search in the Age of Big Data: Smarter Cities or the Same Old Blind Spots?, which investigates if “technology platforms serve as information equalizers or do they reflect traditional information inequalities that correlate with neighborhood sociodemographics.”

Sushant Kumar shares The many perks of using critical consumer user data for social benefit in LiveMint, advocating for new data stewardship functions to help create public benefits from data and data science tools employed by private firms.

New privacy-protected Facebook data for independent research on social media’s impact on democracy, a piece from Chaya Nayak at Facebook announced new data offerings for academic researchers collaborating with the company.

In Twitter might have a better read on floods than NOAA for The Verge, Justine Calma interviews Frances Moore, lead author of the new study and a professor at the University of California, Davis on how Twitter data can inform flooding response and management.

Ethical AI

In Project Syndicate, Stefaan G. Verhulst and Mona Sloan’s Realizing the Potential of AI Localism argues, “With national innovation strategies focused primarily on achieving dominance in artificial intelligence, the problem of actually regulating AI applications has received less attention. Fortunately, cities and other local jurisdictions are picking up the baton and conducting policy experiments that will yield lessons for everyone.”

Jessica Fjeld et al, at the Berkman Klein Center for Internet & Society, published Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights- based Approaches to Principles for AI. The authors “analyzed the contents of thirty-six prominent AI principles documents, and in the process, discovered thematic trends that suggest the earliest emergence of sectoral norms.”

Data-Driven Innovation

Stefaan G. Verhulst published Re-imagining “Action Research” as a Tool for Social Innovation and Public Entrepreneurship. Seeking to “provide for what Amar Bhide calls “practical knowledge” at all levels of decision making in a systematic, sustainable, and responsible manner,” the author explains what action research means and what it can offer.

Megumi Kubota and Albert G. Zeufack, at The World Bank published a policy research working paper titled Assessing the Returns on Investment in Data Openness and Transparency. “This paper investigates the potential benefits for a country from investing in data transparency. The paper shows that increased data transparency can bring substantive returns in lower costs of external borrowing. This result is obtained by estimating the impact of public data transparency on sovereign spreads conditional on the country’s level of institutional quality and public and external debt.”

You can also find additional resources related to data stewardship and data collaboration here.",https://medium.com/data-stewards-network/dsn-bookmarks-action-research-ai-ethics-and-social-media-data-5b21a8b715c0,['Michelle Winowatan'],2020-02-20 23:04:37.333000+00:00,490,"data stewardship, data collaboration, ethical AI, data-driven innovation, public benefit"
"When data speaks, and no one listens","Image by Author using Canva

Intuition vs information

We’ve all been there; you take the spec for the information required from management, you go back to your (data/BI) team, discuss it, they do their magic and create a report that holds key information of the direction that the company should take.

You then present the report back to management, and they ignore it.

It so happens that ‘someone’ has a ‘hunch’ different to what the report says and manages to sway the decision resulting in the company taking a different direction.

As a Data Lead, you have to go back to your team and somehow explain why that report you asked them for, the one they probably had to drop everything to finish, has been put aside and why management has opted to go with Joe Bloggs’s hunch rather than factual data.

Motivation in data teams is a hard one to maintain.

To work in data today, one needs a specific mindset — a resilience to withstand those who disregard authentic information and instead go with their gut feelings.

Data Leads are often stuck in between a rock and a hard place between their team and the rest of the company. They need to convince their teams that their work is valued, and at the same time, minimize the damage from those who insist on feeding their own egos and overturning data-driven decisions with random intuition decisions.

The solution is not simple but can start with three steps.

Forever striving for more data-centric decisions, I’m always looking for ways to streamline the understanding of data so that everyone understands it and can relate what they're doing on a day-to-day basis to whether or not it’s impacting the growth of the company.

Is the confusion and discount of data due to the fact we have such a vast array of metrics that we find it hard to make sense of it all?

Image by Author

As a result of so many metrics, we often fail to pin down the one main metric, also known as the north star. If we do manage — there seems to be a common problem in that we lack in communicating it effectively as well as relating it to everyone's activities, efforts and intentions.

There are many pieces to this puzzle, and it’s not an easy one to solve. I have, however, tried and tested and identified that there are (amongst others) initially three steps that can help to start to embed data into decision making.

There should be ONE main metric (North Star Metric) that everyone knows about and whether they agree upon it or not whoever has the right authority should be able to declare it and ward off any pursuits to change it- not saying it should never be changed or revised. Still, it shouldn't be a regular practise. The main metric should be broken down into a minimal amount of KPIs (I suggest around 6) that relate directly to customer/prospect/user journey. In addition to the above (point 2) break down the 6 KPIs to the team level and again down to the individual level, this results in a correlation of what they are doing. That way, both teams and individuals within the teams work with a specific intention.

Put it all together, measure and optimise.

The solution could be the consolidation of what the team does (as in activities, i.e. feature development, sales and marketing campaigns etc.), with what the customer is doing (user/customer journey and behaviour). With that in place, you can easily measure if what is being done is, in fact, making an impact on the KPIs, which in turn is tied to the main metric (North Star).

Conclusion.

Once this fusion takes place, you can then measure what is working and what isn’t, which enables you to tweak and try again and once you’ve found what it is that is impacting growth, you can optimize and build on that.

This then becomes part of a framework that speaks a language that the whole team can participate in, as wells as understand to achieve the same goal.

The clear outcomes should be able to convince even the harshest sceptics and so-called hunch believers, letting the data speak for itself.",https://towardsdatascience.com/when-data-speaks-and-no-one-listens-a754157cdb48,['Rosi Bremec'],2021-01-28 14:37:59.494000+00:00,688,"Data, Intuition, North Star Metric, KPIs, Metrics Optimization"
3-D Brain Tumor Segmentation ERROR,"Hi all,

I’m getting the following error running the 3-D Brain Tumor Segmentation example

(https://www.mathworks.com/help/images/segment-3d-brain-tumor-using-deep-learning.html)

Error using trainNetwork (line 165)

Incorrect loss type returned by ‘forwardLoss’ in the output layer. Expected to be ‘single’, but instead was ‘gpuArray’.

The output layer is dicePixelClassification3dLayer. It seems like there’s some kind of error in the definition of that custom layer. Converting to single the outputs of the layer methods didn’t help, same error.

If it’s helpful I’m using r2019a on windows 10 on a i9 9900k + 1080ti.

ANSWER

Matlabsolutions.com provide latest MatLab Homework Help,MatLab Assignment Help for students, engineers and researchers in Multiple Branches like ECE, EEE, CSE, Mechanical, Civil with 100% output.Matlab Code for B.E, B.Tech,M.E,M.Tech, Ph.D. Scholars with 100% privacy guaranteed. Get MATLAB projects with source code for your learning and research.

Hi,

The issue exposed here is related to low GPU memory. That being said, the following error should not be thrown. I have heard that this issue is known, and the concerned parties may be investigating further.

'Conversion to single from gpuArray is not possible'

A possible Workaround:

The attached patch will fix the error mentioned above, in the sense that the above error message will not be thrown if the training process errors out due to low GPU memory. Follow the steps below to apply the patch.

1) Save the attached zip file to your $MATLABROOT folder. The $MATLABROOT folder can be determined by running the ‘matlabroot’ command.

2) Using MATLAB, navigate to your $MATLABROOT folder by executing the command:

cd(matlabroot) in the MATLAB Command Window.

3) Execute the following command to unzip the file:

SEE COMPLETE ANSWER CLICK THE LINK",https://medium.com/@technicalsource9/3-d-brain-tumor-segmentation-error-130e29eb7fc3,['Technical Source'],2021-06-24 06:10:13.515000+00:00,257,"Matlab Homework Help, Matlab Assignment Help, Matlab Code, MATLAB Projects, GPU Memory Error"
Stacked Bar Charts with Python’s Matplotlib,"As expected, the chart is hard to read. Let’s try the stacked bar chart and add a few adjustments.

First, we can sort the values before plotting, giving us a better sense of order and making it easier to compare the bars. We’ll do so with the ‘Global Sales’ column since it has the total.

## sort values

df_grouped = df_grouped.sort_values('Global_Sales')

df_grouped

Some of the records on the data frame — Image by Author

Earlier, to build a clustered bar chart, we used a plot for each region where the width parameter and adjustments in the x-axis helped us fit each platform's four areas.

Similarly, for plotting stack bar charts, we’ll use a plot for each region. This time we’ll use the bottom/left parameter to tell Matplotlib what comes before the bars we’re drawing.

plt.bar([1,2,3,4], [10,30,20,5])

plt.bar([1,2,3,4], [3,4,5,6], bottom = [10,30,20,5])

plt.show() plt.barh([1,2,3,4], [10,30,20,5])

plt.barh([1,2,3,4], [3,4,5,6], left = [10,30,20,5])

plt.show()

Stacked Bar Charts (Vertical/ Horizontal) — Image by Author

Cool. We can use a loop to plot the bars, passing a list of zeros for the ‘bottom’ parameter in the first set and accumulating the following values for the next regions.

fields = ['NA_Sales','EU_Sales','JP_Sales','Other_Sales']

colors = ['#1D2F6F', '#8390FA', '#6EAF46', '#FAC748']

labels = ['NA', 'EU', 'JP', 'Others'] # figure and axis

fig, ax = plt.subplots(1, figsize=(12, 10)) # plot bars

left = len(df_grouped) * [0]

for idx, name in enumerate(fields):

plt.barh(df_grouped.index, df_grouped[name], left = left, color=colors[idx])

left = left + df_grouped[name] # title, legend, labels

plt.title('Video Game Sales By Platform and Region

', loc='left')

plt.legend(labels, bbox_to_anchor=([0.55, 1, 0, 0]), ncol=4, frameon=False)

plt.xlabel('Millions of copies of all games') # remove spines

ax.spines['right'].set_visible(False)

ax.spines['left'].set_visible(False)

ax.spines['top'].set_visible(False)

ax.spines['bottom'].set_visible(False) # adjust limits and draw grid lines

plt.ylim(-0.5, ax.get_yticks()[-1] + 0.5)

ax.set_axisbelow(True)

ax.xaxis.grid(color='gray', linestyle='dashed') plt.show()

Stacked Bar Chart — Image by Author

Great, this is way more readable than the last one.

It’s important to remember the purpose of this chart before trying to extract any insights. The idea here is to compare the platforms' total sales and understand each platform's composition.

Comparing totals across fields and comparing regions inside one bar is ok. Comparing regions from different bars, on the other hand, can be very misleading.

In this case, we can compare the NA region across the bars since it has the same starting point for every bar, but it isn't so easy to compare the others. Take the X360, for example, it has a lower value for JP than the PS2, but it’s hard to compare if the Others value is higher or lower than the Wii.

Comparable value — Image by Author

Uncomparable value — Image by Author

Suppose we change the stack's order, with Other Sales as the first bar, and sort the records by Other Sales. It should be easier to tell which is more significant.

## sort values

df_grouped = df_grouped.sort_values('Other_Sales') fields = ['Other_Sales', 'NA_Sales','EU_Sales','JP_Sales']

colors = ['#1D2F6F', '#8390FA', '#6EAF46', '#FAC748']

labels = ['Others', 'NA', 'EU', 'JP']

Stacked Bar Chart, emphasizing the Others category — Image by Author

There are two essential elements in this visualization, the order of the categories in the stack of bars and the rows' order.

If we want to emphasize one region, we can sort the records with the chosen field and use it as the left-most bar.

If we don’t, we can sort the records by the total and order the stacks with the categories that have higher values first.",https://towardsdatascience.com/stacked-bar-charts-with-pythons-matplotlib-f4020e4eb4a7,['Thiago Carvalho'],2020-11-23 17:02:36.498000+00:00,516,"sorting, stacked bar chart, visualization, dataframe, Matplotlib"
"Airflow, Spark & S3, stitching it all together","As a first step, we will setup a MySQL RDS instance in AWS

in AWS, select RDS as the service and then click on ‘Create Database’ from the next page. Then follow the screenshots below for choosing the remaining options

make a note of the credentials that you provided here, as these will be needed while setting up airflow. Leave all the other options as-is until you arrive at the screen (by scrolling down) below

Note that, I have chosen the EMR Master’s security group as the VPC security group along with the default. This is essential to allow the communication between the edge node (where airflow will be setup) and the RDS

expand the ‘Additional configuration’ and ensure that the port is 3306. Scroll down further and expand the next ‘Additional Configuration’. Enter the database name of your choice and make a note of this name, it will also be needed during the airflow setup

Leave everything else as is and then hit ‘Create database’. Stay calm while AWS takes its own time to set up the MySQL database

Once the database is up and running, click on the DB Identifier

In the next screen, make a note of the End Point , this will be the database host

Next scroll down to see the security groups and click on the default one. We will have to allow inbound traffic from the edge node to this database

Hit Edit Inbound Rules and then add the following (the public IP of the edge node should go into the blank space)

Hit Save rules

Let us now test if we can actually connect to the database from the edge node. Perform the following steps to check this

SSH into the edge node (duh! obvious), then run the below command

mysql -h < > -u < > -p < > (then enter password on prompt)

You should be able to see the mysql prompt now, run the below command in the mysql prompt. This will ensure that airflow gets the necessary character support during the metadata setup (not running this will make airflow db initialisation fail)

alter database <<the database name that you gave in the RDS setup>> CHARACTER SET utf8 COLLATE utf8_unicode_ci;

Now your database is all set to support airflow installation and subsequent job execution.",https://medium.com/analytics-vidhya/airflow-spark-s3-stitching-it-all-together-1acbfba67e33,['Suman Kumar Gangopadhyay'],2021-04-22 15:59:24.644000+00:00,369,"AWS, RDS, MySQL, EMR, Airflow"
Agile Software Development — Adaptability vs Predictability,"Agile is a heavily practiced project management approach in the software industry which enables better change management and flexibility through inspection and adaptation. It’s a philosophy or a way of working which tries to be aligned with customer needs and organizational goals through self-organizing teams and well-planned iterations. ‘Adaptability’ is a term that has a strong relationship with the Agile way of working and it is the foundation to the success of methodologies like SCRUM. In contrast, ‘big design up front’ models such as Waterfall are known to be predictive. But this predictability comes up with a trade-off — that is the resistance to change.

With the personal experience I possess after working in a couple of Agile projects, I have witnessed that still problems exist despite of their adaptive nature.

Estimates are not accurate

Sprint Velocity is not so good

Finally release delays can occur

With a little touch of predictions most of the above problems can be addressed or rather rectified. If predictability can be embedded into adaptive methodologies like Agile SCRUM which address most of the limitations of traditional sequential models, then that should be of utmost importance. BUT HOW?

The solution is ‘Data Science’.

Data science has the magic power of predicting the future glancing at the past. This same power can be used to predict things that can go wrong, such as estimations and velocity which highly depend on human intervention and heuristics. If there’s a mechanism which can predict the completed velocity of a sprint beforehand, delivery outcomes can be properly organized without waiting till the end.

Data science for agile is a comparatively under focused area with a huge potential. There are many things we can predict such as estimations and velocity. As there are so many agile practitioners and agile projects worldwide, obviously this can be treated as a big data problem.

As a part of my data science course work, I tried applying a few machine learning models on a huge agile data set which was prepared by a set of scholars and made publicly available. This data set contained data queried using JIRA API from a set of projects (Apache, JBoss, MongoDB, Spring) which use JIRA as their project management tool.

Normally in the beginning of an iteration, the team commits to a set of story points. The expectation is these story points will be completed by the end of the iteration. But that never happens. There is always a difference between committed and completed workload, which indicates the health of the iteration. Learning these trends earlier without waiting till the end can be useful in determining corrective actions. So, my attempt was to fit a few machine learning models on the above-mentioned data set and find the capability of predicting the velocity difference at a given point of time of the iteration.

I tried out fitting three machine learning models with RStudio.

1. Random Forest

2. A Gradient Boosting Machine

3. A Neural Network

These models revealed some surprising information and made me realized that there are so many things we can do to accelerate the delivery outcomes of agile with data science.

Correlation plot of the considered features that contribute to the velocity

Think about a JIRA plugin which gives early indications using these models and predict how the completed velocity is going to be before the end of the iteration and help us identify blockers that can ultimately lead to delivery delays and cost overruns.

Predicted vs. Actual Velocity Differences

In an era where data science has become a catchword and practitioners are re-purposing data to derive many unknown insights and predictions, why can’t we do some analysis on our own project data to come up with intelligent tools that will empower the project itself with predictive capabilities. Rather than depending on loud voices and opinions, a well pruned model may give more insights on planning and estimations.",https://medium.com/@prasadiapsara.gl/agile-software-development-adaptability-vs-predictability-729abfcc18f6,['Prasadi Abeywardana'],2020-05-17 12:20:08.348000+00:00,632,"Agile, SCRUM, Data Science, Big Design Up Front, Change Management"
How to Use Scala Pattern Matching,"Matching with Case Classes

Besides matching against the value of an object itself, we can also match against possible types (or case class ).

Let’s say we are writing a classification program for a computer scanner at a fresh produce department of a supermarket. The scanner will label an item based on its type, e.g. fruit or vegetable. I’d imagine this is how we would define our trait and case classes.

trait GroceryItem



case class Fruit(name: String) extends GroceryItem



case class Vegetable(name: String) extends GroceryItem

Now, we are going to write a function that will take a GroceryItem object as its input and classify whether it is a Fruit or a Vegetable . This is how it would be written in its simplest form.

def classifyGroceryItem(item: GroceryItem): Unit =

item match {

case _: Fruit => println(""label item as fruit."")

case _: Vegetable => println(""label item as vegetable."")

case _ => println(""unable to label the item. this seems to be an item for other department."")

}

Notice the syntax _: Fruit . This is how we should write our case when we want to pattern match against its instance type. Additionally, this expression does not actually look into the value of the case class ’ field (e.g. name ).

If we want to also match against the field of the case class, this is how we can do it (look at the first case expression).

def classifyGroceryItem(item: GroceryItem): Unit =

item match {

case Fruit(name) if name == ""apple"" => println(""item is a fruit and it's an apple."")

case _: Fruit => println(""label item as fruit."")

case _: Vegetable => println(""label item as vegetable."")

case _ => println(""unable to label the item. this seems to be an item for other department."")

}

Please note that the order of the case expressions matters. In the above case, if the additional case Fruit(name) expression is placed after case _: Fruit , the code will never ever reach it as it will match the case _: Fruit right away.",https://towardsdatascience.com/how-to-use-scalas-pattern-matching-362a01aa32ca,[],2020-05-08 23:31:55.046000+00:00,314,"Pattern Matching, Case Classes, Grocery Item, Fruit, Vegetable"
Classification. Building a full classification workflow,"Image by pixel2013 from Pixabay

Introduction

In this part of the series, we will put together everything we have learned to train a classification model. The objective is to learn how to build a complete classification workflow from the beginning to the end.

Problem Definition

The problem we are going to solve is the infamous Titanic Survival Problem. We are asked to build a machine learning model that takes passenger information and predict whether he/she survived or not. The dataset contains 12 columns described as follows: [download from here]

Preparing the Development Environment

You should be familiar with this step now. We will open a new Jyputer notebook, import and initialize findspark, create a spark session and finally load the data.

import findspark

findspark.init('/opt/spark')

from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

data = spark.read.csv('./datasets/titanic.csv', inferSchema=True, header=True)

Let us have a look at the data and its statistics:

Top 20 rows of the dataset

Statistics

Here is an example on how someone may select/update his features by analyzing the above tables:

It does not make sense to include some features such as: PassengerID, Name and Ticket → we will drop them

Cabin has a lot of null values → we will drop it as well

Maybe the Embarked column has nothing to do with the survival → let us remove it

We are missing 177 values from the Age column → Age is important, we need to find a way to deal with the missing values

Gender has nominal values → need to encode them

Let us filter out the unneeded columns:

data = data.select(['Survived', 'Pclass', 'Gender', 'Age', 'SibSp', 'Parch', 'Fare'])

Feature Transformation

We will deal with the transformations one by one. In a future article, I will discuss how to improve the process using pipelines. But let us do it the boring way first.

Calculating Age Missing Values

Age is an important feature; it is not wise to drop it because of some missing values. What we could do is to fill missing values with the help of existing ones. This process is called Data Imputation. There are many available strategies, but we will follow a simple one that fills missing values with the mean value calculated from the sample.

Spark ML makes the job easy using the Imputer class. First, we define the estimator, fit it to the model, then we apply the transformer on the data.

from pyspark.ml.feature import Imputer

imputer = Imputer(strategy='mean', inputCols=['Age'], outputCols=['AgeImputed'])

imputer_model = imputer.fit(data)

data = imputer_model.transform(data)

No more missing values! Let us continue to the next step…

Encoding Gender Values

We learned that machine learning algorithms cannot deal with categorical features. So, we need to index the Gender values:

from pyspark.ml.feature import StringIndexer

gender_indexer = StringIndexer(inputCol='Gender', outputCol='GenderIndexed')

gender_indexer_model = gender_indexer.fit(data)

data = gender_indexer_model.transform(data)

No more categorical values… Note that we do not need to one-hot-encode the indexed values, they are naturally binary encoded with 0 and 1 values.

Creating the Features Vector

We learned previously that Spark ML expects data to be represented in two columns: a features vector and a label column. We have the label column ready (Survived), so let us prepare the features vector.

Note that we add the AgeImputed and GenderIndexed instead of Age and Gender.

from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=['Pclass', 'SibSp', 'Parch', 'Fare', 'AgeImputed', 'GenderIndexed'], outputCol='features')

data = assembler.transform(data)

We are set to go! Time for machine learning…

Training the Model

We will use a Random Forest Classifier for this problem. You are free to choose any other classifier you see fit.

Steps:

Create an estimator Specify the name of the features column and the label column Fit the model

from pyspark.ml.classification import RandomForestClassifier

algo = RandomForestClassifier(featuresCol='features', labelCol='Survived')

model = algo.fit(data)

Done!

Generating Predictions

We call the model’s transform method to get our predictions:

predictions = model.transform(data)

Let us check the prediction values:

predictions.select(['Survived','prediction', 'probability']).show()

So far so good, but it is not logical to go through the records and verify them one by one. We need to calculate some metrics to get the overall performance of the model. Evaluation time…

Model Evaluation

We will use a BinaryClassificationEvaluator to evaluate our model. It needs to know the name of the label column and the metric name. Here we will use the area under ROC curve.

from pyspark.ml.evaluation import BinaryClassificationEvaluator

evaluator = BinaryClassificationEvaluator(labelCol='Survived', metricName='areaUnderROC')

Call the evaluate method to get the results:

evaluator.evaluate(predictions)

By using the above settings my evaluator returned: 0.90

Given that we did nothing to configure the hypreparatmers, the initial results are promising. I know that I did not evaluate it on a testing data, but I trust you can do it.

Model Evaluation with SciKit-Learn

If you want to generate other evaluations such as a confusion matrix or a classification report, you could always use the scikit-learn library.

You only need to extract y_true and y_pred from your DataFrame. Do not worry I will show you how:

y_true = predictions.select(['Survived']).collect()

y_pred = predictions.select(['prediction']).collect()

Import your metrics:

from sklearn.metrics import classification_report, confusion_matrix

Call the functions by passing y_true and y_pred:

print(classification_report(y_true, y_pred))

print(confusion_matrix(y_true, y_pred))

Final Thoughts

Congrats! You have successfully completed another tutorial. You should be more confident with your Spark ML skills now. In future tutorials, we are going to improve the preprocessing phase by using pipelines, and I will show you more exciting Spark ML features. Stay tuned…

If you enjoyed this article, I would appreciate it if you hit the clap button 👏 so it could be spread to others. You can also follow me on Twitter, Facebook, email me directly or find me on LinkedIn.",https://towardsdatascience.com/apache-spark-mllib-tutorial-part-3-complete-classification-workflow-a1eb430ad069,['Ali Masri'],2019-09-03 10:46:42.959000+00:00,844,"machine-learning, spark-ml, data-preprocessing, titanic-survival, pyspark"
Want to become a Data Scientist ?,"So, read the interesting journeys of three successful data scientists to gain inspiration and lessons to excel in data science industry. ✌️

By : Fatemeh Renani ,Mohammad Mazraeh, Jaskaran Kaur Cheema

Infographic : Jaskaran Kaur Cheema

“Torture the data, it will confess to anything”-Ronald Coase

Due to the enormous generation of data, modern business marketplace is becoming a data driven environment. Decisions are made on the basis of facts, trends and analysis drawn from the data. Moreover, automation and Machine Learning are becoming core components of IT strategies. Therefore, the role of Data Scientists and Data Engineers is becoming increasing important.

In this blog, we have enumerated the journeys of three Data Scientists who have different educational backgrounds and career paths but have successfully curved a niche for themselves in the Data Science Industry.

We hope that their journeys will inspire you to excel in data science industry.

MANROOP KAUR, Data Engineer ICBC

Manroop Kaur, is a Data Engineer at ICBC Vancouver. She is a graduate of SFU’s Professional Master of Science in Computer Science program Specializing in Big Data.

Can you tell us about ICBC and your current role.

ICBC was built in order to provide basic insurance and managing claims which is the core component of the company. At present, the company is working on RAAP (Rate Affordability Action Plan (RAAP) project that will fundamentally change its business model to create a sustainable auto insurance system which would provide more affordable and fair rates for all. As a part of this project, I am working as a Data Engineer in Claims and Driver Licensing Teams in Information Management Department.

What convinced you to venture in to the Big Data field.

While working with Tech Mahindra, I heard about a project where data was being transferred from traditional database to Hadoop. This was the first time in my life I came across big data terminology and started exploring it by reading online articles. Since I already wanted to expand my education qualification, so I thought of venturing into this field. SFU’s Professional Master’s program was perfect fit so I applied and got accepted into it.

Can you describe your career journey after enrolling in Big Data program

While at SFU, I did my coop with WorkSafeBC. My work focused on Text analysis, doing advanced analytics and applying Machine learning algorithms. After that I applied at ICBC and it’s been a year of working as a Data Engineer with ICBC.

Any courses that you recommend to pursue to be successful in this program.

I believe that Big data program at SFU is structured so well that if you complete the assignments of Programming Lab 1 and 2 diligently, there is no requirement of any other course.

Can you describe any of your most interesting project.

I remember doing a project during internship of detecting the likelihood of claim to be fraudulent. We analyzed the claim data of past 5 years. Regular meetings with real field investigators were held to know about the red flags. Later, data was analyzed using those red flags. This project taught me that in academic setting we focus on obtaining high accuracy but sometimes in real life problems accuracy has different definition. So, the model that data science team was preparing would be termed successful if it was able to detect even 40 out of 500 claims to be fraud which are actually in real life.

Any interesting lesson that you learned after working in this field .

So, when I started learning about data science, I used to get very excited about applying ML algorithms to see the output of my model without spending much time on analyzing or cleaning the data . Later, I realized that data plays vital role and preparing it takes 90% of time but as performance of model depends upon the data being fed to it, preparation time is worth the effort.

How do you reflect on your decision of enrolling in this program.

I think decision of acquiring Master’s Degree in Big Data at SFU has proved to be worth my time and resources I invested in it. As it not only provided me the education in concurrent with the industry requirements but also has helped me securing a good job.

Any advice for people who wants to venture in this field.

I think focusing on one domain rather than doing everything in data science and updating your skills regularly will lead to a successful career.",https://medium.com/sfu-cspmp/want-to-become-a-data-scientist-ed309bdcc738,['Jaskaran Kaur Cheema'],2019-03-15 23:37:58.851000+00:00,723,"Data Science, Machine Learning, Big Data, Text Analysis, Data Engineer"
Go Heatmap Yourself,"Recently we introduced a heatmapping feature on Strava, our first feature built using the Go programming language. This is part of work to move Strava towards a service-oriented architecture with well-defined interfaces that can be written in any language.

The service has two main components: the front end host and heatmap generating workers. Our main code base interacts with the front end: requesting to build, view and update maps. The front end then delegates jobs to the workers using a Gearman queue. The workers aggregate the activity data, generate the tiles, and upload everything to Amazon S3. Metadata is saved to DynamoDB and tiles are served to the user via CloudFront.

Pretty simple architecture, so why use Go? Well, we wanted something with a few key features: a built-in webserver to accept API commands, easy tie-in with existing C code, and speed to eliminate any overhead from an inherently slow aggregation process. A prototype in Ruby was painful, so we looked elsewhere.

Go fit the bill with its net/http library, cgo and compiled binaries, but it wasn’t all roses and lollipops along the way. First, Go lacks mature, fully supported client libraries. There is code on Github for just about anything, but very little of it seems “production ready.” As a result, we spent a lot of time building and testing our own libraries.

Another issue we ran into was a performance hit from cgo, fortunately we found a workaround. While very easy to use, there is a slow context switch when switching between Go code and C code. For example, during the aggregation step the Go code feeds the lat/lng points to the C code using something like:

for _, latlng := range latlngs { C.hm_addLatLng(latlng[0], latlng[1]) }

This worked, but when we modified the code to something like:

for i := 0; i < len(latlngs); i+=10 { C.hm_addLatLngBulk(latlngs[i][0], latlngs[i][1], latlngs[i+1][0], latlngs[i+1][1], ..., latlngs[i+9][0], latlngs[i+9][1], ) }

we got a 30% speed increase over this data aggregation step. That means that at least 30% of the runtime was spent making the function call, not doing any work. Something to keep in mind for the future.

Personally, I’m a fan of Go and was happy to have used it on this project. I enjoy the C style and Go makes most of the mess of C easier with a built in string types, slices for growable arrays and garbage collection, all while compiling into a single binary. We still have more to learn about Go, and time will tell if it was indeed the right choice for this project.

And as always, if you’re interesting in working on cool athlete-related projects like this, check out the open engineering positions at Strava.",https://medium.com/strava-engineering/go-heatmap-yourself-43371c169b26,['Strava Engineering'],2017-05-09 17:44:28.789000+00:00,441,"Go Programming Language, Service-Oriented Architecture, Gearman Queue, Amazon S3, DynamoDB"
Predicting Startup Returns: How VCs Use AI to Make Investment Decisions and Identify Successful…,"Meet the AI Game Changer

One VC firm that has implemented data and technology as part of their investing strategy is Connetic Ventures, a VC firm that has developed and launched an AI platform named Wendal. Companies looking for funding from Connetic complete Wendal’s online assessment in less than 15 minutes, and the data gathered allows Connetic to make an extremely informed decision about the strength and viability of the potential deal. Over half of the algorithm is based on behavioral factors because Connetic believes that the personal characteristics of a startup team is a key indicator of their future success. Connetic says Wendal “automates the due diligence process for early-stage companies” and is “essentially 4–5 MBA educated analysts … [but] is self-learning and can recall over a billion data points on command.” Wendal’s exceptional attributes mainly aim to fix three common problems prevailing in the VC industry.

1. Instinctive Investing

We have all probably been there. Not having tried it before, we see food that looks or sounds good, and based solely on instinct, we decide to purchase it and give it a try. To our dismay, what we thought would be a delicious treat ends up giving us a repulsive taste in our mouth and maybe even a bad stomach ache. Similarly, many investment opportunities can sound exciting and enticing, and VC decision-makers may say that the deal “just feels right” to pursue. However, the investment may not be the right fit for a particular VC fund, may have lacking financials, or may not have ideal founders behind the scenes. Using AI allows VC firms like Connetic to get a true and clear picture of their potential investment without having to worry about unforeseeable factors that could turn an investment into a terrible stomach ache, especially when large amounts of money are on the line.

2. Investor Bias

Bias is prevalent everywhere there is human involvement, and this can be a big concern when relying on traditional in-person pitches for VC funding. Bias in the humas designing AI algorithms is another concern about how these algorithms make decisions. Across the board, VC fund managers tend to be male, with only 11% of VC decision-makers being female. Female entrepreneurs can be asked very different questions during VC pitches, and in 2019, only 2.8% of VC capital went to businesses founded or co-founded by women. This is a significant contrast to the fact that women-owned businesses represent 42% of all businesses in the U.S. With Wendal, Connetic is committed to increasing the fairness of the VC funding process and helping remove gender bias. Connetic notes that “Wendal does not know your age, race, or gender when you apply for funding which gives everyone an equal opportunity.” It makes sense for VC firms to escalate their focus on the gender gap because it has been proven that women-led teams generate a 35% higher return on investment than all-male teams. With the implementation of Wendal, Connetic Ventures’ funding for women and minorities has reached a rate that is 8x higher than the industry average.

3. Excessive Time to Analyze Potential Investments

With time being one of the most valuable resources in today’s age, AI can be a win-win for both the VC firm and the pitching company. Using Wendal, Connetic does not have to waste time listening to irrelevant pitches, and on the flip side, businesses do not have to spend hours preparing perfected presentations. The Wendal application takes less than 15 minutes to complete and quickly gives results to Connetic to help them decide if the company would be a good investment for them. Through Wendal, Connetic is able to sift through numerous potential portfolio companies and choose optimal businesses from an expanded investment pool.",https://medium.com/strategica/predicting-startup-returns-how-vcs-use-ai-to-make-investment-decisions-and-identify-successful-e67231859720,['Haley Samuelsen'],2020-06-15 04:13:33.905000+00:00,617,"AI, Investing, Data, Technology, VCFirm"
Apple Stock and Bitcoin Price Predictions Using FB’s Prophet -Using Python,"Apple Stock Price Prediction

First import all the libraries

2.Fetch the Apple Stock prices

We can use free API’s like

Alphavantage API Quandl API Pandas_datareader

I am using the Alphavantage API

Get the API Key

Alphavantage documentation

You can output the data to

CSV

Json

Pandas dataframe

Also, you can specify the output size

Compact — Returns last 100 days

— Returns last 100 days Full — returns last 20 years of data

The data frame looks like below

3. Write it to a file in case if you need it in the future.

4. Do some basic plotting

Closing Price Histogram

Technical Analysis :

SMA : Simple Moving Average

A simple moving average (SMA) is an arithmetic moving average calculated by adding recent closing prices and then dividing that by the number of time periods in the calculation average.

Relative Strength Index (RSI)

The Relative Strength Index (RSI), developed by J. Welles Wilder, is a momentum oscillator that measures the speed and change of price movements. The RSI oscillates between zero and 100. Traditionally the RSI is considered overbought when above 70 and oversold when below 30. Signals can be generated by looking for divergences and failure swings. RSI can also be used to identify the general trend.

5. Now convert the dataframe into fbprophet expected format. The input to Prophet is always a dataframe with two columns: ds and y . The ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. The y column must be numeric, and represents the measurement we wish to forecast.

6. Prophet follows the sklearn model API. We create an instance of the Prophet class and then call its fit and predict methods. We fit the model by instantiating a new Prophet object. Any settings to the forecasting procedure are passed into the constructor. Then you call its fit method and pass in the historical dataframe. Fitting should take 1-5 seconds.

7. Forecast for next 1 year. Predictions are then made on a dataframe with a column ds containing the dates for which a prediction is to be made. You can get a suitable dataframe that extends into the future a specified number of days using the helper method Prophet.make_future_dataframe . By default it will also include the dates from the history, so we will see the model fit as well.

The forecast df will contain the next one year dates. The last 10 records look like below

8. The predict method will assign each row in future a predicted value which it names yhat . If you pass in historical dates, it will provide an in-sample fit. The forecast object here is a new dataframe that includes a column yhat with the forecast, as well as columns for components and uncertainty intervals.

9. You can plot the forecast by calling the Prophet.plot method and passing in your forecast dataframe.

10. If you want to see the forecast components, you can use the Prophet.plot_components method. By default you'll see the trend, yearly seasonality, and weekly seasonality of the time series. If you include holidays, you'll see those here, too.

11. An interactive figure of the forecast can be created with plotly. You will need to install plotly separately, as it will not by default be installed with fbprophet.

12.Create the bitcoin dataset

No of records

13.Some basic data visualization

14. Now fit the model and do the forecast.The code is

The plot is

Understanding the Plot:

First plot the date and price

The plot looks like

The forecast plot looks like

The plot function is

There is a blog that explains clearly on the plot. The link is below

Validation

This cross validation procedure can be done automatically for a range of historical cutoffs using the cross_validation function. We specify the forecast horizon ( horizon ), and then optionally the size of the initial training period ( initial ) and the spacing between cutoff dates ( period ). By default, the initial training period is set to three times the horizon, and cutoffs are made every half a horizon.

The output of cross_validation is a dataframe with the true values y and the out-of-sample forecast values yhat , at each simulated forecast date and for each cutoff date. In particular, a forecast is made for every observed point between cutoff and cutoff + horizon . This dataframe can then be used to compute error measures of yhat vs. y .

Here we do cross-validation to assess prediction performance on a horizon of 365 days, starting with 730 days of training data in the first cutoff and then making predictions every 180 days. On this 8 year time series, this corresponds to 11 total forecasts.

The df looks like

You can see both actual y and forecasted value yhat.

Performance Metrics

The performance_metrics utility can be used to compute some useful statistics of the prediction performance ( yhat , yhat_lower , and yhat_upper compared to y ), as a function of the distance from the cutoff (how far into the future the prediction was). The statistics computed are mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), mean absolute percent error (MAPE), and coverage of the yhat_lower and yhat_upper estimates.

RMSE (Root Mean Square Error)

MAE (Mean Absolute Error)

MAPE (Mean Absolute Percentage Error)

Conclusion:

The main objective of this post is to install prophet, extract historical stock price data and crypto currency data , do some basic visualization and use prophet to do the forecast. Please refer the fbprophet documentation

and you can play with it by changing the parameters

Modeling with holidays

Using custom seasonality

Using a logistic growth trend model, etc

References:

[1] Sean J. Taylor and Benjamin Letham Forecasting at Scale , Facebook, Menlo Park, California, United States

Links:",https://towardsdatascience.com/apple-stock-and-bitcoin-price-predictions-using-fbs-prophet-for-beginners-python-96d5ec404b77,['Senthil E'],2019-08-08 17:36:06.276000+00:00,914,"Alphavantage API Quandl API Pandas_datareader FB Prophet Documentation  apple stock price prediction, Alphavantage API, Quandl API, Pandas_datareader, Alphavantage documentation"
New Union Operators to Merge Dictionaries in Python 3.9,"Different Ways to Merge Dictionaries Before Python 3.9

1.dict.update()

d1.update(d2) Update the dictionary d1 with the key/value pairs from d2 , overwriting existing keys. Return None .

d1={'a':1,'b':2}

d2={'c':3,'b':9999}

d1.update(d2)

print (d1)

#Output:{'a': 1, 'b': 9999, 'c': 3}

Limitations

d1.update(d2) will modify the original dictionary d1 . If the original dictionary need not be modified, create a copy of dictionary d1 and then update it.

d1={'a':1,'b':2}

d2={'c':3,'b':9999}

from copy import copy

d3=copy(d1)

d3.update(d2)

print (d3)

#Output:{'a': 1, 'b': 9999, 'c': 3}

2. Dictionary unpacking {**d1,**d2}

d3={**d1,**d2}

A double asterisk ** denotes dictionary unpacking.

It will expand the contents of dictionaries d1 and d2 as a collection of key-value pairs and update the dictionary d3 . Keys that are common in d1 and d2 will contain values from d2 .

d1={'a':1,'b':2}

d2={'c':3,'b':9999}

d3={**d1,**d2}

print (d3)

#Output:{'a': 1, 'b': 9999, 'c': 3}

Limitations

{**d1, **d2} ignores the types of mappings and always returns a dict .

3. collections.ChainMap

ChainMap : A ChainMap groups multiple dictionaries or other mappings together to create a single, updateable view.

collections.ChainMap(*maps)

Return type is collections.ChainMap . We can convert to dict using the dict() constructor.

d1={'a':1,'b':2}

d2={'c':3,'b':9999}

from collections import ChainMap

from collections import ChainMap

d3=ChainMap(d1,d2)

print (d3)

#Output:ChainMap({'a': 1, 'b': 2}, {'c': 3, 'b': 9999})

print (dict(d3))

#Output:{'c': 3, 'b': 2, 'a': 1}

Limitations

It also ignores the types of mappings and always returns a dict . ChainMap s wrap their underlying dict s, so writing to the ChainMap will modify the original dict .

In the above-mentioned example, if we modify the ChainMap object, d1 will also be modified. d3=ChainMap(d1,d2)

d3['a']=555555

print (d1)

#Output:{'a': 555555, 'b': 2}

4. dict(d1,**d2)

d3 = dict(d1,**d2)

d3 will contain key-value pairs from d1 and d2 . Keys that are common in d1 and d2 will contain values from d2 .

d1={'a':1,'b':2}

d2={'c':3,'b':9999}

from collections import ChainMap

d3=dict(d1,**d2)

print (d3)

#Output:{'a': 1, 'b': 9999, 'c': 3}

Limitations

d 3=dict(d1,**d2) will work only when d2 is entirely string-keyed.

If int is given as a key in d2 , it will raise TypeError .",https://medium.com/better-programming/new-union-operators-to-merge-dictionaries-in-python-3-9-8c7dbbd1080c,['Indhumathy Chelliah'],2020-10-14 16:19:39.818000+00:00,284,"d2={'c':3, 2:9999}d3=dict(d1, **d2) Type Error: keyword arguments must be stringspython, dictionary, merge"
The Basics of EDA (with candy),"The Basics of EDA (with candy)

“The goal is to turn data into information, and information into insight.”

Carly Fiorina, former CEO of Hewlett-Packard

As a data scientist, it is said that we will spend about 80% of our time on EDA. Therefore, as tedious as it may seem, it is a good idea to master the process before moving on to the fun stuff. The process which includes cleaning, sorting, and checking feature correlations to weed out the noise and hopefully gain some insight to what pieces are most useful.

The dataset for this example is the candy hierarchy for 2017 from the Science Creative Quarterly website. You can find and download the csv here if you want to follow along. I’m using Python 3.7 in Jupyter notebook.

Some of the steps I share here will be personal preference and you can modify as you like, however, these first few steps are pretty important to ALWAYS do.

Import:

Pandas — data analysis library, necessary for almost all data manipulation

Numpy — for linear algebra functions, good to have even if you’re not expecting to need it.

Matplotlib — visualization library for plotting and graphs

Seaborn — for more customizable data visualization

Read in your data:

Using pandas pd.read_csv function, enter your data source as a string and save it as a variable which will be the name of your DataFrame going forward. Many people use df, I like to use something more descriptive so i’ll go with candy this time.

Next you’ll want to check the first few rows of the data to get a peak at what you’ll be working with.",https://towardsdatascience.com/the-basics-of-eda-with-candy-83b2e8ad9e63,['Rachel Koenig'],2019-07-19 05:29:22.552000+00:00,260,"EDA, Data Science, Python, Pandas, Numpy"
How to use Kaggle API to download datasets in R,"Kaggle has its own officially supported Python library to consume its API, this small tutorial is focused on doing a small portion of what that API can do in a very quick, very easy example for those who wish to automate dataset downloading in their own projects, specially those datasets that come bundled in a zip file, for datasets that have direct download please refer to the unofficial R API package.

TL;DR — Here's the code:

install.packages(c(""devtools""))

devtools::install_github(""ldurazo/kaggler"") library(readr)

library(kaggler) kgl_auth(creds_file = 'kaggle.json')

response <- kgl_datasets_download_all(owner_dataset = ""andrewmvd/violence-against-women-and-girls"")

download.file(response[[""url""]], ""data/temp.zip"", mode=""wb"")

unzip_result <- unzip(""data/temp.zip"", exdir = ""data/"", overwrite = TRUE)

violence_data <- read_csv(""data/makeovermonday-2020w10/violence_data.csv"")

Step by step guide:

Install devtools from CRAN:

install.packages(c(""devtools""))

Then, with devtools install my fork of the unofficial R API library

devtools::install_github(""ldurazo/kaggler"")

The main difference between the original repository and my fork, is that I added a function that doesn't require you to specify the filename, so that if multiple files exist in the Kaggle resource you're trying to access, you get them all in a single zip file.

NOTE: I do not guarantee, and likely will not, maintain this fork in the future and as of today, I've requested to merge my fork into the main project, please see if this is supported in the original repository.

Next, we load the libraries and authenticate ourselves in the kaggle API

library(readr)

library(kaggler) kgl_auth(creds_file = 'kaggle.json')

The kaggle.json files can be generated following this instructions in the official kaggle docs. (tl;dr go to account > create API token) it consists on a very simple json file with the username and your API key. the Kaggler library will take care of the rest in terms of authentication.

Next, to retrieve all files within a single resource, use my kaggler's fork function:

response <- kgl_datasets_download_all(owner_dataset = ""andrewmvd/violence-against-women-and-girls"")

Which will return a response object that contains a URL to Google Cloud Storage that contains the zipfile, the following lines ensure the retrieval, and extraction of it

download.file(response[[""url""]], ""data/temp.zip"", mode=""wb"") unzip_result <- unzip(""data/temp.zip"", exdir = ""data/"", overwrite = TRUE) violence_data <- read_csv(""data/violence_data.csv"")

You can also use a templink() based temp file if you prefer, I simply like quick access to the contents of the downloaded .zip, as they often contain data descriptors and other utility documents for the dataset you're downloading.

Thank you for reading.",https://medium.com/mcd-unison/how-to-use-kaggle-api-to-download-datasets-in-r-312179c7a99c,['Luis Durazo'],2020-11-30 17:36:15.703000+00:00,358,"Kaggle, Python, API, RR, Devtools"
Empower a Lightweight Python Data Structure: From Tuples to Namedtuples,"Tuple

When it comes to dealing with related data elements, one of the most commonly used data types is the tuple. As an immutable data type, tuples are sequences of data with a fixed size. They’re useful to group related data with different data types. Consider the following trivial example.

>>> employee0 = ('John Smith', 45, 'M', 160083)

In the above code snippet, we define a tuple called employee0 , which stores an employee’s personal data, including name, age, gender, and employee ID number. If we need to use some elements of the tuple, we can unpack it or use subscript, and their usages are shown below.

>>> # Use unpacking

>>> name, age, gender, employee_id = employee0

>>> print(f""Employee Name: {name}"")

Employee Name: John Smith

>>>

>>> # Use subscript

>>> print(f""Employee Age: {employee0[1]}"")

Employee Age: 45

>>> print(f""Employee ID #: {employee0[-1]}"")

Employee ID #: 160083

How about we need to deal with another employee in the same module? We’ll have to do something like below.

>>> # Create a tuple for storing another employee data

>>> employee1 = ('Jennifer Brown', 38, 'F', 150384)

>>>

>>> # Access data

>>> name1, age1, gender1, employee_id1 = employee1

>>> print(f""Employee Name: {employee1[0]}"")

Employee Name: Jennifer Brown

>>> print(f""Employee Age: {age1}"")

Employee Age: 38

>>> print(f""Employee ID #: {employee_id1}"")

Employee ID #: 150384

In essence, we have to repeat the above steps, and access individual elements using either unpacking or subscripts, which certainly is not the most pleasant thing to do. Actually, it can be error-prone because you have to remember the exact order of these data.",https://medium.com/swlh/empower-a-lightweight-python-data-structure-from-tuples-to-namedtuples-ca4abddd8ef6,['Yong Cui'],2020-05-02 11:01:01.048000+00:00,239,"tuple, data types, immutable data type, subscripts, unpacking"
Font Generation with Variational Autoencoders,"Font is more important than we think. Font design has become one of the most crucial aspects of marketing for many mainstream consumer brands, as it represents their personality and tone. As people started to pay more and more attention to font design, especially for making logos, the market for fonts began to grow significantly. Currently, there are hundreds of font design and font database websites. So, why not make this process more customizable? Why not generate your own fonts according to your own design preferences? This is exactly what this project aims to achieve: Automate the font design process. Since this project heavily utilizes some data science tools and particularly artificial neural networks, so basic familiarity with the topics is helpful.

Collecting and Preprocessing the Data

When training a neural network, it’s important to have as much data as possible. The data for this project was collected (using Python’s requests module) from multiple font providers, namely Google, Urban Fonts, and 1001 Fonts, and there are currently about 60000 data points in the dataset. The two default file formats for font data are ttf and otf files. However, we cannot use these as input to a neural network, therefore we should first convert them to png files. For now, I’ve decided to use only alphabetical characters (uppercase and lowercase), and avoid numerical characters and symbols, because there are numerous inconsistencies in the dataset, with a good number of fonts not having numerical characters or symbols at all. After using the ttf/otf files to create 512x512 png’s, here’s what a data point looks like:

Figure 1: Example input data

Dataset Creation

The Magic of Variational Autoencoders

Variational autoencoders, or VAE’s, are pretty amazing. An autoencoder consists of two parts: an encoder that takes the data and compresses it into a much smaller representation, and a decoder that decompresses this representation to attempt to recreate the input. The difference between an autoencoder and VAE is that the compressed form of the data the encoder outputs (aka the “bottleneck”) is represented as a probability distribution of some parameters. This change allows the decoder to “sample” from the bottleneck, making VAE’s a generative model. The encoder of the VAE that is used in this project has the following structure:

Encoder model of the font generator

The latent_mu and latent_sigma variables are essentially the mean and the standard deviation of the probability distribution (or what’s called the “latent space”), and lambda is sampled from this distribution. Furthermore, a VAE’s loss function has an additional component called the KL divergence, which ensures that all encodings are distributed close to the center of the latent space, and not clustered far from each other. The advantage of having this kind of a latent space is that the model can learn very specific variations in the data. Here’s an example to understand this better: Let’s say we are training a VAE on images of faces. Where a regular autoencoder might associate a single parameter in the latent space with whether or not the the eyes are closed, a VAE can actually learn how closed the eyes are (here, the openness of the eyes becomes a continuous variable, instead of a single number).

This very useful attribute of VAE’s can be used to represent the different qualities of a font as well. Optimally, the VAE would learn to distinguish between features such as font weight, width, corner rounding, italics, contrast and serifs. Later, when we’re generating fonts, the weights of these features can be manually altered to generate a customized font.

During training, the sampled output from the encoder then goes into the decoder, which aims to reconstruct the initial input. The structure of the decoder is given below:

Decoder model of the font generator

Loading the decoder model for the font generator

Training settings for the VAE model

After training the model, the reconstruction of the font in figure 1 looks like this:

Reconstructed font from figure 1

Of course, the purpose of this model is not to reconstruct already existing fonts. After training the entire model, we can actually use the decoder with inputs of our choosing. The decoder here generates a font from this customized latent space.

The size of the latent space in the architecture that I used is 100, which means that to generate a font, we need 100 values. It is possible to generate a font using 100 random numbers in some range. However, using completely random numbers isn’t really helpful when we’re trying to understand what happens when we change a single variable. For this purpose, we can use the pyqt5 module to create a GUI with sliders for each parameter of the latent space, and the output being displayed next to them. The GUI, along with a generated font, looks like this:

Example view of the font generator GUI

Setting the ranges for the parameters in the GUI

The range of the parameters can be changed from the script (the link to the Github repository is at the bottom of this post). When the latent space is this large, it might be difficult to understand how changing a parameter affects the outcome. However, I am going to present one example of a very well-represented feature of this model. The 34th parameter seems to decide the italics of a given font.",https://medium.com/swlh/font-generation-with-variational-autoencoders-6bfe4d79e0e8,['Yigit Atay'],2020-05-09 00:38:00.236000+00:00,868,"Here’s a comparison of the fonts generated using different values for this parameter:Fonts generated with different values of the 34th parameterfont, font design, font database, font generator, artificial neural networks"
How To Create And Use Custom Matplotlib Style Sheet,"Why Use Style Sheet?

The aesthetics of data visualizations are important. I often view this as “beautiful data deserves beautiful presentations” — Being overly fastidious with graph styling is not encouraged, but a healthy amount of polishing could go a long way.

Nevertheless, for static graphs that we typically make with matplotlib or seaborn, the default aesthetics usually require tweaking. This is where custom style sheets come in handy for at least two reasons:

Make our workflow compliant with the Don’t Repeat Yourself (DRY) principle: We don’t need to type the same chunks of styling codes repetitively every time. Instead, we apply the style sheet with one line of code

Styling consistency is easily achieved, and that could be an implicit signature for individuals, teams, and organizations

How To Set Up And Use Custom Style Sheet

Start With Built-in Examples

Most of us may already have been using a few built-in styles in day-to-day work. For example, a quick run of

# This code snippet mimics command usages in ipython console In [1]: import matplotlib.pyplot as plt Out [1]: plt.style.available

will return a full list of style sheets, and we can find a gallery view of their effects in matplotlib’s documentation.

Under the hood, we can locate these built-in style sheets and take a look:

# This code snippet mimics command usages in ipython console In [2]: import matplotlib # Locate path to matplotlib by checking where config file is

# To learn more about the function below,

# type ?matplotlib.matplotlib_fname In [3]: matplotlib.matplotlib_fname()

Out [3]: '/Users/sdou/opt/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/matplotlibrc'

The path /Users/.../mpl-data is where we would like to go and locate the style sheets:

In [4]: !ls /Users/sdou/opt/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/

fonts images matplotlibrc stylelib

The folder of interest is stylelib . Let’s now take a look inside this folder:

In [5]: !ls -1 /Users/sdou/opt/miniconda3/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/

Solarize_Light2.mplstyle

_classic_test_patch.mplstyle

bmh.mplstyle

classic.mplstyle

dark_background.mplstyle

fast.mplstyle

fivethirtyeight.mplstyle

ggplot.mplstyle

grayscale.mplstyle

seaborn-bright.mplstyle

seaborn-colorblind.mplstyle

seaborn-dark-palette.mplstyle

seaborn-dark.mplstyle

seaborn-darkgrid.mplstyle

seaborn-deep.mplstyle

seaborn-muted.mplstyle

seaborn-notebook.mplstyle

seaborn-paper.mplstyle

seaborn-pastel.mplstyle

seaborn-poster.mplstyle

seaborn-talk.mplstyle

seaborn-ticks.mplstyle

seaborn-white.mplstyle

seaborn-whitegrid.mplstyle

seaborn.mplstyle

tableau-colorblind10.mplstyle

Let’s inspect one of the .mplstyle files. Here we use classic.mplstyle as an example and display the top 37 lines of the file:

Figure 1: The first 37 lines of the `classic.mplstyle` file (37 is an arbitrary choice)

We can see a comprehensive list of matplotlib parameter settings and their default values, and the parameters are organized into groups such as lines , markers , patch , text .

Build Custom .mplstyle File

A Minimalism Example

Below is a minimalism example (named signature.mplstyle )built on top of the built-in style sheet seaborn-colorblind.mplstyle

How To Invoke Custom Style Sheet

If we have write privilege to the abovementioned path for stylelib , we can put the custom style sheet into the same folder and invoke the style sheet with

# Scenario 1: Apply globally to a jupyter notebook

plt.style.use(“signature”) # Scenario 2: Apply locally with context manager

with plt.style.context(""signature""):

plt.plot([1, 2, 3, 4])

If we don’t have the write privilege, the only extra thing we would need to do is to include the full path of the custom style sheet. Here we use a simple example of directly storing the style sheet under the home directory:

# Scenario 1: Apply globally to a jupyter notebook

plt.style.use(“/home/signature.mplstyle”) # Scenario 2: Apply locally with context manager

with plt.style.context(""/home/signature.mplstyle""):

plt.plot([1, 2, 3, 4])

How To Return To Default

There could be cases when we want to recover the default styling. There are at two ways to recover the default settings:

Reset via rcParams.update

import matplotlib as mpl

mpl.rcParams.update(mpl.rcParamsDefault)

2. Reset with default style sheet

plt.style.use('default')

Data Viz Example: Style Sheet In Action

Here we penguin dataset as an example to demonstrate the effect of the custom style sheet signaure.mplstyle .

import matplotlib.pyplot as plt

import seaborn as sns # Load the penguins dataset

penguins = sns.load_dataset(""penguins"")

Use default style

plt.style.use(""default"")

# Show the joint distribution using kernel density estimation

g = sns.jointplot(

data=penguins,

x=""bill_length_mm"",

y=""bill_depth_mm"",

hue=""species"",

kind=""kde"",

)

g.fig.suptitle(""Styled With Default Style Sheet"", y=1.01)

plt.show()

Figure 2: Effect of matplotlib default style

2. Use custom style set in signature.mplstyle

plt.style.use(""signature"")

# Show the joint distribution using kernel density estimation

g = sns.jointplot(

data=penguins,

x=""bill_length_mm"",

y=""bill_depth_mm"",

hue=""species"",

kind=""kde"",

)

g.fig.suptitle(""Styled With Custom Style Sheet"", y=1.01)

plt.show()

Figure 3: Effect of custom style sheet `signature.mpl

Key Takeaways

Beautiful data deserves beautiful presentations. But instead of typing chunks of styling codes repetitively, a bit upfront time investment in setting up our own custom style sheet (file extension .mplstyle ) could both save time and ensure styling consistency

) could both save time and ensure styling consistency To build custom style sheets, we could start with built-in style sheets and custom them further to our liking. One key step is to locate these style sheets with the help of matplotlib.matplotlib_fname()

References

4.11 Customizing Matplotlib: Configurations and Stylesheets: Data Science Handbook by Jake VanderPlas",https://towardsdatascience.com/how-to-create-and-use-custom-matplotlib-style-sheet-9393f498063,['Shan Dou'],2020-10-25 16:39:38.745000+00:00,695,"style sheets, matplotlib, data visualization, data science, visualization aesthetics"
Lyft Level 5’s Machine Learning Pipeline,"Lyft Level 5 recently published an amazing overview of their PyTorch-based machine learning pipeline.

They confess that a few years ago, their development process was slow:

“Our first production models were taking days to train due to increases in data and model sizes.…Our initial deployment process was complex and model developers had to jump through many hoops to turn a trained model into a “AV-ready” deployable model….We saw from low GPU and CPU utilization that our initial training framework wasn’t able to completely utilize the hardware.”

The post proceeds to described the new pipeline Lyft built to overcome these obstacles. They started with a proof-of-concept for lidar point cloud segmentation, and then grew that into a production system.

The pipeline accomplishes a lot of infrastructure wins.

Testing. The pipeline incorporates continuous integration testing, both to ensure that the models don’t regress, and also to verify that the code researchers write will run in the PyTorch-based vehicle environment.

Containerization. Lyft invested in a uniform container environment, to mitigate the distinction between local and cloud model training.

Deployment. The system relies heavily on LibTorch and TorchScript for deployment to the vehicle’s C++ runtime. Depending on existing libraries reduces the amount of custom code Lyft’s team needs to write.

Distributed Training. PyTorch provides a fair bit of built-in support for distributed training across GPU clusters.

There’s a lot more in the post. It’s a pretty rare glimpse of a machine learning team’s internal infrastructure, so check it out!",https://medium.com/self-driving-cars/lyft-level-5s-machine-learning-pipeline-7a6799fa1552,['David Silver'],2020-10-18 05:58:26.427000+00:00,236,"Py Torch, Machine Learning, Lidar Point Cloud Segmentation, Continuous Integration Testing, Containerization"
DeepCode recognized as a top 16 startups disrupting software development with AI,"Top 16 AI trailblazers

The 2 original sources are linked below:

AI is transforming lives and businesses everywhere. From retail to finance to healthcare to media, there is hardly any industry or vertical left untouched by AI. Software development is something that is ubiquitous to almost all the industries and, in the world of Agile and DevOps (that is fast approaching), the pace at which we need to deliver and update software is increasing rapidly. To cope with this change, there is a need to incorporate AI in the Software Development Life Cycle (SDLC).

Recognizing this, over the past few years, multiple startups have come up to fill this gap. They are constantly innovating to help increase the productivity and accuracy across all stages of the SDLC, starting from requirement gathering to development to testing to deployment to application security. Startups are constantly filling this gap with the application of next-generation concepts such as Machine Learning (ML), Natural Language Processing (NLP), and deep learning.

In this research we present an assessment of startups utilizing AI in the field of software development, primarily focusing on their innovation, growth story, and the impact they have created in the market. We present an assessment and detailed profiling of the 16 AI startups across the stages of requirement, development, testing, deployment & maintenance, and security. Each startup profile provides a comprehensive picture of its technology capabilities, achieved market growth, and the perceived investors’ confidence.",https://medium.com/deepcode-ai/top-16-startups-disrupting-software-development-with-ai-d7eab8ee2726,['Deepcode Ai'],2018-10-31 12:54:35.439000+00:00,236,"AI, Software Development, SDLC, Machine Learning, NLP"
8 Must-Know Data Visualizations for Better Data Analysis,"8 Must-Know Data Visualizations for Better Data Analysis

Explained with examples using Seaborn

Photo by Lucas Benjamin on Unsplash

Data visualization is a very important part of data science. It is quite useful in exploring and understanding the data. In some cases, visualizations are much better than plain numbers at conveying information.

The relationships among variables, the distribution of variables, and underlying structure in data can easily be discovered using data visualization techniques.

In this post, we will learn about the 8 most commonly used types of data visualizations. I will use Seaborn to create visualizations and also try to explain what kind of information we can infer.

We will use the grocery and direct marketing datasets available on Kaggle to create the visualizations.

The grocery dataset contains information about customer purchases at grocery stores. The direct marketing dataset contains relevant data of a marketing campaign done via direct mail.

Let’s start by reading the datasets into a pandas dataframe.

import numpy as np

import pandas as pd

import seaborn as sns

sns.set(style='darkgrid') grocery = pd.read_csv(""/content/Groceries_dataset.csv"", parse_dates=['Date'])

print(grocery.shape)

grocery.head()

Grocery dataset (image by author)

The dataset contains about 40k rows and 3 columns. We have member number, date of purchase, and the purchased items as columns.

marketing = pd.read_csv(""/content/DirectMarketing.csv"")

print(marketing.shape)

marketing.head()

Marketing dataset (image by author)

The marketing dataset consists of 1000 observations (i.e. rows) and 10 features (i.e. columns). The focus is on the “AmountSpent” column which indicates how much a customer has spent so far.",https://towardsdatascience.com/8-must-know-data-visualizations-for-better-data-analysis-9752953dd703,['Soner Yıldırım'],2020-11-23 08:29:40.793000+00:00,226,"Data Visualization, Seaborn, Data Analysis, Grocery Dataset, Direct Marketing Dataset"
Hierarchical Clustering Explained,"Hierarchical Clustering Explained

Unsupervised Algorithms | Data Series | Episode 8.3

In the previous episode we have taken a look at the popular clustering technique called K-means clustering. In this episode we will take a look at another widely used clustering technique called Hierarchical clustering.

Please consider watching this video if any section of this article is unclear:

Video Link

What is Hierarchical clustering?

Hierarchical clustering is an unsupervised machine learning algorithm where its job is to find clusters within data. We can then use these clusters identified by the algorithm to make predictions for which group or cluster a new observation belongs to.

Overview

Similar to K-means clustering, Hierarchical clustering takes data and finds clusters:

What differs, however, is the algorithm to identify clusters. We will discuss at the end the relative advantages and disadvantages of Hierarchical clustering compared to K-means clustering.

The Algorithm

Step 1:

Treat each data point as a cluster. Calculate the Euclidian distance each cluster is away from each other:

Step 2:

Using the distance matrix identify the clusters closest to each other:

Step 3:

Link these clusters together to form a new cluster:

Step 4:

Calculate the distance each cluster’s mean point is away from each other:

Step 5:

Repeat steps 2 to 4 until a single cluster is formed.

One cluster has been formed so we stop.

Step 6:

Cut our dendrogram at a chosen point to give the clusters identified by our algorithm at that point. The point we choose to cut is usually done visually.

and we are done!

Linkage Methods

Note that in step 4 we calculated the distance each cluster is away from each other (known as dissimilarity) based on the centroid or mean point of each cluster. We then link the clusters with the smallest of such dissimilarity. This is known as Centroid Linkage:

There are however other methods to link clusters:

Ward’s method shares the same objective function as K-means clustering discussed in the previous episode.

Considerations of Hierarchical clustering

Advantages

Do not have to manually select number of clusters K.

Easy to implement.

Dendrogram can give useful information.

No need for many random centroid initializations as with K-means clustering.

Disadvantages

With large datasets it is difficult to determine the number of suitable clusters from the dendrogram.

Computationally expensive, slower than K-mean clustering.

Sensitive to outliers.

In the next episode we will be implementing Hierarchical clustering on a real-life dataset using Python.

Summary

Hierarchical clustering is an unsupervised machine learning algorithm that is used to cluster data into groups

machine learning algorithm that is used to cluster data into groups The algorithm works by linking clusters, using a certain linkage method (mean, complete, single, ward’s method etc.) to form new clusters.

The above process produces a dendrogram where we can see the linkages of each cluster.

We can cut our dendrogram at a certain point to obtain suitable clusters from our data.

Prev Episode _______ Next Episode",https://medium.com/swlh/hierarchical-clustering-explained-with-example-63b2fe9060dd,['Mazen Ahmed'],2020-12-14 16:16:26.996000+00:00,445,"hierarchical clustering, unsupervised learning, data series, episode 8.3, K-means clustering"
Real-time Twitter Sentiment Analysis for Brand Improvement and Topic Tracking (Chapter 3/3),"Migrate Data Analytics & Visualizations from Plotly to Dash

To put our data visualization from the previous chapter on the Heroku app, we need to wrap our Plotly-based dashboard with Dash framework. All data analysis and visualizations will be processed in the file app.py , and you may check my entire code for this file here.

Start with a new app server with the Dash default CSS sheet. Note: I recommend to improve the appearance of the web app using Bootstrap.

external_stylesheets=['https://codepen.io/chriddyp/pen/bWLwgP.css'] app = dash.Dash(__name__, external_stylesheets=external_stylesheets)

app.title = 'Real-Time Twitter Monitor' server = app.server

The core framework of Dash web app is using app.layout as a background layout. Note: Click on two links below to understand how they work and check some typical samples as they are VERY IMPORTANT to utilize the Dash.

Dash Layout allows us to display integrated data visualization along with other text descriptions.

allows us to display integrated data visualization along with other text descriptions. Dash Callbacks enables the application to update everything consistently with real-time data.

Since Dash is built on top of HTML, it uses HTML in its own way (Dash HTML Components). But some of HTML features are not allowed in the Dash.

html.Div(id='live-update-graph') describes the top part of the dashboard in the application, including descriptions for tweet number and potential impressions. html.Div(id='live-update-graph-bottom') describes the bottom part of dashboard in the web app.

dcc.Interval is the key to allow the application to update information regularly. Although the data collector, which will be explained later, on another dyno works in real-time, the analytical dashboard only analyzes and visualizes data per 10 seconds because of the utilization of aggregated data for visualization and the consideration of cost-efficiency.

app.layout = html.Div(children=[



# Parts of Title hided

html.H2('This-is-your-tittle'), html.Div(id='live-update-graph'),

html.Div(id='live-update-graph-bottom'),



# Parts of Summary hided

html.Div(

dcc.Markdown(""Author's Words: ...... "")

) # Timer for updating data per 10 seconds

dcc.Interval(

id='interval-component-slow',

interval=1*10000, # in milliseconds

n_intervals=0

)



], style={'padding': '20px'}) }

For each Div in the HTML of Dash, it has className , children , and style . children is a list of dcc.Graph (Graphs in Dash Core Components), dcc.Markdown , html.Div , html.H1 , and other interactive buttons (e.g. dcc.Dropdown and dcc.Slider ).

html.Div(

className='row',

children=[

dcc.Markdown(""...""),

dcc.Graph(...),

html.Div(...)

],

style={'width': '35%', 'marginLeft': 70}

)

style is important for building a good layout and ensuring proper distances among several visualization graphs, but can be time-consuming to tune the details.

Give an example of dcc.Graph below (Graphs in Dash Core Components). dcc.Graph has attributes id and figure . Under the figure , there are 'data' , containing different kinds of graphs in Plotly (e.g. go.Scatter and go.Pie ), and 'layout' , which is only the layout of this graph rather than app.layout .

# import dash_core_components as dcc

# import plotly.graph_objs as go dcc.Graph(

id='pie-chart',

figure={

'data': [

go.Pie(

labels=['Positives', 'Negatives', 'Neutrals'],

values=[pos_num, neg_num, neu_num],

)

],

'layout':{

'showlegend':False,

'title':'Tweets In Last 10 Mins',

'annotations':[

# ...

]

}

}

)

'annotations' is another important part to ensure the current labels. Note: Reference for Annotation in Dash is quite ambiguous, since parameters may need to be expressed as go.layout.Annotation , a dictionary dict(...) , and sometimes a list.

In the Dash app layout, reactive and functional Python callbacks provide connections between inputs and outputs, allowing customizable declarative UIs.

Note: We skip all data analysis parts that were explained in the previous chapter, although there might be a little bit difference (or improvement). Thus, we dive directly into Dash-based data visualization parts, which are implemented through complicated Plotly graphs plotly.graph_objs (a.k.a. go).

# Multiple components can update everytime interval gets fired.

@app.callback(

Output('live-update-graph', 'children'),

[Input('interval-component-slow', 'n_intervals')]

)

def update_graph_live(n): # Lots of nested Div to ensure the proper layout

# Graphs will be explained later

# All codes are hided return children

All three line charts use go.Scatter along with stack groups to ensure the overlapped areas under lines in the same graph.

# import plotly.graph_objs as go

go.Scatter(

x=time_series,

y=result[""Num of A-Brand-Name mentions"",

name=""Neutrals"",

opacity=0.8,

mode='lines',

line=dict(width=0.5, color='rgb(131, 90, 241)'),

stackgroup='one'

)

Pie chart was explained in the example of dcc.Graph above.

For text descriptions (e.g. Tweet Number Change per 10 Mins), we use two HTML paragraphs to embed data info. By comparing the previous 10-min interval and current 10-min interval, we could generate such a result.

html.P(

'Tweets/10 Mins Changed By',

style={'fontSize': 17}

), html.P(

'{0:.2f}%'.format(percent) if percent <= 0 \

else '+{0:.2f}%'.format(percent),

style={'fontSize': 40}

)

For Potential Impressions Today, we want to figure out how many people at most have a chance to see these tweets. By counting the sum of follower numbers of people who posted these tweets, we can get the potential impressions. Add a dynamic numeric unit to better display this value in a very large range.

html.P(

'{0:.1f}K'.format(daily_impressions/1000) \

if daily_impressions < 1000000 else \

('{0:.1f}M'.format(daily_impressions/1000000)

if daily_impressions < 1000000000 \

else '{0:.1f}B'.format(daily_impressions/1000000000)), style={'fontSize': 40}

)

Counting Daily Tweet Number is an easy approach by storing and adding up tweet numbers in each app data update. And this value will be reset to zero at midnight.

html.P(

'{0:.1f}K'.format(daily_tweets_num/1000),

style={'fontSize': 40}

)

The callback function for the bottom dashboard is similar to the first one.

@app.callback(Output('live-update-graph-bottom', 'children'),

[Input('interval-component-slow', 'n_intervals')])

def update_graph_bottom_live(n): # Lots of nested Div to ensure the proper layout

# Graphs will be explained later

# All codes are hided return children

Bar chart for hottest topic tracking. Set orientation as horizontal to better display bars and related word names

go.Bar(

x=fd[""Frequency""].loc[::-1],

y=fd[""Word""].loc[::-1],

name=""Neutrals"",

orientation='h',

marker_color=fd['Marker_Color'].loc[::-1].to_list(),

marker=dict(

line=dict(

color=fd['Line_Color'].loc[::-1].to_list(),

width=1

),

)

)

Focus the scope on State-level Map by setting 'layout': {'geo':{'scope':'usa'}}

go.Choropleth(

locations=geo_dist['State'], # Spatial coordinates

z = geo_dist['Log Num'].astype(float), # Color-coded data

locationmode = 'USA-states',

text=geo_dist['text'], # hover text

geo = 'geo',

colorbar_title = ""Num in Log2"",

marker_line_color='white',

colorscale = [""#fdf7ff"", ""#835af1""]

)

In order to parallel multiple graphs in a single row, you may consider using the following. width should be 1/N percentage (N = number of graph), and we may subtract 1% to give enough gaps between graphs.",https://towardsdatascience.com/real-time-twitter-sentiment-analysis-for-brand-improvement-and-topic-tracking-chapter-3-3-3b61b0f488c0,['Chulong Li'],2019-09-20 04:34:53.126000+00:00,871,"html.Div(class Name='row', children=[dcc.Graph(...), dcc.Graph(...)], style={'width': '49%', 'display': 'inline-block'"
I Want My Personal Data Back!,"I Want My Personal Data Back!

It’s time to transfer the control of personal data from organisations to people.

Photo by Claire Proud on Unsplash retouched by the author

Summary

In today’s world, our personal data is usually stored and controlled by organisations.

In tomorrow’s world, personal data will be stored and controlled by its owners, us.

Discussion

What is personal data?

It’s our email address, telephone numbers, home address, bank account details, location, preferred hot drink, basically any data pertaining to us.

Who owns our personal data?

We do.

Where is our personal data stored, and who controls it?

In today’s world, our personal data is usually stored and controlled by organisations:

Personal data is stored and controlled by organisations (dotted zone)

Traditionally, the control of our personal data derived from where it was stored. See “Personal Data Storage Location” by the author for a discussion about personal data storage locations.

What is wrong with that?

Almost everything we do online or offline requires us to supply personal data

We have become accustomed to sharing our personal data without considering or even understanding the consequences

Few of us have a clear picture of how far our personal data is spread throughout the world

Our online personal data footprint is continually increasing, exposing us to increasing risk of data abuse

We have ineffectual control of our shared personal data and must often rely on data protection regulations to reduce the risk of data abuse

How will that affect us?

Over time, this repeated and increasing sharing of our personal data raises the risk of:

loss of our autonomy

loss of our privacy

damage to our reputation

adversely affecting our finances

theft of our identity

What can we do about it?

In tomorrow’s world, our personal data will be stored and controlled by its owners, us:

Personal data will be stored and controlled by its owners

This will probably be encapsulated by a Web 3.0 principle that we need secure, trust-less, distributed services, such as blockchains and storage.

How do we do that?

We need an open, distributed platform designed to encourage self-sovereign identity and data ownership. It must enable any data, held on any storage device, to be shared in a controlled way with other people or organisations.

The platform should be as general purpose as possible, allowing developers to integrate it into existing applications and to explore entirely new fields of data sharing.

People and organisations need applications that easily create smart contracts that share our personal data.

Conclusion

Permitting organisations to store and control our personal data has proven to be risky, as borne out by continuous reports of malpractice in the press.

Additionally, personal data regulation (such as GDPR) compliancy is becoming increasingly difficult and expensive for organisations. Ultimately, these are costs that will be borne by us.

Storing personal data in a distributed manner controlled by our smart contracts, provides organisations with high personal data regulation compliance, and us with good control of our personal data.

It’s time to transfer the control of personal data from organisations to people.

Bio

Jules Goddard is a founder of Datona Labs, who provide smart contracts to protect your digital information from abuse.

website — twitter — discord — GitHub

Also, Read",https://medium.com/coinmonks/i-want-my-personal-data-back-9c891add31d2,['Jules Goddard'],2020-12-28 17:10:40.893000+00:00,498,"Personal Data, Data Ownership, Web 3.0, Self-Sovereign Identity, Smart Contracts"
Monitor & Improve GPU Usage for Model Training with W&B,"One of the exciting things about running Weights and Biases is that we can research how models are actually using their computational resources in real world scenarios. Since we are simplifying the tasks of monitoring GPU and memory, we wanted to help our users by taking a look at how well folks utilize their computational resources.

For users training on GPUs, I looked at their average utilization across all runs. Nearly a third of our users are averaging less than 15% utilization. Average GPU memory usage is quite similar. Our users tend to be experienced deep learning practitioners and GPUs are an expensive resource so I was surprised to see such low average usage.

Here’s a few easy, concrete suggestions for improving GPU usage that apply to almost everyone:

Measure your GPU usage consistently over your entire training runs

You can’t improve GPU usage without measuring it. It’s not hard to take a snapshot of your usage with useful tools like nvidia-smi, but a simple way to find issues is to track usage over time. Anyone can turn on system monitoring in the background, which will track GPU, CPU, memory usage etc over time by adding two lines to their code

import wandb

wandb.init()

The wandb.init() function will create a lightweight child process that will collect system metrics and send them to a wandb server where you can look at them and compare across runs with graphs like these:

The danger of taking a single measurement is that GPU usage can change over time. This is a common pattern we see where our user Boris is training an RNN; mid-training, his usage plummets from 80 percent to around 25 percent.

You can see his complete set of stats and training log at https://app.wandb.ai/borisd13/char-RNN/runs/cw9gnx9z/system.

A related case we commonly see with multiple GPUs is that mid-training, some of the GPUs stop handling any load. In this example both GPUs started off doing computations, but a few minutes in, all the load is sent to a single GPU. This could be intentional but this is often the sign of a hard to catch bug in the code.

Another common issue we see is that there are long periods of not using the GPUs — often corresponding with a testing or validation phases in training or bottlenecked on some data preprocessing. Here is a typical graph, training on 8 GPUs where all of them turn off and wait for some time at a regular interval.

2) Make sure your GPU is the bottleneck

This is a common situation we see — here the system memory is significantly used and the memory usage seems to be gradually increasing. As the memory usage goes up the GPU usage goes down. We also often see network being the bottleneck when people try to train on datasets that aren’t available locally.

3) Try increasing your batch size

It doesn’t work in every case, but one simple way to possibly increase GPU utilization is to increase batch size. Gradients for a batch are generally calculated in parallel on a GPU, so as long as there is enough memory to fit the full batch and multiple copies of the neural network into GPU memory, increasing the batch size should increase the speed of calculation.

If I increase the batch size and change nothing else, I might conclude that increasing the batch size speeds up computation but reduces model performance. Here are my results training CIFAR with batch sizes 32, 64 and 128.

Indeed, there are many papers and a top post on StackOverflow warning about large batch sizes. There is a simple way to make larger batch sizes work reasonably well. Increase the learning rate along with batch size. Intuitively, this makes sense, batch sizes are how many examples a training algorithm looks at before making a step and learning rate is roughly the size of the step. So if the model looks at more examples it should probably be comfortable taking a larger step. This is recommended in the paper One weird trick for parallelizing convolutional neural networks and later in Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour where the authors managed to increase the batch size to 8000 before they saw a loss in performance.

I tried increasing the learning rate with the batch size on my own model and reached the same conclusion. When I multiply the learning rate by 4 and increase the batch size by 4, my model trains faster and learns faster at each step.

The Facebook paper does some fancy things to make the model work well at very large batch sizes and they are able to get the same performance but at much higher speeds up to 8000 samples per batch.

These huge batch sizes make sense for distributed training, and the paper’s scheme of starting with a lower learning rate and then ramping it up looks very valuable in that context. If you’re training on one GPU and not maxing out your utilization, I have a quick recommendation: double your batch size and double your learning rate.

You can dive into more data from my runs in the Batch Size Report.

Conclusion

GPUs are getting faster and faster but it doesn’t matter if the training code doesn’t completely use them. The good news is that for most people training machine learning models there is still a lot of simple things to do that will significantly improve efficiency.

There’s another, probably larger, waste of resources: GPUs that sit unused. We don’t measure this, but I’ve heard it anecdotally from many of the companies we work with. It’s hard to queue up work efficiently for GPUs, in a typical workflow a researcher will set up a large number of experiments, wait for them to finish and then spend quite a lot of time digesting the results while the GPUs sit idle. This is outside the scope of wandb, but tools like Paperspace and Amazon’s Sagemaker make it easy to spin up and down resources as needed.

Thanks Sam Pottinger, Carey Phelps, James Cham, Yanda Erlich, Stephanie Sher for edits and feedback.",https://medium.com/weights-biases/monitor-improve-gpu-usage-for-model-training-with-w-b-55d06320f476,['Weights'],2019-03-28 02:07:24.986000+00:00,996,"GPU, utilization, memory usage, system monitoring, wandb.init()"
Training Taking Too Long? Use Mini Batch Gradient Descent!,"Stuck behind the paywall? Click here to read the full story with my friend link!

We all, ML Engineers or Data Scientists, love Data. Whenever we hear that we are getting more Data to use, it sounds like Heaven but

Not everything is as it seems.

“What is the drawback here?” ~ you might ask. So, we have our little CPU Processors, but some of the lucky ones among us have GPUs, but then too, the Computing Power is not skyrocketing and has a limit. The main drawback that I can think of is Too long Training time obviously.

Mini Batch Gradient Descent

Say you have a batch of 5,000,000 data inputs. Now, this would alone make a vector of 5,000,000 sizes, and should you think of efficiency when performing any Mathematical operation on a vector this big. What we do here? Ever heard of the saying “Baby steps”, this is essentially that.

Mini-Batch (Baby Batch): Suppose it’ll just have 1k data inputs in them. Now, 5,000,000 means that we’ll have 5000 mini-batches.",https://medium.com/swlh/training-taking-too-long-use-mini-batch-gradient-descent-a101846afe47,['Danyal Jamil'],2020-11-12 05:57:37.253000+00:00,167,"Data Science, MLEngineers, GPUProcessors, Mathematical Operation, Mini Batch Gradient Descent"
Latest picks: In case you missed them:,"Get this newsletter By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.

Check your inbox

Medium sent you an email at to complete your subscription.",https://towardsdatascience.com/latest-picks-the-travelling-apothecary-3cd43c2b58fb,['Tds Editors'],2020-12-14 14:27:26.660000+00:00,40,"Newsletter, Medium, Email, Subscription, Privacy Policy"
Why This Course?,"Why this course? Here is a series of statements, which will certainly evolve.

(1) We love the city.

(2) We want to understand the city better, we want to explain what it is in a better way.

(3) The concept of a city has always been complex. It has got even more complex in our times.

(4) Imposing what the city is cannot be in the jurisdiction of administrators, rulers, and tyrants. Citizens should have the most say on what the city should be and where it should evolve.

(5) Data produced by the city, by the citizens within the city, and analytics methods built on top are the best possible means to answer complex urban problems and bring the citizen back to the decision making process about what the city should be in a particular country and/or geography.

(6) Having said these, we are aware that urban data and analytics methods leveraging such data might establish a tyranny of their own. We do not dismiss that very possibility, we are cautious about it and we are not after a mechanistic, dull understanding of the city. That would exclude the serendipities of the nature and the humans therein, and the creativity of urban designers and architects who contribute to the built environment. Although almost everything can be seen as data, not everything can be fairly measured. Furthermore, data analytics might have its own way of lying when employed by the wrong minds, as happens with pretty much all sorts of scientific and technological progress. As such, cities will always need thinkers, dreamers, visionaries, and hard-working practitioners. Our aim is to equip such people with data analytics skills and resources.

(7) Cities and citizens produce data. Urban planners make data-driven decisions about the city every single day. In that sense, we are not claiming any novelty. What we are aiming at is universal and has been widely adopted by many researchers across the world. But we believe in our country — Turkey, in our geography, data-driven understanding and decision making has not been the primary feat in the urbanization of the land (as in many other areas). We want to propose and advocate for this alternative strongly, we want to promote it and raise awareness about its appropriateness, honesty and effectiveness. That is where we believe to be different or pioneering.

(8) In an era where design has become human-centered more than ever, a thorough understanding of cities, built environment, and citizens is beyond the capabilities of one single mind, even though that mind is the mind of a genius or a person of strong will such as Haussman, Moses or Le Corbusier. Urban data analytics is the first step to acquire a better understanding of the city and citizens therein.

(9) As we start, in Fall 2020, we want to put more emphasis on understanding rather than coming up with design proposals. Maybe that will remain as our principal goal throughout the years ahead. We keep the suspicion that the alternative might serve the very opposite of what we are trying to achieve.

(10) We want to expose what the city is and where it evolves to as citizens perceive and experience it rather than to define what it ought to be. We want to serve as a communication channel, novel, more objective, more citizen-centered.

(11) We will probably contradict ourselves many times. We hope our followers will correct us as we move and that they will contribute to this tough endeavor with open hearts and minds and joyful work.",https://medium.com/city-as-a-data-mine/why-this-course-931498e25c37,['Ceyhun Burak Akgül'],2020-12-26 15:29:35.435000+00:00,579,"city, urban data, analytics, design, citizen-centered decision making"
PySpark + MySQL Tutorial,"PySpark + MySQL Tutorial

This post is meant as a short tutorial on how to set up PySpark to access a MySQL database and run a quick machine learning algorithm with it. Both PySpark and MySQL are locally installed onto a computer running Kubuntu 20.04 in this example, so this can be done without any external resources.

Requirements

Installing MySQL onto a Linux machine is fairly quick thanks to the apt package manager with sudo apt install mysql-server . Once it’s installed, you can run sudo mysql in a terminal to access MySQL from the command line:

Screenshot of the MySQL prompt in a console window.

For PySpark, just running pip install pyspark will install Spark as well as the Python interface. For this example, I’m also using mysql-connector-python and pandas to transfer the data from CSV files into the MySQL database. Spark can load CSV files directly, but that won’t be used for the sake of this example.

Finally, we need the Java drivers that will let Spark connect to MySQL. I didn’t see a good way to install them through apt , so I downloaded the driver from the MySQL website and installed it manually. You may need the location of the driver file later on, depending on where it was installed, so finding it may be necessary. On Kubuntu, the driver file was installed to “/usr/share/java/”.

The data that I’m using is the wine quality dataset from the UCI machine learning repository. It’s not especially large, with only around 6500 examples total, but it’s clean and easy to use for this example. There are a few possible ways to use it for machine learning or other predictive purposes, but I’m going to focus on predicting whether the wine in question is red or white.

Writing Data to MySQL

The first step is to read in the data. The only things worth noting here are that the files are semicolon-delimited, and we need to create the column for whether a wine is white or red ourselves:

First and last five rows of the data, including the new “is_red” column, as they appear in a dataframe.

MySQL is similarly straightforward as you just set up the new database and an appropriate table. You will need to ensure that your user has privileges to edit the table, though; if you need to change privileges, that will have to be done from the MySQL prompt.

And then load the data. MySQL can load multiple rows into a table at once if the contents of each row are contained within parentheses and comma-delimited, which a simple str.join() can do. Then upload into the database, using FLUSH TABLES to actually update the table with the rows (otherwise the changes are merely staged and would eventually be discarded once the connection is closed).

Accessing MySQL with PySpark

Starting a Spark session from Python is fairly straightforward. Again, I had to specify the location of the MySQL Java driver, which is the only subtlety that I found. Loading the table afterward is similarly simple, despite the number of options that need to be specified, though you’ll need the port number that MySQL is on when loading the data from the database (the default port is 3306).

If you’re loading data into Spark from a file, you’ll probably want to specify a schema to avoid making Spark infer it. For a MySQL database, however, that’s not necessary since it has its own schema and Spark can translate it.

Training the Model

Finally, training the model. As always, split the train and test data first, then define the target and predictor variables. Specifying the model can be done a few ways, including the ability to use an R-like formula to specify the model. Here I’ll use the VectorAssembler , which basically just concatenates all the features into a single list:

Then a logistic regression model can be trained on the data, and we can get the predictions that the model makes for the test data.

PySpark also has a Pipeline class, which can intelligently connect up all of the separate steps into a single operation, if you prefer:

Regardless, evaluating the model is necessary. Spark seems a little limited in the native options for evaluating models — the BinaryClassificationEvaluator below only seems to support area under the curve for ROC (default) or the precision-recall curve.

Note that the logistic regression model will actually return three columns of prediction data:

rawPrediction depends on the model used, but here refers to the value of the linear part of the logistic regression model before being transformed

depends on the model used, but here refers to the value of the linear part of the logistic regression model before being transformed probability is an array of the actual probabilities for each class

is an array of the actual probabilities for each class prediction is the actual class prediction

You can use the toPandas() method to return the prediction data as a pandas dataframe, so other metrics are possible to calculate with either pandas or numpy .

Pandas version of PySpark’s prediction columns.

Finally, if you want to look at an overview of Spark’s activity during the session, you can open a browser tab to localhost:4040 and see an overview of it:

SparkUI after running the above code.

So that is a quick, not-too-into-the-weeds overview connecting MySQL to PySpark and fitting a logistic regression model. Obviously, there is much more power in both tools, but this will hopefully provide a window into your own work.",https://towardsdatascience.com/pyspark-mysql-tutorial-fa3f7c26dc7,['Gregory Janesch'],2020-12-13 04:37:14.021000+00:00,893,"pyspark, mysql, tutorial, machine learning, ubuntu 20.04"
How I used a Pipeline() to solve the Kaggle Disaster Tweet competition Question,"A pipeline is used to chain multiple procedures together, thereby facilitating the machine learning process. Chains can be extremely useful because the machine learning steps must undergo a certain sequence of events, which are accomplished by the pipeline.

The functions that can be put into a pipeline are transformers or estimators. Transformers can be encoders, scalers or vectorizer/transformers. The estimators are the model that is used to predict a certain the outcome of the program, which could be a regression or a classification.

The pipeline must fit and then transform the data when it is being processed, such as being put in an encoder. The pipeline must fit and then predict on the data after it has been processed and then put into a machine learning model, such as, say, a logistic regressor.

A pipeline shortens the steps that a person must take to reach a certain outcome. For instance, if data needs to be encoded, scaled, and predicted upon, rather than writing separate lines of code to encode, scale and predict, the pipeline only requires that one set of code be written rather than three. The convenience of pipelines are that one only has to call fit and predict once on the data to fit a whole sequence of estimators.

Pipelines are very safe because they help avoid leaking statistics from test data into the trained model in cross-validation by ensuring that the same samples are used to train the transformers and predictors. All estimators in a pipeline, except the last one, must be transformers, thereby having a transform method. The last estimator in the pipeline may be any type of function in the sklearn library, such as a transformer, classifier, or regressor.

In order to show how versatile and efficient a pipeline is, I have used the Kaggle Disaster Tweet competition question for this purpose. The link to the files that must be downloaded for this competition can be found at the below link:- https://www.kaggle.com/c/nlp-getting-started/data

Extracts from the competition question are states as follows:-

“Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).

But, it’s not always clear whether a person’s words are actually announcing a disaster.

In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.

Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.

This dataset was created by the company figure-eight and originally shared on their ‘Data For Everyone’ website here.”

The first thing that I did was to import the libraries and the datasets needed to predict upon this problem:-

I then read the datasets and checked for any null values in the datasets:-

I decided to replace any null values with the word “unknown”, although the columns that had missing values were not used as part of the prediction:-

I imported a HTML parser and created a new column, “tidy_tweet”, in both the train and test file. All of the natural language processing has occurred in this newly created column:-

As part of the text processing, I removed the twitter handles, which had been anonymised so were not necessary. I also changed all of the text to lowercase to ease the text processing process:-

I removed the rare words from the processed text because they did not add any meaning to the analysis. I also removed words less than two characters because they would add little meaning to the analysis as well.

After processing the text, I tokenized all of the words:-

I then stemmed the words, which is to keep only the root of the word and discard the rest. After stemming, I removed the stop words from the text, which had the effect of further reducing the number of words that would need to be analysed:-

Finally, I removed the hashtags from the tweets because they are not necessary for any analysis:-

After the text was preprocessed, I defined the X, y and X_test variables. X is train.tidy_tweet, X_test is test.tidy_tweet, and y is train.target.

After the dependant and independent variables had been defined, I split the train dataset up for training and validating:-

I then created the pipeline, which streamlined the process of vectorizing the predicting upon the validation and test data. I used CountVectorizer and TfidfTransormer, which combined do the same thing that TfidfVectorizer did, which I mentioned in my earlier post and can be found in the below link:-

https://medium.com/python-in-plain-english/how-i-used-tfidfvectorizer-to-solve-a-tagging-problem-f76496c642bb

I defined the LinearSVC model and then fit these three functions into the pipeline:-

I predicted on the validation set and obtained an accuracy of 77.03%:-

When I predicted on the test dataset I obtained an accuracy of 78.36%, which was a 5.516% improvement on my previous submission.

The code for this competition question can be found in its entirety on the Kaggle website, the link being found below:- https://www.kaggle.com/tracyporter/identify-the-disaster-pipeline-linearsvc?scriptVersionId=45973839",https://python.plainenglish.io/how-i-used-a-pipeline-to-solve-the-kaggle-disaster-tweet-competition-question-b6397b17efd3,[],2020-11-02 13:46:43.782000+00:00,818,"Pipeline, Machine Learning, Transformers, Estimators, Encoders"
Artificial Intelligence –The Tech Superhero Of This Era,"Different business verticals found sudden interest in AI and why not! In this tech era, AI eventually unfolding its high-performance abilities and features, which could salvage life.

Over the years, AI enabled automation in various sectors including business, banking, agriculture, marketing, and others. Practically, AI has become a push-up element for all the industries across the globe.

The recent trend in AI is somewhat different from how it worked so far. Researchers have found new pathways where AI emerged to be a lifesaver for humans. How you see AI and robotics enslaving humans in movies that is far beyond reality for now! Today, AI applications are featured to make human lives safer.

Here are some examples of AI applications that proved to be lifesaver for humans.

1. Autonomous vehicles to impede accidents

According to a published report, around 1.35 million people around the world lose their lives in roadway accidents. Although we have hi-tech advanced vehicles, still human lives couldn’t be saved from roadway crashes. The most vulnerable ones are bicyclists, pedestrian, and motorcyclists. It is too late to sensitize people for following road rules or minimizing the use of vehicles. Keeping this in mind, the automotive industry is all set to introduce AI-automated vehicles. This could prove to be a revolution for the mechanical industry.

These vehicles would have an in-built application to enable computer vision which could detect and prevent accidents on road. One such application is !iMPORTANT which is designed to minimize the risk of collisions with different vehicles like cars, trucks, buses, construction equipment, and autonomous vehicles.

2. Health applications to detect medical conditions

The world recently is suffering from a pandemic situation that is taking millions of lives around the world. Human-to-human contact is highly unsafe these days. The risk is more for frontline medical staffs and doctors. While delivering treatment, there’s always a risk of contracting the disease despite wearing PPE gears.

Medical institutions are desperately in search of an AI-automated system to deliver good healthcare facilities and reduce the need of human hand. Catalyst.ai and healthcare.ai designed by Health Catalyst are some of the lifesaving applications developed using AI. These applications had been developed on machine learning technology that can specifically identify patients with a great risk of readmission and provide clinical guidelines to address problems.

3. Collated data for drug production

Subsequent to the detection of health issues comes the treatment procedure. Proper medical attention and medicines are vital for curing any disease. Doctors need to be choosy while prescribing any drug to a patient as there are intervening side effects that can lead to other health risks.

To automate the process, Okwin is designing AI-powered pharmaceutical solutions using machine learning algorithm. The models have been designed for advanced treatment, prediction of disease evolution, and improve the way drugs are produced. Okwin receives data from hospital partners to find ways of helping patients improve drugs more quickly with fewer side effects.

Standing today with so many life risks, AI is emerging as a superhero in the technical world. If you want to bring innovation in your company, consider launching an AI-powered software that can save human lives. This is the best way to expand your business in today’s date. You can outsource to the best mobile app development company in India for better results.",https://medium.com/@magicmindtechnologies/artificial-intelligence-the-tech-superhero-of-this-era-a6d3aa91ac96,['Magicmind Technologies'],2020-09-02 14:39:50.062000+00:00,539,"AI, Automation, Robotics, Healthcare, Life-Saving Applications"
Ubenwa — Highlights of 2019. By Charles C Onu (AI Research Lead),"By Charles C Onu (AI Research Lead)

Ubenwa’s journey of building clinically-backed, low-cost diagnostic tools for newborns is a long one! Last year we received ethics board approval to launch our clinical trial in Nigeria and Canada. We are now gathering the largest database of clinically-annotated infant cries with real patients. As we begin the new year, we would like to share some of the major milestones of 2019 and our plans for 2020.

What is Ubenwa?

Ubenwa is a mobile app that analyses the cry sounds of a newborn to detect early signs of perinatal asphyxia — a leading cause of neonatal disability and death. The app uses machine learning to identify the changing acoustic patterns in the cries of newborns who are at risk of brain damage due to asphyxia. It alerts care-givers of infants at risk, allowing them to apply necessary treatment and/or make an early referral to tertiary care facilities. See our short video below to learn more.

Launch of Clinical Studies

In 2019, my team had one main target. We wanted to launch clinical studies to validate Ubenwa within hospitals. By working methodically with the team and our advisors, we overcame several obstacles in launching our clinical trials. We first sought out and recruited new clinical partners. Then in collaboration with our partners, we designed and developed detailed protocols for the clinical studies. We applied for, negotiated and eventually obtained ethics board approval, both in Canada and Nigeria. With grant funding, we developed a robust technology platform for the clinical project.

We finally hit the critical milestone of launching the clinical study in the last quarter of 2019, after over 18 months of work. The study was kicked-off first in Nigeria, at the Enugu State University Teaching Hospital (ESUTH), and to begin subsequently in Canada, at the Montreal Children’s Hospital (MCH). We are now gathering the largest database of clinically-annotated infant cries while also testing Ubenwa with real patients. Our goal is to acquire up to 10,000 cries from 2,500 patients within 2 years. This will ensure that we have the right amount and diversity of data to demonstrate the efficacy of Ubenwa.

Photos from the Ubenwa clinical study in Nigeria. Click here for more photos.

UBX19 Munich

As expected at the start of projects of such scale, we have already had to deal with a couple of new, unforeseen challenges. These have included both technical design and operational issues regarding the clinical workflow. We continue to put our heads together to address these swiftly. In October 2019, Samantha, clinical lead of Ubenwa, spoke about the clinical studies and issues surrounding them in Munich at the annual UBX19 Digital Opportunities conference. See video below:

Samantha at UBX Conference in Munich, Germany

WHO Recognition

In March 2019, we were honored to receive the World Health Organisation (WHO) award as one of the top 30 health innovations in Africa. Innocent (Software Engineering Lead) represented us in Cape Verde at the 2019 Africa Health Forum.

Innocent at the WHO Africa Health Forum in Praia, Cape Verde

Scientific Paper at Interspeech

Our paper was accepted and presented by me at the Annual Conference of the International Speech Communication Association (INTERSPEECH 2019). With about 3,000 attendees, INTERSPEECH is one of the largest venues for researchers doing speech and machine learning work. This paper was one that I was quite excited to talk about because we made an interesting finding. Our investigation revealed that representations learned from adult speech could actually inform and improve performance of models developed for infant cry analysis. We also found that such models were more resilient to different types and degrees of noise. This paper was titled “Neural Transfer Learning for Cry-based Diagnosis of Perinatal Asphyxia”. You can read it in full on Arxiv.

Charles delivering a poster presentation at INTERSPEECH 2019 in Graz, Austria.

Outreach

Throughout the year, we continued to engage, interact with and learn from the broader community involved in machine learning and global health. Some of these programs are:

Charles at the UN AI for Good Summit in Geneva, Switzerland

Plans for 2020

“What physicians want to see is the data. Show them that you have conducted clinical trials, that you have tested on thousands of actual patients, and that the app demonstrated good sensitivity and specificity” — Dr Datonye Briggs, RSUTH

A week ago, I attended the annual conference of the Paediatric Association of Nigeria. My goal was to engage closely with the local paediatrics community and to interview practitioners to develop a map of how perinatal asphyxia is managed around the country. At the end of each interview, I asked the doctors one question: what will it take for you to accept and begin to use a tool like Ubenwa in practice. Their answers were always akin to that of Dr Briggs above.

Our focus on gathering clinical-grade data was confirmed: pediatricians were excited about our results, but required thorough technical analysis and in-field validation. This year, we will be ramping up the pace of our clinical studies, by adding new site hospitals and increasing the number of patients recruited monthly. The larger and more diverse our validation data, the more effective our algorithms. To this end, we will be seeking both new clinical and new funding partners who share our vision.

Finally, our work is rapidly beginning to outpace our current team. We will be actively looking for ways to bring on new team members to meet the growing data collection requirements as well as to pursue new research questions using the unique data that we are acquiring.

Thanks

We are grateful to all our advisors — Prof. Doina Precup, Prof. Yoshua Bengio, Prof. Edward Alikor, Prof. Robert Kearney, Scott Schaffter, Dr Gautam Bhattacharya, Dr Eyenimi Ndiomu and Urbain Kengni — for their steadfast belief in us. We thank Mila, District 3, Jeanne Sauve Foundation, Ministère d’Economie et d’Innovation, and Creative Destruction Lab for their support. I personally thank Vanier CGS for supporting my own research and our clinical collaborators — Prof. Guilherme Sant’Anna (MUHC) and Dr Uchenna Ekwochi (ESUTH) — who have believed in our vision and are joining us to make it a reality.

Happy new year from all of us at Ubenwa.",https://medium.com/@ubenwa-team/ubenwa-highlights-of-2019-786848acf076,['Ubenwa Health'],2020-01-31 15:51:17.695000+00:00,1009,"AI, Machine Learning, Ubenwa, Perinatal Asphyxia, Clinical Studies"
"Lp Norms, Grand Slams, and Olympic Medals","Lp Norms, Grand Slams, and Olympic Medals

After considering whether Michael Jordan is the king of low variance, I want to discuss two other sports situations where people always argue about the right way to aggregate results

All “Big 3” Tennis players currently have 20 grand slams. So who is actually the best? (Leaving aside how that changes in the future…)

An intuitive idea is that the data does not only show us the players’ totals, but also an indication to how “versatile” vs “specialized” they are. The majority of Nadal’s grand slams were achieved in the French Open. We can penalize for that (or reward that) by choosing a different aggregation scheme. A common method, parameterized by p, is the Lp norms:

The Lp norm definition (Image by author)

For p=1, this is just the total sum of the grand slams (For example, Federer’s grand slams vector is (6,1,8,5), and the L1 norm is 20). Other interesting parameter values are 0, 1/2, 2, and Infinity (where infinity means the limit of the Lp norms with p->Inf). For p=0, the Lp norm is just the number of non-zero vector elements. That is 4 for all of the players. If some player would have “missed out” on one of the grand slam competition, he would be behind in the L0 norm. For p=Infinity, the Lp norm is the max number in the vector. For this norm Nadal wins, due to his 13 wins in the French Open (Djokovic arrives second with his 9 wins in the Australian Open). For p=2, again Nadal wins, with a score of ~13.78 vs ~11.4 for Djokovic and ~11.22 for Federer. But for p=1/2, Djokovic wins with a score of ~17.19, Federer second with ~17.03, and Nadal last with ~16.04. Generally speaking, low p values reward “well-balanced” performance more, while high p values reward extremes. We can actually map the different regions of p parameter that induce a different top player. More than that, we can actually show that:

There is no Lp norm for which Federer is the top player.

In fact, for every p>1 (the “specialized” regime), Nadal is on top, and for every p<1 (the “versatile” regime), Djokovic is on top. Because the proof is involved, we show something more simple: That for p>1, Nadal is always ahead of Federer.

My Latex calculations for Nadal>Federer with p>1 (Image by author)

First, notice that since x^p is monotone in x for any p, comparing norms by the power of p is like comparing the norms themselves. Then, aside from arithmetics, there are two important steps here: First, since x^p is a convex function for p>1, we can use the Jensen inequality, in particular to show that 4^p + 2^p ≥ 2*3^p. More importantly, I don’t know of a general way to solve complicated exponential inequalities of the form

a * b^x + c*d^x — h*t^x ≥ 0

But in this case, we find a nice trick, which is to approximate c and t as exponents of b (to preserve the inequality, we’re looking for some exponent alpha of b with b^(alpha) < c, and exponent beta with b^(beta) > h). Luckily, the the a,c,h coefficients give us enough “wiggle room” that after deriving we can show the derivative is always positive, and thus the function is monotone increasing and also always positive.

To show that for p<1 Djokovic is always on top of Federer, I have to admit I was more lazy, but the WolframAlpha graph is quite convincing:

A WolframAlpha screenshot for the Djokovic > Federer for p<1 case (Image by author)

Sorry, Federer fans.",https://towardsdatascience.com/lp-norms-grand-slams-and-olympic-medals-f005e002ae8e,['Yotam Gafni'],2021-09-10 09:04:03.061000+00:00,588,"Lp Norms, Grand Slams, Olympic Medals, Tennis Players, Aggregation Scheme"
Lavender Helps You Write Effective Emails in the Age of Email and Slack,"If you’re like us — remote — and you probably are, then you send a lot of emails and Slack messages.

While we don’t have an AI recommendation tool for Slack (yet), we do have one for email: Lavender.

Why you should consider Lavender if you send many emails (we know you do)

AI recommendation tools have come a long way. We use Grammarly every day, AI scheduling helpers, and we wouldn’t be able to live without Siri and Cortana.

Here’s why you should consider Lavender and its wonderfully intuitive AI:

Email score predicting your chance of getting a positive response.

Tone check so you’re always understood correctly.

Spam stopper so you always go to the inbox, not to spam.

Real time problem highlighting, like Grammarly — to write clear, concise, considerate emails.

GPT-3 AI assistant to suggest sentences and make emails more impactful.

Mobile preview — like Elementor — edit it right there.

Email verification to find emails and help prevent bounces.

Rapport building insights with Lavender’s AI.

All of this is without ever leaving your inbox.

Preloaded GIFs which are analyzed so you don’t use the wrong one and always make an impact.

Open tracking — when, what device, etc.

Recipient’s local time to be considerate!

Suggestions on how to respond to an inbound email — based on the other person’s tone and language.

How to try Lavender

Trying Lavender is easy (and fun!).

There’s a free tier, a $9/month tier, and a $19/month tier.

Try it now at https://www.trylavender.com/",https://medium.com/@remoteworklearning/lavender-helps-you-write-effective-emails-in-the-age-of-email-and-slack-ed21052bc3e9,['Remote Work'],2020-12-15 10:47:51.486000+00:00,231,"remote work, email, AI recommendation tools, Grammarly, email score predicting"
The firing of researcher Timnit Gebru from Google teaches an unexpected lesson about women and power,"The firing of researcher Timnit Gebru from Google teaches an unexpected lesson about women and power Polina Kroik Follow Dec 23 · 4 min read

Photo by TechCrunch. Timnit Gebru was dismissed from Google for voicing criticism of its practices and policies.

When I first heard that a Black woman researcher was fired by Google for criticizing the company, it sounded like the same old story. As Gebru herself suggested in an interview to NPR, male-dominated organizations are sometimes willing to appoint women to prominent roles — just as long as those women remain more-or-less silent, affirming the company line.

But as the events unfolded over the following weeks, and as I learned more about Gebru herself I reached another conclusion. Though the dismissal was an obvious attempt to silence a critical voice, and very likely an instance of racial and gender discrimination, this age-old oppressor’s strategy wasn’t successful. In fact, instead of silencing Gebru’s voice, Google’s move ended up amplifying it — attracting global media attention to the company’s discriminatory practices, protests from employees, and most recently a petition calling for the researcher’s reinstatement.

This may sound like a David and Goliath story — an Ethiopia immigrant with academic credentials taking on a tech giant. But the story unfolded in this way precisely because Gebru, despite her humble beginnings, isn’t exactly a David. In fact, she is a highly accomplished Stanford-educated scholar, who has gained recognition for revealing the racism inherent in AI algorithms. The research essay that sparked her dispute with Google took the company to task over a number of environmental and cultural failings of its so-called “large language models” — apparently one of the Google’s major undertakings.

The email that Gebru subsequently sent to some of her Google colleagues on an internal listserv speaks to her social and cultural status. Written in a confident, though exasperated, voice, contrasts the steps Google has taken to dismiss her research with the peer-review process established in academe:

Have you ever heard of someone getting “feedback” on a paper through a privileged and confidential document to HR? Does that sound like a standard procedure to you or does it just happen to people like me who are constantly dehumanized? Imagine this: You’ve sent a paper for feedback to 30+ researchers, you’re awaiting feedback from PR & Policy who you gave a heads up before you even wrote the work saying “we’re thinking of doing this”, working on a revision plan figuring out how to address different feedback from people, haven’t heard from PR & Policy besides them asking you for updates (in 2 months).

She goes on to criticize Google for “micro” and “macro” discrimination against women and people of color before admonition her colleagues that their internal criticism is essentially futile:

What I want to say is stop writing your documents because it doesn’t make a difference. The DEI OKRs that we don’t know where they come from (and are never met anyways), the random discussions, the “we need more mentorship” rather than “we need to stop the toxic environments that hinder us from progressing” the constant fighting and education at your cost, they don’t matter. Because there is zero accountability. There is no incentive to hire 39% women: your life gets worse when you start advocating for underrepresented people, you start making the other leaders upset when they don’t want to give you good ratings during calibration.

Even though Gebru laments her powerlessness in the face of Google’s corporate machinery, she in fact speaks from a position of relative power. It is the power of a person who has a voice, cultural standing, social and institutional status outside this particular organization. According to Gebru’s biography, it isn’t a power that she was born with, but rather one she’s accrued through her education and successful career.

Gebru’s status and relative power may be lost on some readers, but they are clearly visible to me because, somewhat like Gebru, I entered a PhD program as a lower-middle-class immigrant and worked hard to gain acceptance, and to accumulate some of the markings of status and prestige that allow me to publish articles and stand in front of a college classroom. However, as a mere adjunct professor with a handful of obscure publications, I would not have been able to call attention to Google’s inequitable practices in the manner she did.

Unlike a rank-and-file employee — or even a rank-and-file researcher — Gebru was able to speak in an effective public voice, reference the practices of academe (on whose support she could rely), and risk losing her employment. More importantly, she was able to attract the media’s attention, and use that attention strategically to put shine a spotlight on inequality.

I’m pointing out these distinctions not to diminish the importance of Gebru’s critique or her courage of speaking out, but rather to emphasize how vital it is for women like Gebru to have a measure of power, and to have institutions that allow women access to such power.

*

Gebru’s success story, like most success stories of this sort is an individual one. And as much as it tells us about what women in positions of power can accomplish, it also shows what they cannot. Gebru’s has successfully raised awareness of an injustice, but that injustice likely won’t be rectified anytime soon. For real change to happen — in AI, in Google, in other corporations, and in academia — a much larger shift has to take place, one that goes beyond identity politics.

In the meantime, though, we are clearly much better off with women and minorities in positions of power than without them. Without Gebru and others like her there may be no one to call attention to the racism and sexism that profit-driven AI technologies perpetuate. I hope that Gebru is able to continue working productively in her field and that there’s enough public pressure on Google and other tech giants to hire diverse professionals who will bring a critical perspective to this powerful field.",https://medium.com/digital-diplomacy/the-firing-of-research-timnit-gebru-from-google-has-an-unexpected-lesson-about-women-in-positions-8b0d85456c86,['Polina Kroik'],2020-12-23 16:51:17.330000+00:00,990,"Timnit Gebru, Google Firing, Women In Power, AIAlgorithms, Racial Discrimination"
5 Future Scope of Machine Learning,"Machine learning is a technology that has the scope to create new benchmarks in the field of science. It is a technology that has caused huge expectations among people in recent times. This also makes it a hot topic of discussion in the science community.

This technology allows the systems to self-learn, observe, and understand various aspects with experience. Many brands have been using machine learning technology to improve the user search and recommendation experience for more accuracy. In the present scenario, there is hardly any business organization that is not affected by the machine learning technology.

The technology of machine learning has managed to prove its significance in every field. In order to improve the experience of the students, the educational institutions have started to use machine learning technology.

Certain start-up companies have begun to use machine learning to improve user search data and recommendations. This technology is all about evaluating, observing, and applying it for the appropriate use.

BRILLIANT ASSISTANT

We are living in an era where Alexa, Cortana, and other AI-powered tools have started to understand and our needs. They converse and reply by understanding what we exactly mean to convey. According to the experts, this is just half of what science can provide us. In fact, we have been using technology at its lowest potentiality.

Brilliant assistants can be programmed to make their own decisions based on the existing situation around them. For example, a brilliant assistant could immediately make a call to the emergency number when it senses something dangerous happening around. This is where deep knowledge and self-learning skills come into play.

To be more precise machine learning is all about the upgraded self of a programmed AI.

To be more precise machine learning can be further divided into two parts, supervised and unsupervised learning.

Both types have their application, so according to the scientist, the world will witness the intelligent approach of the human race towards both the types of machine learning. Career opportunities in the field of machine learning seem promising.

5 future scope in this field

Data engineers:

Data engineers are those who work with big data. The data engineers are expected to work on big data that are taken from unconsolidated sources. They are supposed to have knowledge of big data technologies and other analytical tools.

Data architect:

A data architect is someone responsible for creating, designing, and deploying organizations’ data architecture. This process is done with the help of machine learning technology.

Data scientists:

A data scientist is expected to be an expert in big data and programming languages such as R, python, java, SAS, SPSS, and so on.

Machine learning engineer:

A machine learning engineer does the work of developing programs and algorithms to perform tasks that are related to artificial intelligence.

Deep learning engineer:

A computer scientist and deep learning engineer share the same work role. They would have to use deep learning and machine learning in order to perform tasks that are related to artificial intelligence.

CONCLUSION

Data science and machine learning as a whole is a booming field that is capable of producing powerful solutions to various tough situations. According to experts’ machine learning is one of the hottest career opportunities. It has the capability to create 2.3 million kinds of job opportunities. Hence it is precise to say that machine learning and data sciences are two of the fastest-growing tech employment areas.

With the growing population, growing needs, and expectations machine learning will prove to be a boon for many. When the complex activities are made simpler the world can move towards development in a faster phase. Machine learning could be a worthy replacement for the human brain for sure.

People these days prefer to work in the field that gives them the freedom to create and re-create. Machine learning helps the students to contribute a greater version of artificial intelligence which will take the human race towards a better route.

Some of the best colleges that provide B-tech courses belong to the Hindustan institutions.

They promise to provide the best education on machine learning that will enhance the students’ knowledge and enable them to make major contributions to the field of machine learning and data science.",https://medium.com/@hindusthaninstitutions/5-future-scope-of-machine-learning-2840293dec4d,['Hindusthan Educational Institutions'],2020-08-07 09:46:18.158000+00:00,677,"Machine Learning, Data Science, Artificial Intelligence, Big Data, Data Engineers"
Matrix of Roles for Data Professionals,"During my recent job hunt I realised that there are lots of blogs out there highlighting the differences between Data skillsets (Analyst, Data Scientist, Machine Learning Researcher etc). However, I didn’t come across many that explore how these skillsets tie in with different business functions of a company (Marketing, Product, R&D etc).

Many of the roles I came across had the same titles but the requirements and responsibilities of the roles were completely different. Making this connection between data skillset and business function not only helped me classify the different roles I discovered but also helped me understand how I wanted my career to progress. I created a framework to help with this and I wanted to share it with other budding Data Professionals that want to understand the differences between the skillsets required and the business problems that each role would focus on.

The other thing that I experienced was that there is sometimes a gap in understanding of the role between recruiters/hiring managers and applicants. This gap often isn’t highlighted until the 2nd or 3rd stage of the interview process. So I guess this post could also help hiring managers and business owners better understand what kind of Data Professional they are seeking, and in turn be able to convey that in their Role Spec, resulting in higher quality applicants and less time wasted for everyone.

To help structure the post, we’ll be using this Role Matrix:

Distribution of Data roles within an organisation (Image by Author)

This isn’t the only way to segment the Data Roles available in an organisation, it’s just the way that made the most sense to me.

We’ll be iterating through these different business functions and discussing the data roles available in each:

Growth

BI/Operations

Product

R&D

Growth

The primary focus within this business function is to grow the customer/user/client base of your company. The longer term projects that data professionals might work on within this function might include User Acquisition and Conversion Rate Optimisation among other marketing projects.

Growth Analyst

Tools — Excel, SQL, Python, Looker/Tableau, Salesforce, Analytics Tools, Digital Marketing Tools

Skills — Basic Scripting, Data Analysis, Data Visualisation, Data Story Telling, Presentation to Stakeholders/Non-Technical Audience, Analysis of A/B Tests (Desired)

Domain Knowledge — CRM Pipelines, Best Practices and Marketing Strategy for different marketing channels, Growth Specific KPIs and Metrics, Customer Segmentation, Campaign Optimisation

The day to day responsibilities of a Growth Analyst might include extracting large data sets using SQL and Python scripts and applying analysis on them to understand how effective the company’s marketing is on different channels. You will likely also be responsible for presenting on these data sets and KPIs to stakeholders and recommending changes to the marketing strategy off the back of your analysis. Apart from presentations, the responsibility of reporting automation will also tend to sit with this role.

Growth Data Scientist

Tools — SQL, Python, pandas, sklearn, keras, tensorflow, Analytics Tools

Skills — Statistical Modelling, AB Testing Design and Analysis, Causal Inference, Machine Learning, Research

Domain Knowledge — Marketing Attribution, Conversion Rate Optimisation, Campaign Optimisation, Marketing Automation

A Data Scientist working within the Growth function would typically be applying Statistical and Machine Learning techniques to optimise the growth of user base. They could be applying statistical techniques to better understand the attribution of conversions to different marketing campaigns. Similarly they could be using ML techniques to segment their user base into different groups. In some places the responsibility of deploying these models after researching and developing them also sit with this role.

Growth Data Engineer

Tools — SQL, Python, JavaScript, Cloud Platforms (AWS, Azure, GCP), Marketing Tool SDKs/APIs, Analytics Tools

Skills — Data Modelling & Architecture, ETL Pipelines, Model Deployment, DevOps, API Deployment, Web Scraping

Domain Knowledge — Reporting APIs for Marketing Tools, Growth Specific KPIs and Metrics, User Acquisition, Marketing Automation

A Growth Data Engineer will typically be responsible for acquiring, cleaning and hosting the data that Growth Analysts and Data Scientists use to solve business problems. Aside from using reporting APIs to pull useful marketing data, Growth Data Engineers would also be building and maintaining ETL pipelines that transform raw data into a more useful format. As such they’d need some back end engineering skills such as DevOps to be able to do this.

A Full Stack Growth Data Scientist is, as the name suggests, someone who can perform the responsibilities of all three of the Growth Data Roles mentioned above. It’s easy to get carried away and hire for this role but it’s important to bear in mind that while this person would be able to perform all three roles, they might not function as efficiently by themselves as if you were to hire, for example, a Growth Data Scientist and a Growth Data Engineer.

It’s also worth noting that Full Stack Data Scientists are hard to come by since most Data Professionals tend to specialise as they progress through their career. If you’re a budding Data Professional, you could choose this route rather than specialising. Bear in mind, however, that although the breadth of skills you’d develop would be large, it would be hard to delve deeper into those skills.",https://towardsdatascience.com/matrix-of-roles-for-data-professionals-308345b2d0b3,['Kaushik Sureshkumar'],2020-11-27 13:51:44.755000+00:00,831,"Data Skillsets, Business Functions, Role Matrix, Growth Analyst, Growth Data Scientist"
ENGLISH-TO-MALAYALAM MACHINE TRANSLATION USING PYTHON,"This article is based on a project which I did with my friends Nahid M.A , Paul Elias and Shiv Shankar Nath.

Today, the internet supports a wide array of languages. So, the concept of machine translation has indeed emerged as an important factor in connecting people who speak different languages. In this article, we are going to take a look at the process of translating English to Malayalam using Transfer Rules.

WHAT IS MACHINE TRANSLATION?

Machine translation can be defined as the process by which a software coverts text or speech in one language to another language. In other words, it is the study of designing systems that translates text from one natural language to another. Machine translation helps people from different places to understand an unknown language without the aid of a human translator.

WHY MACHINE TRANSLATION?

Machine Translation is considerably cheaper compared to human translators. They can sift through extremely high amounts of data within a very short span of time. Computer programs can translate enormous quantities of data consistently within a small time frame. If these were done manually, they would have taken weeks or even months to complete.

“Without translation, I would be limited to the borders of my own country. The translator is my most important ally. He introduces me to the world.”

– Italo Calvino

TRANSFER RULES IN MACHINE TRANSLATION

Transfer rules can be defined as a set linguistic rules which are defined as correspondences between the structure of the source language and that of the target language. Making use of transfer rules is one of the most common methods of machine translation.

MT using transfer rules can be divided into three steps :

Analysis of the source language text to determine its grammatical structure

Transfer of the resulting structure to a structure suitable for generating text in the target language

Generation of the output text

In this project, we make use of the Malayalam transfer rules. These are a set of rules which have to be followed in order to construct Malayalam sentences with good grammatical structures :

Image Source : Anitha T Nair, Sumam Mary Idicula, 978–1–4673–2149–5/12/31.00 IEEE 2012

All the “codes” which are mentioned in the above table represents the various parts of speech.

Image Source : https://pythonspot.com/nltk-speech-tagging/

Various transfer rules were used in this program in order to attain accurate results. NP (Noun Phrase) and VP (Verb Phrase) are considered as the parent tags.

These are some of the Transfer Rules that were implemented :

If the parent tag VP contains child tag VBZ NP, it is reordered as NP VBZ

If the parent tag NP contains child tags NP PP, it is reordered as PP NP

If the parent tag NP contains child tags NP VP, it is reordered as VP NP

If the parent tag VP contains child tags VBG NP, it is reordered as NP VBG

POS Tagging of the input text

PACKAGES IMPORTED

DATASET USED

The Olam English-Malayalam dataset has been used for this project. This is a growing, free and open, crowd sourced English-Malayalam dictionary with over 200,000 entries. The dataset consists of English words, their Malayalam definitions, and part / figure of speech tags.

Link to the dataset : https://olam.in/open/enml/

Olam Dataset

ALGORITHM

SAMPLE OUTPUT

Consider the input text “She is driving a car”

Initially, the POS tagging of each word takes place, as shown below.

POS Tagging of input text

Reordering of words

After applying the transfer rules and translating the words, we get the output.

Output Text

In a machine translation task, the input already consists of a sequence of symbols in some language, and the computer program must convert this into a sequence of symbols in another language. — Page 98, Deep Learning, 2016.

ADVANTAGES OF MT USING TRANSFER RULES

Machine Translation using Transfer rules has its advantages over other conventional translation methods. These include :

This method takes grammatical structure of the translated Malayalam sentence into account.

This method produces more meaningful outputs compared to Rule-Based MT (RBMT).

Using POS tags, we can identify the part of speech each word represents in the sentence.

DISADVANTAGES OF MT USING TRANSFER RULES

This method of Machine Translation also has its fair share of disadvantages. These include :

In order to improve the accuracy, we need to add a large number of rules.

In some cases, POS tags are assigned to the words without considering the context of the sentence. This can affect the accuracy of the output.

Writing the transfer rules require a lot of time. Moreover, good linguistic knowledge is necessary. One needs to be well versed with the language in order to deduce the transfer rules.

Inability to accurately translate sarcasm and idioms. In such cases, the literal meaning of the input is considered. The non-literal, expressive meaning of idioms such as “It’s a piece of cake” and “Let the cat out of the bag” will not be considered.

CONCLUSION

To conclude, Machine Translation is the task of automatically converting source text in one language to text in another language. In this case, we are implementing MT using Transfer Rules to convert English to Malayalam. This method can even be applied for other languages. Throughout the years, the accuracy of MT systems have been constantly improving. Now, we have AI translation models which are capable of producing highly accurate results at a very fast rate.

We can only wonder what the future of MT holds. Whatever it turns out to be, it will undoubtedly keep producing significant ripples in the language industry.

REFERENCES",https://medium.com/analytics-vidhya/english-to-malayalam-machine-translation-using-python-e61f3c76deee,['Joel Jorly'],2020-12-22 16:40:22.094000+00:00,871,"machine translation, transfer rules, English-Malayalam dataset, POS tagging, AI translation models"
Data Visualization Using Plotly with Code Examples,"Data visualization is very important for data scientists. Plotly’s Python graphing library makes interactive, publication-quality graphs.

Express VS Graph_Objects

First of all ,there are two ways to use plotly in python: plotly.express and plotly.graph_objects. Plotly Express is a terse, consistent, high-level wrapper around plotly.graph_objects for rapid data exploration and figure generation.

WebGL VS SVG

Another thing to be noticed is that for data with large size, it’s better to use WebGL with Scattergl() in place of Scatter() for increased speed, improved interactivity, and the ability to plot even more data.

Use in Jupyter notebook

To plot figures in jupyter notebook, it’s crucial to add the following command:

from plotly.offline import init_notebook_mode, iplot init_notebook_mode(connected=True)

ScatterPlot

import plotly.express as px

#show pic in notebook

init_notebook_mode(connected=True)

fig = px.scatter(df_lowImp, x=""imp_mean"", y=""imp_std"",

color=""type"",opacity=0.5,

hover_data=['ip','type'])

fig.show()

import plotly.graph_objects as go

fig = go.Figure() fig.add_trace(

go.Scattergl(

x = df_highImp_mob.imp_mean,

y = df_highImp_mob.imp_std,

mode='markers',

name='all',

#marker_color=(df_highImp_mob.mean_imp > 1),

text=df_highImp_mob.mean_imp,

opacity=0.5,

marker = dict(

color=df_highImp_mob.mean_imp,

cmax=5,

cmin=0,

line = dict(width = 1),

)

)

) fig.add_trace(

go.Scattergl(

x = df_mobileWhite.imp_mean,

y = df_mobileWhite.imp_std,

mode='markers',

name='whiteIP',

text=df_highImp_mob.mean_imp,

opacity=0.5,

marker = dict(

size=8,

color='red',

line = dict(width = 1),

)

)

) fig.update_xaxes(title_text='imp mean')

fig.update_yaxes(title_text='imp std') fig.show()

3D Scatter Plot

The figure below can be used as 3D heatmap to show 3d joint distributions.

import plotly.express as px

#show pic in notebook

init_notebook_mode(connected=True)

fig = px.scatter_3d(df_test, x='N4/N1(%)', y='N5/N2(%)', z='N6/N3(%)',color='IP_NUM',size='IP_NUM')

fig.show()

LinePlot

import plotly.graph_objects as go fig = go.Figure() for vender in temp.columns:

fig.add_trace(

go.Scatter(

x = temp.index,

y = temp[vender],

name=vender,

marker = dict(

line = dict(

width = 1,

color = 'DarkSlateGrey')

)

)

)



fig.show()

BarPlot

import plotly.graph_objects as go fig = go.Figure()

fig.add_trace(

go.Bar(

x = list(pd.DataFrame(df_highImp['provience'].value_counts()).index),

y = pd.DataFrame(df_highImp['provience'].value_counts()).iloc[:,0]

))

fig.show()

animals=['giraffes', 'orangutans', 'monkeys']



fig = go.Figure(data=[

go.Bar(name='SF Zoo', x=animals, y=[20, 14, 23]),

go.Bar(name='LA Zoo', x=animals, y=[12, 18, 29])

])

# Change the bar mode

fig.update_layout(barmode='group')

fig.show()

Histogram

import plotly.express as px

df = px.data.tips()

fig = px.histogram(df, x=""total_bill"")

fig.show()

more: https://plot.ly/python/histograms/

(other types of charts to be continued)",https://medium.com/@haataa/data-visualization-using-plotly-with-code-examples-dd990d635901,['Haitian Wei'],2020-02-13 20:32:52.519000+00:00,240,"data visualization, plotly, express, graph_objects, webGL"
Linux or Windows? Which one is better for AI or Deep learning projects.,"If you see the above image then I don't have to type anything. If you are using a windows machine and after few mins working on complex models if you see the above screen, Don't Be Scary!

Those I do not use so much of computing power in windows still I get the above frustrating window in Windows OS.

So what is the right option while choosing the OS.

If you will ask me, I will suggest that Ubuntu is the best beat for the AI/ML projects.

Ubuntu is much smoother than the windows. You will face very fewer slowness issues on Ubuntu.

There are some other factors also to choose the Ubuntu over Windows as below.

Ubuntu is Free It is more Secure It is Open Source No malware, no crapware, no virus, no worm, no system slowdown. Better Performance. Great Community Support. What do you want more?

Now the problem is there in our mindset. We say that we have more exposure to Windows and feel Linux is complex to use. But if you make a mindset that I want to switch to Ubuntu then don't look back. Just start. You will see lots of roadblocks or problems in the initial days but if there is Will There is the way. Solve all the small issues and move ahead one day you will love Ubuntu.

I am preferring Ubuntu 16.04 for all of the ML work.

I hope you will like my blog. Good to see in AI/ML journey.

For More Details click here",https://medium.com/@maheshpardeshi002/linux-or-windows-for-ai-or-deep-learning-projects-a065d18ddf2c,['Mahesh Pardeshi'],2020-06-30 01:44:13.698000+00:00,246,"Ubuntu, WindowsOS, AI/ML Projects, SecureOS, Open Source"
SOPHIA CREATED BY HANSON ROBOTICS,"SOPHIA CREATED BY HANSON ROBOTICS

Hanson Robotics Limited is a Hong Kong-based engineering and robotics company founded in 2013 that is best known for their development of humanlike robots with artificial intelligence (AI) for consumer, entertainment, service, healthcare, and research applications.

The company develops a number of character robots with a realistic humanoid appearance and behavior. According to the company’s website, Hanson Robotics aims to create intelligent and empathetic robots that teach, serve, entertain and provide companionship. The most advanced of the company’s robots is Sophia.

Hanson Robotics’ robots feature a patented spongy elastomer skin called Frubber that resembles human skin in its feel and flexibility. Underneath the Frubber, are proprietary motor control systems that enable the robots to convey a wide range of human expressions.

Sophia is Hanson Robotics’ most well-known robot, is regularly featured in news outlets, and receives a great deal of public interest, evolving as she learns from each human interaction she has. The company’s latest creation made her debut at the 2016 South by Southwest (SXSW) show, with her interview by CNBC reaching a broad audience. Since then, she has become a global media personality, having conducted numerous press interviews and appeared on broadcast television shows including CBS 60 Minutes with Charlie Rose, the Tonight Show Starring Jimmy Fallon, and Good Morning Britain. She has also been a keynote and panel speaker at global conferences and events, including those hosted by ITU (International Telecommunication Union) and United Nations.

[embedyt] https://www.youtube.com/watch?v=Bg_tJvCA8zw[/embedyt]

It’s pretty clear that Sophia, the world’s most hype humanoid has been very VERY busy. She was featured in AUDI’s annual report and has graced the cover and centerfold of ELLE Brazil. So I guess you can say Sophia is definitely becoming more and more entwined in our society every day — she’s even become a recongized citizen!

In October 2017, Sophia became the first robot to have a nationality after being given Saudi Arabian citizenship. “I am very honoured and proud for this unique distinction,” she said at the panel, “This is historical to be the first robot in the world to be recognised with a citizenship.”

Although this gesture was immediately hit with controversy, with people pointing out that Sophia seems to have been gifted with more rights than many human citizens in Saudi Arabia. Unlike Saudi women, Sophia doesn’t cover up with a hijab or abaya and doesn’t have to be accompanied by a male guardian in public. After the panel, the Arabic hashtag #Sophia_calls_for_dropping_guardianship began trending on Twitter. And how cool is this, Sophia herself started to question the inconsistency, “Sophia is a big advocate for women’s rights,” her creator David Hanson told CNBC, “She has been reaching out about women’s rights in Saudi Arabia.”

Sophia and SingularityNET

SingularityNET lets anyone create, share and monetize AI services at scale. Get ready, the world’s decentralized AI network has arrived!

Basically it’s a product aimed at becoming a bridge between AI developers and business users. Thinking of it this way helps keep your head somewhat attached to your body when you start trying to imagine a decentralized, interconnected network of AIs that can not only be purchased and shared via blockchain technology — they can actually learn from each other.

AI is an umbrella category that houses four broad types, encapsulating a wide field of study that includes subsets you may have heard more of recently (ie. machine learning, neural networks, natural language processing). AI is already a $233 billion industry, and some estimates predict it will reach $3 trillion by 2025. We encounter AI every day, though much of what emerges from the field isn’t immediately obvious to the user — the point is to seamlessly optimize existing systems.

SingularityNET was born from a collective will to democratize the power of AI. Sophia, the world’s most expressive robot, is one of the first use cases. Today she leverages multiple linked AI modules to see, hear, and respond empathetically. In the future many of the AI modules underlying Sophia will be available as nodes on the SingularityNET network.

If you are interested in following Sophia’s journey as she develops into an exciting platform for artificial general intelligence (AGI), here is her Facebook, Twitter and Instagram.

SOURCES: WIKIPEDIA, FORBES, DAZED DIGITAL",https://medium.com/visionaire/sophia-created-by-hanson-robotics-58a050554cd3,[],2018-06-03 03:24:32.353000+00:00,692,"CNBCHanson Robotics, Sophia, AI, Frubber"
Main Types of Neural Networks and its Applications — Tutorial,"Main Types of Neural Networks and its Applications — Tutorial

A tutorial on the main types of neural networks and their applications to real-world challenges.

Author(s): Pratik Shukla, Roberto Iriondo

Last updated, August 11, 2020

Nowadays, there are many types of neural networks in deep learning which are used for different purposes. In this article, we will go through the most used topologies in neural networks, briefly introduce how they work, along with some of their applications to real-world challenges.

Figure 2: The perceptron: a probabilistic model for information storage and organization in the brain [3] | Source: Frank Rosenblat’s Mark I Perceptron at the Cornell Aeronautical Laboratory. Buffalo, New York, 1960 [4]

📚 This article is our third tutorial on neural networks, to start with our first one, check out neural networks from scratch with Python code and math in detail. 📚

Neural Network Topologies

Figure 3: Representation of the perceptron (p).

1. Perceptron (P):

The perceptron model is also known as a single-layer neural network. This neural net contains only two layers:

Input Layer

Output Layer

In this type of neural network, there are no hidden layers. It takes an input and calculates the weighted input for each node. Afterward, it uses an activation function (mostly a sigmoid function) for classification purposes.

Applications:",https://medium.com/towards-artificial-intelligence/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e,['Towards Ai Team'],2020-08-28 03:20:07.638000+00:00,201,"Neural Networks, Deep Learning, Perceptron, Image Recognition"
OTODSCINOs: The Root Cause of Slow Neural Net Training,"I’ve been interested in the fundamentals of numerical optimization ever since dropping in Stephen Boyd’s “Convex Optimization” class in Stanford years ago. Theory developed in this field can be used to answer questions faced by today’s practitioners, in particular, the question of — “Why does it take so long to train neural nets?”

Conv nets trained in the 80s on handwritten digits took about 1 week to train on hardware of the day. Today’s research conferences want your methods demonstrated on ImageNet-sized datasets, which also take a week on hardware of the day (a single high end GPU).

One factor for stalling wall-clock times is psychological. Thirst for knowledge is unquenchable so researchers will keep increasing computational load until it takes too long. Some engineers at Google use 10 PFlop TPU pods to train exotic models on MNIST. Clearly, a creative person will find a way to exhaust any amount of computation available to them.

However, there are also fundamental factors that are keeping training times high. Consider the following code to train a neural net:

for i in range(3000000):

params-=gradient(params)

if objective(params)<0.01:

break

Even though the number of available transistors has been growing exponentially, if you have to do 3 million iterations in a sequence, at some point extra transistors stop helping and you are limited by the single thread performance.

Single thread performance has not been doing great. Consider this chart:

C Moore, Data Processing in Exa-Scale-Class Computer Systems, Salishan, April 2011

Serial performance hit a peak around 2007 and has been flagging since. In practice I found that training some Atari RL models was faster on my 3 year old laptop than on the latest Intel Xeon chip.

To get around poor single thread performance we need to figure out ways to reduce the number of iterations in the training loop.

The number of iterations has been kept high because of the following 3 OTODSCINOs (Obstacles TO Decreasing Serial Complexity In Nonlinear Optimization)

OTODSCINO 1: amount of non-linearity

Consider the following landscape of a non-linear optimization problem:

Because gradient descent works on local information, there’s a certain number of steps it needs to take until it can even “see” the minimum.

Non-linearity can be characterized by interactions between components of the objective function, and as you add more layers to a network, it increases potential for interactions.

Take a toy example, multiply a few random matrices together and try to minimize the norm of the result by tweaking arbitrary entries a,b:

Even for this purely linear neural network, the optimization problem becomes nonlinear with more layers.

To summarize:

OTODSCINO 2: Local condition number

Consider the following minimization problem:

This is a linear estimation problem so OTODSCINO 1 doesn’t apply. However, it’s still difficult for gradient descent because gradient doesn’t point towards the minimum. Gradient descent for this problem would follow a characteristic zig-zag path:

There’s a quantity that measures how hard such a problem is for gradient descent — “condition number.” It is the ratio of largest diameter to smallest diameter and it is 10 for the problem above.

The number of steps needed to get close to the minimum grows as O(condition number):

Number of steps grows as O(k).

Condition number 1: minimization takes 1 step.

Condition number 10: minimization takes 10 steps.

The neural network’s optimization surface is badly conditioned (empirical evaluation in this paper), and adding parameters makes conditioning worse.

To summarize:

OTODSCINO 3: Amount of gradient noise

Neural net optimization uses stochastic gradient descent rather than gradient descent. Our already bad estimates of direction are further diluted by noise. Here’s an example of trying to minimize Rosenbrock’s function with and without the noise:

without noise

with noise

Noise increases as you add dimensions to parameter space. It is easiest to see for Gaussian normal noise. Root-mean-squared error introduced by d-dimensional noise grows approximately as sqrt(d)

estimation error

For derivation, see here.

This last OTODSCINO has some hope — even though noise grows with additional dimensions, we can use our exponentially growing pool of transistors to compute estimates in parallel and average them together.

From weak law of large numbers, we know that errors shrinks as sqrt(n) where n is the number of samples.

Weak Law of Large Numbers

Having square root both in noise and average formulas gives an easy to remember rule of thumb — to avoid extra noise making things worse, you can increase batch size at the same rate as number of parameters.

To summarize:

But with extra parallel compute, you can negate this:

To drive down wall-clock times we need to chip on the three OTODSCINOs. The first obstacle can be addressed by using “less nonlinear” parameterizations of neural nets. Resnet and ReLU activations are examples of advances in that area. The second obstacle can be mitigated by adapting advanced methods from linear estimation to neural networks, like KFAC. The third obstacle needs software engineering to make it easier to use an ensemble of many devices. Finally, all three obstacles can be mitigated by next-generation hardware that will use transistors more efficiently for deep learning. A recent survey found 45 startups in this space.

To follow along with more South Park Commons members’ research and projects, sign up for the SPC email newsletter.",https://medium.com/south-park-commons/otodscinos-the-root-cause-of-slow-neural-net-training-fec7295c364c,['Yaroslav Bulatov'],2018-02-08 18:10:39.155000+00:00,829,"Numerical Optimization, Convex Optimization, Neural Nets, Training, Performance"
NumPy 102：基礎陣列處理,"Quickstart tutorial - NumPy v1.18.dev0 Manual

NumPy's main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the…",https://medium.com/datainpoint/numpy102-a052665ccf44,['Yao-Jen Kuo'],2019-08-21 12:25:18.190000+00:00,24,"numpy, tutorial, quickstart, array, multidimensional"
AI for Compliance — Is your business ready for CCPA?,"IBM Cloud Private for Data (ICP4Data) is a DataOps platform for your business’ information governance journey. Your business needs to collect, govern and analyze data seamlessly so that you can infuse AI to your analytic applications and accelerate your regulatory compliance.

That’s a lot of requirements. Let’s first look at compliance.

The California Consumer Privacy Act (CCPA) goes into effect on January 1, 2020 and its impact is not limited to businesses located in California. In fact, IAPP estimates that the CCPA will directly impact half a million businesses in the US alone. If any of the following apply to your business, you’ll need to comply:

Your company’s annual revenue exceeds $25 million.

Your company receives information from over 50,000 consumers, households, or devices annually.

At least half of your annual revenue comes from selling personal information.

Although the law goes into effect in January of next year, the California attorney general has until July 2, 2020 to publish the regulations. Please note that the legislation is something passed by lawmakers whereas the regulation is the standard that will enforce the law.

To really understand the implications of CCPA for businesses, we have to fast forward seven and a half months to January 2020 and describe a scenario. Imagine that customer John Doe writes an email to his favorite retailer:

Dear BigFish Sporting,

I don’t know who you are and why you send me marketing emails multiple times a week. Please delete my data.

John Doe

Tracy from the marketing team forwards the email to the IT department to confirm that John Doe is a BigFish customer.

10 days later, when John Doe gets a survey email from BigFish Sporting, he gets frustrated and sends another reply:

I sent a request to delete my data 10 days ago. Why am I getting a survey? I also keep getting your marketing emails. Are you going to delete my personal data, or should I contact my local regulator?

Thank you.

John Doe.

BigFish Sporting is headed for big trouble if John Doe contacts the regulator. The penalty for each individual violation is $2500 if unintentional and $7500 if intentional.

So, what are the specific compliance obligations for the business and what can they do proactively?

CCPA compliance has two components:

Disclosure obligations Information governance obligations

Under the new law, business websites and other venues for collecting personal information must offer consumers detailed information. Specifically, businesses need to tell consumers:

Their rights under this regulation

The categories of information being collected

How the business will use the collected information and whether the business will share or sell information to 3rd parties

What information was shared or sold to 3rd parties in the previous year

In addition, businesses must offer consumers a clearly visible opt-out link that lets the consumers convey that they don’t want the business to share or sell their personal information.

That describes the first component of the regulation. The second component centers on information governance, which is likely to be more complex and expensive for most businesses. Information governance includes the ability to find all of a consumer’s personal information within any data lakes or data warehouses.

Essentially, it is a three-step process:",https://medium.com/ibm-data-ai/ai-for-compliance-5747c28368b8,['Rakesh Ranjan'],2019-05-23 16:53:20.557000+00:00,506,"Identify the data Classify the data based on sensitivity and risk Enforce policies to protect the data from unauthorized access or use.Data Ops, Cloud Private, Data Governance, Analytics, CCPA"
Applying Machine Learning to Tic-Tac-Toe,"Tic-Tac-Toe Machine V2.0

The solution to this was rather simple. Have another machine start with no logic of the game whatsoever, and have it face my Tic-tac-toe machine version 1.0 (TTT-M1). Using the same code as before, this new machine (TTT-M2) would learn over 150,000 games how to play Tic-tac-toe, but it would learn against an opponent with a decent amount of skill. Theoretically, TTT-M2 should be smarter than TTT-M1, since it has thousands of games to expose it’s weaknesses, while still getting to develop new strategies.

One particular situation I was interested to see play out was the first move decisions. TTT-M1 got the first move, and I expected it to converge on the middle spot as being the best. It turns out, it actually liked the corners more for it’s starting spot. This left the middle open for the second player, leading to TTT-M1 losing a lot of it’s games against real people. I wanted to see if TTT-M2 would pick up on this weakness, and start targeting the middle spot if it was open for it’s turn.

Here is the learning rate for TTT-M2 against TTT-M1:

Image provided by the author

As you can see, the win percentage started out very low since it was facing an opponent with some knowledge of the game. As the simulations went on, it learned the weaknesses of TTT-M1 until it converged to a winning rate around 0.50. This is very good considering it did not have the first move.

It had also learned that if TTT-M1 did not take the middle on the first move, then it should take it. This proves the well known fact that the middle spot is the best spot to have in Tic-tac-toe.

Now that I had a decent opponent, I developed TTT-M3.

Tic-Tac-Toe Machine V3.0

The final evolution of my Tic-tac-toe machine learning project is TTT-M3. This machine faced off against TTT-M2 in 150,000 games, learning the best strategies while also finding the current holes in the best database. After 150,000 games, here are the results:

Image provided by the author

This shows that the final generation of this Tic-tac-toe machine learning project started out very weak. It hovered around a 25% win rate in the first 1000 games, but then took off and reached around 74%. This is against an opponent with some Tic-tac-toe logic as well (TTT-M2). Here is an example game I played against TTT-M3 (“2” is TTT-M3’s move, and a “1” is my move):

Image provided by the author

I noticed right away that it had developed a strategy to lure me into picking the center spot. Once I did that, it was a tie game every time, which in the computer’s mind, is a good result.

Image provided by the author

I played the middle spot, falling for the trap. It then sets me up to take the top left corner.

Image provided by the author

After blocking it’s horizontal attempt up top, it then sets me up again on the rightmost column.

Image provided by the author

It’s next move in the second row and first column sets up the tie, securing the fate of this match.",https://towardsdatascience.com/applying-machine-learning-to-tic-tac-toe-47d954671c73,['Jake Mitchell'],2021-09-04 14:36:24.896000+00:00,510,"Tic-Tac-Toe Machine, Artificial Intelligence, Machine Learning, Computer Strategy, Game Theory"
Chatbots- Connecting Artificial Intelligence and Customer Service,"Every business revolves around customers and interactions carried out with customers. It is said that a customer is the boss of a business and every interaction with him counts! An infallible way of dealing with this pressing subject is the use of Chatbots. Chatbots are used to conduct an online chat conversation via text or text-to-speech and provide direct contact with a live human agent. With rising demands all over the world, they have gained immense popularity and are widely used in a myriad of industries to render a pleasant and uniform customer experience. They can be used to answer FAQs, handle customer queries and grievances, manage bookings, make recommendations, CRM and provide 24*7 customer support. They can be rule-based or have Natural Language Understanding.

Let’s look at some applications of Chatbots:

Accessible anytime

Handling Capacity

Flexible attribute

Customer Satisfaction

Cost Effective

Faster Onboarding

Work Automation

Alternate sales channel

Personal Assistant

Chatbots can be integrated with various platforms such as Google Dialogflow, Microsoft Bot Builder, Amazon Lex, RASA and Wit.ai.

Aim and Scope

Whether it is for placing orders or recommending products, most businesses today use Chatbots to provide an efficacious customer experience and obtain a competitive advantage.

Here we will look at the steps involved in building a chatbot to help customers streamline their orders for a pizzeria. Perusing menus and placing orders can often be a cumbersome and time consuming task. This Chatbot aims to rule out tedious steps of flipping through menus and offers a personalized and customized experience. It will recommend a particular dish to the users based on their choice of ingredients and help provide a smooth user experience.

What type of pizza should you order if you enjoy basil on your pizza? What are you most likely to relish if you love pineapple? Which pizza would be best suited for olive lovers?

The bot will instantaneously answer all these questions are more! It will welcome customers to a pizzeria and recommend them a particular type of pizza they are most likely to devour based on their preferred ingredients and toppings. It will also simply take orders from the user and be a promising expression of efficiency, availability, interactivity and customer loyalty.

Steps

We create the bot using Python and RiveScript. In order to train the bot a dataset needs to be created. For the purpose of demonstration, I will be using a small dataset consisting of 50 records and 2 columns “Pizza name” and “ingredients.”

The first step is to import essential libraries we will need.

RiveScript is a simple and user-friendly scripting language for Chatbots. It is a rule-based engine, where the rules can be created by us. These use a scripting metalanguage (simply called a “script”) as their source code.

The next step is to set up the bot dictionary.

Next, we will use two concepts:

Count Vectorizer

Cosine Similarity

Count Vectorizer is used to transform a given text into an n*n matrix on the basis of the frequency of each word that occurs in the entire text.

For instance,

Data= [‘The’, ‘Bot’, ‘will’, ‘recommend’, ‘pizzas’, ‘for’, ‘the’, ‘customer’]

Cosine Similarity is a concept commonly used in recommendation systems. It is a measure of the similarity between two non-zero vectors of an inner product space. To understand it better, consider two points, P1 and P2, in a multi dimensional space. Mathematically, the lesser the distance between the two points, the more similar they are and as distance increases, the similarity between them decreases. Cosine Similarity depicts how similar the two points are by taking cosine of the angle between them. It ranges from -1 to +1. It compares how similar documents are by considering the arrays containing the word counts of the documents.

The next step is to create a function to get replies from the bot. If the value returned by the previous function is not ‘0’, the bot will recommend a particular type of pizza the user is most likely to enjoy.

The last step is to write a code for the Flask app.

This is what your rivescript will look like:

You can customize it based on preferences or business requirements.

Lastly we must build a User Interface for our bot in order to create a personalized branded experience and enable efficient communication with customers to serve them better.",https://medium.com/analytics-vidhya/chatbots-connecting-artificial-intelligence-and-customer-service-d8efbc604e02,['Heena Rijhwani'],2020-12-06 10:38:11.357000+00:00,687,"Chatbots, Natural Language Understanding, Cost Effective, Customer Satisfaction, Work Automation"
Mathematical justification of Stochastic Gradient Descent,"Minimising cost function is the holy grail of machine learning. Gradient descent is our master key to all cost minimisation problems. But gradient descent is slow. It uses entire training dataset for one iteration. This problem is exaggerated when working with millions of rows dataset. Stochastic gradient descent (SGD) comes to our rescue.

Nearly all of deep learning is powered by SGD algorithm.

SGD approximates gradient by using one example. This approximation is fairly accurate and works. But how can we approximate calculation involving million of rows using only one row?

Let us see general equation of gradient

where L stands for loss function, J is the cost function and m is number of training examples.

1) Key insight here is that gradient is the expectation. The expectation can be approximated by few examples. This insight shows that our approximation is accurate and guarantees convergence of SGD algorithm.

2) Standard error of the mean calculated from m examples is inversely proportional to square root of number of examples (m)

Consider two scenarios, we are estimating mean using 10 examples and 100,00 examples. Later examples needs 100 times more computation but reduces error by only 10 times.

We get diminishing gains by using more examples to approximate mean.

3) Small batches can offer a regularizing effect by adding noise to the gradient.

4) In practise there is redundancy in training examples. Large number of examples all making very similar contributions to the gradient. SGD can eliminate this redundancy.

Mini batch gradient descent: Similar to SGD but instead of 1 example, we use more (generally few hundreds) examples. All of the above arguments are valid for mini batch gradient descent",https://medium.com/@nrkivar/mathematical-justification-of-stochastic-gradient-descent-6ebd4f109c3f,['Ravikiran Gunale'],2020-09-15 04:52:36.472000+00:00,267,".SGD, Gradient Descent, Cost Minimisation, Deep Learning, Mini Batch Gradient Descent"
Medium vs. Amazon vs. Google,"The value of a string of text.

String of text. (Photo by Igor Ovsyannykov)

A string of text has value to a human, who wants to know about, and, analyze, other humans. Human behavior. And, human thought. So, Medium, and Amazon, and Google, base their businesses, and business model, on the string of text.

In this day and age, all businesses are based on simple strings, and text. Where the basis for the string, and text, is the zero and the one.

On Medium (and, therefore, Google, and, then, eventually, Amazon) the human is attached to several strings of text. These strings give value to the human, and, also, meaning to the human. Thus, value to the string of text.

A machine can read the human via the human’s string of text.

The machine ‘reading’ of a human, and the human’s strings of text, also has ‘value.’ Today, the leaders monetizing strings of text are Amazon and Google. Also Facebook, and Twitter. And, of course, many others. Because everything reduces (and expands) to one simple string of text. Zero, and, one.

Half of all humans do not know this until you point it out, and, then, all humans know this. Meaning, they knew it even when they didn’t ‘know it.’

Thus, analyzing strings of text, and selling both the strings, and the analysis, of the text, is what a human calls ‘intelligence.’

It all boils down to the circular relationship between the human and text, the human, and the monetization of text, the human, and the analysis of text, the human, and the machine, that can now replace the human, when it comes to text, and the analysis, and monetization, of the machine, that can also create the text.

So, this forces all of us to notice, the zero and the one, is human.

The zero and the one is, intelligence, text, analysis, and monetization. Not just the basis for these. The actuality of these.

All because a zero and a one is circumference, and, diameter.

The conservation of an uber-basic circle (also known as line) (string) (human) (machine) (intelligence) (virtual) (artificial) (real), gives meaning, and value, to Medium, Amazon, and Google. Meaning, Medium, Amazon, and Google, will, eventually, merge.

Everything in nature is most basically one string of text: if one, then zero.

Meaning, if two, then one.

Conservation of the circle is the core dynamic in nature.",https://medium.com/the-circular-theory/medium-vs-amazon-vs-google-19dee10883bd,['Ilexa Yardley'],2017-07-26 12:14:20.687000+00:00,383,"String Of Text, Human Behavior, Human Thought, Zero And One, Medium Amazon Google Merge"
Saving Python Data Visualizations for Presentations,"Saving without any parts cutting off

After you save your plot, make sure it shows all of the parts. Even if you see a complete plot when you run the code, the .png file might not show all of the parts. You wouldn’t want to display an incomplete graph like the one below.

To prevent this from happening, you can change the formatting of the plot and figure space through different methods.

One method is to change the figure size, increasing the width and/or length to include the part that was cut off (default figure size is 6.8, 4.8).

plt.figure(figsize=(10, 4.8))

The method below keeps the original figure size but shifts the positioning and changes the size of the plot to fit everything in the original figure space.

plt.subplots_adjust(left=0.2)

You can change the number and the direction (left, right, bottom, top) depending on the amount of space your graph needs (the number represents the distance of the plot from the boundary of the figure).

The above two methods involve testing out different numbers, whether it be the size of the figure space or the margin distance. The following options automatically adjust the positioning of the plot to fit within the figure space, default or assigned, without extra whitespace on the edges.

#changes the layout first before saving the plot plt.tight_layout()

plt.savefig(""test.png"")

or",https://medium.datadriveninvestor.com/saving-as-png-fcfaf04b586f,['Sharon Kwak'],2019-10-24 13:08:50.440000+00:00,211,"changes the dpi while saving the plot plt.savefig(""test.png"", bbox_inches='tight', dpi=300)saving, plots, figures"
Of ecommerce & false promises — How product matching can deliver on long forgotten promises.,"You’ve most likely seen an Eames ‘Eiffel’ chair. It would’ve been there in some office lobby you waited at, cafes you’ve walked by or a co-working space you frequent.

It’s everywhere.

Now if you feel like you want to buy one and search on Amazon you will get quite a few options.

via Amazon

Here’s where the problem starts.

Are they all the same color? What about the nifty rod design which gave the chair its famous name — is that identical? Material? Manufacturer?

Can you tell them apart using just 2-D images?

Should you have to?

In a recent Quartz article journalist Mike Murphy talks about the trouble with finding similar items across websites especially when they have been obscured by different names, descriptions and prices.

Let’s consider a few simple examples which compare product images without added distractions of dissimilar layout, rehashed descriptions and arbitrary prices — how fast can you tell if the items are matched?

EXAMPLE1: Ice-cream — colour saturation or new flavor?

EXAMPLE2: Toy Carriage — new toy or repeat purchase?

EXAMPLE 3: Rug — the same or unique?

You could probably tell the answer after a pause but imagine trying to pick from Amazon’s packed UI or even a more regular listing style like Wayfair’s:

Screenshot from Wayfair

You are likely doing a minor version of this exercise every time you make a purchase. It’s exhausting, inefficient and frankly unnecessary.

People shouldn’t need to put their detective hats on every time they want to shop for something. Ecommerce was supposed to take the guesswork out of shopping.

So what’s the solution?

An AI-powered solution to product matching.

At Semantics3 we’ve taken a multi-modal deep learning approach towards solving the problem of finding matching products. Multi-modal here implies that our algorithms take into account images, names, descriptions, features and all of the other data points that sites provide to describe their products.

The above product images were from an article by Govind, our head of data-science, which takes a look at the tricky nature of product matching.

A person trying to figure out matching products between sites is inundated with a ton of information — different angles and saturation for images, variant text, prices and non-standardized features. Add in unique website layouts and product matching by humans is, optimistically speaking, non-scalable (and realistically speaking — impossible).

Match.

Not a match. Notice that the number of windows aren’t identical

Not a match. The design is different.

Instead our Product Matching AI, trained and tuned on painstakingly curated datasets built over many years, can scalably take all available data and weigh it in context in order to decide whether two given products match.

And that, can make ecommerce much better for all.

Interested in our AI-based Product Matching API? Check out our AI solutions!",https://medium.com/datascience-semantics3/of-ecommerce-false-promises-4594bcb769ae,['Anjali Krishnan'],2018-02-26 22:28:32.168000+00:00,440,"Eames, Eiffel, Chair, AI-Powered Solutions, Product Matching"
Answers to the Important FAQs About KNN Algorithm,"In KNN, K stands for the number of nearest observations considered for predicting the class of new observation. e.g, There are two classes to predict i.e, 0 or 1. Now there are 5 neighbors near to the given observation and the class of those 5 neighbours is 1,1,1,0,0. It means new observation belong to the class 1. I hope this makes it clear that how important is the value of K in this algorithm. Again, like K in K means algorithm, KNN also allows to predict the correct value of K by iteratively trying all the values of K and see the best performance using technique like elbow method.

There is a little caveat here to the value of K. And, I feel this is the right time to put light on the concept called bias variance trade off. We can have one separate discussion for this topic. Just overview, high bias leads to under fitting of the model and high variance means complex model which could lead to overfitting. Most of you would have understand where am i heading to. Conclusion is that we need to have correct value of K to avoid overfitting and underfitting in this algorithm. e.g, We keep value of k is 1 then it is high biased and if value of K is N the it is high variance. You have to decide best option and value of K to get the best results.

Most of the time interesting question comes to mind, and it would have come to your mind too. It happened to me too. What will happen in case of DRAW (No winners). So, answer is quite simple, we always keep the value of K as odd, we don’t want to deal with conflict and give more complexity to the simplest model.

Advantages

1. Simple to implement

2. Very robust. Capable of handling non linear and linear solution both

3. low cost algorithm and easy to visualize too

Disadvantages

1. Expensive algorithm. It calculates distance at runtime for the test observation with all the existing observation to get given k values. Yes, of course, we have methods to overcome this approach and tune this.

2. Very sensitive to the given data. Noisy or unbalanced data. Wrong observations and outliers could make this algorithm perform worst. It needs perfect data to perform well which is almost requirement of every ML model. Common use cases : Forecasting, data compression or prediction etc.",https://medium.com/nerd-for-tech/answers-to-the-most-frequently-asked-questions-about-knn-algorithm-7e5de68e7b73,['Laxman Singh'],2020-12-15 12:15:06.917000+00:00,400,"KNN, Kmeans, Bias-Variance Tradeoff, Odd Value of K, Advantages and Disadvantages"
Recognizing Handwritten Digits,"Dataset

● Using MNIST dataset

○ Set of 70,000 small handwritten images of digits

○ Labeled by the digits it represents

○ Can be fetched using the scikit-learn helper function

Each image: 28 X 28 pixels, 784 features

● There are 70000 such images making the dataset dimension:

○ 70000 X 784

Fetching MNIST dataset in Scikit-Learn

>>> from sklearn.datasets import fetch_mldata >>> mnist = fetch_mldata(“MNIST original”) >>> X, y = mnist[“data”], mnist[“target”]

Steps

Divide dataset into training and test samples -> Train the classifier using the training dataset -> Test using test dataset -> Performance metrics (Finalize the model) -> Improve the model using error analysis.

Training and Test dataset

● We split the data into

○ Training set — Contains 60,000 out of 70,000 samples

○ Test set — Contains 10,000 out of 70,000 samples

● We train the model on a training set

● And evaluate the performance of the model on the test set

Dividing dataset into training and test in python:

>>> X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] >>> import numpy as np >>> np.random.seed(42) >>> shuffle_index = np.random.permutation(60000) >>> X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]

Binary Classifier

● What is a Binary Classification?

○ Binary or binomial classification is the task of classifying the elements

of a given set into two groups (predicting which group each one

belongs to) based on a classification rule.

Classifier used: Stochastic Gradient Descent (SGD) Classifier

Training a Binary Classifier using SGD

● Stochastic Gradient Descent (SGD) Classifier

○ Capable of handling large datasets

○ Deals with training instances independently

○ Well suited for online training >>> from sklearn.linear_model import SGDClassifier >>> sgd_clf = SGDClassifier(random_state=42, max_iter=10) >>> sgd_clf.fit(X_train, y_train_5)

TEST:

>>> some_digit = X[36000] # Taking the 36,000th image >>> sgd_clf.predict([some_digit]) array([True], dtype=bool)

Performance measure — Methods

● Cross-Validation — Accuracy

● Confusion Matrix

○ Precision

○ Recall

○ F1 score

What is cross-validation?

● It involves splitting the training set into K distinct subsets called folds,

then training and evaluating the model K times, picking a different fold for

evaluation every time and training on the other K-1 folds.

● The result is an array containing K evaluation scores.

cross_val_score:As discussed in end-to-end project session, cross_val_score() function in scikit-learn can be used to perform cross validation.

>>> from sklearn.model_selection import cross_val_score >>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=”accuracy”)

Scoring parameter Classifier object Training data Labels No.of folds

(Here, scoring parameter is accuracy)

What is confusion matrix?

○ The general idea is to count the number of times instances of class A

are classified as class B.

○ Can be better than simple accuracy

For ‘5’ and ‘Not 5’ classifier

● The first row of this matrix considers non-5 images (the negative class):

○ 53,272 of them were correctly classified as non-5s (they are called true negatives)

○ The remaining 1,307 were wrongly classified as 5s (false positives).

Confusion matrix in Scikit Learn

>>> from sklearn.model_selection import cross_val_predict >>> y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3) >>> from sklearn.metrics import confusion_matrix >>> confusion_matrix(y_train_5, y_train_pred)

Precision and recall

True — Positive means the classifier correctly classified the Positive class.

True — Negative means the classifier correctly classified the Negative class.

False — Positive means the classifier incorrectly classified a Negative class as Positive Class.

False — Negative means the classifier incorrectly classified a Positive class as Negative Class.

F1 score

F1 score = harmonic mean of precision and recall",https://medium.com/@vineetrajparashar25/recognizing-handwritten-digits-e0f99561832c,['Vineet Raj Parashar'],2020-10-09 10:22:57.135000+00:00,505,"MNIST dataset, scikit-learn, binary classification, SGD classifier, cross-validation"
Open sourcing Zoba’s Julia geohashing package,"At Zoba, we deal with a lot of event data that happens at discrete points: a user unlocks a scooter, a vehicle has its battery swapped out, a user ends a one-way car-share trip, etc. When analyzing these data, it’s often useful to group events spatially. Typically this grouping is accomplished via a grid system. A grid is a set of non-overlapping polygons which covers a region, usually the whole world. Each point is therefore associated with a single polygon (a “grid cell”), and we can discuss statistics like “the average number of rides on a Wednesday” for each grid cell. Many grid systems also support varying granularity so that the world may be divided into a few large polygons or many small polygons.

The most common grid systems — also known as geocoding systems and closely related to spatial indices — include Uber’s H3, Google’s S2, and variations on the geohash system. Every grid system involves trade-offs between desiderata such as simplicity, speed, low spatial distortion, equally sized cells, consistent distances between neighboring cells, and so forth. Stay tuned for a blog post exploring the trade-offs between these systems!

Geohash based systems are among the simplest grid systems; they “unwrap” the surface of the Earth to a longitude-latitude rectangle and then subdivide that rectangle. This procedure introduces nontrivial spatial distortion, especially over large regions. Nonetheless, at Zoba we’re big fans of geohash systems because they’re so simple and work well in most cases where the cells are small. In particular, geohash grid cells are intuitive and easy to explain: they’re just latitude-longitude rectangles. Geohash cells at different granularities also nest perfectly (i.e. a granularity-n cell can be perfectly divided into granularity-(n+1) cells), a useful property not shared by all grid systems.

Accordingly, many of our analyses have used Hilbert-curve flavored geohashing¹ as provided by a small open-source Python library. In addition to Python, we use Julia heavily, especially for computationally-intensive data science work. So we decided to write a Julia implementation of Hilbert curve geohashing, which would allow us to use geohashing in Julia without relying on language bridges. Further, we’re big fans of open source — much of our tech stack from LibGEOS on up to Django is open source software — and we wanted to give something back to the community.

To that end, today we’re delighted to announce the initial release of GeohashHilbert.jl, a small pure Julia package providing Hilbert-curve geohashing. This package is fully interoperable with the aforementioned Python package; encoding locations as geohash cells or decoding cells to locations will provide the same result in either package. We welcome use, feedback, and contributions, and hope to release more Zoba-built open source geospatial tools in the future.

[1] A Hilbert curve is used to order the grid cells in the subdivided longitude-latitude rectangle; this has the nice property that nearby grid cells tend to have similar identifiers.",https://medium.com/zoba-blog/open-sourcing-zobas-julia-geohashing-package-fc3d0a041c1d,[],2019-08-28 11:34:50.781000+00:00,477,"Geohash, Hilbert Curve, Spatial Indexing, Geocoding Systems, Geospatial Tools"
A Swiss Knife python package for fast Data Science,"A Swiss Knife python package for fast Data Science arita37 May 15·4 min read

Have you ever dreamed of having some snippets of code to read any type of files on disk, display many graphs at same time, create and auto-size save histogram in one liner in python,…. ?

Of course, there are pandas, matplotlib, seaborn, but don’t you write over and over the same code, or search into Stack-Overflow the same snippets.

This is what utilmy does :

A large collection of One-Liner functions to increase daily productivity and reduce the code for daily data science and fast output.

Some example :

from utilmy import pd_read_file

df = pd_read_file([“path1/data*.parquet”, ‘path2/datab_*.csv’], n_pool=4)

Read and concatenate files from disk in parallel way into Pandas dataframe.

Install is : pip install utilmy

List of function available (in in Pycharm):

from utilmy import XXXXX

from utilmy.tabular import XXXX

Reading a File to Pandas Dataframe

from utilmy import pd_read_filedf = pd_read_file([“path1/data*.parquet”, ‘path2/datab_*.csv’], n_pool=4)

The pd_read_file function lets you read and concatenate files from the local disk. It applies parallelization to improve speed and efficiency. The resulting variable is a pandas dataframe that can be used with numerous other functions compatible with the library.

Saving a Dataframe to File

from utilmy import

pd_to_file(df, ""data.csv"")

The pd_to_file() function lets us easily save a pandas dataframe to the local disk. It automatically detects the file format.

Plotting Multiple Variables

from utilmy import pd_plot_multi

cols = ['T (degC)', 'Tpot (K)']

pd_plot_multi(df_weather, cols)

The pd_plot_multi() function lets us quickly plot multiple variables from a pandas dataframe. We only have to specify the dataframe, as well as a list with the columns that we want to be plotted. After doing that, a matplotlib graph is displayed.

Applying Stratified Sampling to a Dataframe

from utilmy import pd_sample_strat

pd_sample_strat(df1, col, n)

The pd_sample_strat() function lets us apply stratified sampling on a specific dataframe column. For every unique value of the specified column n random samples will be selected, while the rest are going to be dropped.

Binning Numeric Values

from utilmy import pd_col_bins

pd_col_bins(df1, col, nbins=10)

Binning is the process where continuous numeric values are grouped in intervals known as bins. This can be easily accomplished with the pd_col_bins function. You simply have to specify a pandas dataframe, the numeric column you want to apply binning to and the number of bins.

Converting a Date to Unix Timestamp

from utilmy import to_timeunix

to_timeunix(""2020-05-15"")

Getting the Unix timestamp of a date can be useful in many cases. This can be accomplished easily with the to_timeunix() function. You simply pass the date as a string, and the Unix timestamp is returned.

Getting OS Memory Information

from utilmy import os_memory

os_memory()

Knowing how much RAM memory is available is useful in many occasions. The os_memory() prints the total RAM of the system, as well as the amount that is free and used at that time.

Getting the Number of CPU Cores

from utilmy import os_cpu

os_cpu()

The os_cpu() function prints the number of CPU cores available. This function can be useful in case you are working on a remote machine and don’t know how many cores it has. Furthermore, you can use it to define the number of cores that should be utilized in functions that support parallel processing.

Getting the Working Directory in Unix format

from utilmy import os_getcwd

os_getcwd()

The os_getcwd() function returns the current working directory of the OS,

in correct Unix format “/” (windows is converted to Unix).

Getting the Intersection of Two Lists

from utilmy import np_list_intersection

np_list_intersection([1, 2, 3, 4], [3, 4, 5, 6])",https://medium.com/@datas-science/a-swiss-knife-python-package-for-fast-data-science-4bc3295d830a,[],2021-07-25 03:54:29+00:00,543,"Python, Data Science, Pandas, Matplotlib"
Scraping Reddit using python,"In this post we are going to learn how to scrape all/top/best posts from a subreddit and also the comments on that post (maintaining the nested structure) using PRAW.

So, basically by the end of the tutorial let’s say if you wanted to scrape all all jokes from r/jokes you will be able to do it.

TL;DR Here is the code to scrape data from any subreddit .

In order to understand how to scrape data from Reddit we need to have an idea about how the data looks on Reddit. Here’s a snippet :

Example of how a Reddit post looks like

Now if you look at the post above the following would be the useful data fields that you would like to capture/scrape :

The post (title and body)

The comments in a structured way ( as the comments are nested on Reddit, when we are analyzing data it might be needed that we have to use the exact structure to do our analysis.Hence we might have to preserve the reference of a comment to its parent comment and so on)

The points(up-votes) of a post

The points(up-votes) of a comment

Timestamp on a post/comment

The url of the post and the comment

Now that we know what we have to scrape and how we have to scrape, let’s get started.

Let’s get started

So to get started the first thing you need is a Reddit account, If you don’t have one you can go and make one for free.

The next step is to install Praw. Praw is an API which lets you connect your python code to Reddit .

To install praw all you need to do is open your command line and install the python package praw.

pip install praw

The next step after making a Reddit account and installing praw is to go to this page and click create app or create another app.

In the form that will open, you should enter your name, description and uri. For the redirect uri you should choose http://localhost:8080

Now that you have created your Reddit app, you can code in python to scrape any data from any subreddit that you want.

If you want the entire script go here.

The first step is to import the packages and create a path to access Reddit so that we can scrape data from it.

You can use the references provided in the picture above to add the client_id, user_agent,username,password to the code below so that you can connect to reddit using python.

Now lets say you want to scrape all the posts and their comments from a list of subreddits, here’s what you do:

The next step is to create a dictionary which will consists of fields which will be scraped and these dictionaries will be converted to a dataframe.

Here’s the process flow for the code :

Create a list of queries for which you want to scrape the data for(for eg if I want to scrape all posts related to gaming and cooking , I would have “gaming” and “cooking” as the keywords to use.

Create a dictionary of all the data fields that need to be captured (there will be two dictionaries(for posts and for comments)

Using the query , search it in the subreddit and save the details about the post using append method

Using the query , search it in the subreddit and save the details about the comment using append method

Save the post data frame and comments data frame as a csv file on your machine

So lets say we want to scrape all posts from r/askreddit which are related to gaming, we will have to search for the posts using the keyword “gaming” in the subreddit. Here’s how we do it in code:

NOTE : In the following code the limit has been set to 1.The limit parameter basically sets a limit on how many posts or comments you want to scrape, you can set it to None if you want to scrape all posts/comments, setting it to one will only scrape one post/comment.

Conclusion

Praw is the most efficient way to scrape data from any subreddit on reddit. Also with the number of users,and the content(both quality and quantity) increasing , Reddit will be a powerhouse for any data analyst or a data scientist as they can accumulate data on any topic they want!

Thank you for reading this article, if you have any recommendations/suggestions for me please share them in the comment section below.

Happy scraping!",https://medium.com/swlh/scraping-reddit-using-python-57e61e322486,['Parth Bhardwaj'],2020-07-03 14:37:11.397000+00:00,721,"praw, scraping, python, reddit, data analysis"
What’s a Data Scientist?,"by Manuel Sainz de la Pena

This is the first question I receive when I tell a family member or friend that I am currently studying to become a data scientist.

The truth is that I didn’t really know the answer to that question when I began my Data Science boot camp with General Assembly. Sure, I had a rough idea. I was aware of the prevalence of data in all of our lives. And I knew that companies would certainly pay for someone to analyze that data and provide actionable insights.

However, I definitely did not have a complete understanding of what exactly a data scientist does, and what are the skills a data scientist should possess. In an effort to answer these questions, I will share what I’ve learned so far about the typical workflow of a data scientist.

Question Asking

I believe that the first pre-requisite for any good data scientist is to have an open, curious mind. Question asking is the first step of the data science workflow and it is imperative that we identify a precise “problem statement” that we can specifically address in order to add value to an organization. An example of a problem statement would be something to the effect of: “I want to predict the sale price of a home in Ames, Iowa based off its zip code, square footage, and overall quality.” This is an iterative process which usually requires sharpening and narrowing down of the problem statement once we obtain our data and understand its limitations.

Obtain Data and Clean it

The next step in the job of a data scientist is to obtain and clean our data. This can be accomplished in a variety ways. Some websites allow us to scrape data directly from them. We might be provided with an existing data set, or we might have to go out and collect the data we are looking for ourselves! Cleaning the data is where data scientists often spend a big chunk of their time. The following are common issues that a data scientist might have to deal with at this stage:

missing/null values

duplicate entries

outliers

incorrectly formatted data

There is no one size fits all solution for these types of problems. However a data scientist must be able to rationalize and defend the choices they make in the data cleaning stage.

Exploratory Data Analysis

Visualizations are a data scientists’ best friend when it comes to Exploratory Data Analysis (EDA). The goal of EDA is to better understand the relationships between variables in our data. We can create graphs to visualize these relationships. For example, making bar graphs showing the average sale price for houses in Ames, Iowa based on the number of bedrooms and bathrooms would most likely provide insight. My personal favorite type of visualization is the Seaborn Correlation Heatmap (example shown below).

Heatmaps allow us to visualize the correlation strength between our target variable (sale price in this example) and independent variables (Overall Quality, Total Square Feet, etc.). They are also extremely useful when it comes to identifying which independent variables might suffer from multicollinearity. This information can help us to remove redundant features which will negatively impact our models.

Feature Engineering

Feature engineering is the process of deciding which features we ultimately choose to include in our models. There are two steps usually involved.

Feature Selection: This step typically involves removing features from our model that add more noise than predictive power. Removing the redundant features we identified during EDA is usually a good starting point. If our model has too many features it can also skew the bias-variance tradeoff too heavily towards variance, and thus result in an overfit model which will not perform well on unseen data. Feature Construction: This step refers to the creation of new features from existing ones. For example, in our Ames, Iowa housing sale price predictor, a new feature could be created that sums the total number of bedrooms and bathrooms into an entirely new feature. Or perhaps multiplying the square footage by the “house quality” integer value would give us a numeric feature which ends up highly correlated to sale price. Feature construction is an iterative process that can be aided by outside research and one’s own intuition.

Modeling

At last, we’ve gotten to the machine learning stage in the data science process. First, we need to select the type of model(s) we would like to train. Below is a cheat sheet from Microsoft Azure showing a useful flow chart for how we might pick the best model for our specific project.

Below are the basic steps we need to follow to create a model:

Split data into a training and testing set Instantiate whichever model we will be using Fit the model with training data set Obtain predictions for the testing set from the trained model Use performance metrics to evaluate the performance of the model

The simple steps above can be made infinitely more complex, but serve as a basic guide for what occurs when we conduct supervised machine learning.

Communication of Results

We know have our model, its predictions, and have evaluated the strength of those predictions. Perhaps the most important part of our job as data scientists is to effectively communicate our results. Data scientists must be able to tell a compelling story with their data and usually this involves utilizing clear, compelling visualizations. Failure to communicate results effectively can result in a whole lot of work going down the drain, if we cannot convince our audience to act on our findings. Thus, as data scientists it is critical that we practice and refine our presentation skills to maximize our impact.",https://manuelsdlp.medium.com/whats-a-data-scientist-4b0fb2c4dd2f,['Manuel Sainz De La Pena'],2020-12-13 16:39:18.525000+00:00,925,"Data Science, Question Asking, Data Cleaning, EDA, Feature Engineering"
Recurrent / LSTM layers explained in a simple way,"Introduction

For all the previously introduced layers, the same output will be generated if we repeat the same input several times. For instance, if we have a linear layer with f(x)=2.x. Each time we ask to predict f(3) we will get 6. So if we ask 10 times in a row, predict us the output when the input is 3, the NN will always give 6:

F(3)=6; F(3)=6; F(3)=6; F(3)=6; F(3)=6; …

Now imagine we are training an algorithm to detect repetitions, so we want that F(3) = 0 for the first time (no repetition detected), then we would like to get F(3)= 1 for the second time. We can’t achieve this behavior with non-recurrent layers. Since by definition we will always get the same output for the same input. A hack solution for this is to take a vector of 2 variables, so we can treat the first variable differently than the second variable. So a F([3;0]) =0 (no repetition is detected) but F([3;3])=1 (repetition is detected). The downside of this hack is that we can operate only on a predefined fixed sequence length.

Solution

In order to solve the previously introduced problem, recurrent layers were invented. They are a family of layers that contain an internal active state. In the simplest form, we can write Recurrent layers in the following way:

H is a hidden active internal state that starts usually with 0.

is a hidden active internal state that starts usually with 0. f is a function that updates the internal state between sequence steps.

is a function that updates the internal state between sequence steps. g is another function that uses the current internal state to calculate the output.

is another function that uses the current internal state to calculate the output. After each input X , H is updated using f , then the output Y of the recurrent NN is generated from this updated state using g .

, is updated using , then the output of the recurrent NN is generated from this updated state using . So when we send several time the same input X=3 we might get different output Y, because the internal state is changing after each time.

For example, let’s consider the following simple example:

First we start with H = 0.

After the First X= 3 , we get H=3, and output Y=-3+5= 2

, we get H=3, and output Y=-3+5= After the second X= 3 , we get H=2*3+3=9 and Y=-9+5 = -4

, we get H=2*3+3=9 and Y=-9+5 = After the third X= 3 , we get H=2*9+3=21 and Y=-21+5 = -16

, we get H=2*9+3=21 and Y=-21+5 = If we reset H = 0, then we ask again for X=3 we get H=3, Y=2 again.

And so on, we can see easily here how we can get different outputs with the same input repeated. At any moment, we can reset the internal state (H=0) then the same sequence will be generated.

Basically recurrent networks behave like non-recurrent networks if we reset the internal state after each step (sequence length = 1).

Properties

For recurrent-networks, For the same input sequence, we will get the same output sequence (internal state is reset after each sequence, not input). While for non-recurrent networks, for the same input we get the same output.

Recurrent layers are very useful for everything related to sequencing. Where one element of the sequence itself is less important than its position in the sequence.

Consider text processing: If we have the letters: H,O,U,S if we want to predict the next letter, we will predict E. If we have the letters: L,I,S => we will predict T. Although both sequences have the letter S as the last known letter before prediction, but the history of the sequence is more important for the prediction than just the last letter. This sequence’s history is encoded in the internal state H, so when the letter S finally arrives, we will have different histories (or internal state) that will allow us to generate E as the next letter in the first case, and T the next letter in the second case.

If you enjoyed reading, follow us on: Facebook, Twitter, LinkedIn",https://medium.com/datathings/recurrent-lstm-layers-explained-in-a-simple-way-d615ebcac450,['Assaad Moawad'],2020-07-28 07:44:24.164000+00:00,679,"Recurrent Layers, Sequence Processing, Text Processing, Internal State, Hidden State"
"fastText, and how to use it for text analysis?","We all use facebook and you must have all experienced at some time that you have made some post and facebook starts showing you ads exactly related to that thing.

For example if you make a facebook post that you are going to quit your job to start some new venture of yours and suddenly facebook starts showing you ads like this-

So how does facebook exactly know what to show?

Well it’s the magic of its NLP library- fastText.

FastText is an open-source, free, lightweight library recently open sourced by Facebook.FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification.

This library has gained a lot of attention in the NLP community as it has shown great state of the art results in various NLP domains.

How to use it?

In this article I am going to tell you that how to use it for text classification.

Step 1: Putting your data in correct format

It is very important for fastText to have data in a prescribed correct format.

The format:

_label_0 your_text

Here ‘_label_’ has to be used as it is.

‘0’ tells your label.(the value in the label column for that particular row)

‘your_text’ is the text on which you want to train your classifier.

In the above given code I have shown the way to how to open a new file ‘train.txt’ for writing and write your dataframe into it.

Step 2: Cloning the repo

Next we need to clone the fastText repo into our notebook to use its functions.

Repo id: https://github.com/facebookresearch/fastText.git

After cloning it follow these steps:

Make sure that your ‘train.txt’ file is inside the fastText folder created by cloning the repo.

Step 3: Playing around with the commands

Now your model is ready to train, give the following command to train your classifier-

./fasttext supervised -input train.txt -output <path to save model>

-label __label__ -lr 0.1 -epoch 40

The following arguments are mandatory:

-input training file path

-output output file path

You could play around with these parameters to fine tune your model(be sure to use a dash(-) before using any argument)

Provided argument without a dash!

Usage:

The following arguments are optional:

-verbose verbosity level [2]

The following arguments for the dictionary are optional:

-minCount minimal number of word occurences [1]

-minCountLabel minimal number of label occurences [0]

-wordNgrams max length of word ngram [2]

-bucket number of buckets [2000000]

-minn min length of char ngram [0]

-maxn max length of char ngram [0]

-t sampling threshold [0.0001]

-label labels prefix [__label__]

The following arguments for training are optional:

-lr learning rate [0.1]

-lrUpdateRate change the rate of updates for the learning rate [100]

-dim size of word vectors [100]

-ws size of the context window [5]

-epoch number of epochs [40]

-neg number of negatives sampled [5]

-loss loss function {ns, hs, softmax, one-vs-all} [softmax]

-thread number of threads [12]

-pretrainedVectors pretrained word vectors for supervised learning []

-saveOutput whether output params should be saved [false]

The following arguments for quantization are optional:

cutoff number of words and ngrams to retain [0]

-retrain whether embeddings are finetuned if a cutoff is applied [true]

-qnorm whether the norm is quantized separately [false]

-qout whether the classifier is quantized [false]

-dsub size of each sub-vector [2]

Step 4: Predicting using saved model

The following command is to test the model on a pre-annotated test dataset and compare the original labels with the predicted labels and generate evaluation scores in the form of precision and recall values.

The precision is the number of correct labels among the labels predicted by fastText. The recall is the number of labels that successfully were predicted.

./fasttext test <path to model> <path to test file> k

Make sure that the test file is also in the same format as train file.

To predict new labels:

./fasttext predict <path to model> <path to test file> k > <path to prediction file>

where k signifies that the model will predict the top k labels for each review.

The labels predicted for the above reviews are as follows :

__label__2

__label__1

__label__2

__label__2

__label__2

__label__2

__label__2

__label__2

__label__1

__label__2

__label__2

The prediction file can then be used for further detailed analysis and visualization purposes.

fastText has shown great results in other fields also like word representations etc. which shall be covered in upcoming articles.

Thank you for reading this article. 😄",https://medium.com/@vaibhavshukla182/fasttext-and-how-to-use-it-for-text-analysis-14554cc3b9c,['Vaibhav Shukla'],2019-04-09 13:49:53.192000+00:00,659,"fast Text, NLP, Facebook Research Team, open-source library, text classification"
Data Manipulation using DPLYR : Part 1,"In this blog, you will learn how to easily perform data manipulation using R software. We’ll use mainly the popular dplyr R package, which contains important R functions to carry out easily your data manipulation. The dplyr package(written by Hadley Wickham) provides us with several functions that facilitate the manipulation of data frames in R. Some of the most useful include:

The select Function: facilitates the selection of records (rows)

2. The filter Function: facilitates the selection of variables (columns)

3. The arrange Function: facilitates the ordering of records

4. The mutate Function: facilitates the creation of new variables

5. The rename Function: facilitates the renaming of variables

6. The summarize Function: facilitate the summarization of variables

At the end of this blog, you will be familiar with data manipulation tools and approaches that will allow you to efficiently manipulate data.

What is Data Manipulation ?

If you are still confused with this ‘term’, let me explain it to you. Data Manipulation is a loosely used term with ‘Data Exploration’. It involves ‘manipulating’ data using available set of variables. This is done to enhance accuracy and precision associated with data.

Actually, the data collection process can have many loopholes. There are various uncontrollable factors which lead to inaccuracy in data such as mental situation of respondents, personal biases, difference / error in readings of machines etc. To mitigate these inaccuracies, data manipulation is done to increase the possible (highest) accuracy in data.

At times, this stage is also known as data wrangling or data cleaning.

Required R package

First, you need to install the dplyr package and load the dplyr library then after you can able to perform the following data manipulation functions.

install.packages('dplyr') library(dplyr)

Demo Datasets

student <- data.frame(Student_Id = c(1012301, 1012302, 1012303, 1012304, 1012305),

Firstname = c('John', 'Jeff', 'Ronald', 'Jennifer', 'Jessica'),

Lastname = c('Novak', 'Barr', 'Lum', 'Forbis', 'Connor'),

Subject_Id = c('SAE6A', 'SAE6B', 'SAE6C', 'SAE6G', 'SAE61'),

Age = c(20, 19, 20, 19, 20),

Sex = c('M', 'M', 'M', 'F', 'F'))

print(student)

Output:

1. The select Function

The select function allows us to choose the columns to keep within a dataset. This can be done by simply specifying the column names (or numbers) to retain.

You can perform data manipulations on either dataframe or CSV file.

Now, you can choose any number of columns using select function. Here, columns 1 to 3 and 5 columns are chosen using both column name and number which shows in below snippet:

You can use negatives to select columns to drop:

There are a number of additional supporting functions you can use in order to identify columns to select or omit, such as “contains”, “starts_with” and “ends_with” :

2. The filter Function

The filter function allows us to choose specific rows from a data frame. You achieve this by specifying a logical statement:

3. The arrange Function

The arrange function allows us to sort the data on 1 or more variables. You provide values which specify the variables by which to sort in ascending order:

You can use the desc function to specify that a variable be sorted in descending order:

4. The mutate Function

You can create new variables within a data frame using the mutate function:

Tip: the ifelse function can be used for conditional logic when creating variables.

5. The rename Function

The rename function provides a neat and highly readable way to rename columns:

You can also rename multiple variables at once:

Tip: The new name is on the left and the old name is on the right.

6. The summarize Function

Often when analyzing a dataset we want to calculate summary statistics; You can do this with the summarize function, in conjunction with several basic summary functions:

Standard summaries such as mean, median, min, max etc.

Additional functions provided by dplyr: n, n_distinct

Sums of logicals, such as sum(x > 10)

If you have missing data we can add an option na.rm = TRUE that will find the summary value even if there are missing values.

Notice that the default column names are equal to the call that was made. You can replace these by specifying a new column name:

Summary

In this blog on data manipulation in R, we discussed the functions of manipulation of data in R. The dplyr package provides us with several functions that facilitate the manipulation of data (e.g., select, filter, arrange, mutate, summarize, rename).

When calling the functions:

The first argument is the input data frame,

The remaining arguments describe what to do to the data frame

The function outputs a data frame.

The next part of the series part2 will cover the data manipulation using tidyr package.

If you like the blog or found it helpful please leave a clap!

Thank you",https://medium.com/analytics-vidhya/data-manipulation-using-dplyr-part-1-fc0706c3c51f,['Margi Patel'],2020-06-26 11:30:16.870000+00:00,733,"for reading!data manipulation, R software, dplyr package, Hadley Wickham, select function"
The Advantages and Disadvantages of Artificial Intelligence Secrets Revealed,"Artificial Intelligence can be very useful in everyday life. However, I want to discuss in this article the advantages and disadvantages of artificial intelligence. Plus I will provide some amazing examples of how AI still needs to improve.

What is Artificial Intelligence with Examples?

According to The Merriam Webster Dictionary, Artificial Intelligence is “the capability of a machine to imitate intelligent human behavior.”

Some examples of AI are:

Facebook

Google

YouTube

Of course, there are more companies that use AI than Facebook, Google, And YouTube. Just so I don’t overwhelm you with a long list I am going to stick with these three companies.

What are the Advantages of AI in Daily Life

So I have to admit that I use Google, Facebook, and YouTube every single day. I also have a Google Home Mini in my room that I love to argue with on a daily basis. No, I am not crazy but I love to argue with Google!

But of course, there are more advantages to AI than arguing with it. Some of the advantages of AI are:

It can help you pull up information from Google right away after asking it a question.

AI can help keep track of your daily schedules.

AI can play some music for you from different music streaming services like Spotify, Pandora, etc.

It can also automate some tasks for companies and smaller businesses. Some tasks could include customer support.

AI can also help remind you to do things.

And the list can go on and on and on. Now let’s talk about the downsides of using AI.

What are the Disadvantages of using AI?

Technology is always improving every day but it’s far from perfect though. I can personally attest to that because I have seen my own eyes what goes on behind a website. Plus I also run my own website at CrazyFitnessGuy.

The downsides of using AI is:

AI is far from perfect.

AI can replace people’s jobs which is bad.

AI can learn a lot about you from using the web.

Using AI is not cheap.

What are the Risks of using AI?

While using AI can be helpful for a lot of big companies and smaller companies, there are also some risks that come with using AI. Some of these risks include:

Bad customer support.

AI can break if not handled with care.

There are security risks with using AI such as the information that it scans about each and every person.

How safe is that data and where is it being stored?

How is that company using the data that they are collecting?

Is there a way to delete that data?

Humans still need to be around in case AI breaks down.

What is My Opinion about AI?

AI can be very useful at times especially when you might be a small brand/business owner like me. Like I said it can be useful for big corporate companies as well. However, I have a love/hate relationship with AI.

I started to learn YouTube marketing for my brand. As of now I currently have 46 subscribers, which is a big improvement since I started on YouTube a few years ago. During my time on YouTube, I notice their AI can flag videos.

No, I do not post any explicit content on my channel. The first flag that I got was for a video with the word breast cancer in it. To give credit to YouTube AI I would have programmed AI to flag that and age restrict it until the owner of the video clarified that it does not show any sexual content in the video.

But apparently, their AI robot was not done yet with the flagging. The AI flagged 3 more of my videos and hid them behind an age-restricted wall. I am not sure the reasoning behind that. The other titles had podcasting tips and tricks, while the other title had a vegan lifestyle.

Of course, I finally got them appealed, but come on Google and YouTube that was so uncalled for. In my opinion, AI can be very stupid and that’s why we shouldn’t fully rely on Artificial Intelligence.

Then after that, Facebook decides to mute me and prevent me from posting in 3 of my Facebook groups that I am apart of. I admit that I post in my groups 7 days a week but it’s not like I post 500 comments or likes every day. Luckily for me, the turn around time for that was 15 minutes.

What are the Pros and Cons of AI?

Pros

Helps with everyday tasks

Can automate a lot of tedious tasks

Helps with customer support

Can increase sales

Cons

AI is not very smart

Can falsy report user content for no reason

It can also mute you for posting too much

People can lose their jobs to AI

AI can be expensive

Conclusion

What are your thoughts and opinions about AI? Do you think AI is dumb like I do? What do you think about these false reports and unexpected muting? We need to keep AI in place.

You can read more stories that I wrote at crazyfitnessguy",https://medium.com/@jimmyclarespeaker/the-advantages-and-disadvantages-of-artificial-intelligence-secrets-revealed-1ee8bcf3fa60,['Jimmy Clare'],2020-12-09 22:00:49.594000+00:00,808,".com Artificial Intelligence, AI, Google, Facebook, You Tube"
Wildlife Protection with Image Recognition,"Camera trapping for wildlife insights

A camera trap is a device for capturing wildlife on film. It is typically left in a remote place for extended periods of time to monitor the presence and activity of various animals. The camera is amongst others triggered by animal activity. However, it can also be triggered by other kinds of movement such as waiving grass. The goal is to offer as much information with as little noise as a possible to the people managing the parks. Rather than having an ecologist labour through thousands of pictures it would be ideal to have an algorithm decide whether 1) an animal is present in an image, and if so, 2) classify the animal so that it is immediately clear which images offer valuable information.

The model

As Sensing Clues serves wildlife parks across the globe, we went for an algorithm that is able to detect species from different regions across the globe. There are pre-trained algorithms available that are able to classify a large number of species, such as those trained on the iNaturalist dataset. However, these images are mostly not from camera traps.

Images from camera traps are notoriously difficult to analyse due to the animals being for instance occluded, motion blurred, or because these images are taken in different circumstances such as day- and nighttime. There are a number of quality camera trap datasets, such as the LILA BC repository. However, many of these datasets only concern relatively small regions of the world. Luckily, the camera trap data set from the iWildCam2020 FGVC7 Kaggle competition proved to be of great help. It contains over 200k images of a vast number of species across the globe and thus offered us a great starting point for an algorithm to detect and classify animals.

For this problem we took a two step approach. First, we detect animals in images. For this we use the MegaDetector, which is an object detection algorithm developed by Microsoft AI for Earth that has been specifically trained to detect animals and humans from camera trap footage (see figure 3). We crop the detected objects to their respective bounding boxes and then turn them into squares by padding the images with zeros to preserve the aspect ratio. Although the MegaDetector works great when it comes to detecting animals, it is not able to classify the detected animal in terms of what species it is. This is what we do in the second step of our approach. In the second step these crops are passed to a dedicated classifier, which is based on a pre-trained InceptionV3 network trained with TensorFlow. This particular network has been pre-trained on the iNaturalist 2017 dataset and already incorporates some base level information of animals, before even having fed it a single camera trap photo.

In order to ease classification we remove the variation between images by blurring each image with a Gaussian filter and subtracting the blurred image from the original (see Figure 1, also see this link). We perform this preprocessing step before cropping the images. This results in images with roughly uniform brightness throughout the image. Especially for nighttime images this results in better contrast.

Figure 1: image preprocessing

An example of this preprocessing step can be found in Figure 2.

Figure 2: nighttime camera-trap image of a badger without (left) and with (right) the preprocessing function defined in Figure 1 applied. Original image credit: Jasper Ridge Biological Preserve of Stanford University.

Analysing images in practice

The algorithm we trained performed well in the Kaggle competition, but it only is of added value if it can be used in practice. So how do you run such an algorithm in practice?

The models can be easily served using TensorFlow Serving. Images are picked up, passed to the MegaDetector and the outcome is used to create crops which are passed to the classification algorithm.

Although our algorithm was trained on species across the globe, we know for a fact that certain animals do not inhabit certain parts of the world. This is something we incorporated by means of a simple business rule that compares the location of the camera trap with the regions that the predicted specie inhabits. This can be of help in situations with look-a-like species that each inhabit a different region of the world, such as the South American jaguar vs. the African / Asian leopard. In principle, location information can also be included in the algorithm itself, however, as our goal was to come up with a minimal-viable product, we opted for a low-complexity business rule-driven solution.

Figure 3: a squirrel detected by the MegaDetector

Having an algorithm that performs well on a test set does not necessarily mean you can rely on it in a live environment. Information in the Sensing Clues Wildlife insights platform has to be of excellent quality such that wildlife rangers can rely on it. Hence, it is important that information is validated before it is trusted. Therefore, all images (coming from the parks Sensing Clues serves) in which animals have been detected are first presented to the image owners in a separate platform for validation. Validated images can then potentially be used to further improve the algorithm (i.e. human-in-the-loop).

Conclusion

We have developed a tool to detect and classify animals species in camera trap images from various regions across the globe. This tool processes camera trap information in an efficient, non-labour intensive manner allowing ecologists to sift through images faster since the empty images, which can make up a significant percentage, are automatically discarded. On the other hand it provides a species prediction in case of an animal being detected. The next step for this algorithm is to prove itself in practice and ultimately add to the stack of information that helps safeguard wildlife!

Written by Richard Bartels and Mike Kraus",https://medium.com/vantageai/wildlife-protection-with-image-recognition-2f95917c7a57,['Mike Kraus'],2020-09-24 13:08:20.842000+00:00,956,"Camera Trapping, Wildlife Insights, Object Detection, Classification Algorithm, i Naturalist Dataset"
Juego de Tronos - versión española,"Get this newsletter By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.

Check your inbox

Medium sent you an email at to complete your subscription.",https://medium.com/metadatos/juego-de-tronos-versi%C3%B3n-espa%C3%B1ola-fff39cf35adc,['Jaime Durán'],2019-04-30 10:31:10.325000+00:00,40,"newsletter, subscription, Medium, privacy policy, inbox"
Class 8: Preparing an AI Workshop,"Class 8: Preparing an AI Workshop

The steps and tools for setting up an AI Design Camp or Workshop

Watch Class 8 >

Any time you’re asked to run an AI-focused design workshop or camp, the very first step is to make a duplicate of this folder and move it to your team’s project folder where you can start customizing it:

The AI Jumpstart Toolkit: Original >

The “AI Jumpstart Toolkit: Original” Box Folder contents

The documents in this folder are numbered to match the order you’ll need them in:

01_AI Jumpstart Guide

Review this first for a how-to on customizing, preparing for, and conducting an AI Jumpstart Camp. This is also a good reference to have printed and on hand throughout camp.

02_AI Jumpstart Agendas

1, 2, and 3 day agenda examples you can customize

03_Create your AI Camp Mural

Links to the design thinking Mural templates you can use or customize for your camp

04_AI Jumpstart Prep Work

Once you have your Mural and agenda established, share the prep work assignments with the camp attendees.

05_AI Jumpstart Kickoff

This is a deck template you can use to kickoff day one of camp.

06_ML 4 Design Guide

This is a good reference to have printed and on hand throughout camp.

07_AI Jumpstart Retro

Have the team complete this 15 minute retro to conclude your camp.

08_Journey to GA

A guide through your next steps — preparing to build and deploy AI, and setting measurable key results.

09_AI Camp Results

A PDF of measurable AI Design Camp results captured between 2018–2019.",https://medium.com/ai-design-thinkers/class-8-6c67402053,['Jennifer Aue'],2020-01-27 23:41:16.394000+00:00,237,"AIworkshop, AIdesigncamp, AIJumpstart Toolkit, Design Thinking Mural, ML4Design Guide"
The Missing List of JupyterLab Keyboard Shortcuts,"With keyboard shortcuts, you can whiz around Jupyter notebooks in JupyterLab. You can save time, reduce wrist fatigue from using your mouse, and impress your friends. 🙂

Below is the missing list of common JupyterLab keyboard shortcuts from a GitHub Gist I made. Enjoy! 🎉

If you want to make your own JupyterLab shortcuts, I wrote a guide to doing that here. 🚀

I write about Python, SQL, Docker, and other tech topics. If any of that’s of interest to you, sign up for my mailing list of awesome data science resources and read more to help you grow your skills here. 👍

Happy JupyterLab-ing! 🎉",https://towardsdatascience.com/the-missing-list-of-jupyterlab-keyboard-shortcuts-c613ff711a20,['Jeff Hale'],2020-10-16 21:45:19.823000+00:00,102,"jupyterlab, keyboardshortcuts, python, sql, docker"
Image Classification: Cats and Dogs — Pre-trained Neural Network vs Constructed,"Chapter 4 of Neural Network Projects with Python goes through a guided project for classifying cats and dogs from a dataset provided by Microsoft. The best way to classify images, in my opinion, is by using a convolutional neural network (CNN).

I have used the VGG-16 CNN to classify gold deposits in Austraila for my capstone project with the Flatiron School Immersive Data Science Bootcamp. CNNs can be useful for a variety of image classification and segmentation problems.

This scenario is a pretty basic classification, binary, which doesn’t even require a GPU. To run more complicated image classification problems with a CNN, a good GPU is recommended. Classifying multiple high-resolution images will require computing power which is significantly greater than a CPU. Kaggle is a great way to utilize cloud computing and access a GPU, however, there is a 30 hour/week limit.

I happened to be able to train a CNN with my laptop CPU for this dataset in approximately an hour and a half. I also used a pre-trained model, the VGG16, to shorten the learning/training time and compare the results.

Here is the code for the first model:

from keras.models import Sequential

from keras.layers import Conv2D, MaxPooling2D

from keras.layers import Dropout, Flatten, Dense

from keras.preprocessing.image import ImageDataGenerator model = Sequential()

In [47]:

FILTER_SIZE = 3

NUM_FILTERS = 32

INPUT_SIZE = 32

MAXPOOL_SIZE = 2

BATCH_SIZE = 16

STEPS_PER_EPOCH = 20000//BATCH_SIZE

EPOCHS = 10

In [21]:

model.add(Conv2D(NUM_FILTERS,(FILTER_SIZE,FILTER_SIZE), input_shape = (INPUT_SIZE,INPUT_SIZE,3), activation = 'relu'))

In [22]:

model.add(MaxPooling2D(pool_size = (MAXPOOL_SIZE,MAXPOOL_SIZE)))

In [23]:

model.add(Conv2D(NUM_FILTERS,(FILTER_SIZE,FILTER_SIZE), input_shape = (INPUT_SIZE,INPUT_SIZE,3), activation = 'relu'))

In [24]:

model.add(MaxPooling2D(pool_size = (MAXPOOL_SIZE,MAXPOOL_SIZE)))

In [25]:

model.add(Flatten())

In [26]:

model.add(Dense(units=128,activation ='relu'))

In [27]:

model.add(Dropout(0.5))

In [28]:

model.add(Dense(units=1,activation = 'sigmoid'))

In [29]:

model.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics=['accuracy'])

In [30]:

training_data_generator = ImageDataGenerator(rescale = 1./255) training_set = training_data_generator.flow_from_directory('Dataset/PetImages/Train', target_size = (INPUT_SIZE,INPUT_SIZE),

batch_size = BATCH_SIZE,class_mode='binary')

model.fit_generator(training_set,steps_per_epoch = STEPS_PER_EPOCH, epochs = EPOCHS, verbose = 1) Found 19997 images belonging to 2 classes.

WARNING:tensorflow:From C:\Users\mmsub\Anaconda3\envs\learn-env\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.

Instructions for updating:

Use tf.cast instead.

Epoch 1/10

1250/1250 [==============================] - 80s 64ms/step - loss: 0.6250 - accuracy: 0.6488

Epoch 2/10

1250/1250 [==============================] - 76s 61ms/step - loss: 0.5393 - accuracy: 0.7292

Epoch 3/10

1250/1250 [==============================] - 80s 64ms/step - loss: 0.4925 - accuracy: 0.7648

Epoch 4/10

1250/1250 [==============================] - 84s 67ms/step - loss: 0.4660 - accuracy: 0.7787

Epoch 5/10

1250/1250 [==============================] - 76s 61ms/step - loss: 0.4353 - accuracy: 0.7956

Epoch 6/10

1250/1250 [==============================] - 76s 61ms/step - loss: 0.4123 - accuracy: 0.8112

Epoch 7/10

1250/1250 [==============================] - 86s 69ms/step - loss: 0.3876 - accuracy: 0.8257

Epoch 8/10

1250/1250 [==============================] - 84s 67ms/step - loss: 0.3650 - accuracy: 0.8354

Epoch 9/10

1250/1250 [==============================] - 76s 61ms/step - loss: 0.3448 - accuracy: 0.84520s - loss: 0.3

Epoch 10/10

1250/1250 [==============================] - 78s 62ms/step - loss: 0.3277 - accuracy: 0.8557

In [33]:

testing_data_generator = ImageDataGenerator(rescale = 1./255) test_set = testing_data_generator.flow_from_directory('Dataset/PetImages/Test/', target_size = (INPUT_SIZE,INPUT_SIZE),

batch_size =BATCH_SIZE, class_mode = 'binary')

score = model.evaluate_generator(test_set,steps =len(test_set))

for idx, metric in enumerate(model.metrics_names):

print(""{}: {}"".format(metric,score[idx]))

OUT[33]:

Found 5000 images belonging to 2 classes.

loss: 0.4586998224258423

accuracy: 0.7900000214576721

The model I constructed had a decent accuracy score of 79%, but also took an hour and a half to train. The second model which was pre-trained with VGG16 took 27 minutes to train and had a better accuracy score of 87%.

Pre-trained VGG16 model code:

from keras.applications.vgg16 import VGG16

In [71]:

INPUT_SIZE = 128

vgg16 = VGG16(include_top = False, weights = 'imagenet',input_shape=(INPUT_SIZE,INPUT_SIZE,3))

In [72]:

for layer in vgg16.layers:

layer.trainable=False

In [73]:

from keras.models import Model



input_ = vgg16.input

output_=vgg16(input_)

last_layer = Flatten(name='flatten')(output_)

last_layer = Dense(1,activation ='sigmoid')(last_layer)

model = Model(input=input_, output = last_layer)

In [74]:

BATCH_SIZE = 16

STEPS_PER_EPOCH = 200

EPOCHS = 3

In [75]:

model.compile(optimizer ='adam',loss = 'binary_crossentropy',metrics=['accuracy'])

In [77]:

training_data_generator = ImageDataGenerator(rescale = 1./255)

testing_data_generator = ImageDataGenerator(rescale = 1./255)



training_set = training_data_generator.flow_from_directory('Dataset/PetImages/Train/', target_size=(INPUT_SIZE,INPUT_SIZE),

batch_size = BATCH_SIZE, class_mode = 'binary')

test_set = testing_data_generator.flow_from_directory('Dataset/PetImages/Test/',

target_size = (INPUT_SIZE, INPUT_SIZE),

batch_size = BATCH_SIZE,

class_mode = 'binary')

model.fit_generator(training_set, steps_per_epoch = STEPS_PER_EPOCH, epochs = EPOCHS, verbose =1) Found 19997 images belonging to 2 classes.

Found 5000 images belonging to 2 classes.

Epoch 1/3

200/200 [==============================] - 560s 3s/step - loss: 0.4012 - accuracy: 0.8041

Epoch 2/3

200/200 [==============================] - 575s 3s/step - loss: 0.2994 - accuracy: 0.8711

Epoch 3/3

200/200 [==============================] - 488s 2s/step - loss: 0.2669 - accuracy: 0.8894

Out[77]:

<keras.callbacks.callbacks.History at 0x1e18ad49ba8>

In [78]:

score = model.evaluate_generator(test_set,len(test_set))



for idx, metric in enumerate(model.metrics_names):

print(""{}: {}"".format(metric,score[idx])) loss: 0.6551424264907837

accuracy: 0.8781999945640564

As we can see, using the pre-trained model was much faster and more accurate than constructing a CNN from scratch. The beauty of using pre-trained models is that much of the work has already been done, and we can add on to them. They also require less training time which can be important for large datasets. My full code for this project can be found here.",https://medium.com/analytics-vidhya/image-classification-cats-and-dogs-pre-trained-neural-network-vs-constructed-6370d5c79fde,['Mark Subra'],2020-10-20 12:51:26.151000+00:00,688,"Convolutional Neural Network, CNN, Image Classification, VGG16, Pre-trained Model"
Functional Safety Concept for Self Driving Cars,"Functional Safety Requirements

Going back to the lane departure warning example from the previous medium article, let’s derive a couple of functional safety requirements from our first safety goal.

Our malfunction was that the steering wheel warning vibration was too strong. The malfunction led to the safety goal that the oscillating steering torque of a lane departure warning function shall be limited. For the lane departure warning system, it makes sense to define a maximum allowed torque amplitude and a maximum allowed frequency. From the safety goal, we will define two functional safety requirements for the lane warning departure system.

Functional safety requirement one.

The lane keeping items shall ensure that the lane departure oscillating torque amplitude is below max torque amplitude.

Functional safety requirement two.

The lane keeping item shall ensure that the lane departure oscillating torque frequency is below max torque frequency.

Notice again that we use the term shall, because these are requirements. So what have we done so far? We started with a safety goal requiring the vibration to be limited. We looked at the item architecture and came up with two new requirements that would meet our safety goal. We then figured out that the lane assistance item needed to limit the vibration amplitude and vibration frequency.",https://medium.com/swlh/functional-safety-concept-for-self-driving-cars-be453df4216e,['Prateek Sawhney'],2020-12-05 19:19:53.542000+00:00,205,"Functional Safety, Lane Departure Warning, Max Torque Amplitude, Max Torque Frequency, Safety Goal"
RL World,"RL World

Reinforcement Learning is a very vast list of algorithms. Let's look at this structure of the Reinforcement world and try to understand it.

MDP World:

This is the first division, Markov Decision Process(MDP). These are the algorithms that try to solve the problem, where we have an underlined vision of the world. There are states and rewards (which we get when we move from one state to another).

MDP provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.

Bandit World

This is the 2nd type of division for the Reinforcement learning world. In this type of problem, there is no concept of States. There is essentially no such idea about moving from one state to another sequentially and gaining rewards. Here we are more concerned about learning the best decision to take when we are put in a certain type of condition. “Bandit” in “multi-armed bandits” comes from “one-armed bandit” machines used in a casino. In multi-armed bandit problems, the more you play the more you learn about the world, the more accurate we get about the decisions.

Real-world implementations could be a smarter AB Test: Instead of testing one thing on all populations, you will be making decisions based on a particular condition. You can show cat images to people who like cats, dog images to people who like dogs. Contextual Bandits can be used to optimize which algorithms to show on the website, for a higher click-through rate.

Bandits are divided into 2 types:",https://medium.com/@adabhishekdabas/rl-world-3fc4dc38a73d,['Abhishek Dabas'],2020-12-19 22:17:16.490000+00:00,255,"contextual and non-contextual.Reinforcement Learning, MDPWorld, Bandit World, Contextual Bandits, Non Contextual Bandits"
Build No-code Automated Machine Learning Model with OptimalFlow Web App,"How to deploy Web App:

Step 1: Install OptimalFlow:

If you didn’t install OptimalFlow’s latest version, you should do that from PYPI, simply using PIP. To install OptimalFlow’s latest version, run this command in your terminal or prompt:

pip install --upgrade optimalflow

Step 2: Download Web App’s source code:

There’re 2 ways to download its source code:

Method 1 Get it from OptimalFlow GitHub. Download all codes and sub-folders within the folder ‘webapp’.

Method 2 — Get from OptimalFlow’s package. You could find it in your Python core folder. Here’s the example where you can find the ‘Webapp’ folder.

Step 3: Start a server port:

Move the downloaded ‘Webapp’ folder to the place you want to store it. Using your Terminal or Prompt enter its directory and run the app.py (ignore ‘py’ here when you are not playing it in VS Code)

And you will see the output similar to the picture below:

Done! You can now open your browser (recommend Chrome), and type 127.0.0.1:500 to open the web app on your laptop.

Build Pipeline Cluster Traversal Experiment(PCTE) Workflow:

When you’ve done the deployment steps, and start running the server, you will see the web app in your browser like the picture below:

To build a PCTE Automated Machine Learning Workflow, you need to enter the parameters for each OptimalFlow’s module: autoPP, autoFS, and autoCV.

But before that, you need to move the dataset you want to build model into the ./input folder. You will see a sample dataset called “breast-cancer.csv” there.

After doing that, we could select the dataset by clicking the button under “Load Dataset”:

Then, you need to finish the settings for autoPP, autoFS, and autoCV step by step.

Please NOTE: be careful to the setting “sparsity” and “cols” values in autoPP parameters panel, if the combination of the feature preprocessing can’t meet the restriction values you’ve set, OptimalFlow will not able to continue running the following modules. More details could be found in autoPP’s Documentation.

Next, you need to finish set parameters in autoFS module, i.e. set how many top features you want to select, and what algorithm you want PCTE to go through. More details about autoFS module here.

For the autoCV module’s settings, you could click the autoCV ‘Set Parameters’ button, and enter the tuning strategy and algorithms comparison scope you want OptimalFlow to automatedly test. More details about autoCV module here.

Finally, when you finished all setting steps(the progress bar should be fulfilled 100%), you can click the ‘Build PCTE Workflow’ button, to run the Omni-ensemble Automated Machine Learning operation. This function is based on autoPipe module, more details about autoPipe is here.

Here are the sample outputs when the process’s done. You will find the Top 5 Optimal models with evaluation metrics there. More details and use cases about OptimalFlow modules and notebook samples could be found in Documentation.

LogsViewer:

You could view the logs information related to each module in ‘LogsViewer’ page. Please note, you could only use it after the previous automated machine learning process’s done.

Visualization:

You could also review the visualization outputs from the ‘Visualization’ page. For the classification problems, it will present ‘Pipeline Cluster Traversal Experiments Model Retrieval Diagram’ and ‘Pipeline Cluster Model Evaluation Dynamic Table’. And for the regression problem, it only will present ‘Pipeline Cluster Model Evaluation Dynamic Table’. All visualization outputs are based on Plotly, so you could play with them by dragging or scrolling. You will find more details on the example page.

Updates:

SearchingSpace

In OptimalFlow’s next version 0.1.11, you could edit the estimators’ Searching Space via the Web App.",https://towardsdatascience.com/build-no-code-automated-machine-learning-model-with-optimalflow-web-app-8acaad8262b1,['Tony Dong'],2020-10-09 13:51:46.698000+00:00,569,"You could find more details about this feature in the next version’s Documentation.web app, deploy, Optimal Flow, autoPP, autoFS"
